[{"categories":null,"content":"在生产环境使用 Kubernetes ，绕不过去的一个问题就是持久化卷。\n如果是使用阿里 ACK 托管平台的话，可以用 OSS 来持久化卷，如果是自搭的 kubernetes，那么存储就需要仔细考虑了。\nceph比较复杂，容易出故障。nfs 也不可用，毛病多多。minio倒是可以。\n这种情况下使用双副本的 GlusterFS 就是不错的选择。\n生产环境就不能随意了，最好不要使用 Heketi，因为凡是要持久化的东西，都是比较重要的东西，最好都有 yaml 记录。\nGlusterFS 的搭建就不说了。说说实际使用过程：\n一、装GFS，生产新卷 安装就不说了，我们的GFS有两个节点，172.19.20.18 和 172.19.20.36，我们强制建立一个两副本的卷： kuaijian-vol\n1gluster volume create kuaijian-vol replica 2 transport tcp 172.19.20.18:/glusterfs/kuaijian-vol 172.19.20.36:/glusterfs/kuaijian-vol force 二、为k8s产生GFS的endingpoint和service 1cat \u0026lt;\u0026lt; EOF \u0026gt;\u0026gt; ep-svc.yaml 2--- 3apiVersion: v1 4kind: Service 5metadata: 6name: gfs-cluster_svc 7spec: 8ports: 9- port: 1 10--- 11apiVersion: v1 12kind: Endpoints 13metadata: 14name: gfs-cluster_svc 15subsets: 16- addresses: 17- ip: 172.19.20.18 18ports: 19- port: 1 20- addresses: 21- ip: 172.19.20.36 22ports: 23- port: 1 24EOF 25 26kubectl apply -f ep-svc.yaml 这里要提一个概念，通常情况下 service 是通过 selector 标签来选择对应的 pod 来增加 endingpoint 的。如下：\n1apiVersion: v1 2kind: Service 3metadata: 4 name: go-api_svc 5spec: 6 ports: 7 - port: 8080 8 protocol: TCP 9 targetPort: 8080 10 selector: 11 app: go-api 12 type: ClusterIP 而上面，我们没有通过标签，而是让 endingpoint 和 svc 同名而手动增加 endingpoint 到 svc 的。\n三、为k8s生产创建 PV和PVC 静态环境不使用 Storageclass 持久化卷的图解如下，pod做pvc声明，pvc连接到pv，pv从GFS中拿到卷。\n首先声明一个 pv：\n1cat \u0026lt;\u0026lt; EOF \u0026gt;\u0026gt; kuaijian-pv.yaml 2apiVersion: v1 3kind: PersistentVolume 4metadata: 5name: gfs-kuaijian-50G_pv 6labels: 7name: gfs-kuaijian-50G_pv 8spec: 9capacity: 10storage: 50Gi 11accessModes: 12- ReadWriteMany 13glusterfs: 14endpoints: gfs-cluster_svc 15path: kuaijian-vol 16readOnly: false 17persistentVolumeReclaimPolicy: Retain 18EOF 19 20kubectl apply -f kuaijian-pv.yaml 然后声明一个 pvc 通过 matchLabels 来跟之前的 pv 绑定。\n1cat \u0026lt;\u0026lt; EOF \u0026gt;\u0026gt; kuaijian-pvc.yaml 2kind: PersistentVolumeClaim 3apiVersion: v1 4metadata: 5name: gfs-kuaijian-50G_pvc 6spec: 7accessModes: 8- ReadWriteMany 9resources: 10requests: 11storage: 50Gi 12selector: 13matchLabels: 14name: gfs-kuaijian-50G_pv 15EOF 16 17kubectl apply -f kuaijian-pvc.yaml 注意上面的 pvc，我们一下子申请了50G，把整个 pv 空间全用光了；当然我们也可以只申请个 10Gi，下个 pvc 再 10Gi，这样也是行的通的。\n四、POD使用PVC 声明一个 Nginx 的 Deployment 来使用这个 pvc\n1apiVersion: apps/v1 2kind: Deployment 3metadata: 4 name: nginx-deployment 5 labels: 6 app: nginx 7spec: 8 replicas: 2 9 selector: 10 matchLabels: 11 app: nginx 12 template: 13 metadata: 14 labels: 15 app: nginx 16 spec: 17 containers: 18 - name: nginx 19 image: nginx 20 ports: 21 - containerPort: 80 22 volumeMounts: 23 - name: data-www 24 mountPath: /data/www 25 volumes: 26 - name: data-www 27 persistentVolumeClaim: 28 claimName: gfs-kuaijian-50G_pvc 这样 kubernetes 的存储部分就搞定了。GFS 用于生产非常稳定，基本跑了 7 年了没有大毛病。\n","date":"2021-11-10","img":"","permalink":"/posts/20211110-k8s_gfs/","series":null,"tags":null,"title":"生产环境kubernetes使用持久化卷GlusterFS"},{"categories":null,"content":"在生产环境中，ES 通常是不会在 k8s 集群中存在的，一般 MySQL 和 Elasticsearch 都是独立在 k8s 之外。\n那么无论哪种 pod，要甩日志到 ES，最轻量的方案肯定是用 filebeat 甩过去了。\n当然，如果是阿里的 ACK，logtail 和 logstore 配搭已经非常不错了，根本用不到 filebeat 和 ES。\n如果我们不想为阿里 sls、logstore 出钱买单，就只能用 filebeat + ES 了\n我们说说 filebeat 的 sidecar 僚机用法：\n如上图所示，简单说就是起一个 filebeat 的 logging-agent 僚机来收集日志并发送到 ES，而不用动 app-container 分毫。\n我们以部署一个 Tomcat 应用为例来说明：\n一、打造 filebeat 镜像 首先准备 Dockerfile\n1FROM alpine:3.12  2 3ARG VERSION=7.15.1  4 5COPY docker-entrypoint.sh /  6 7RUN set -x \\  8 \u0026amp;\u0026amp; cd /tmp \\  9 \u0026amp;\u0026amp; wget https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-${VERSION}-linux-x86_64.tar.gz \\  10 \u0026amp;\u0026amp; tar xzvf filebeat-${VERSION}-linux-x86_64.tar.gz \\  11 \u0026amp;\u0026amp; mv filebeat-${VERSION}-linux-x86_64 /opt \\  12 \u0026amp;\u0026amp; rm /tmp/* \\  13 \u0026amp;\u0026amp; chmod +x /docker-entrypoint.sh  14 15 16ENV PATH $PATH:/opt/filebeat-${VERSION}-linux-x86_64  17 18WORKDIR /opt/filebeat-${VERSION}-linux-x86_64  19 20ENTRYPOINT [\u0026#34;/docker-entrypoint.sh\u0026#34;]  我们以 alphine:3.12 为底版，然后下载 filebeat 7.15.1的二进制包并释放到 /opt 下，最后指定入口文件 /docker-entrypoint.sh\n奥妙全在这个 docker-entrypoint.sh 中了\n1#!/bin/bash 2 3cat \u0026gt; /etc/filebeat.yaml \u0026lt;\u0026lt; EOF 4filebeat.config.modules: 5path: /opt/filebeat-7.15.1-linux-x86_64/modules.d/*.yml 6reload.enabled: true 78# 加入自定义的字段 9fields_under_root: true 10fields: 11project: kuaijian-tomcat 1213# 收集云厂商的数据和docker的变量 14processors: 15- add_cloud_metadata: ~ 16- add_docker_metadata: ~ 1718filebeat.modules: 19- module: apache 20access: 21enabled: true 22var.paths: 23- \u0026#34;/usr/local/tomcat/logs/localhost_access_log.*.txt\u0026#34; 24error: 25enabled: true 26var.paths: 27- \u0026#34;/usr/local/tomcat/logs/application.log*\u0026#34; 28- \u0026#34;/usr/local/tomcat/logs/catalina.*.log\u0026#34; 29- \u0026#34;/usr/local/tomcat/logs/host-manager.*.log\u0026#34; 30- \u0026#34;/usr/local/tomcat/logs/localhost.*.log\u0026#34; 31- \u0026#34;/usr/local/tomcat/logs/manager.*.log\u0026#34; 3233setup.template.name: \u0026#34;tomcat-logs\u0026#34; 34setup.template.pattern: \u0026#34;tomcat-logs-*\u0026#34; 35output.elasticsearch: 36hosts: [\u0026#34;172.19.20.xxx:9200\u0026#34;,\u0026#34;172.19.20.xxx:9200\u0026#34;] 37index: \u0026#34;tomcat-logs-%{+yyyy.MM}\u0026#34; 38EOF 39 40set -xe 41 42# If user don\u0026#39;t provide any command  43# Run filebeat  44if [[ \u0026#34;$1\u0026#34; == \u0026#34;\u0026#34; ]]; then 45 exec /opt/filebeat-7.15.1-linux-x86_64/filebeat -c /etc/filebeat.yaml 46else 47 # Else allow the user to run arbitrarily commands like bash  48 exec \u0026#34;$@\u0026#34; 49fi 我们为什么不在 k8s 里用 configmap 来配置 filebeat.yml 呢？\n理由是收集日志文件多且路径、类型各不相同，这么一堆的配置都放在 configmap 里会让人癫狂的。所以干脆放到镜像里，便于调试也便于修改。\n上面我们也充分利用了 filebeat 的 module，有 module 可用就必须用 module，而不是手动指定 filebeat.inputs ，可用的 mudole 实在太多了，一定要善用！！！另外 tomcat 和 apache 的日志格式是一样的。\n我们在最后执行的时候，也加了 exec $@ 便于调试，如果没有指定 CMD，就启动 filebeat，如果指定了比如 /bin/bash，就进入调试状态。\n我们打好镜像就 push 到 harbor 里待用\n附录：https://www.elastic.co/guide/en/beats/filebeat/current/configuration-general-options.html filebeat的配置列表\n二、sidecar部署 我们写一个 k8s 的 tomcat deployment文件：\n1apiVersion: apps/v1 2kind: Deployment 3metadata: 4 name: tomcat 5 labels: 6 app: tomcat 7spec: 8 replicas: 1 9 selector: 10 matchLabels: 11 app: tomcat 12 template: 13 metadata: 14 labels: 15 app: tomcat 16 spec: 17 containers: 18 - name: filebeat-sidecar 19 image: xxxx.xxxx.xxx/filebeat:xxx 20 env: 21 - name: POD_NAMESPACE 22 valueFrom: 23 fieldRef: 24 apiVersion: v1 25 fieldPath: metadata.namespace 26 - name: NODE_NAME 27 valueFrom: 28 fieldRef: 29 apiVersion: v1 30 fieldPath: spec.nodeName 31 - name: POD_IP 32 valueFrom: 33 fieldRef: 34 apiVersion: v1 35 fieldPath: status.podIP 36 - name: POD_NAME 37 valueFrom: 38 fieldRef: 39 apiVersion: v1 40 fieldPath: metadata.name  41 volumeMounts: 42 - name: logs-volume 43 mountPath: /usr/local/tomcat/logs 44 - name: tomcat 45 image: tomcat 46 ports: 47 - containerPort: 8080 48 volumeMounts: 49 - name: logs-volume 50 mountPath: /usr/local/tomcat/logs 51 volumes: 52 - name: logs-volume 53 emptyDir: {} 可以看到我们在这个 deployment 里定义了 pod 是单副本，里面跑了两个 container，一个是 filebeat，一个是 tocmat，两者通过同一个 volume 连接在一起，这样就可以做到不修改 tomcat container 而拿到里面的日志了。\n这样就把 tomcat 应用的日志收到 ES 去了。\n","date":"2021-11-10","img":"","permalink":"/posts/20211110-k8s_sidecar/","series":null,"tags":null,"title":"Kubernetes生产环境使用filebeat Sidecar收集日志"},{"categories":null,"content":"我们选择 haproxy 1.8 版本以上的，编译安装到路径 /export/servers/haproxy\n1make TARGET=linux2628 PREFIX=/export/servers/haproxy USE_GETADDRINFO=1 USE_ZLIB=1 USE_REGPARM=1 USE_OPENSSL=1 \\ 2 USE_SYSTEMD=1 USE_PCRE=1 USE_PCRE_JIT=1 USE_NS=1 3 4make install PREFIX=/export/servers/haproxy 编辑 haproxy.conf 配置文件：\n1global  2 maxconn 5120  3 chroot /export/servers/haproxy  4 daemon  5 quiet  6 nbproc 2  7 pidfile /tmp/haproxy.pid 8 9defaults  10 timeout connect 5s  11 timeout client 50s  12 timeout server 20s 13 14listen http  15 bind :80  16 timeout client 1h  17 tcp-request inspect-delay 2s  18 acl is_http req_proto_http  19 tcp-request content accept if is_http  20 server server-http :8080 21 use_backend ssh if !is_http 22 23backend ssh  24 mode tcp  25 timeout server 1h  26 server server-ssh :22 解释一下：我们在 8080 端口开了 http 服务，在 22 端口开了 ssh 服务，80端口由 haproxy 做代理转发，首先判断客户端请求是否是 http 请求，如果是就转发到 8080 端口，如果不是，就转发到 22 端口，这样就实现了 80 端口同时跑 http 和 ssh 两个服务。\n","date":"2021-11-10","img":"","permalink":"/posts/20211110-haproxy_multiple_port/","series":null,"tags":null,"title":"Haproxy一个端口跑多个服务"},{"categories":null,"content":"Ucloud的机器在两会期间干脆端口全灭，firewall设置进来的端口全关闭！！！！！！\n还好有个Global ssh的服务，可以 ssh ubuntu@111.129.37.89.ipssh.net 登录上去，注意，直接ssh ubuntu@111.129.37.89 是不通的。\n那我们就搭建一个SSLH服务，可以把ssh和openvpn以及ssl服务统统塞到一个端口22里\n动手吧\n1、修改openssh的端口 从22端口改成2222，千万别重启，22这会先得归ssh用\n2、安装sslh\n1sudo apt install sslh 2vi /etc/default/sslh 3 4找到Run=no 5改成Run=yes 6 7然后到下面，按需配置（不跑443的话可以不配） 8DAEMON_OPTS=\u0026#34;--user sslh --listen 0.0.0.0:22 --ssh 127.0.0.1:2222 --ssl 127.0.0.1:443 --openvpn 127.0.0.1:1194 --pidfile /var/run/sslh/sslh.pid --timeout 5\u0026#34; 3、配置sslh并且重启服务器\n1sudo systemctl enable sslh 2sudo reboot 就搞定了\n","date":"2021-11-10","img":"","permalink":"/posts/20211110-sslh_multiple_port/","series":null,"tags":null,"title":"Sslh的一个端口同时跑多个服务"},{"categories":null,"content":"这个要求挺古怪的，背景是防火墙只开了 nginx 443 端口。我也想同时 ssh 登录进去，但是F5没开IP\n就只能这么干了，让 Nginx 一个端口跑多个服务\n在 nginx.conf 加一段，stream 配置，nginx 的 ip 是 192.168.8.110：\n1 2#Multi Ports 3stream { 4 upstream ssh { 5 server 192.168.8.112:22; 6 } 7 8 upstream https { 9 server 192.168.8.111:443; 10 } 11 12 map $ssl_preread_protocol $upstream { 13 default ssh; 14 \u0026#34;TLSv1.2\u0026#34; https; 15 \u0026#34;TLSv1.3\u0026#34; https; 16 \u0026#34;TLSv1.1\u0026#34; https; 17 \u0026#34;TLSv1.0\u0026#34; https; 18 } 19 20 # SSH and SSL on the same port 21 server { 22 listen 443; 23 24 proxy_pass $upstream; 25 ssl_preread on; 26 } 27} 28 测试一下：\n1curl -v https://192.168.8.110 2 3ssh 192.168.8.110 -p 443 4 这样就可以了。\n","date":"2021-11-09","img":"","permalink":"/posts/20211109-nginx_multiple_port/","series":null,"tags":null,"title":"Nginx的一个端口同时跑SSH和HTTPS服务"},{"categories":null,"content":"kubernetes 装好正常运行一段时间后，会出现要把研发和运维权限分开的场景：\n比如：\n 给某个用户某一指定名称空间下的管理权限 给用户赋予集群的只读权限 …  非常麻烦，我们这里不讨论过多的概念，从运维的角度出发，简单实用化\n我们需要明确三个RBAC最基本的概念\n Role: 角色，它定义了一组规则，定义了一组对Kubernetes API对象的操作权限 RoleBinding: 定义了\u0026quot;被作用者\u0026quot;和\u0026quot;角色\u0026quot;的绑定关系 Subject: 被作用者，既可以是\u0026quot;人\u0026quot;，也可以是机器，当然也可以是 Kubernetes 中定义的用户(ServiceAccount主要负责kubernetes内置用户)  我们的操作过程流程如下，首先创建客户端证书；其次创建Role角色；再创建RoleBinding，把Subject和Role绑定，就完事了；最后一步是生成 kubectl 的配置文件。\n一、创建客户端证书 我们以已建好的阿里 ACK 为例，或者自建好的 Kubernetes 也行；确定已经有了 .kube/config 配置文件，拥有集群最高权限，并且可以正常执行 kubectl 命令。\n首先是生成证书，并向集群提出证书请求并签发，脚本如下：\n1#!/bin/sh 2 3useraccount=reader 4 5openssl req -new -newkey rsa:4096 -nodes -keyout $useraccount-k8s.key -out $useraccount-k8s.csr -subj \u0026#34;/CN=$useraccount/O=devops\u0026#34; 6csr=$(cat $useraccount-k8s.csr | base64 | tr -d \u0026#39;\\n\u0026#39;) 7cat \u0026lt;\u0026lt; EOF \u0026gt; k8s-csr.yaml 8apiVersion: certificates.k8s.io/v1beta1 9kind: CertificateSigningRequest 10metadata: 11name: $useraccount-k8s-access 12spec: 13groups: 14- system:authenticated 15request: $csr 16usages: 17- client auth 18EOF 19 20 21kubectl create -f k8s-csr.yaml 22kubectl certificate approve $useraccount-k8s-access 23 24kubectl get csr $useraccount-k8s-access -o jsonpath=\u0026#39;{.status.certificate}\u0026#39; | base64 --decode \u0026gt; $useraccount-k8s-access.crt 25kubectl config view -o jsonpath=\u0026#39;{.clusters[0].cluster.certificate-authority-data}\u0026#39; --raw | base64 --decode - \u0026gt; k8s-ca.crt 解释一下：我们定义了一个用户CN=reader，然后向集群发送了证书请求并签发，最终从集群获得了 reader-k8s-access.crt 的客户端证书和 k8s-ca.crt 的 CA 证书。\nk8s-csr.yaml 和 reader-k8s.csr 都是中间产物，最终我们有了客户端私钥 reader-k8s.key ，客户端证书 reader-k8s-access.crt ，CA 证书 k8s-ca.crt 这三个有用的文件。\n二、创建 Role 角色 1cat \u0026lt;\u0026lt; EOF \u0026gt;\u0026gt; role.yaml 2apiVersion: rbac.authorization.k8s.io/v1 3kind: Role 4metadata: 5name: role-pods_reader 6namespace: default 7rules: 8- apiGroups: 9- \u0026#34;\u0026#34; 10resources: 11- pods 12- pods/log 13verbs: 14- get 15- list 16- watch 17EOF 18 19kubectl apply -f role.yaml 解释：我们创建了一个 role 角色，名字叫做 role-pods_reader，所属命名空间是 default ，它对 pods 和 pods/log 有 get 、list、watch的权限，也就是说role-pods_reader 可以查看 default 空间的 pods 和 pods 的日志。\n三、创建 Rolebinding 1cat \u0026lt;\u0026lt; EOF \u0026gt;\u0026gt; rolebinding.yaml 2apiVersion: rbac.authorization.k8s.io/v1 3kind: RoleBinding 4metadata: 5name: rolebinding-default_pods_reader 6namespace: default 7roleRef: 8apiGroup: rbac.authorization.k8s.io 9kind: Role 10name: role-pods_reader 11subjects: 12- apiGroup: rbac.authorization.k8s.io 13kind: User 14name: reader 15EOF 16 17kubectl apply -f rolebinding.yaml 解释：我们创建了一个 rolebinding，名字叫做 rolebinding-default_pods_reader，同样所属命名空间是 default，它绑了两个东西，一个是 role，就是上面第二步创建的 role-pods_reader；另一个是 subject，对应了一个用户，就是我们第一步创建的那个 reader。\n四、生成 kubectl 配置文件 1#!/bin/sh 2useraccount=reader 3namespace=default 4 5kubectl config set-cluster $(kubectl config view -o jsonpath=\u0026#39;{.clusters[0].name}\u0026#39;) --server=$(kubectl config view -o jsonpath=\u0026#39;{.clusters[0].cluster.server}\u0026#39;) --certificate-authority=k8s-ca.crt --embed-certs --kubeconfig=$useraccount-k8s-config 6 7kubectl config set-credentials $useraccount --client-certificate=$useraccount-k8s-access.crt --client-key=$useraccount-k8s.key --embed-certs --kubeconfig=$useraccount-k8s-config 8 9kubectl config set-context $useraccount --cluster=$(kubectl config view -o jsonpath=\u0026#39;{.clusters[0].name}\u0026#39;) --namespace=$namespace --user=$useraccount --kubeconfig=$useraccount-k8s-config 10 11kubectl config use-context $useraccount --kubeconfig=$useraccount-k8s-config 解释：上面看起来很复杂，其实就是四步，在配置文件里设置 cluster 、设置 credentials 证书、设置 context 上下文、设置当前上下文。\n完事后会产生一个完整的 reader-k8s-config 文件，如下：\n五、测试 我们验证一下：\n1KUBECONFIG=reader-k8s-config kubectl get pods 2 3KUBECONFIG=reader-k8s-config kubectl auth can-i delete pods 4 5KUBECONFIG=reader-k8s-config kubectl auth can-i delete svc 6 这样一个对 default 空间的只读用户就建立好了\n六、集群只读用户 我们这里给出集群只读用户的 role，命名 role-cluster_reader\n1kubectl apply -f - \u0026lt;\u0026lt;EOF 2apiVersion: rbac.authorization.k8s.io/v1 3kind: ClusterRole 4metadata: 5name: role-cluster_reader 6rules: 7- apiGroups: 8- \u0026#34;\u0026#34; 9resources: 10- nodes 11- pods 12- pods/exec 13- pods/log 14- services 15- configmaps 16- secrets 17- serviceaccounts 18- endpoints 19verbs: 20- get 21- list 22- watch 23- apiGroups: 24- apps 25resources: 26- deployments 27- replicasets 28- daemonsets 29- statefulsets 30verbs: 31- get 32- list 33- watch 34- apiGroups: 35- batch 36resources: 37- jobs 38- cronjobs 39verbs: 40- get 41- list 42- watch 43EOF 以及 rolebinding，还是绑到第一步的用户 reader 的例子\n1kubectl apply -f - \u0026lt;\u0026lt;EOF 2apiVersion: rbac.authorization.k8s.io/v1 3kind: ClusterRoleBinding 4metadata: 5name: rolebinding-cluster_reader 6roleRef: 7apiGroup: rbac.authorization.k8s.io 8kind: ClusterRole 9name: role-cluster_reader 10subjects: 11- apiGroup: rbac.authorization.k8s.io 12kind: User 13name: reader 14EOF 生成配置文件的步骤跟上一步是一样的。\n从上面大家可以看到，其实最主要的就是 role 的 yaml 文件，里面控制着到底有怎么样的权限。\n","date":"2021-11-09","img":"","permalink":"/posts/20211109-k8s_rbac/","series":null,"tags":null,"title":"Kubernetes创建普通账号"},{"categories":null,"content":"没啊办法，翻墙翻墙还是翻墙。\n上游有若干 trojian 、v2ray 、sock5 、http各种各样的代理，这样多种的选择，那么就装一个 clash 客户端就可以全接管了。\n说下我们的做法：找个小Linux做旁路由，DNS和网关都设置在这台机器上，局域网内的机器都通过这台上网。\n我们用到的是 clash 的 Tproxy redir-host 和 udp-proxy 模式，这种模式比较强大。用就用最强大的。\n安装很简单，操作系统 centos 或者 ubuntu 都行，项目地址：\nhttps://github.com/Dreamacro/clash 说明书：\nhttps://lancellc.gitbook.io/clash/clash-config-file/proxy-groups/load-balance 首先下载二进制文件，现在版本是 v1.7.1，解压后放到 /usr/local/bin 目录下\n1wget https://github.com/Dreamacro/clash/releases/download/v1.7.1/clash-linux-amd64-v1.7.1.gz 2gzip -d clash-linux-amd64-v1.7.1.gz 3chmod 755 clash-linux-amd64-v1.7.1 4mv clash-linux-amd64-v1.7.1 /usr/local/bin 然后生成 clash.service\n1cat \u0026lt;\u0026lt; EOF \u0026gt;\u0026gt; /etc/systemd/system/clash.service 2[Unit] 3Description=clash service 4After=network.target 56[Service] 7Type=simple 8User=root 9ExecStart=/usr/local/bin/clash-linux-amd64-v1.7.1 10Restart=on-failure # or always, on-abort, etc 1112[Install] 13WantedBy=multi-user.target 14EOF 然后最重要的，就是配置文件了\n我这里这个旁路由的设备 IP 地址是 192.168.2.2，网卡设备是 enp2s0\n下卖弄\n1# http的代理端口 2port: 7890 3# sock5的代理端口 4socks-port: 7891 5# 重定向的端口 6redir-port: 7892 7 8ipv6: false 9 10allow-lan: true 11bind-address: \u0026#39;192.168.2.2\u0026#39; 12interface-name: enp2s0 13 14mode: rule 15log-level: info 16external-controller: 0.0.0.0:9090 17secret: \u0026#34;Fuck2021\u0026#34; 18external-ui: dashboard 19 20profile: 21 store-selected: false 22 tracing: true 23 24hosts: 25 # 把cantv的域名解析屏蔽掉，禁止它自动升级 26 \u0026#39;tms.can.cibntv.net\u0026#39;: 0.0.0.0 27 28dns: 29 enable: true 30 listen: 0.0.0.0:1053 31 enhanced-mode: redir-host 32 nameserver: 33 - 114.114.114.114 34 - tls://dns.rubyfish.cn:853 # dns over tls 35 - https://1.1.1.1/dns-query # dns over https 36 - tcp://1.1.1.1 37 38proxies: 39 - name: \u0026#34;trojan1\u0026#34; 40 type: trojan 41 server: www.linuxboy.net 42 port: 443 43 password: Fuck2021 44 sni: www.linuxboy.net 45 skip-cert-verify: true 46 47 - name: \u0026#34;vmess1\u0026#34; 48 type: vmess 49 server: 101.59.201.93 50 port: 41555 51 uuid: 7a17ae5e-fb86-42e2-abd4-b8c33cfabcd 52 alterId: 64 53 cipher: auto 54 55proxy-groups: 56 - name: Proxy 57 type: select 58 proxies: 59 - trojan 60 61 - name: \u0026#34;auto\u0026#34; 62 type: url-test 63 proxies: 64 - vmess1 65 - trojan1 66 url: \u0026#39;http://www.gstatic.com/generate_204\u0026#39; 67 interval: 300 68 69rules: 70 - DOMAIN-SUFFIX,v2ex.com,Proxy 71 - DOMAIN-SUFFIX,t66y.com,auto 72 - DOMAIN-SUFFIX,reddit.com,Proxy 73 - SRC-IP-CIDR,192.168.1.0/32,DIRECT 74 - SRC-IP-CIDR,192.168.2.0/32,DIRECT 75 - IP-CIDR,127.0.0.0/8,DIRECT 76 - GEOIP,CN,DIRECT 77 - MATCH,Proxy 解释一下：proxy 定义了两个代理，一个是 trojan，一个是 v2ray。然后再集合成组，一个组叫 Proxy， 显式指定用 trojan；另一个组叫 auto，根据 vmess1 和 trojan1 访问 http://www.gstatic.com/generate_204 的页面速度，谁快就用谁，缺省300秒会访问一次这个页面来决定哪个代理快。\n剩下的 rules 就很简单，把自己知道要访问的域名放到代理中去，然后把局域网的 IP 段放进 DIRECT 直接访问，最后 GEO IP 不是中国的由 Proxy 兜底。\n网上有一大堆规则，八戒的建议是不要去学，规则越多越慢，你自己知道要访问什么网站需要翻墙，加进去就好了。弄一堆，自己看着都头蒙\n最后我们在 rc.local 放入以下 iptable 内容，就可以了\n1#clash 2#tcp 3iptables -t nat -N clash 4iptables -t nat -A clash -d 0.0.0.0/8 -j RETURN 5iptables -t nat -A clash -d 10.0.0.0/8 -j RETURN 6iptables -t nat -A clash -d 127.0.0.0/8 -j RETURN 7iptables -t nat -A clash -d 169.254.0.0/16 -j RETURN 8iptables -t nat -A clash -d 172.16.0.0/12 -j RETURN 9iptables -t nat -A clash -d 192.168.0.0/16 -j RETURN 10iptables -t nat -A clash -d 224.0.0.0/4 -j RETURN 11iptables -t nat -A clash -d 240.0.0.0/4 -j RETURN 12iptables -t nat -A clash -d 192.168.2.2 -j RETURN 13iptables -t nat -A clash -p tcp -j REDIRECT --to-port 7892 14iptables -t nat -I PREROUTING -p tcp -d 8.8.8.8 -j REDIRECT --to-port 7892 15iptables -t nat -I PREROUTING -p tcp -d 8.8.4.4 -j REDIRECT --to-port 7892 16iptables -t nat -A PREROUTING -p tcp -j clash 17 18#udp 19ip rule add fwmark 1 table 100 20ip route add local default dev lo table 100 21iptables -t mangle -N clash 22iptables -t mangle -A clash -d 0.0.0.0/8 -j RETURN 23iptables -t mangle -A clash -d 10.0.0.0/8 -j RETURN 24iptables -t mangle -A clash -d 127.0.0.0/8 -j RETURN 25iptables -t mangle -A clash -d 169.254.0.0/16 -j RETURN 26iptables -t mangle -A clash -d 172.16.0.0/12 -j RETURN 27iptables -t mangle -A clash -d 192.168.0.0/16 -j RETURN 28iptables -t mangle -A clash -d 224.0.0.0/4 -j RETURN 29iptables -t mangle -A clash -d 240.0.0.0/4 -j RETURN 30iptables -t mangle -A clash -d 192.168.2.2 -j RETURN 31iptables -t mangle -A clash -p udp -j TPROXY --on-port 7892 --tproxy-mark 1 32iptables -t mangle -A PREROUTING -p udp -j clash 33iptables -t nat -N CLASH_DNS 34iptables -t nat -F CLASH_DNS 35iptables -t nat -A CLASH_DNS -p udp -j REDIRECT --to-port 1053 36iptables -t nat -I OUTPUT -p udp --dport 53 -j CLASH_DNS 37iptables -t nat -I PREROUTING -p udp --dport 53 -j REDIRECT --to 1053 最后启动clash\n1systemctl start clash ","date":"2021-11-08","img":"","permalink":"/posts/20211108-clash/","series":null,"tags":null,"title":"Clash的搭建教程"},{"categories":null,"content":"上篇简单介绍了 onedev ，这篇我们具体拿个 java spring 的项目来实际编译一下\n首先必须确认环境：\nonedev 和 agent 都是用 root 安装运行的，然后已经安装了 docker，且 selinux 设置为 disabled，否则会出权限麻烦。\n我们用的例子是 spring 的 petclinic，正常的 build 的步骤如下：\n1git clone https://github.com/spring-projects/spring-petclinic.git 2cd spring-petclinic 3./mvnw package 我们首先在 onedev 的 projects 新建一个项目 spring-boot\n然后到 clone 下来的源代码目录下\n1cd spring-petclinic 2 3git init 4git add . 5git commit -m \u0026#34;Spring boot demo project\u0026#34; 6 7git remote add origin http://192.168.86.101:6610/spring-boot 8git push --set-upstream origin master 这样就可以在 spring-boot 里看到代码了\n然后看上图，有个紫色灯泡，Enable build support by adding.onedev-buildspec.yml，点那个链接\n我们增加一个 build 的步骤，里面再增加两个 job，一个是 Get code ，一个是 build\n第一步肯定是把代码拿下来，选择 Checkout Code ，命名为 Get code ，然后其他保持缺省配置，保存\n第二步就是 build 代码，选择 Execute Shell/Batch Commands，然后实际是启动了一个 docker 镜像来执行 build 的过程\n  Image 填入：maven:3.5-jdk-8-alphine\n  Commands 填入：\n1unset MAVEN_CONFIG 2cat \u0026lt;\u0026lt; EOF \u0026gt;\u0026gt; /root/.m2/settings.xml 3\u0026lt;settings\u0026gt; 4\u0026lt;proxies\u0026gt; 5\u0026lt;proxy\u0026gt; 6\u0026lt;id\u0026gt;default\u0026lt;/id\u0026gt; 7\u0026lt;active\u0026gt;true\u0026lt;/active\u0026gt; 8\u0026lt;protocol\u0026gt;http\u0026lt;/protocol\u0026gt; 9\u0026lt;host\u0026gt;192.168.1.10\u0026lt;/host\u0026gt; 10\u0026lt;port\u0026gt;3128\u0026lt;/port\u0026gt; 11\u0026lt;/proxy\u0026gt; 12\u0026lt;/proxies\u0026gt; 13\u0026lt;/settings\u0026gt; 14EOF 15./mvnw package 解释一下，maven:3.5-jdk-8-alphine 这个镜像，如果在里面执行 mvnw package ，会报错 repository 的错，所以必须把 MAVEN_CONFIG 的环境变量给删除，另外整个 build 过程会去拉 N 多包和配置，大概200多兆，如果不翻墙，基本是失败。所以这里强制 mvn 使用了代理！！！否则 build 一天都不会成功。\n  第三步是配置 mvn repository的缓存，大家不想每次build都去下一遍依赖包吧，在 More Settings 设置\n在Caches里填入：\n  key: maven-cache\n  path: /root/.m2/repository\n  然后执行 Build , 然后等待，第一次时间会很长，终于 Successful 了\n接下来的步骤就可以用 Dockerfile 生成镜像，然后推到 Harbor，再下载或者 gitops下载来各种 yaml 文件，拉 kubectl 和配置文件，就可以推送到 kubernetes 了。\n补充一下，在别的地方突然看到有 maven 的阿里镜像地址，记录一下：\n1 \u0026lt;mirror\u0026gt; 2 \u0026lt;id\u0026gt;alimaven\u0026lt;/id\u0026gt; 3 \u0026lt;name\u0026gt;aliyun maven\u0026lt;/name\u0026gt; 4 \u0026lt;url\u0026gt;http://maven.aliyun.com/nexus/content/groups/public/\u0026lt;/url\u0026gt; 5 \u0026lt;mirrorOf\u0026gt;*\u0026lt;/mirrorOf\u0026gt; 6 \u0026lt;/mirror\u0026gt; ","date":"2021-11-08","img":"","permalink":"/posts/20211108-onedev_maven/","series":null,"tags":null,"title":"Onedev构建一个实际java Spring应用"},{"categories":null,"content":"vpn的搭建一直是一个难解的题目，openvpn、ipsec、tinc 都在用，都不够简洁。\nn2n 是一种 peer to peer 端到端的 vpn，搭建起来非常简单方便。必备利器\n首先了解一下概念\nn2n 的 vpn 分为两种角色，Supernode 超级节点和 Edgenode 边缘节点。\n很简单，Supernode 最好是公网地址，需要开放端口供其它节点连接上来；剩余的 Edgenode 可以没有公网地址，各个 Edgenode 边缘节点之间会尝试绕过 Supernode 直接连接。这大大提高了节点之间通讯的效率。通讯是加密的，非常安全。\n安装的话完全不用安装，直接一个命令就搞定了。\nhttps://github.com/ntop/n2n 弄出两个可执行文件 supernode 和 edge 就好了\n超级节点：\n1/usr/local/bin/supernode -p 11111 -v 边缘节点：\n1/usr/local/bin/edge -d n2n0 -c ThisisaSecret2012 -k Fuck2020 -a 192.168.0.2 -l 41.22.59.112:11111 -f 参数解释：\n -d 生成的虚拟网卡的命名 -c 某组vpn节点的口令。大家看到启动 supernode 的时候没有任何参数，supdernode 只负责转发。supernode 支持多组不同的 vpn。这里就是用来区分不同的 vpn 组的。 -k 节点之间通讯是用的 twofish 加密算法，这里指定的是密钥 -a 节点的ip -l supernode的ip和端口 -f 放到后台 daemon 执行  这样就建立好了，系统中会多出一张 n2n0 的网卡。\n","date":"2021-11-05","img":"","permalink":"/posts/20211105-n2n_vpn/","series":null,"tags":null,"title":"N2n一种peer to Peer的VPN的使用"},{"categories":null,"content":"其实一直在用 github、gitlab、jenkins，但是 github 时不时的抽风， gitlab 的 runner 套 Docker in docker 的方法很难用。\n所以 CI/CD 这一块挺喜欢阿里云效这种简便易行的。但确实找不到合适类似的\n从 V2EX 上看到一个老哥发的 onedev，就是一站式的工具，这不就试试先\n以 centos7 为例，安装过程如下：\n一、安装 java 1.8 版本 1rpm -ivh jdk-8u201-linux-x64.rpm 二、安装 git 高版本 缺省 centos7 和 epel 带的 git 版本太低，不符合要求，得加个新的源装新版本\n1yum -y install https://packages.endpoint.com/rhel/7/os/x86_64/endpoint-repo-1.7-1.x86_64.rpm 2yum -y install git curl 三、安装配置 onedev https://github.com/theonedev/onedev 下载压缩包，然后解压运行 bin/server.sh start ，很简洁，不错！\n运行完打开 http://192.168.86.101:6610 进行初始配置，就两步就 ok 了。\n三、例子 我们是要用的，所以在 projects 新建一个项目 spring-boot\n然后到源代码目录下\n1git init 2git add . 3git commit -m \u0026#34;Spring boot demo project\u0026#34; 4 5git remote add origin http://192.168.86.101:6610/spring-boot 6git push --set-upstream origin master 这样就可以在 spring-boot 里看到代码了\n然后看上图，有个紫色灯泡，Enable build support by adding.onedev-buildspec.yml，点那个链接就可以进入一系列 build 、push、deploy 的 pipeline 了。\n友情提示，单机的话，机器需要装 onedev 的 agent，然后还有 docker，就可以build了。用起来感觉跟云效差不多，很不错。\n","date":"2021-11-05","img":"","permalink":"/posts/20211105-onedev/","series":null,"tags":null,"title":"一站式Git软件onedev的安装使用"},{"categories":null,"content":"Nginx 可以用 kill -HUP 来重启，不会丢失已建连接。Haproxy 如何做才能做到 zero downtime 无缝重载呢？\n做法如下：\n一、配置 编译harpoxy的时候带上参数 USE_SYSTEMD，选择 Haproxy 1.8 以上版本\n1make TARGET=linux2628 USE_GETADDRINFO=1 USE_ZLIB=1 USE_REGPARM=1 USE_OPENSSL=1 \\ 2 USE_SYSTEMD=1 USE_PCRE=1 USE_PCRE_JIT=1 USE_NS=1 3make install Haproxy无缝重载技术：\n  旧进程当前管理的连接根据 file descriptor 文件描述符通过 socket 套接字传输到新进程。\n  在这个过程中，文件socket（unix socket）的连接没有断开。\n  新进程在充当 master-worker 主工作者的同时执行此任务。\n  综上所述，通过使用unix socket来维护连接状态并在旧进程和新进程之间传递，防止了连接丢失。\nhaproxy 的运行使用 Systemd 重载，使用 -Ws 方式。 （此外，在构建时必须启用 USE_SYSTEMD）\n1 -D : start as a daemon. The process detaches from the current terminal after 2 forking, and errors are not reported anymore in the terminal. It is 3 equivalent to the \u0026#34;daemon\u0026#34; keyword in the \u0026#34;global\u0026#34; section of the 4 configuration. It is recommended to always force it in any init script so 5 that a faulty configuration doesn\u0026#39;t prevent the system from booting. 6 7 -W : master-worker mode. It is equivalent to the \u0026#34;master-worker\u0026#34; keyword in 8 the \u0026#34;global\u0026#34; section of the configuration. This mode will launch a \u0026#34;master\u0026#34; 9 which will monitor the \u0026#34;workers\u0026#34;. Using this mode, you can reload HAProxy 10 directly by sending a SIGUSR2 signal to the master. The master-worker mode 11 is compatible either with the foreground or daemon mode. It is 12 recommended to use this mode with multiprocess and systemd. 13 14 -Ws : master-worker mode with support of `notify` type of systemd service. 15 This option is only available when HAProxy was built with `USE_SYSTEMD` 16 build option enabled. 17 18 具体的启动脚本：/etc/systemd/system/haproxy.service\n1[Unit] 2Description=HAProxy Load Balancer 3After=network-online.target 4Wants=network-online.target 5 6[Service] 7Environment=\u0026#34;CONFIG=/etc/haproxy/haproxy.cfg\u0026#34; \u0026#34;PIDFILE=/run/haproxy.pid\u0026#34; 8ExecStartPre=/usr/sbin/haproxy -f $CONFIG -c -q 9ExecStart=/usr/sbin/haproxy -Ws -f $CONFIG -p $PIDFILE 10ExecReload=/usr/sbin/haproxy -f $CONFIG -c -q 11ExecReload=/bin/kill -USR2 $MAINPID 12KillMode=mixed 13Restart=always 14SuccessExitStatus=143 15Type=notify 16 17# The following lines leverage SystemD\u0026#39;s sandboxing options to provide 18# defense in depth protection at the expense of restricting some flexibility 19# in your setup (e.g. placement of your configuration files) or possibly 20# reduced performance. See systemd.service(5) and systemd.exec(5) for further 21# information. 22 23# NoNewPrivileges=true 24# ProtectHome=true 25# If you want to use \u0026#39;ProtectSystem=strict\u0026#39; you should whitelist the PIDFILE, 26# any state files and any other files written using \u0026#39;ReadWritePaths\u0026#39; or 27# \u0026#39;RuntimeDirectory\u0026#39;. 28# ProtectSystem=true 29# ProtectKernelTunables=true 30# ProtectKernelModules=true 31# ProtectControlGroups=true 32# If your SystemD version supports them, you can add: @reboot, @swap, @sync 33# SystemCallFilter=~@cpu-emulation @keyring @module @obsolete @raw-io 34 35[Install] 36WantedBy=multi-user.target 二、验证 验证是否真的是无缝重载的步骤如下：\n在haproxy.cfg的global段落中加入stat的配置：\n1stats socket /var/run/haproxy.sock level admin expose-fd listeners process 1 运行一个不断循环重启的脚本：\n1while true ; do systemctl reload haproxy ; sleep 3 ; done 用 apache 的压测工具 ab 来压一下。\nSend request while service is reloading:\n1ab -r -c 20 -n 100000 http://127.0.0.1/ 最后检查结果中 \u0026ldquo;Failed requests\u0026rdquo; 是否为零就可以了\n","date":"2021-11-04","img":"","permalink":"/posts/20211104-haproxy_restart/","series":null,"tags":null,"title":"Haproxy的Zero Downtime重启如何做"},{"categories":null,"content":"用 alphine 镜像的一些常用技巧：\n会随时增加：\n一、修改源，用国外的源非常慢，替换成国内的中科大源 1sed -i \u0026#39;s/dl-cdn.alpinelinux.org/mirrors.ustc.edu.cn/g\u0026#39; /etc/apk/repositories 二、更新apk库 1apk update 三、安装软件 1#安装telnet 2apk add busybox-extras 3 4#安装curl 5apk add curl 6 7#安装时间组件 8apk add tzdata 9 10#更新并且安装软件 11apk add --update tzdata 四、进入容器一步执行换源、更新、安装 1sed -i \u0026#39;s/dl-cdn.alpinelinux.org/mirrors.ustc.edu.cn/g\u0026#39; /etc/apk/repositories \u0026amp;\u0026amp; apk update \u0026amp;\u0026amp; apk add curl \u0026amp;\u0026amp; apk add busybox-extras 五、解决缺少glibc库的问题 如果不ln会报错，原因是缺少glibc库！！！解决方法如下：\n1RUN mkdir /lib64 \u0026amp;\u0026amp; \\  2 ln -s /lib/libc.musl-x86_64.so.1 /lib64/ld-linux-x86-64.so.2 六、一些调试的CMD 1CMD [\u0026#34;/bin/bash\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;while true; do echo hi; sleep 10; done\u0026#34;] 2 3kubectl run curlpod --image=radial/busyboxplus:curl --command -- /bin/sh -c \u0026#34;while true; do echo hi; sleep 10; done\u0026#34; 七、pod的等待技巧 这儿里启动正式的pod之前，先临时起了两个容器等待其他服务的完成\n1 containers: 2 - name: myapp-container 3 image: busybox 4 command: [\u0026#39;sh\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#39;echo The app is running! \u0026amp;\u0026amp; sleep 3600\u0026#39;] 5 initContainers: 6 - name: init-myservice 7 image: busybox 8 command: [\u0026#39;sh\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#39;until nslookup myservice; do echo waiting for myservice; sleep 2; done;\u0026#39;] 9 - name: init-mydb 10 image: busybox 11 command: [\u0026#39;sh\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#39;until nslookup mydb; do echo waiting for mydb; sleep 2; done;\u0026#39;] ","date":"2021-11-04","img":"","permalink":"/posts/20211104-alphine_usage/","series":null,"tags":null,"title":"Alphine镜像的使用技巧"},{"categories":null,"content":"我们都喜欢用 alphine 的镜像做底包，来生产自己的镜像\nalphine 的底包的时间设定就非常重要了\n直接给出 Dockerfile\n1FROM alpine:3.12 2 3# latest certs 4RUN apk add ca-certificates --no-cache \u0026amp;\u0026amp; update-ca-certificates 5 6# timezone support 7ENV TZ=Asia/Shanghai 8RUN apk add --update tzdata --no-cache \u0026amp;\u0026amp;\\ 9 cp /usr/share/zoneinfo/${TZ} /etc/localtime \u0026amp;\u0026amp;\\ 10 echo $TZ \u0026gt; /etc/timezone 11 12# install chrony and place default conf which can be overridden with volume 13RUN apk add --no-cache chrony \u0026amp;\u0026amp; mkdir -p /etc/chrony 14COPY chrony.conf /etc/chrony/. 15 16# port exposed 17EXPOSE 123/udp 18 19# start 20CMD [ \u0026#34;/usr/sbin/chronyd\u0026#34;, \u0026#34;-d\u0026#34;, \u0026#34;-s\u0026#34;] 21 时间设定重要的就是下面这几行\n1# timezone support 2ENV TZ=Asia/Shanghai 3RUN apk add --update tzdata --no-cache \u0026amp;\u0026amp;\\ 4 cp /usr/share/zoneinfo/${TZ} /etc/localtime \u0026amp;\u0026amp;\\ 5 echo $TZ \u0026gt; /etc/timezone 还有如果在 image: maven:3.5-jdk-8-alpine 这种镜像里，无论怎么改时间都不是东八区，可以试试下面的命令：\n1export COMMIT_TIME=$(TZ=CST-8 date +%F-%H-%M) ","date":"2021-11-03","img":"","permalink":"/posts/20211103-alphine_timezone/","series":null,"tags":null,"title":"Alphine镜像中timezone的设定"},{"categories":null,"content":"其实我们的生产环境一直是 KVM ，然后用 shell 脚本控制虚机的生成，也是用到了 Cloud-init 的标准镜像。\n听说 Proxmox 也很不错，于是想看看能否也在生产环境中用上\n如果在生产环境中用，必须要让 proxmox 支持 cloud-init ，否则无意义，下面也说一下跑在生产的注意事项\n首先我们用光盘安装：\n然后第一个注意的地方就是硬盘，选 Options 后：\n会冒出一堆选项，公司的生产环境，服务器如果没有 raid 卡是很奇怪的，所以 zfs 反而不是标配，因为我们会事先在 raid 卡上划分好硬盘，生产环境基本必然是 raid10 ，接下来就是 ext4 和 xfs 二选一了，八戒选 ext4 ，因为坏了好修理，xfs_repair 用起来相当龟毛：\n那么，选定了 ext4 ，接下来就比较重要了\n  hdsize 1116.0 ，单位是G，这个是自动收集上来的，不用改\n  swapsize，交换分区大小，这个给 8 G（最大8G）\n  maxroot，这个分区是第一个分区，存放 iso 和 template 的，需要给够，100 G\n  minfree，第一个分区最小留多大，给 10 G（缺省16G）\n  maxvz，这个分区是第二个分区，存放实际的虚机文件，全都用上，什么也不填写\n  然后安装成功。\n打开网页，我们可以看到一个 local，100G，对应上面的 maxroot\n然后 local-lvm ，就是剩余放虚机的空间\nssh登录系统，首先换成中科大的 apt 源，并升级一下系统：\n1$ sed -i \u0026#39;s|^deb http://ftp.debian.org|deb https://mirrors.ustc.edu.cn|g\u0026#39; /etc/apt/sources.list 2$ sed -i \u0026#39;s|^deb http://security.debian.org|deb https://mirrors.ustc.edu.cn/debian-security|g\u0026#39; /etc/apt/sources.list 3$ CODENAME=`cat /etc/os-release |grep CODENAME |cut -f 2 -d \u0026#34;=\u0026#34;` 4$ echo \u0026#34;deb https://mirrors.ustc.edu.cn/proxmox/debian $CODENAMEpve-no-subscription\u0026#34; \u0026gt; /etc/apt/sources.list.d/pve-no-subscription.list 5$ cat /etc/apt/sources.list.d/pve-no-subscription.list 6$ rm /etc/apt/sources.list.d/pve-enterprise.list 7$ apt update 8$ apt upgrade 那生产使用，是必须用 Cloud-init 的标准化镜像的。我们需要造出一个 template 。\n以 Centos7 为例子\n1wget http://cloud.centos.org/centos/7/images/CentOS-7-x86_64-GenericCloud.qcow2 2apt-get install libguestfs-tools 然后准备脚本 modify.sh ：\n1#!/bin/sh 2image_name=CentOS-7-x86_64-GenericCloud.qcow2 3# virt-edit -a ${image_name} /etc/cloud/cloud.cfg 4virt-edit -a ${image_name} /etc/cloud/cloud.cfg -e \u0026#39;s/disable_root: [Tt]rue/disable_root: False/\u0026#39; 5virt-edit -a ${image_name} /etc/cloud/cloud.cfg -e \u0026#39;s/disable_root: 1/disable_root: 0/\u0026#39; 6virt-edit -a ${image_name} /etc/cloud/cloud.cfg -e \u0026#39;s/lock_passwd: [Tt]rue/lock_passwd: False/\u0026#39; 7virt-edit -a ${image_name} /etc/cloud/cloud.cfg -e \u0026#39;s/lock_passwd: 1/lock_passwd: 0/\u0026#39; 8virt-edit -a ${image_name} /etc/cloud/cloud.cfg -e \u0026#39;s/ssh_pwauth: 0/ssh_pwauth: 1/\u0026#39; 9virt-edit -a ${image_name} /etc/ssh/sshd_config -e \u0026#39;s/PasswordAuthentication no/PasswordAuthentication yes/\u0026#39; 10virt-edit -a ${image_name} /etc/ssh/sshd_config -e \u0026#39;s/PermitRootLogin [Nn]o/PermitRootLogin yes/\u0026#39; 11virt-edit -a ${image_name} /etc/ssh/sshd_config -e \u0026#39;s/#PermitRootLogin [Yy]es/PermitRootLogin yes/\u0026#39; 12virt-edit -a ${image_name} /etc/ssh/sshd_config -e \u0026#39;s/#PermitRootLogin prohibit-password/PermitRootLogin yes/\u0026#39; 13virt-edit -a ${image_name} /etc/ssh/sshd_config -e \u0026#39;s/[#M]axAuthTries 6/MaxAuthTries 20/\u0026#39; 14virt-customize --install cloud-init,atop,htop,nano,vim,qemu-guest-agent,curl,wget,telnet,lsof,screen -a ${image_name} 运行它，以上命令其实是侵入镜像，修改 sshd_config 允许 root 用 password 登录，然后又安了几个常用软件，大家可以按需修改。\n最后生成 template , 脚本： vm.sh 1#!/bin/sh 2vm_id=9999 3image_name=CentOS-7-x86_64-GenericCloud.qcow2 4 5qm create ${vm_id} --memory 8196 --net0 virtio,bridge=vmbr0 6qm importdisk ${vm_id} ${image_name} local-lvm 7qm set ${vm_id} --scsihw virtio-scsi-pci --scsi0 local-lvm:vm-${vm_id}-disk-0 8qm set ${vm_id} --ide2 local-lvm:cloudinit 9qm set ${vm_id} --boot c --bootdisk scsi0 10qm set ${vm_id} --serial0 socket --vga serial0 11qm template ${vm_id} cloud-init 技术的核心其实就是用配置文件，在虚机启动的时候动态修改，这里把配置放到了 ide2 的一个虚拟 cdrom 中\n最终会生成一个 id 为 9999 的 template\n我们还需要改两处：\n一是 CPU、MEMORY、硬盘大小，缺省是 8G，我们生产的镜像标配是80G，需要 resize , 加 72G，合计80G\n二是 cloud-init 部分，用户名、密码、DNS、IP、MASK、GATEWAY\n这样这个 template 就做好了，在生产的时候，只需要 clone 这个模板（模式要选 Full Clone），然后记得修改为不同的IP，就可以了。\n总体来说，这个东西偏小白，对于习惯了 KVM 的人来说，反而不如脚本来的快。\n相关文档：https://whattheserver.com/proxmox-cloud-init-os-template-creation/\n","date":"2021-11-03","img":"","permalink":"/posts/20211103-proxmox/","series":null,"tags":null,"title":"生产环境Proxmox 7.02的安装和配置"},{"categories":null,"content":"kubernetes 中 nginx ingress 的优化分两部分\n一、系统sysctl部分优化 首先是对nginx启动前的系统性能进行优化，这部分调整网络的缓冲区，减小闲置 socket 关闭的时间\n以阿里 ACK 为例，我们可以编辑 deployments 的 nginx-ingress-controller\n1 initContainers: 2 - command: 3 - /bin/sh 4 - -c 5 - | 6 mount -o remount rw /proc/sys 7 sysctl -w net.core.somaxconn=65535 8sysctl -w net.ipv4.ip_local_port_range=\u0026#34;1024 65535\u0026#34; 9sysctl -w net.ipv4.tcp_tw_reuse=1 10sysctl -w fs.file-max=1048576 11sysctl -w net.ipv4.tcp_keepalive_time = 300 12sysctl -w net.ipv4.tcp_keepalive_probes = 5 13sysctl -w net.ipv4.tcp_keepalive_intvl = 15 14 二、nginx ingress 参数优化 大家制动，nginx ingree 其实是做为一个中间代理，所以上下游的socket参数也需要优化\n同样以阿里ACK为例，我们可以编辑 configmaps 的 nginx-configuration\n1apiVersion: v1 2data: 3 allow-backend-server-header: \u0026#34;true\u0026#34; 4 enable-underscores-in-headers: \u0026#34;true\u0026#34; 5 generate-request-id: \u0026#34;true\u0026#34; 6 ignore-invalid-headers: \u0026#34;true\u0026#34; 7 log-format-upstream: $remote_addr - [$remote_addr] - $remote_user [$time_local] 8 \u0026#34;$request\u0026#34; $status $body_bytes_sent \u0026#34;$http_referer\u0026#34; \u0026#34;$http_user_agent\u0026#34; $request_length 9 $request_time [$proxy_upstream_name] $upstream_addr $upstream_response_length 10 $upstream_response_time $upstream_status $req_id $host [$proxy_alternative_upstream_name] 11 proxy-body-size: 20m 12 proxy-connect-timeout: \u0026#34;10\u0026#34; 13 reuse-port: \u0026#34;true\u0026#34; 14 server-tokens: \u0026#34;false\u0026#34; 15 ssl-redirect: \u0026#34;false\u0026#34; 16 17 upstream-keepalive-timeout: \u0026#34;900\u0026#34; 18 keep-alive-requests: \u0026#34;10000\u0026#34; 19 upstream-keepalive-connections: \u0026#34;500\u0026#34; 20 max-worker-connections: \u0026#34;65536\u0026#34; 21 22 worker-cpu-affinity: auto 23kind: ConfigMap ","date":"2021-11-02","img":"","permalink":"/posts/20211102-ingress_nginx/","series":null,"tags":null,"title":"K8s中nginx Ingress的性能优化"},{"categories":null,"content":"这篇是纯配置篇，解释都在配置里了，是生产服务器 sysctl.conf 的配置\n1### KERNEL ### 2 3# Reboot after 10sec. on kernel panic 4kernel.panic = 10 5 6### IMPROVE SYSTEM MEMORY MANAGEMENT ### 7 8# Increase size of file handles and inode cache 9fs.file-max = 2097152 10 11# Insure we always have enough memory 12vm.min_free_kbytes = 8192 13 14# Do less swapping 15vm.swappiness = 10 16vm.dirty_ratio = 10 17vm.dirty_background_ratio = 2 18 19 20### GENERAL NETWORK SECURITY OPTIONS ### 21 22# Avoid a smurf attack 23net.ipv4.icmp_echo_ignore_broadcasts = 1 24 25# Turn on protection for bad icmp error messages 26net.ipv4.icmp_ignore_bogus_error_responses = 1 27 28# Turn on syncookies for SYN flood attack protection 29net.ipv4.tcp_syncookies = 1 30net.ipv4.tcp_max_syn_backlog = 1024 31net.ipv4.tcp_max_syn_backlog = 8192 32 33 34# Turn on timestamping 35net.ipv4.tcp_timestamps = 1 36 37# Turn on and log spoofed, source routed, and redirect packets 38net.ipv4.conf.all.log_martians = 1 39net.ipv4.conf.default.log_martians = 1 40 41# No source routed packets here 42net.ipv4.conf.all.accept_source_route = 0 43net.ipv4.conf.default.accept_source_route = 0 44 45# Turn on reverse path filtering 46net.ipv4.conf.all.rp_filter = 1 47net.ipv4.conf.default.rp_filter = 1 48 49# Make sure no one can alter the routing tables 50net.ipv4.conf.all.accept_redirects = 0 51net.ipv4.conf.default.accept_redirects = 0 52net.ipv4.conf.all.secure_redirects = 0 53net.ipv4.conf.default.secure_redirects = 0 54 55# Don\u0026#39;t act as a router 56net.ipv4.ip_forward = 0 57net.ipv4.conf.all.send_redirects = 0 58net.ipv4.conf.default.send_redirects = 0 59 60# Number of times SYNACKs for passive TCP connection. 61net.ipv4.tcp_synack_retries = 2 62 63# Allowed local port range 64net.ipv4.ip_local_port_range = 1024 65000 65 66# Protect Against TCP Time-Wait 67net.ipv4.tcp_rfc1337 = 1 68 69# Decrease the time default value for tcp_fin_timeout connection 70net.ipv4.tcp_fin_timeout = 15 71 72# Decrease the time default value for connections to keep alive 73net.ipv4.tcp_keepalive_time = 300 74net.ipv4.tcp_keepalive_probes = 5 75net.ipv4.tcp_keepalive_intvl = 15 76# This means that the keepalive process waits 300 seconds for socket  77# activity before sending the first keepalive probe, and then resend 78# it every 15 seconds. If no ACK response is received for 5 consecutive  79# times (75s in this case), the connection is marked as broken. 80 81### TUNING NETWORK PERFORMANCE ### 82 83# Disable IPv6 84net.ipv6.conf.all.disable_ipv6 = 1 85net.ipv6.conf.default.disable_ipv6 = 1 86net.ipv6.conf.lo.disable_ipv6 = 1 87 88# Default Socket Receive Buffer 89net.core.rmem_default = 31457280 90 91# Maximum Socket Receive Buffer 92net.core.rmem_max = 12582912 93 94# Default Socket Send Buffer 95net.core.wmem_default = 31457280 96 97# Maximum Socket Send Buffer 98net.core.wmem_max = 12582912 99 100# Increase number of incoming connections 101net.core.somaxconn = 5000 102 103# Increase number of incoming connections backlog 104net.core.netdev_max_backlog = 65536 105 106# Enable TCP window scaling 107net.ipv4.tcp_window_scaling = 1 108 109# Increase the maximum amount of option memory buffers 110net.core.optmem_max = 25165824 111 112 113# Increase the maximum total buffer-space allocatable 114# This is measured in units of pages (4096 bytes) 115net.ipv4.tcp_mem = 65536 131072 262144 116net.ipv4.udp_mem = 65536 131072 262144 117 118# Increase the read-buffer space allocatable 119net.ipv4.tcp_rmem = 8192 87380 16777216 120net.ipv4.udp_rmem_min = 16384 121 122# Increase the write-buffer-space allocatable 123net.ipv4.tcp_wmem = 8192 65536 16777216 124net.ipv4.udp_wmem_min = 16384 125 126 127# Increase the tcp-time-wait buckets pool size to prevent simple DOS attacks 128net.ipv4.tcp_max_tw_buckets = 1800000 129 130# TIME_WAIT socket policy 131# Note: if both enabled then disable 132# net.ipv4.tcp_timestamps for servers  133# behind NAT to prevent dropped incoming connections 134net.ipv4.tcp_tw_recycle = 1 135net.ipv4.tcp_tw_reuse = 1 136 137# Enable TCP MTU probing (in case of Jumbo Frames enabled) 138#net.ipv4.tcp_mtu_probing = 1 139 140# Speedup retrans (Google recommended) 141net.ipv4.tcp_slow_start_after_idle = 0 142net.ipv4.tcp_early_retrans = 1 143 144# Conntrack 145# 288bytes x 131072 = 37748736 (~38MB) max memory usage 146net.netfilter.nf_conntrack_max = 131072 147net.netfilter.nf_conntrack_tcp_loose = 1 148 149#TCP的直接拥塞通告(tcp_ecn)关掉 150net.ipv4.tcp_ecn = 0 151 152#路由缓存刷新频率，当一个路由失败后多长时间跳到另一个路由，默认是300。 153net.ipv4.route.gc_timeout = 100 154 155#设定系统中最多允许在多少TCP套接字不被关联到任何一个用户文件句柄上。 156#如果超过这个数字，没有与用户文件句柄关联的TCP 套接字将立即被复位 157#防简单Dos 158net.ipv4.tcp_max_orphans = 655360 159 160# NOTE: Enable this if machine support it 161# -- 10gbe tuning from Intel ixgb driver README -- # 162# turn off selective ACK and timestamps 163net.ipv4.tcp_sack = 0 164net.ipv4.tcp_timestamps = 1 ","date":"2021-11-02","img":"","permalink":"/posts/20211102-sysctl_conf/","series":null,"tags":null,"title":"Linux内核sysctl内核参数优化"},{"categories":null,"content":"Custom Configuration of TCP Socket Keep-Alive Timeouts 这是个古老的话题，我们在机器的优化中，需要设置 TCP Socket 的 Timeout 参数\n用来加快 TCP 关闭无用闲置连接的时间\nLinux 内核中有三个缺省参数:\n  1 tcp_keepalive_time  缺省是 7200 秒    1 tcp_keepalive_probes  缺省是 9    1 tcp_keepalive_intvl  缺省是 75 秒    处理流程如下：\n一、客户端打开一个 TCP socket 连接，开始跟服务器通讯\n二、如果这条 socket 连接空闲没有任何数据传输，静默了 tcp_keepalive_time 秒后，那么客户端会主动发送一个空的 ACK 包到服务器\n三、那么，根据服务器是否回应了一个相应的 ACK 包来判断\n1ACK  未回应  等待 tcp_keepalive_intvl 秒，然后再发一个 ACK 包 重复以上等待并发送 ACK 包的过程，直到次数等于 tcp_keepalive_probes 如果第2步做完还收不到任何回应，发送一个 RST 包并关闭连接   回应了: 回到上述第二步  那么缺省情况下，7200+75×9，一个没有任何数据传输的 socket 才会被关闭，大概是2小时11分钟。\n这个时间太长了。需要优化一下：\n1net.ipv4.tcp_keepalive_time = 300 2net.ipv4.tcp_keepalive_probes = 5 3net.ipv4.tcp_keepalive_intvl = 15 上面的时间是 300 + 5x15，大概是6分钟，大大缩短释放空闲 socket 的时间\n","date":"2021-11-02","img":"","permalink":"/posts/20211102-tcp_keealive/","series":null,"tags":null,"title":"Linux内核TCP连接Keep-Alive Timeout的配置"},{"categories":null,"content":"这属于Shell的高级技巧了，我们可能需要在 bash 中并发 wget rsync 文件，下面就讨论一下这个问题。\n首先从简单的单线程开始：\n1$ for i in $(seq 1 2); do echo $i; done 21 32 可以看到是顺序执行的，下面变多线程：\n1$ for i in $(seq 1 2); do echo $i \u0026amp; done 2[1] 245505 31 4[2] 245506 52 6[1] Done echo $i 7[2] Done echo $i 可以看到我们只把 ; 号改成了 \u0026amp; 号，程序就变成了多线程执行。\n区别在于 ; 号会等待之前的命令执行完毕再执行下一条，而 \u0026amp; 不等待，直接继续执行下一条；相当于后台运行了前一条命令。\n下面说说 find 的单线程和多线程：\nfind 的 exec 用法\n1$ find /path [args] -exec [cmd] {} \\;  {} 占位符号，存放find找到的记录 ; 对于每一条找到的单独记录，执行的cmd是一条一条单独执行的 执行的顺序如下: cmd result1; cmd result2; \u0026hellip;; cmd result N  1$ find /path [args] -exec [cmd] {} \\+  {} 占位符号，存放find找到的记录 + 对于找到的所有记录，执行的cmd是合并了所有记录集执行的 执行顺序如下: cmd result1 result2 \u0026hellip; result N  多个exec可以串起来：\n1$ find /tmp/dir1/ -type f -exec grep howtouselinux {} \\; -exec echo {} \\; | sed \u0026#39;s/howtouselinux/deep/g\u0026#39; 至此，find 也还是单线程执行的，并没有并发。\nfind 要并发，就只能跟 xargs 结合在一起：\nxargs 通常配合管道使用，将前面命令产生的参数，逐个传入后续命令，作为参数。xargs 传来的参数，默认位于 xargs 后面命令的最后，如果要改变位置，需要用**-I**参数。xargs 如果不带命令，缺省是 echo\n  -d 分隔符\n 1$ echo -e \u0026#34;a\\tb\\tc\u0026#34; | xargs -d \u0026#34;\\t\u0026#34; echo 2a b c    -I{} 指定占位符，-I %那就是 % 替代从之前管道取得的参数\n 1$ find . -type d | xargs -I % -0 rsync -auvPR % 192.168.1.38::new/    -0 跟find的-print0配合，find命令有一个特别的参数-print0，指定输出的文件列表以null分隔。然后，xargs命令的-0参数表示用null当作分隔符。\n 1$ find ./new -mindepth 6 -maxdepth 6 -type d -print0 | xargs -I % -0 rsync -auvPR % 172.18.34.38::new/    -P 最大并发线程数，下面是并发30线程\n 1$ find ./new -mindepth 6 -maxdepth 6 -type d -print0 | xargs -P 30 -I % -0 rsync -auvPR % 172.18.34.38::new/    -n 选项限制单个命令行的参数个数，下面是 rsync 一行命令传带60个文件，30个进程那就是30个 rsync，每个 rsync 同时传60个文件。\n 1$ find ./new -mindepth 6 -maxdepth 6 -type d -print0 | xargs -P 30 -n 60 -I % -0 rsync -auvPR % 172.18.34.38::new/ 2 3$ echo {0..10} | xargs -I{} -n 2 40 1 52 3 64 5 76 7 88 9 910    使用 bash -c 并发的例子：\n1$ time for i in $(seq 1 5); do echo $[$RANDOM % 5 + 1]; done | xargs -I{} echo \u0026#34;sleep {}; echo \u0026#39;Done! {}\u0026#39;\u0026#34; | xargs -P5 -I{} bash -c \u0026#34;{}\u0026#34; ","date":"2021-10-29","img":"","permalink":"/posts/20211029-bash_multithread/","series":null,"tags":null,"title":"Shell以及find的多线程执行"},{"categories":null,"content":"之前讲过如何对 opnvpn 总体限速，这次来了一个更严格的程序限速需求：\n场景如下：\n  两个机房间有一条专线 100M\n  两个机房间需要同步数据，同步需要限制到60M，给别的程序留出带宽空间\n  传输是多个文件，用 rsync 并发传送\n  分析了一下脚本的核心部分\n1find . -type f | grep $(date -d\u0026#34;-1 day\u0026#34; +\u0026#39;%Y%m%d\u0026#39;) |xargs -I % -P 30 rsync -auvPR % 192.168.9.17::mysql 发现是利用 xargs 的并发，-P 30 最大并发30个，启动了 rsync 同步\nrsync 没有限速，这就麻烦了。\n一、单文件单独限速 首先是使用 rsync 的 \u0026ndash;bwlimit=600 参数，把速度限制为 600KB/s ，600×8=4800，单进程基本是5M的速度，最多只能跑12个了，就会跑到60M。\n这样也不太对，尤其是 rsync 并发进程逐渐减少，少于12个的时候，这样就出现跑不满60M的现象。\n二、多文件整体限速 那么 rsync 支持多文件传输 ，使用如下格式整体限速\n1rsync --bwlimit=7200 -auvPR 文件1 文件2 文件3 192.168.9.17::mysql 问题又来了，文件1 文件2 文件3 的路径非常长，而文件个数不定，有撑爆命令行单行长度限制的可能，也不可行\n三、tc 使用 tc 可以控制源ip或者目的ip的带宽，但是本机网卡是万兆光卡，生产环境，每时每刻都有数据读写。\n一旦错了，就直接完蛋了。也不太可行\n四、杀器trickle 寻找了半天，终于找到了个大杀器trickle，可以对程序单独限速，也可以对一堆程序整体限速\n安装：\n1$ yum install -y epel-release 2$ yum install trickle 参数解释：\n -u 上载速度 KB/s ，乘以8换算成网络速度 -d 下载速度 KB/s ，乘以8换算成网络速度 -s standalone独立模式，不参与 trickled 的整体模式  如果要对一个程序单独限速，10KB\n1trickle -s -u 10 -d 10 wget http://mirrors.163.com/rocky/8/isos/x86_64/Rocky-8.4-x86_64-dvd1.iso wget 的下载完全被限制在了10KB\n如过要对使用trickle的所有程序做整体限速\n1# 首先启动守护进程 trickled 2$ trickled -u 7200 -d 7200 3 4# 然后所有用trickle执行的命令就会整理限速在7200KB/s （网络速度60M） 5find . -type f | grep $(date -d\u0026#34;-1 day\u0026#34; +\u0026#39;%Y%m%d\u0026#39;) |xargs -I % -P 30 trickle rsync -auvPR % 192.168.9.17::mysql ","date":"2021-10-28","img":"","permalink":"/posts/20211028-trickle/","series":null,"tags":null,"title":"Linux下的程序限速软件Trickle"},{"categories":null,"content":"Dockerfile 是造出镜像的基础，是必须熟知并了解的知识：\n一、编写Dockerfile 先给个例子，是 minio 代理访问阿里的 OSS\n1FROM alpine:3.12  2 3RUN apk add --update bash \u0026amp;\u0026amp; rm -rf /var/cache/apk/*  4 5COPY minio.RELEASE.2020-04-15T19-42-18Z /data/minio.RELEASE.2020-04-15T19-42-18Z  6 7ENV MINIO_ACCESS_KEY=LTAI5tFFTbsxxxxxuLb  8ENV MINIO_SECRET_KEY=t78PyGnHZilxxxxxdxBCjvNgtVC5Y  9 10WORKDIR /data  11EXPOSE 9000  12 13CMD [\u0026#34;/data/minio.RELEASE.2020-04-15T19-42-18Z\u0026#34;,\u0026#34;gateway\u0026#34;,\u0026#34;oss\u0026#34;,\u0026#34;http://oss-cn-shanghai-internal.aliyuncs.com\u0026#34;]  14# CMD /bin/sh -c \u0026#34;while true; do echo hi; sleep 10; done\u0026#34; 15  详细解释每一条语句：\n  FROM\n基板，alpine 3.12 是个比较微小的版本，注意它的毛病，/bin/sh其实是busybox，没有/bin/bash，某些bash的函数功能支持不全，比如for循环\n  RUN\n在容器中运行命令，上例中我们添加了 bash ，并清理了缓存。命令间用 \u0026amp;\u0026amp; 可以避免镜像过多分层。\nRUN分两种模式shell和exec模式:\n我们只用 exec 模式，因为在 image 里装入多个 shell，没什么意义。\n  COPY 和 ADD\n作用都是将文件或目录复制到 Dockerfile 构建的镜像中\n我们只用COPY，如果遇到要把URL的文件放进去，可以先wget，然后放；如果要解压tarr包放进去，那就先解压再放。\n注意源文件路径都使用相对路径，目标路径使用绝对路径。\n如果dest不指定绝对路径，则是想对于WORDIR的相对路径\n  CMD 入口\n用 [] 分割， 把所有 \u0026quot;\u0026quot; 的部分合并为一行，中间用空格隔开执行；或者直接一行没任何分割符。\n所以上面的例子就是执行了一句：\n1/data/minio.RELEASE.2020-04-15T19-42-18Z gateway oss http://oss-cn-shanghai-internal.aliyuncs.com 技巧：\n把几个命令合在一起执行\n()表示在当前shell合并执行\n{}表示派生出一个子shell，在子shell中合并执行，{ echo \u0026ldquo;aaa\u0026rdquo; }必须有空格\n\u0026amp;表示后台运行\n命令之间使用 \u0026amp;\u0026amp; 连接，实现逻辑与的功能。\n 只有在 \u0026amp;\u0026amp; 左边的命令返回真（命令返回值 $? == 0），\u0026amp;\u0026amp; 右边的命令才会被执行。 只要有一个命令返回假（命令返回值 $? == 1），后面的命令就不会被执行。  \u0026amp;\u0026amp;左边的命令（命令1）返回真(即返回0，成功被执行）后，\u0026amp;\u0026amp;右边的命令（命令2）才能够被执行；换句话说，“如果这个命令执行成功\u0026amp;\u0026amp;那么执行第二个命令”。\n最下的语法用了seq而不是for循环，是因为busybox的sh不支持for语法\n所以才有如此怪异的语法，在容器中后台跑10个php think queue，1个crond，前台跑一个php-fpm：\n1CMD for i in $(seq 10); do (php think queue \u0026amp;) ; done \u0026amp; crond \u0026amp;\u0026amp; php-fpm   ARG 参数\nARG VERSION=7.6.1\n定义后可以用${VERSION}引用，build的时候可以加\u0026ndash;build-arg 传参数进去\n1docker build --build-arg VERSION=${LATEST} -t $(ORG)/$(NAME):$(BUILD) .   还有很多 Build 的技巧，如果造一个 go 语言编译环境的中间层镜像，然后造最终镜像。\n但是八戒还是推荐直接造出二进制可执行文件，然后直接拷贝进去就好，不要弄的过于麻烦，中间层那种适用于用源码 CI/CD 中无编译环境的情况。\n二、调试容器 很多情况下我们造好了 image ，一跑就掉下来了，也不知道是什么情况\n这个时候，我们把 CMD 换成一个 sh 执行一个死循环\n1CMD /bin/sh -c \u0026#34;while true; do echo hi; sleep 10; done\u0026#34; 然后进入容器，然后再执行之前的 CMD 命令，看看报错信息是什么，就可以调试了\n1$ docker exec -it 89174asklja /bin/sh 三、pod的等待技巧 这里启动正式的pod之前，先临时起了两个容器等待其他服务的完成\n1 containers: 2 - name: myapp-container 3 image: busybox 4 command: [\u0026#39;sh\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#39;echo The app is running! \u0026amp;\u0026amp; sleep 3600\u0026#39;] 5 initContainers: 6 - name: init-myservice 7 image: busybox 8 command: [\u0026#39;sh\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#39;until nslookup myservice; do echo waiting for myservice; sleep 2; done;\u0026#39;] 9 - name: init-mydb 10 image: busybox 11 command: [\u0026#39;sh\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#39;until nslookup mydb; do echo waiting for mydb; sleep 2; done;\u0026#39;] ","date":"2021-10-28","img":"","permalink":"/posts/20211028-dockerfile/","series":null,"tags":null,"title":"Dockerfile的编写与调试技巧"},{"categories":null,"content":"给同事做了个 PHP 接口，转发发送短信的请求，同时要把发送记录发送到远程的 cacti 的 syslog 去\n很简单，但是也不简单\n首先是 PHP 服务器，是最简化编译的，php -m 查了一下\n1php -m 2[PHP Modules] 3Core 4ctype 5curl 6date 7dom 8fileinfo 9filter 10gettext 11hash 12iconv 13json 14libxml 15openssl 16pcre 17PDO 18pdo_sqlite 19Phar 20posix 21Reflection 22session 23SimpleXML 24SPL 25sqlite3 26standard 27tokenizer 28xml 29xmlreader 30xmlwriter 31 32[Zend Modules] 居然没有 socket 模块，没办法，找到源代码，编译一个安装，原有的 php 安装路径是 /export/servers/php\n1$ tar zxvf php-7.4.0.tar.gz 2$ cd php-7.4.0/sockets 3$ /export/servers/php/bin/phpize 4$ ./configure --enable-sockets --with-php-config=/export/servers/php/bin/php-config 5$ make 6$ make install 又看了一眼，是 php-fpm，居然没有 php.ini ，得，再生成一个，放在 /export/servers/php/lib/php.ini\n1extension_dir = \u0026#34;/export/servers/php740/lib/php/extensions/no-debug-non-zts-20190902/\u0026#34; 2extension = sockets.so 然后重启 php-fpm ，重新 php -m 检查，发现有 socket 模块就 ok 了。\n接下来就是 php 源代码了\n1\u0026lt;?php 2date_default_timezone_set(\u0026#39;Asia/Shanghai\u0026#39;); 3 4function send_remote_syslog($message) { 5 $sock = socket_create(AF_INET, SOCK_DGRAM, SOL_UDP); 6 foreach(explode(\u0026#34;\\n\u0026#34;, $message) as $line) { 7 $syslog_message = \u0026#34;\u0026lt;22\u0026gt;\u0026#34; . date(\u0026#39;M d H:i:s \u0026#39;) . \u0026#39;Qi_an_xin sms_log: \u0026#39; . $line; 8 socket_sendto($sock, $syslog_message, strlen($syslog_message), 0, \u0026#39;172.18.31.6\u0026#39;, 514); 9 } 10 socket_close($sock); 11} 12 13 send_remote_syslog(\u0026#34;ABC 验证码: 39792192\u0026#34;); 14?\u0026gt;这样就可以了，上面代码比较难理解的是\u0026lt;22\u0026gt;，那是报错级别的计算方法，Facility + Severity：\n1 * Facility values: 2 * 0 kernel messages 3 * 1 user-level messages 4 * 2 mail system 5 * 3 system daemons 6 * 4 security/authorization messages 7 * 5 messages generated internally by syslogd 8 * 6 line printer subsystem 9 * 7 network news subsystem 10 * 8 UUCP subsystem 11 * 9 clock daemon 12 * 10 security/authorization messages 13 * 11 FTP daemon 14 * 12 NTP subsystem 15 * 13 log audit 16 * 14 log alert 17 * 15 clock daemon 18 * 16 local user 0 (local0) (default value) 19 * 17 local user 1 (local1) 20 * 18 local user 2 (local2) 21 * 19 local user 3 (local3) 22 * 20 local user 4 (local4) 23 * 21 local user 5 (local5) 24 * 22 local user 6 (local6) 25 * 23 local user 7 (local7) 26 * 27 * Severity values: 28 * 0 Emergency: system is unusable 29 * 1 Alert: action must be taken immediately 30 * 2 Critical: critical conditions 31 * 3 Error: error conditions 32 * 4 Warning: warning conditions 33 * 5 Notice: normal but significant condition (default value) 34 * 6 Informational: informational messages 35 * 7 Debug: debug-level messages 计算方法就是 （facility*8 + severity），这里的22+0，可以理解成 local6 ，就是级别6\n如果发了一个 \u0026ldquo;local use 4\u0026rdquo; 和 Serverity = 5 的消息，那么就是 20×8+5=165 ，包头就是 \u0026lt;165\u0026gt;\n","date":"2021-10-28","img":"","permalink":"/posts/20211028-php_syslog/","series":null,"tags":null,"title":"PHP程序如何发送syslog到远程服务器"},{"categories":null,"content":"用 kubernetes 越多，用 docker 越多，就愈发感觉到好处多多。\n简简单单的一个可执行文件，用 docker 基板 alphine 封装，就可以运行起一个 pod ，然后指定 deployment、svc、ingress，就可以将服务暴露出去。\n其实很多情况下单可执行文件 + systemd也是不错的选择。\n这不就遇到个问题，ghostunnel 这个软件，github 只释放出了源代码以及 windows 、linux 和 mac 的三个可执行版本。\n可我的执行环境是 nanopi ，是个 arm7 的架构，就无计可施了。\n无奈下，在 nanopi 上装了 go ，编译了个 arm7 的出来。\n但是遇到 vaultwanden ，rust 的，就没法弄了，vps 太弱，根本无法用 cargo 编译。\n那怎么办呢？方法如下，不安装 Docker ，也可以把镜像中的文件抽取出来\n1$ mkdir vm 2$ wget https://raw.githubusercontent.com/jjlin/docker-image-extract/main/docker-image-extract 3$ chmod +x docker-image-extract 4$ ./docker-image-extract vaultwarden/server:alpine 5Getting API token... 6Getting image manifest for vaultwarden/server:alpine... 7Fetching and extracting layer a0d0a0d46f8b52473982a3c466318f479767577551a53ffc9074c9fa7035982e... 8Fetching and extracting layer 3a9a529931676767ec84d35ab19774b24bd94e20f6fff7e6bda57ef5f2a66cfc... 9Fetching and extracting layer f9dcfa9aefe67ce52ab2a73e515ea715d242348b8fc338dbe4ca72a853ea0318... 10Fetching and extracting layer 4249d8cece35148b5faca2c6a98d566a271a1996b127b14480793ee8825e43c0... 11Fetching and extracting layer 72f4873a62cc82eaf28905077df3791e3b235bf5d17670e7aff6d5fb5e280739... 12Fetching and extracting layer 8eb772c524f9d998c8c7c92acc5ba96e3e9ebfb175dbb2441fe6e7b7598874f5... 13Fetching and extracting layer 663794f103b44abb8a90e1376dce14735905e2f938b4ca7e0ff379b09cbf6148... 14Image contents extracted into ./output. 15 这样我们就可以在 output 目录下得到 vaultwarden 和 web-vault\n如果我们要拿到 arm7 的镜像，还需要再费点劲，以 ghostunnel 为例：\n访问： https://hub.docker.com/r/ghostunnel/ghostunnel/tags 找到 linux/arm/v7 的 tag，40247f4b49c3 点开后，找到 DIGEST: ，复制sha256以及后面的字串，sha256:40247f4b49c364046ebf47696dbd31752b4db2ae2b9edf1fd4c52c2573f06e04\n按如下方法释放即可：\n1$ ./docker-image-extract ghostunnel/ghostunnel:sha256:40247f4b49c364046ebf47696dbd31752b4db2ae2b9edf1fd4c52c2573f06e04 ","date":"2021-10-27","img":"","permalink":"/posts/20211027-docker_extract_file/","series":null,"tags":null,"title":"没有装Docker如何从镜像中释放出文件"},{"categories":null,"content":"为什么会有非 Docker 环境这个怪字眼呢？\n无他，因为满网搜索到的教程都是在 Docker 环境下安装使用。\n但是穷啊，八戒的 vps 是个单核 500m 的 justhost 机器，便宜的很，这种廉价机器来跑 Docker，基本要占100M，跑不太动。\n这种一穷二白的环境，就只能把 Bitwarden 从容器里拆出来用。\n好在 Bitwarden_rs 是一个 rust 程序，占内存(16M左右)和cpu极少，本身就适合在 systemd 环境下跑。\n这里就利用 vaultwarden 和 traefik，在一台老破小服务器上运行。\n系统环境是 CentOS 7.9\n步骤如下：\n一、下载bitwarden(vaultwarden) 1wget https://github.com/dani-garcia/vaultwarden/archive/refs/tags/1.23.0.tar.gz 二、安装cargo并编译（可选） 1yum install -y epel-release 2yum install -y openssl-devel cargo 3 4cd vaultwarden-1.23.0 5cargo build --release --features sqlite 直接爆错啊，小小的 vps 连编译都过不去，编译进程都被 kill 掉了\n三、下载vaultwarden主文件 编译不通，就只能想别的办法了。Faint\n找一台有 docker 机器，从里面把文件都解析出来好了\n1docker pull vaultwarden/server:alpine 2docker create --name vw vaultwarden/server:alpine 3docker cp vw:/vaultwarden . 4docker cp vw:/web-vault . 5docker rm vw 这样会得到一个可执行文件 vaultwarden 和一个目录 web-vault\n我们把这两个东西都挪到 /opt/vaultwarden 目录下，并且建立 data 文件夹，用来存放要生成的 sqlite3 数据文件。\n1mkdir -p /opt/vaultwarden/data 2mv vaultwarden /opt/vaultwarden 3mv web-vault /opt/vaultwarden 四、生成systemd启动文件 注意，下面我们设置了 vaultwarden ROCKET_ADDRESS 的监听地址是 127.0.0.1 ，一是为了安全，二是为了下一步我们搭建 traefik，来反代 vaultwarden 用的；因为访问 vaultwarden 必须要加证书，而它本身是没有这个功能的，必须前置一个 nginx 或者 haproxy 或者 traefik 或者 carddy。\n1cat \u0026lt;\u0026lt; EOF \u0026gt;\u0026gt; /etc/systemd/system/vaultwarden.service  2[Unit] 3Description=Bitwarden 4 5[Service] 6Type=simple 7Restart=always 8Environment=\u0026#34;ROCKET_ADDRESS=127.0.0.1\u0026#34; 9WorkingDirectory=/opt/vaultwarden 10ExecStart=/opt/vaultwarden/vaultwarden 11 12[Install] 13WantedBy=local.target 14EOF 五、配置traefik 1wget https://github.com/traefik/traefik/releases/download/v2.4.8/traefik_v2.4.8_linux_amd64.tar.gz 2tar zxvf traefik_v2.4.8_linux_amd64.tar.gz 3 4mkdir -p /opt/traefik/dynamic 5mv traefik /opt/traefik 生成traefik配置文件，利用 traefik 自动申请 Let\u0026rsquo;s encrypt 证书\n1cat \u0026lt;\u0026lt; EOF \u0026gt;\u0026gt; /opt/traefik/traefik.yml 2log: 3 level: DEBUG 4 5api: 6 insecure: false 7 dashboard: true 8 9entryPoints: 10 http: 11 address: \u0026#34;:80\u0026#34; 12 http: 13 redirections: 14 entryPoint: 15 to: https 16 scheme: https 17 https: 18 address: \u0026#34;:443\u0026#34; 19 20certificatesResolvers: 21 letsEncrypt: 22 acme: 23 storage: /opt/traefik/acme.json 24 email: zhangranrui@gmail.com 25 tlsChallenge: {} 26 httpChallenge: 27 entryPoint: http 28 29providers: 30 file: 31 directory: /opt/traefik/dynamic 32 watch: true 配置 vaultwarden 代理\n1cat \u0026lt;\u0026lt; EOF \u0026gt;\u0026gt; /opt/traefik/dynamic/pass.yml 2http: 3 routers: 4 https_01: 5 rule: \u0026#34;Host(`xxx.rendoumi.com`)\u0026#34; 6 service: svc_01 7 tls: 8 certresolver: letsEncrypt 9 http_01: 10 rule: \u0026#34;Host(`xxx.rendoumi.com`)\u0026#34; 11 service: svc_01 12 entryPoints: 13 - http 14 services: 15 svc_01: 16 loadBalancer: 17 servers: 18 - url: \u0026#34;http://localhost:8000\u0026#34; 19EOF 设置 traefik 的 systemd 启动文件\n1cat \u0026lt;\u0026lt; EOF \u0026gt;\u0026gt; /etc/systemd/system/traefik.service  2[Unit] 3Description=traefik 4 5[Service] 6Type=simple 7Restart=always 8WorkingDirectory=/export/servers/traefik 9ExecStart=/export/servers/traefik/traefik 10 11[Install] 12WantedBy=local.target 13EOF 五、启动vaultwarden和traefik 1systemctl daemon-reload 2systemctl enable --now vaultwarden 3systemctl enable --now traefik 打开页面，我们就成功的用一台老破小搭建了自己的密码管理服务器！！！\n","date":"2021-10-27","img":"","permalink":"/posts/20211027-bitwarden/","series":null,"tags":null,"title":"Bitwarden（vaultwarden）如何在非Docker环境下安装使用"},{"categories":null,"content":"在生产环境来创建阿里ACK托管k8s集群的过程：\n完全用于生产，不是搭建来做测试用的。\n授公司委托，给的RAM用户，所以阿里云RAM第一次登录后，强制修改密码\n然后授权资源管理， 正式开始建立过程\n一、准备条件   两台及以上ecs服务器\n  阿里云账户余额100元以上（阿里云要求）\n  阿里云oss一个（oss和ecs在一个区域最好）\n  首先阿里云创建k8s集群要求至少有两台ecs服务器，可以创建集群的时候再购买，不要预先购买。\n二、下面开始创建： 阿里云最左上角的菜单（新版本首页）-\u0026gt;产品与服务-\u0026gt;容器服务kubernetes版本\n第一次创建会让开启ram授权，正常点击授权就可以\n点击创建集群\n点击后如下\n各个选项的详细说明：\n第一部分：\n集群版本： 最上面可以选择ACK托管版，和其他4个版本，着重说一下专有版和托管版的区别\n专有版本：master和worker都需要自己创建，如果需要高可用，那么master需要至少三个，也就是说，如果你不想把master和worker放在同一台服务器上，就要多使用三台服务器。\nACK托管版：master由阿里云给创建，自己只需要购买worker服务器。\n集群名称： k8s-hbb\n地域： 请选择自己ecs和rds等资源所在区域，这里是华东2（上海）\nKubernetes版本：阿里云已经做好充分的测试了，所以选择默认的即可。这里是 1.18.8-aliyun.1\n容器运行时： Docker 19.03.5\n第二部分：\n专有网络: 专有网络选择和ecs，rds同一个专有网络，这里是vpc-uf6pcr7nvp3dqmx86yyk0，网段是172.19.0.0/16\n虚拟交换机： 同一个专有网络下面的交换机是可以互通的，这里新建一个虚拟交换机，网段是172.19.240.0/20\n网络插件： 选Flannel，除去阿里云自己的区别描述，还有一点 如果使用flannel插件，则worker端对外，访问外网（比如短信接口等）使用的是worder所在ecs自己的eip或者如果使用的是snat模式，就是snat绑定的eip。如果使用的terway插件则走的就是snat的eip。注意，创建集群成功后，会为集群创建一个对外服务的ingress的slb，worker内部的容器直接对外访问，使用的不是这个slb的ip。slb只是进来的通道。\npod网络 CIDR：为统一起见，10.240.0.0/20\nservice CIDR：为统一起见，192.168.240.0/20\n注意以上 建立好了三个网段，三个网段中均有240字段，便于记忆\nECS(2台)：172.19.240.0/20\nPOD网段：10.240.0.0/20\nService网段：192.168.240.0/20 ​\n节点IP数量：256，指单个节点可运行 Pod 数量的上限。 一定要拉到最大量256 ，弄到16的话，一个节点本身要跑10多个system的pod，就无法跑应用pod了。\n第三部分:\n配置SNAT：必选配置SNAT，对外主动访问的时候IP需要一致。解释：如果ecs没有访问外网能力，则必须使用snat，snat就是把vpc绑定一个eip，然后给内部的ecs使用nat方式主动外出访问用的，比如主动反问第三方的接口等。如果ecs自己已经绑定了eip或者自带ip带宽，可以不选择。\nAPISERVER访问：必选公网EIP暴露，这个绑定以后ip不收费，可以使用流量包，管理master用的。如果要使用『云效』必选。\n默认不选中使用EIP暴露API Server。 API Server提供了各类资源对象（Pod，Service等）的增删改查及watch等HTTP Rest接口。 - 如果选择开放，会创建一个EIP，并挂载到内网SLB上。此时，Master节点的6443端口（对应API Server）暴露出来，用户可以在外网通过kubeconfig连接并操作集群。 - 如果选择不开放，则不会创建EIP，您只能在VPC内部用kubeconfig连接并操作集群。\nRDS白名单：这个注意选择，目前阿里云显示出来的只有普通的mysql-rds，redis的和polardb的都不显示\n安全组：选择企业类型就可以，后期可以修改规则。\n第四部分\nKube-proxy模式：选IPVS，比IPTABLES性能高\n集群本地域名： hbb.local，.local结尾的统统是本地域名\n下一步，增加worker\n必须选新增实例，不要选择现有实例\n新增实例：就是新购买ecs，要注意自己选择vpc和交换机\n选择已有实例：可以选择现有的服务器，注意：现有服务器会被更换硬盘，硬盘内容会被清空。\n企业级实例规格族\n实例规格族名称格式为ecs.\u0026lt;规格族\u0026gt;，实例规格名称为ecs.\u0026lt;规格族\u0026gt;.large。\n ecs：云服务器ECS的产品代号。 \u0026lt;规格族\u0026gt;：由小写字母加数字组成。  小写字母为某个单词的缩写，并标志着规格族的性能领域。部分小写字母的含义如下所示。  c：一般表示计算型（computational） g：一般表示通用型（general） r：一般表示内存型（ram） ne：一般表示网络增强型（network enhanced）   数字一般区别同类型规格族间的发布时间。更大的数字代表新一代规格族，拥有更高的性价比，价格低性能好。   large：n越大，vCPU核数越多。  例如，ecs.g6.2xlarge表示通用型g6规格族中的一个实例规格，拥有8个vCPU核。相比于g5规格族，g6为新一代通用型实例规格族。\n按上面选ecs.c6.large，费用0.95/时，似乎比ecs.n1.medium 1.34/时好。\n节点数量：最少是2个，无法减到0，没办法。\n系统盘：选ESSD，速度快，40G即可。不要开云盘备份，会要求设置snapshot策略，要收钱。\n操作系统：选CentOS 7.9，不要选aliyun的自定义版本。更加标准化，便于升级。\n这里选择操作系统的时候，只有两个系统可以选择，一个是centos7，一个是阿里云linux。为什么操作系统不能选很多种？因为阿里云要使用Cloud-init自动安装docker的各种工具包进作为worker角色的ecs，所以他对系统的要求更高，否则很可能出现各种各样的问题。这里才会有这种限制。\n密钥对选则新建一个k8s-ssh。\n下一步组件配置\n安装ingress组件：需要对外服务，这个必选，会给分配一个slb负载均衡\n如果想K8S集群的服务直接提供服务给用户访问，可以选择『公网』，它会创建一个SLB并用EIP暴露公网，后端是k8s-ingress入口。\n负载类型： 公网，对外服务就要写公网。\n存储插件： 必选CSI，对之后创建数据卷语法没影响\n监控插件： 基础版是免费的，可以放心使用\n日志服务： 日志服务可以加，尤其是以后如果想采集内部doker里面的日志，这里还是推荐加一下最好，他会自动创建标记采集，后面使用这个标记可以方便的自动添加日志节点。\nslb费用： 0.66/小时,带宽费用0.8/g\n下一步确认配置\n核对一下是否和自己选择的一样\n核对无误后点击创建集群。注意：创建集群的时候，会检测一些权限，如果权限未开通，可以令开页面进行开通授权，比如ess弹性伸缩，开通后点小按钮刷新状态， 状态都ok以后，点击创建集群。\n等待十分钟左右集群创建成功。到此创建集群已经完成。\n然后到控制台可以查看，这样ACK集群就创建好了。\n后记：\n毁掉ACK的时候，切忌去删除arms-prom的helm，再删除ack集群，否则会清不干净东西。\n下次重建的时候会装不上prometheus\n","date":"2021-10-26","img":"","permalink":"/posts/20211026-ack_build/","series":null,"tags":null,"title":"阿里云ACK完全生产环境规划和搭建"},{"categories":null,"content":"之前介绍过如何制作一个 centos live cdrom 系统\n那么，某些情况下我们可能无法弄一个 pxe 系统，而只能通过 idrac 挂载 iso 的方式安装系统\n该如何去做呢？\n步骤如下：\n一、下载Centos的minimal安装光盘 1wget http://mirrors.163.com/centos/7.9.2009/isos/x86_64/CentOS-7-x86_64-Minimal-2009.iso 2yum install -y mkisofs 二、准备kickstart安装文件 下载： centos7.ks 1text 2skipx 3install 4 5auth --useshadow --enablemd5 6authconfig --enableshadow --passalgo=sha512 7 8firstboot --disable 9keyboard us 10lang en_US.UTF-8 11reboot 12cdrom 13 14firewall --disable 15selinux --disabled 16 17services --enabled=\u0026#34;chronyd\u0026#34; 18logging level=info 19 20 21#ignoredisk --only-use=vda 22ignoredisk --only-use=sda 23#bootloader --location=mbr --append=\u0026#34;net.ifnames=0 biosdevname=0 crashkernel=auto\u0026#34; 24bootloader --location=mbr --append=\u0026#34;crashkernel=auto\u0026#34; 25 26rootpw --plaintext Renren2021! 27timezone Asia/Shanghai --isUtc 28 29network --device=lo --hostname=localhost.localdomain 30user --name=supdev --gid=511 --groups=\u0026#34;supdev\u0026#34; --uid=511 --password=\u0026#34;Renren2021!\u0026#34; 31 32zerombr 33clearpart --all --initlabel  34 35part biosboot --fstype=biosboot --size=1 36part /boot --fstype ext4 --size=2048  37part swap --asprimary --size=8192 38part / --fstype ext4 --size=1 --grow 39 40#part biosboot --fstype=biosboot --size=1 41#part /boot --fstype ext2 --size 250 42#part pv.01 --size 1 --grow 43#volgroup vg pv.01 44#logvol / --vgname=vg --size=1 --grow --fstype ext4 --fsoptions=discard,noatime --name=root 45#logvol /tmp --vgname=vg --size=1024 --fstype ext4 --fsoptions=discard,noatime --name=tmp 46#logvol swap --vgname=vg --recommended --name=swap 47 48#uefi 49#partition /boot/efi --asprimary --fstype=vfat --label EFI --size=200 50#partition /boot --asprimary --fstype=ext4 --label BOOT --size=500 51#partition / --asprimary --fstype=ext4 --label ROOT --size=4096 --grow 52 53 54services --enabled=network 55 56reboot 57 58%pre 59parted -s /dev/sda mklabel gpt 60%end 61 62%packages 63@core 64@system-admin-tools 65@additional-devel 66@virtualization-client 67@virtualization-platform 68@virtualization-tools 69libguestfs-tools-c 70perl-Sys-Virt 71qemu-guest-agent 72qemu-kvm-tools 73curl 74dstat 75expect 76openssl 77initscripts 78ipmitool 79lrzsz 80lsof 81mtools 82nc 83nmap 84perl 85perl-CPAN 86procps 87python 88screen 89sysstat 90systemtap 91systemtap-client 92systemtap-devel 93tcpdump 94telnet 95vim 96wget 97wsmancli 98zip 99chrony 100kexec-tools 101net-tools 102ntp 103ntpdate 104man 105acpid 106chrony 107telnet 108%end 三、准备生成iso的脚本 下载： makeiso.sh 1#!/bin/bash 2rm -rf /tmp/bootiso /tmp/bootcustom /tmp/boot.iso 3mkdir /tmp/bootiso 4mount -o loop CentOS-7-x86_64-Minimal-2009.iso /tmp/bootiso 5 6mkdir /tmp/bootcustom 7cp -r /tmp/bootiso/* /tmp/bootcustom 8umount /tmp/bootiso 9rmdir /tmp/bootiso 10 11 12chmod -R u+w /tmp/bootcustom 13 14cp centos7.ks /tmp/bootcustom/isolinux/ks.cfg 15 16sed -i \u0026#39;/menu\\ default/d\u0026#39; /tmp/bootcustom/isolinux/isolinux.cfg 17sed -i \u0026#39;s/^timeout\\ .*/timeout 10/g\u0026#39; /tmp/bootcustom/isolinux/isolinux.cfg 18sed -i \u0026#39;/^label\\ linux/i label\\ kickstart\u0026#39; /tmp/bootcustom/isolinux/isolinux.cfg 19sed -i \u0026#39;/^label\\ linux/i \\ \\ menu\\ label\\ ^Install\\ Using\\ Kickstart\\ CentOS 7\u0026#39; /tmp/bootcustom/isolinux/isolinux.cfg 20sed -i \u0026#39;/^label\\ linux/i \\ \\ menu\\ default\u0026#39; /tmp/bootcustom/isolinux/isolinux.cfg 21sed -i \u0026#39;/^label\\ linux/i \\ \\ kernel\\ vmlinuz\\ biosdevname=0\u0026#39; /tmp/bootcustom/isolinux/isolinux.cfg 22sed -i \u0026#39;/^label\\ linux/i \\ \\ append\\ initrd=initrd.img\\ ks=cdrom:\\/ks.cfg\u0026#39; /tmp/bootcustom/isolinux/isolinux.cfg 23sed -i \u0026#39;/^label\\ linux/i \\\\n\u0026#39; /tmp/bootcustom/isolinux/isolinux.cfg 24 25cd /tmp/bootcustom 26mkisofs -o /tmp/boot.iso -b isolinux.bin -c boot.cat -no-emul-boot -boot-load-size 4 -boot-info-table -V \u0026#34;CentOS 7 x86_64\u0026#34; -R -J -v -T isolinux/. . 生成的自动安装光盘文件在 /tmp/boot.iso ，在 idrac 中 mount 出来，就可以用 virtual CD-ROM 自动安装了\n","date":"2021-10-25","img":"","permalink":"/posts/20211025-autoinstall_cd/","series":null,"tags":null,"title":"Centos Auto Install Cdrom自动安装cdrom的制作"},{"categories":null,"content":"为了研发方便就给他们在内网开通了 vsftpd 的服务。\n结果 java 直接有封好的 ftp library 可用，大家就直接用了。\n导致任何单独的一个文件上传都会起一个 ftp 实例，没有复用 ftp 的 socket 链接 。系统挤压了大量的socket连接。\n烦恼啊，出了事就麻烦。需要把日志都详细记下来\n做法如下：\n1vi /etc/vsftp/vsftpd.conf 2...... 3dual_log_enable=YES 4log_ftp_protocol=YES 5xferlog_enable=YES 6xferlog_std_format=NO 7...... 解释一下：\n  dual_log_enable \u0026mdash; 和 xferlog_enable 协同，会写两份日志，一份到/var/log/xferlog，一份到/var/log/vsftpd.log\n  log_ftp_protocol \u0026mdash; 和 xferlog_enable 协同，同时xferlog_std_format需要设置为NO，这样所有的 FTP 命令都会记录下来。\n  这样所有人的操作都会被记录下来，就后顾无忧了。\n","date":"2021-10-25","img":"","permalink":"/posts/20211025-vsftpd/","series":null,"tags":null,"title":"Vsftpd的日志设置"},{"categories":null,"content":"一般来说，我们要搭建一个正式的pxe自动装机系统，需要装 dnsmasq 做 dhcp + tftp ，需要编译 ipxe 来获得 undionly.kpxe ，需要 http 服务器来提供资源下载，repo 同步服务来提供 repo。组件非常多，也比较麻烦。\n当然，这么多也是有必要的，因为可以持续提供一个稳定的装机系统。\n场景一换，如果我们在本地机房里，什么都没有，想搭一套环境的步骤就比较繁复了。\nPyPXE 就是非常简单的一个程序，居然自己实现了用于 PXE 的 dhcp、tftp 和 http 全部的功能，而且支持 iPXE。\n太牛逼了，前提啊，PyPXE 是基于 Python 2.7 的，Python 3.x是运行不了的。\n想让它跑起来还必须做一定的修改，步骤如下：\n一、下载PyPXE 1git clone https://github.com/pypxe/PyPXE.git 2cd PyPXE 下载就行了，不用安装。\n二、手动生成config.json配置文件 1{ 2 \u0026#34;DHCP_SERVER_IP\u0026#34;: \u0026#34;192.168.85.27\u0026#34;, 3 \u0026#34;DHCP_FILESERVER\u0026#34;: \u0026#34;192.168.85.27\u0026#34;, 4 5 \u0026#34;DHCP_OFFER_BEGIN\u0026#34;: \u0026#34;192.168.85.200\u0026#34;, 6 \u0026#34;DHCP_OFFER_END\u0026#34;: \u0026#34;192.168.85.250\u0026#34;, 7 \u0026#34;DHCP_SUBNET\u0026#34;: \u0026#34;255.255.255.0\u0026#34;, 8 \u0026#34;DHCP_ROUTER\u0026#34;: \u0026#34;192.168.85.1\u0026#34;, 9 \u0026#34;DHCP_DNS\u0026#34;: \u0026#34;114.114.114.114\u0026#34;, 10 11 \u0026#34;DHCP_SERVER_PORT\u0026#34;: 67, 12 \u0026#34;DHCP_BROADCAST\u0026#34;: \u0026#34;\u0026#34;, 13 \u0026#34;DHCP_MODE_PROXY\u0026#34;: false, 14 \u0026#34;DHCP_WHITELIST\u0026#34;: false, 15 \u0026#34;HTTP_PORT\u0026#34;: 80, 16 \u0026#34;LEASES_FILE\u0026#34;: \u0026#34;\u0026#34;, 17 \u0026#34;MODE_DEBUG\u0026#34;: \u0026#34;dhcp\u0026#34;, 18 \u0026#34;MODE_VERBOSE\u0026#34;: \u0026#34;\u0026#34;, 19 \u0026#34;NBD_BLOCK_DEVICE\u0026#34;: \u0026#34;\u0026#34;, 20 \u0026#34;NBD_COPY_TO_RAM\u0026#34;: false, 21 \u0026#34;NBD_COW\u0026#34;: true, 22 \u0026#34;NBD_COW_IN_MEM\u0026#34;: false, 23 \u0026#34;NBD_PORT\u0026#34;: 10809, 24 \u0026#34;NBD_SERVER_IP\u0026#34;: \u0026#34;0.0.0.0\u0026#34;, 25 \u0026#34;NBD_WRITE\u0026#34;: false, 26 \u0026#34;NETBOOT_DIR\u0026#34;: \u0026#34;netboot\u0026#34;, 27 \u0026#34;NETBOOT_FILE\u0026#34;: \u0026#34;boot.http.ipxe\u0026#34;, 28 \u0026#34;STATIC_CONFIG\u0026#34;: \u0026#34;\u0026#34;, 29 \u0026#34;SYSLOG_PORT\u0026#34;: 514, 30 \u0026#34;SYSLOG_SERVER\u0026#34;: null, 31 \u0026#34;USE_DHCP\u0026#34;: true, 32 \u0026#34;USE_HTTP\u0026#34;: true, 33 \u0026#34;USE_IPXE\u0026#34;: true, 34 \u0026#34;USE_TFTP\u0026#34;: true 35} 上面json文件无法加注解，我们把它分三部分\n  本机配置，本机的地址都是 192.168.85.27\n  dhcp 的配置，开始192.168.85.200，结束192.68.85.250，掩码255.255.255.0，网关192.168.85.1，DNS114.114.114.114\n  第三部分不用动\n  三、下载ISO并修改ipxe脚本 1cd netboot 2wget http://mirrors.163.com/rocky/8/isos/x86_64/Rocky-8.4-x86_64-dvd1.iso 3mkdir rocky8.iso 4mount -o loop Rocky-8.4-x86_64-dvd1.iso rocky8.iso 5 6cat \u0026lt;\u0026lt; EOF \u0026gt;\u0026gt; boot.http.ipxe 7#!ipxe 89:start 10menu PXE Boot Options 11item shell iPXE shell 12item Rocky8 Install rocky8 13item exit Exit to BIOS 1415choose --default rocky8 --timeout 5000 option \u0026amp;\u0026amp; goto ${option} 16:shell 17shell 181920:rocky8 21set root http://192.168.85.27/rocky8.iso 22initrd ${root}/images/pxeboot/initrd.img 23kernel ${root}/images/pxeboot/vmlinuz inst.repo=${root}/ initrd=initrd.img ip=dhcp 24boot 252627:exit 28exit 29EOF 三、修改源代码 运行一下：\n1python -m pypxe.server --config config.json --debug all --verbose all 如果我们起一台机器或者虚机，会报第一个错：\nUnicodeDecodeError: \u0026lsquo;ascii\u0026rsquo; codec can\u0026rsquo;t decode byte 0xc0 in position 0: ordinal not in range(128)\n这个是代码报错，我们需要修改一下\n1vi pypxe/dhcp.py 2 3 def tlv_encode(self, tag, value): 4 \u0026#39;\u0026#39;\u0026#39;Encode a TLV option.\u0026#39;\u0026#39;\u0026#39; 5 6 # 注释掉下面的两行，我们不需要打印出我们一定能看懂的字符，都按bytes处理即可 7 #if type(value) is str: 8 # value = value.encode(\u0026#39;ascii\u0026#39;) 9 value = bytes(value) 10return struct.pack(\u0026#39;BB\u0026#39;, tag, len(value)) + value 然后我们需要修改第二个地方，理由是这个 PyPXE 会判断 Client 发过来的 dhcp 请求，它只实现了针对PXE-Client的 Vendor-class：\n所以我们也要屏蔽一下，否则按照正常过程\n客户端dhcp \u0026ndash;\u0026gt; PyPXE 后，PyPXE 送回客户 ipxe 脚本，然后客户安装，当加载了vmlinuz和initrd之后会进入anaconda-linux进行系统安装，过程中会再次向DHCP服务器申请IP地址， 这个时候他向DHCP Server发出的discover申请是得不到回复的，因此安装过程将被打断。\n1vi pypxe/dhcp.py 2 3 def validate_req(self, client_mac): 4 # client request is valid only if contains Vendor-Class = PXEClient 5 \u0026#39;\u0026#39;\u0026#39;代码整个注释掉，直接返回 True 6 if self.whitelist and self.get_mac(client_mac) not in self.get_namespaced_static(\u0026#39;dhcp.binding\u0026#39;): 7 self.logger.info(\u0026#39;Non-whitelisted client request received from {0}\u0026#39;.format(self.get_mac(client_mac))) 8 return False 9 if 60 in self.options[client_mac] and \u0026#39;PXEClient\u0026#39;.encode() in self.options[client_mac][60][0]: 10 self.logger.info(\u0026#39;PXE client request received from {0}\u0026#39;.format(self.get_mac(client_mac))) 11 return True 12 self.logger.info(\u0026#39;Non-PXE client request received from {0}\u0026#39;.format(self.get_mac(client_mac))) 13 return False 14 \u0026#39;\u0026#39;\u0026#39; 15 return True 这样修改后，就可以正常安装了。\n服务器启动：\n客户端启动pxe开始安装，看下面，系统的ipxe dhcp一次，然后chainload.kpxe 又一次，anaconda 又一次，最少会发三次或更多的dhcp请求。\n用 VNC 连进去可以看到安装画面，如果是 kickstart 就是全自动安装了。\n","date":"2021-10-22","img":"","permalink":"/posts/20211022-pypxe/","series":null,"tags":null,"title":"PyPXE-一个牛逼的一站式PXE安装包"},{"categories":null,"content":"上一篇文章我们介绍了 ETCD 的容器化，搞这件事情的主要目的其实是要动态更新 Nginx 的配置\n这一章我们就来配置 confd 和 Nginx，来达到动态更新 Nginx 配置的目的\n一、安装配置confd 下载并安装：\n1wget https://github.com/kelseyhightower/confd/releases/download/v0.16.0/confd-0.16.0-linux-amd64 2mv confd-0.16.0-linux-amd64 /usr/sbin/confd 3chmod +x /usr/sbin/confd 生成配置文件：\n我们在 etcd 中存放的格式如下\n1etcdctl set /nginx/app01/subdomain app1 2etcdctl set /nginx/app01/upstream/app01_1 \u0026#34;192.168.0.1:5601\u0026#34; 3 4/nginx/app01/subdomain \u0026#34;app01\u0026#34; 5/nginx/app01/upstream/app01_1 \u0026#34;192.168.0.1:5601\u0026#34; 6/nginx/app01/upstream/app01_2 \u0026#34;192.168.0.2:5601\u0026#34; 那么，我们先生成 confd 的配置文件：\n1mkdir -p /etc/confd/{conf.d,templates} 2 3cat \u0026lt;\u0026lt;EOF\u0026gt;\u0026gt;/etc/confd/conf.d/nginx.toml 4[template] 5src = \u0026#34;nginx.conf.tmpl\u0026#34; 6dest = \u0026#34;/etc/nginx/conf.d/nginx-auto.conf\u0026#34; 7keys = [ 8\u0026#34;/nginx/app01/subdomain\u0026#34;, 9\u0026#34;/nginx/app01/upstream\u0026#34;, 10] 11check_cmd = \u0026#34;/usr/sbin/nginx -t\u0026#34; 12reload_cmd = \u0026#34;/usr/sbin/nginx -s reload\u0026#34; 13EOF 14 15cat \u0026lt;\u0026lt;EOT\u0026gt;\u0026gt;/etc/confd/templates/nginx.conf.tmpl 16upstream {{getv \u0026#34;/nginx/app01/subdomain\u0026#34;}} { 17{{range getvs \u0026#34;/nginx/app01/upstream/*\u0026#34;}} 18server {{.}}; 19{{end}} 20} 2122server { 23server_name {{getv \u0026#34;/nginx/app01/subdomain\u0026#34;}}.example.com; 24location / { 25proxy_pass http://{{getv \u0026#34;/nginx/app01/subdomain\u0026#34;}}; 26proxy_redirect off; 27proxy_set_header Host $host; 28proxy_set_header X-Real-IP $remote_addr; 29proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; 30} 31} 32EOT 33 confd 会根据 etcd 的值，结合 nginx.conf.tmpl ，生成 nginx-auto.conf，然后 nginx -t 验证通过后，执行 nginx -s rolad。\n注意：nginx的配置中必须有 include /etc/nginx/conf.d/*.conf;\n二、运行confd 1# 只处理一次 2confd -onetime -backend etcd -node http://etcd-svc.default:2379 3 4# 按时间轮询 5confd -interval=60 -backend etcd -node http://etcd-svc.default:2379 \u0026amp; 6 这样就可以动态更新 Nginx 了。\n","date":"2021-10-21","img":"","permalink":"/posts/20211021-etcd_confd_nginx/","series":null,"tags":null,"title":"ETCD + CONFD + NGINX的配置"},{"categories":null,"content":"由于使用到了阿里的 K8S 托管集群 ACK，于是想占便宜。想用到托管 master node 的 etcd 来保存数据。\n结果是，未遂！！无法使用。\n阿里有单独的配置管理服务，复杂化了，不想用。\n那么解决方案就是，启动只有一个节点副本的 etcd pod，然后数据持久化到 OSS 的 S3 桶中。\n一、实现etcd的单节点docker化 首先我们只想在测试环境中跑一个单节点的 etcd，还没有用到 k8s，做法如下：\n1#!/bin/bash 2 3NODE1=172.18.31.33 4REGISTRY=quay.io/coreos/etcd 5# available from v3.2.5 6#REGISTRY=gcr.io/etcd-development/etcd 7 8docker run \\ 9 -p 2379:2379 \\ 10 -p 2380:2380 \\ 11 --volume=/data/etcd:/etcd-data \\ 12 --name etcd ${REGISTRY}:latest \\ 13 /usr/local/bin/etcd \\ 14 --data-dir=/etcd-data --name node1 \\ 15 --initial-advertise-peer-urls http://${NODE1}:2380 --listen-peer-urls http://0.0.0.0:2380 \\ 16 --advertise-client-urls http://${NODE1}:2379 --listen-client-urls http://0.0.0.0:2379 \\ 17 --initial-cluster node1=http://${NODE1}:2380 18 19 如上就可以了，容器跑起来以后进入容器测试一下：\n1docker exec -it 425f26903466 /bin/sh 2 3etcdctl -C http://127.0.0.1:2379 member list 4c3511611548b7c7c: name=node1 peerURLs=http://172.18.31.33:2380 clientURLs=http://172.18.31.33:2379 isLeader=true 5 6etcdctl ls --recursive / 这样一个单节点的 etcd 就弄好了，对外暴露的是 2379 和 2380 端口\n二、实现 etcd 的单节点 k8s 化 首先编写一个deployment文件etcd-deploy.yaml：\n下载：etcd-deploy.yaml 1apiVersion: apps/v1  2kind: Deployment  3metadata:  4 name: etcd-deploy 5 labels:  6 app: etcd 7spec:  8 replicas: 1  9 selector:  10 matchLabels:  11 app: etcd 12 template:  13 metadata:  14 labels:  15 app: etcd 16 spec:  17 containers:  18 - name: etcd 19 image: quay.io/coreos/etcd:latest 20 ports: 21 - containerPort: 2379 22 name: client 23 protocol: TCP 24 - containerPort: 2380 25 name: server 26 protocol: TCP 27 command: 28 - /usr/local/bin/etcd 29 - --name 30 - etcd 31 - --initial-advertise-peer-urls 32 - http://etcd:2380 33 - --listen-peer-urls 34 - http://0.0.0.0:2380 35 - --listen-client-urls 36 - http://0.0.0.0:2379 37 - --advertise-client-urls 38 - http://etcd:2379 39 - --initial-cluster 40 - etcd=http://etcd:2380 41- --data-dir 42- /etcd-data 43volumeMounts: 44- mountPath: /etcd-data 45name: etcd-data 46lifecycle: 47postStart: 48exec: 49command: 50- \u0026#34;sh\u0026#34; 51- \u0026#34;-c\u0026#34; 52- \u0026gt; 53echo \u0026#34;127.0.0.1 etcd\u0026#34; \u0026gt;\u0026gt; /etc/hosts; 54volumes: 55- name: etcd-data 56persistentVolumeClaim: 57claimName: k8s-etcd-20g 58restartPolicy: Always 注意上面，我们使用了一个 pvc 卷 k8s-etcd-20g，这个卷挂在 /etcd-data，是由 OSS 建立的，用于持久话数据，省得重启 etcd 的 pod，数据消失不见了。\n然后，我们需要把这个 deployment 作为 svc 服务暴露在集群中，再编写一个etcd-svc.yaml\n下载：etcd-svc.yaml 1apiVersion: v1 2kind: Service 3metadata: 4 name: etcd-svc 5spec: 6 ports: 7 - port: 2379 8 name: tcp2379 9 protocol: TCP 10 targetPort: 2379 11 - port: 2380 12 name: tcp2380 13 protocol: TCP 14 targetPort: 2380 15 selector: 16 app: etcd 17 type: ClusterIP kubectl apply 部署到 k8s 中，这样就可以了。\nk8s测试方法，随便启动一个 busybox pod，进去测试一下：\n1kubectl run curl --image=radial/busyboxplus:curl -i --tty --rm 2 3curl http://etcd-svc:2379/version 4 5curl http://etcd-svc.default:2379/version 6 7curl http://etcd-svc.default:2379/v2/keys 8 9curl http://etcd-svc.default:2379/v2/keys/?recursive=true 10 11curl http://etcd-svc.default:2379/v2/keys/service/nginx 12 13curl http://etcd-svc.default:2379/v2/keys/service/nginx/127.0.0.1 14 15curl --location --request PUT \u0026#39;http://etcd-svc:2379/v2/keys/service/nginx/10.240.0.41\u0026#39; --header \u0026#39;Content-Type: application/x-www-form-urlencoded\u0026#39; --data-urlencode \u0026#39;value=10.240.0.41:9000\u0026#39; 16 17curl http://etcd-svc.default:2379/v2/keys/service/nginx/ ","date":"2021-10-21","img":"","permalink":"/posts/20211021-etcd_docker/","series":null,"tags":null,"title":"Etcd单节点应用"},{"categories":null,"content":"在生产环境中我们大量使用了 kvm 的虚拟技术，虚拟机的镜像系统使用的是 Cloud-init 的技术\n不可避免的，虚机会遭到各种损坏，维护的手段就十分必要了\n假设我们有一个虚机文件 vis-16-41-18.qcow2 坏了\n一、安装支持包 1yum install libguestfs libguestfs-tools 二、查看日志 1virt-log -a vis-16-41-18.qcow2 没有什么特殊的报错信息\n三、分析文件系统组成 virt-filesystems和virt-df都可以，用virt-df看的更多一些\n1virt-filesystems -l -a vis-16-41-18.qcow2 2Name Type VFS Label Size Parent 3/dev/sda1 filesystem ext4 - 209715200 - 4/dev/sda2 filesystem ext4 - 214536355840 - 5 6virt-df -a vis-16-41-18.qcow2 7Filesystem 1K-blocks Used Available Use% 8vis-16-41-18.qcow2:/dev/sda1 194241 31706 152295 17% 9vis-16-41-18.qcow2:/dev/sda2 206088704 5639856 189973444 3% 10 四、挂载文件系统开始修复（方法1） 从上面可以看到 vis-16-41-18.qcow2 里面有两个分区，/dev/sda1 和/dev/sda2\n第一个应该是/boot，第二个是/\n把 / mount 出来\n1mkdir 18 2guestmount -a vis-16-41-18.qcow2 -m /dev/sda2 --rw ./18 或者全自动mount\n1guestmount -a vis-16-41-18.qcow2 -i --rw ./18 这样就可以直接进18目录进行修复操作了\n1cd 18/lib64 2ls libc*.* 发现同事胡乱升级glibc，把libc的基础库弄坏了，少libc.so.6的软链接，建立一个修复即可\n1ln -s libc-2.15.so libc.so.6 五、挂载文件系统开始修复（方法2） 我们可以用 guestmount，也可以直接用 guestfish 。\nguestfish 是个命令行工具。它使用 libguestfs 的所有功能。\n1guestfish 2 3Welcome to guestfish, the libguestfs filesystem interactive shell for 4editing virtual machine filesystems. 5 6Type: \u0026#39;help\u0026#39; for help on commands 7 \u0026#39;man\u0026#39; to read the manual 8 \u0026#39;quit\u0026#39; to quit the shell 9 10\u0026gt;\u0026lt;fs\u0026gt; add vis-16-41-18.qcow2 11\u0026gt;\u0026lt;fs\u0026gt; run 12\u0026gt;\u0026lt;fs\u0026gt; list-filesystems 13/dev/sda1: ext4 14/dev/sda2: ext4 15\u0026gt;\u0026lt;fs\u0026gt; mount /dev/sda2 / 16\u0026gt;\u0026lt;fs\u0026gt; cat /etc/fstab 17 18# 19# /etc/fstab 20# Created by anaconda on Mon Dec 29 15:24:53 2014 21# 22# Accessible filesystems, by reference, are maintained under \u0026#39;/dev/disk\u0026#39; 23# See man pages fstab(5), findfs(8), mount(8) and/or blkid(8) for more info 24# 25UUID=9fdc111c-3042-4527-b3f8-a2961e55077e / ext4 defaults 1 1 26UUID=1855d5e1-18f8-48ea-8c3b-c52cdd512a5e /boot ext4 defaults 1 2 27tmpfs /dev/shm tmpfs defaults 0 0 28devpts /dev/pts devpts gid=5,mode=620 0 0 29sysfs /sys sysfs defaults 0 0 30proc /proc proc defaults 0 0 31 32\u0026gt;\u0026lt;fs\u0026gt; guestfish的常用命令：\n1add vis-16-41-18.qcow2 2run 3list-filesystems 4 5ll / 6ls / 7cat /etc/fstab 8write-append /etc/rc.d/rc.local \u0026#34;service sshd start\u0026#34; 9edit /etc/fstab. 10less /var/log/messages 11mkdir /tmp/a 12touch /tmp/a/b.txt 13write /tmp/a/b.txt 14rm /tmp/a/b.txt 15 16upload Upload a local file to the disk. ###注意：是上载本地文件到镜像文件去！！！ 17 六、virt对应guestfish的一些命令 1virt-cat vis-16-41-18.qcow2 /home/supdev/.bash_history 2 3virt-copy-in Copy files and directories into a guest. 4virt-copy-out Copy files and directories out of a guest. 5 6virt-edit Edit a file in a guest. 7virt-ls List files and directories in a guest 七、virt-rescue救援模式 如果虚机系统起不来，可以先尝试进入 rescue 救援模式\nvirt-rescue 类似于救援 CD，但用于虚拟机，且无需提供 CD。\nvirt-rescue 为用户提供救援外壳和一些简单的恢复工具，可用于检查和更正虚拟机或磁盘映像中的问题。\n1virt-rescue -a vis-16-41-18.qcow2 2Welcome to virt-rescue, the libguestfs rescue shell. 3 4Note: The contents of / are the rescue appliance. 5You need to mount the guest\u0026#39;s partitions under /sysroot 6before you can examine them. A helper script for that exists: 7mount-rootfs-and-do-chroot.sh /dev/sda2 8 9\u0026gt;\u0026lt;rescue\u0026gt; 10[ 67.194384] EXT4-fs (sda1): mounting ext3 file system 11using the ext4 subsystem 12[ 67.199292] EXT4-fs (sda1): mounted filesystem with ordered data 13mode. Opts: (null) 14mount: /dev/sda1 mounted on /sysroot. 15mount: /dev bound on /sysroot/dev. 16mount: /dev/pts bound on /sysroot/dev/pts. 17mount: /proc bound on /sysroot/proc. 18mount: /sys bound on /sysroot/sys. 19Directory: /root 20Thu Jun 5 13:20:51 UTC 2014 21(none):~ # ","date":"2021-10-21","img":"","permalink":"/posts/20211021-libguestfs/","series":null,"tags":null,"title":"Libguestfs的救援手段"},{"categories":null,"content":"这是个娱乐话题，Dogecoin 狗币在马斯克的吹捧鼓动下，冲上云霄\n其实真的用CPU挖币，应该是挖 xmb 门罗币才是对的选择，挖狗币只是娱乐一下\n废话不多说，直接放上教程，我的机器是 CentoOS\n首先需要有个狗币钱包地址，这个我就不教大家了\n一、下载xmrig挖矿软件 下载地址：https://github.com/xmrig/xmrig/releases\n我们选择最近的下载就好\n二、做好加密通道 我们需要做好一条加密tcp通道\n用 ghostunnel, localhost:9999 \u0026mdash;\u0026gt; vps:9999 \u0026mdash;\u0026gt; rx.unmineable.com:3333\n三、用screen后台开挖 1screen 2 3#./xmrig -o localhost:9999 -a rx -k -u DOGE:狗币地址.矿工名#heyt-3711 4./xmrig -o localhost:9999 -a rx -k -u DOGE:DLR3DZGucJiSdahARW1vV5B1h3WYiw454a.work01 5 6ctrl+a+d 四、查看挖了多少 查看地址：https://unmineable.com/coins/DOGE/address/\n","date":"2021-10-21","img":"","permalink":"/posts/20211021-dogecoin/","series":null,"tags":null,"title":"如何用CPU挖狗币Dogecoin"},{"categories":null,"content":"chrony 已经成了事实标准，替代了ntp。\n但是，有几个细节，需要非常注意。\n给出我们的配置，/etc/chrony.conf\n1 2# Use public servers from the pool.ntp.org project. 3# Please consider joining the pool (http://www.pool.ntp.org/join.html). 4server 172.10.1.1 iburst prefer minpoll 6 maxpoll 10 5server 172.10.1.2 iburst 6 7# Record the rate at which the system clock gains/losses time. 8driftfile /var/lib/chrony/drift 9 10# Allow the system clock to be stepped in the first three updates 11# if its offset is larger than 1 second. 12makestep 1.0 3 13 14# Enable kernel synchronization of the real-time clock (RTC). 15rtcsync 16 17# Enable hardware timestamping on all interfaces that support it. 18#hwtimestamp * 19 20# Increase the minimum number of selectable sources required to adjust 21# the system clock. 22#minsources 2 23 24# Allow NTP client access from local network. 25#allow 192.168.0.0/16 26 27# Serve time even if not synchronized to a time source. 28#local stratum 10 29 30# Specify file containing keys for NTP authentication. 31#keyfile /etc/chrony.keys 32 33# Specify directory for log files. 34logdir /var/log/chrony 35logchange 0.5 36# Select which information is logged. 37#log measurements statistics tracking rtc 里面有好几个细节，下面逐一解释一下：\n一、server 这里可以添加很多时间服务器，172.10.1.1 和 172.10.1.2 是两台自建的时间服务器。\nibust 会在 chrony 启动的2秒内，去快速poll服务器4次来快速矫正当前系统时间\nprefer 优先使用指定的服务器\nminpoll 6，缺省是6，意思是2的6次方，也就是64秒，最小轮询时间服务器的时间间隔是64秒\nmaxpoll 10，缺省是10，同上，2的10次方，也就是1024秒，最大轮询时间间隔是1024秒\n通常情况下一过minpoll的时间周期，就会触发一次时间同步询问。\n二、makestep 正常情况下如果系统时钟跟时间服务器不一致，chrony调整的方式是慢慢增加，或慢慢减少，不会一步到位，直接去跟时间服务器对齐。\nmakestep 1.0 3，意思就是如果时间服务器跟系统时间相差1秒，那么就在下3个时钟更新中追上时间服务器。\n这样就会立刻快速追平了，这样会带来时间跳跃。\n三、rtcsync 这个是把系统时钟同步到主板的硬件时钟去。\n缺省情况下是11分钟同步一次\n四、logchange logchange 0.5，意思是如果chrony调整的系统时间，超过了0.5秒的时长，就会发一条消息到syslog，这样我们就能在/var/log/messages里看到这条消息了。\n五、验证调试 开发人员会问，什么时候同步的服务器啊，多长时间同步一次，时间到底准不准啊，有没有发生跳跃啊\n我们就用chronyc sources来验证，配置中\n1server 172.10.1.1 iburst prefer minpoll 4 maxpoll 5 2server 172.10.1.2 iburst 解释一下，minpoll 4 maxpoll 5 ，那么最小轮询时间16秒，最大32秒。\n我们可以看到上图 LastRx 就是上次询问时间服务器的间隔时间，14秒、15秒、16秒，然后就变1了，最小间隔16秒后，立即就询问时间服务器，同步时间。\n同样可以看到第二台时间服务器就不受这个限制，缺省minpoll 6，就是64秒。所以上图第二台，63秒、64秒、65秒，变0，才去询问时间服务器。\n总结一下，chrony 调整时间偏差是匀速的，缓慢的。它询问时间服务器的间隔由minpoll来控制。\n我们用logchange来记录大的时间调整，以备追溯和查询。\n","date":"2021-10-20","img":"","permalink":"/posts/20211020-chrony/","series":null,"tags":null,"title":"Chrony的几个详细配置细节"},{"categories":null,"content":"很现实的问题，局域网内有态势感知和网络流量分析，这很讨厌！\n那么，如何把某段流量隐藏起来，让态势感知无法分析呢？\n前提条件，你需要有国外的一台 VPS 作为外援，把 TCP 流量通过 TLS 加密送到国外的服务器，然后再转发到正确的目标服务器上，这样就不会被人追踪了。\n这里推荐 Ghostunnel ，这是个 Go 的程序，只有一个执行文件。配合 certik 证书生成，就完美了。\n项目地址：https://github.com/ghostunnel/ghostunnel\n首先我们使用 certik 生成三个证书，ca.pem server.pem 和 client1.pem\n然后ghostunnel以及三个证书文件都放在/usr/local/bin下\n一、在VPS上运行ghostunnel模式server 1/usr/local/bin/ghostunnel server --listen 0.0.0.0:9999 --target tr.dero.herominers.com:1117 --keystore server.pem --cacert ca.pem --allow-cn client1 --unsafe-target 上面监听了端口0.0.0.0:9999（监听0.0.0.0必须加参数\u0026ndash;unsafe-target），远程转发到dero的矿池端口1117，只允许验证过的cn client1连接。\n二、在本地机器上运行ghostunnel模式client 1/usr/local/bin/ghostunnel client --listen localhost:9999 --target 193.42.114.129:9999 --keystore client.pem --cacert ca.pem 本地监听localhost:9999，所以不用加\u0026ndash;unsafe-target参数，然后连接到远程 vps 服务器，ip地址是193.42.114.129，端口是9999\n这样就完成了。可以放心的启动程序，连接到本地端口localhost:9999，TCP流量就会被隐藏起来，不会被分析到.\n","date":"2021-10-19","img":"","permalink":"/posts/20211019-ghostunnel/","series":null,"tags":null,"title":"Ghostunnel使用TLS加密TCP流量"},{"categories":null,"content":"我们会有很多时候需要用到TLS证书，一个非常方便、小众的工具就是 certik 。\n这个软件纯由 Go 组成，就一个可执行文件，使用了 etcd 的 boltdb 格式存放所有的证书。\n软件地址：https://github.com/opencoff/certik\n使用：\n一、初始化证书库 1./certik -v tls.db init CA 二、签发一张server证书 1#./certik -v tls.db server -i IP.ADDR.ES server.domain.name 2./certik -v tls.db server -i 193.42.114.129 server 上面我们 server 的 ip 是193.42.114.129，域名简洁起见就叫 server 了 。\n三、签发一张client1证书 1./certik -v tls.db client client1 四、查看证书库 1./certik -v tls.db list 我们就可以看到三个证书了，一个 CA ，一个 server ，一个 client1\n五、导出各个证书 1#导出CA 2./certik -v tls.db export --root-ca 3 4#导出server 5./certik -v tls.db export server 6 7#导出client1 8./certik -v tls.db export client1 以上就可以得到各个东西了。\n那么我们 gen 出证书做什么用呢？当然用作 ghostunnel 的验证用\n","date":"2021-10-19","img":"","permalink":"/posts/20211019-certik/","series":null,"tags":null,"title":"Certik 证书签发软件"},{"categories":null,"content":"现在国内都禁止挖币，什么币安、火币、cnspark之类的都不允许国内 IP 访问了。\n如何实现的呢？\n首先需要得到国家IP段，下载地址：http://www.ipdeny.com/ipblocks/。这里以我们国家 cn 为例\n步骤如下：\n一、安装ipset 1#Debian/Ubuntu系统 2apt-get -y install ipset 3 4#CentOS系统 5yum -y install ipset 二、清空iptable规则 1#防止设置不生效，清空之前的防火墙规则 2iptables -P INPUT ACCEPT 3iptables -F 三、创建ipset规则集 1#创建一个名为cnip的规则 2ipset -N cnip hash:net 3 4#下载国家IP段，这里以中国为例 5wget -P . http://www.ipdeny.com/ipblocks/data/countries/cn.zone 6 7#将IP段添加到cnip规则集中 8for i in $(cat /root/cn.zone ); do ipset -A cnip $i; done 四、创建iptable黑名单 1#扔黑名单 2iptables -A INPUT -p tcp --dport 80 -m set --match-set cnip src -j DROP 3iptables -A INPUT -p tcp --dport 443 -m set --match-set cnip src -j DROP 4 5#然后放行其他的 6iptables -P INPUT ACCEPT ","date":"2021-10-18","img":"","permalink":"/posts/20211018-ipset_block_cn/","series":null,"tags":null,"title":"使用IPSET封掉某个国家整个的访问"},{"categories":null,"content":"公司安装了 openvpn ，带来方便，但是也有很多不便的地方，机房的总带宽就那么多。\n很多人共用 vpn 的时候，就会抢占带宽。\n那么，我们需要限制一下，限制 openvpn 所能使用的带宽，避免抢占 WEB 的带宽\n做法如下：\n由于我们不是要单独限制某一个 openvpn 用户，而是限制整体，所以简单用 TC 就可以了\n1#!/bin/sh 2tc qdisc del dev tun0 root 3tc qdisc add dev tun0 root handle 1: htb default 1 4tc class add dev tun0 parent 1: classid 1:1 htb rate 30Mbit ceil 30Mbit 解释一下：\n 我们 openvpn 启的是 tun0 ，所以限制的对象就是 dev tun0 首先第一行清除 tun0 的根队列 然后第二行建立 tun0 的 root 根队列为 1:0 htb ，缺省是1:1的子队列 最后一行，第三行建立 1:1 的子队列，带宽限制是 30Mbit ，注意这里是大B，就是网络术语中的带宽，换算成小b的话，需要除以8  效果很明显，直接被限制住（41兆而不是30M是因为这台机器是虚机，实体机上还有别的流量）：\n","date":"2021-10-18","img":"","permalink":"/posts/20211018-openvpn_limit_bandwidth/","series":null,"tags":null,"title":"OpenVPN 限制流量带宽"},{"categories":null,"content":"本站这个博客的由来：\n源自于 wiredcraft 的面试，这个公司让老八很是向往，可以远程工作，第一次面试是 devops ，老外见面聊了后，由代理中国人面。因为面的是K8S的东西，正好刚给画包包公司做了整体迁往阿里ACK的工程，以为没问题，实际是直接问倒了我。就好比master node上面都跑了什么进程，唉，一言难尽啊，又问到ansible的变量，回答估计也不满意，结果就挂了\n知耻而后勇，后面去恶补了一下ansible和k8s的东西，实际也是实际操作居多，然后第二次面的是 sysadmin，全程老外面。还出了几道题，应该是没问题。但是工资要高了，也被刷了。\n这个博客就是其中一道题，那既然搭建出来了，干脆就好好用吧。\n本身自己对静态的 Blog 系统也比较感兴趣，自己主站 www.rendoumi.com 的 Blog 是由 journey 搭建的，是一个精巧的 go 程序，最妙的是它兼容 Ghost 博客系统，也能使用 Ghost 的 theme ，这样就完美的把自己以前 Ghost 的博客迁移了过去，但是，里面文章有很多过时了，但也不想清理，干脆借着这个机会，从新开始。搭一个自己喜欢的系统继续写新博客\n流行的 Markdown 写作平台有Hexo和Hugo，选Hugo是因为实在是不喜欢node，弄一堆npm，迁移麻烦死。\n这个博客程序基于Hugo，托管在 github，众所周知，github 是托管静态文件的地方，Markdown的最大毛病是图片。图片放在图床也不是好办法，所以图片和静态文件要在一起，下面就说一下搭建过程，我的主机是Ubuntu：\n一、下载Hugo 下载地址： https://github.com/gohugoio/hugo/releases/download/v0.88.1/hugo_0.88.1_Linux-64bit.tar.gz 1wget https://github.com/gohugoio/hugo/releases/download/v0.88.1/hugo_0.88.1_Linux-64bit.tar.gz 2tar zxvf hugo_0.88.1_Linux-64bit.tar.gz 二、初始一个博客写作目录 1./hugo new site MyBlog ​\n三、下载theme 1cd MyBlog 2git clone https://github.com/halogenica/beautifulhugo.git themes/beautifulhugo 3echo theme = \\\u0026#34;beautifulhugo\\\u0026#34; \u0026gt;\u0026gt; config.toml ​\n四、写一篇新文章 1cd MyBlog 2../hugo new posts/my-first-post.md echo \u0026#34;#### This is another Blog\u0026#34; \u0026gt;\u0026gt; content/posts/my-first-post.md 五、运行server，build草稿 1cd MyBlog 2../hugo server --buildDrafts ​\n六、测试一下 1curl http://localhost:1313 ​\n七、推送到github 首先我们要去github开一个xxx.github.io的repo仓库，然后 git 把生成的静态内容推上去就好了\n1cd MyBlog 2 3#生成静态文件 4../hugo --buildDrafts 5 6#文件生成的目录是public 7cd public 8 9#正常git操作就可以了 10git init 11git add . 12git commit -m \u0026#34;first commit\u0026#34; 13git branch -M main 14git remote add origin git@github.com:zhangrr/zhangrr.github.io.git 15git push -u origin main 八、看下结果 打开网页 http://zhangrr.github.io 就能看到网页了\n​\n九、选择写作软件 其实现在开始才是最重要的，用什么软件来写，就用大家推荐的 Typora 来就好了\n1# or use 2# sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys BA300B7755AFCFAE 3wget -qO - https://typora.io/linux/public-key.asc | sudo apt-key add - 4 5# add Typora\u0026#39;s repository 6sudo add-apt-repository \u0026#39;deb https://typora.io/linux ./\u0026#39; 7sudo apt-get update 8 9# install typora 10sudo apt-get install typora 十、选择目录存放格式 这个才是最主要的问题，看下图，post目录是所有文章，下面按目录存放，目录名是日期和文章名，目录里面是index.md和文章附带的图片。\n我觉得这个模式才是符合我的要求的。\n1. 2├── post 3│ ├── 2018-01-11-关联了两款小程序.md 4│ └── index.md 5│ ├── 2018-02-05-一款小小的物流数据产品.md 6│ └── index.md 7│ ├── 2018-03-19-现已加入 Algolia 搜索服务.md 8│ └── index.md 9│ ├── 2018-04-13-我是如何搞砸了本站搜索服务的.md 10│ └── index.md 11│ ├── 2018-04-18-小站构建工具已成功切换到 Hugo.md 12│ └── index.md 13│ ├── 2018-04-19-开始翻译一个文档：Saleor.md 14│ └── index.md 15│ ├── 2018-04-22-Saleor 初稿已翻译完成.md 16│ └── index.md 17│ ├── 2018-04-26-今天全是干货 18│ │ ├── IMG_5991-4755089.jpg 19│ │ ├── IMG_5997-4755064.jpg 20│ │ ├── IMG_5998-4755103.jpg 21│ │ ├── IMG_5999-4755051.jpg 22│ │ ├── IMG_6001-4755073.jpg 23│ │ ├── IMG_6002-4755080.jpg 24│ │ ├── IMG_6003-4755030.jpg 25│ │ ├── IMG_6004-4755009.jpg 26│ │ └── index.md 27│ ├── 2018-05-02-从 Jekyll 到 Hugo 的一些细节.md 28│ └── index.md 29│ └── 2018-05-03-Hugo 的文件管理方案.md 30│ └── index.md 31 十一、微调 Typora   为了能显示目录结构Outline，所以所有副标题需要用 ctrl+2 的标题文本，这样就能自动生成Outline\n     为了能让剪贴板自动把ctrl+v贴上的图片放到目录里面，需要设置Image\n  这样就完美了，以后就在这里写工作博客了。\n  ","date":"2021-10-15","img":"","permalink":"/posts/20211015-hugo_blog/","series":null,"tags":null,"title":"本站博客的由来以及搭建使用教程"},{"categories":null,"content":"上一篇文章，我们实施了Ubuntu下wifi热点的搭建，那么，其实我是想抓我iphone手机的https明文流量包来着。\n怎么抓取呢？\n方法也很简单\n一、安装 mitmproxy 有wifi热点那台机器的wlan网卡地址是192.168.222.1，就在那台上安装，便于抓取\n1pip install mitmproxy 二、运行mitmproxy 1mitmproxy -p 8080 三、配置iphone安装mitm证书 打开手机，用Safari浏览器打开网址：http://mitm.it\n 找到IOS那行，仔细看一看说明\n 点击 Get mitmproxy-ca-cert.pem 安装描述文件\n安装完以后，在设置 \u0026ndash;\u0026gt; 已下载描述文件，安装描述文件\n 安装好以后会显示绿色的已验证\n然后在手机上用 safari 访问网址：https://ip138.com\n回到 Ubuntu 的命令行窗口，上下选中抓到的包，然后按回车查看，左右光标键移动，可以看到response是明文的，q键是返回上一级\n四、包的拦截修改 上面演示的是常规的查看操作，下面介绍一下 mitmproxy 的另一强大功能，拦截修改 request 和 response。\n输入 i，然后输入 ~s 再按回车键，这时候就进入了 response 拦截模式。如果输入  ~q 则进入 request 的拦截模式，更多的命令可以输入 ？ 查看。拦截模式下的页面显示如下图所示：\n其中红色的表示请求正被拦截，这时 Enter 进入后 再按 e 就可以修改 request 或者 response。修改时是用 vim 进行编辑的，修改完成后按 a 将请求放行，如果要放行所有请求输入 A 即可。\n","date":"2021-10-14","img":"","permalink":"/posts/20211014-iphone_hijack/","series":null,"tags":null,"title":"Iphone手机的https抓包"},{"categories":null,"content":"公司的wifi信号很弱，也不保险。省事起见，还是自己建立一个好\nusb无线网卡设备必须是一个 nl80211 兼容的无线设备，所以驱动就是这个：nl80211\n我的操作系统是Ubuntu，如果是CentOS命令基本一样\n插上wifi usb卡后 ip a 看一下网卡的名称，我这里是：wlx00a1b0817651，够长\n一、安装hostapd软件 1sudo apt install -y hostapd 二、建立hostapd.conf文件 1vi /etc/hostapd/hostapd.conf 2driver=nl80211 3ssid=Fast_8188  4channel=10 5interface=wlx00a1b0817651 6wpa=2 7wpa_passphrase=GreatWall2021! 8wpa_key_mgmt=WPA-PSK 9wpa_pairwise=TKIP 三、建立启动脚本/usr/local/bin/initAP.sh 1cat /usr/local/bin/initAP.sh 2 3#!/bin/bash 4 5start() {  6rfkill unblock all 7ifconfig wlx00a1b0817651 up 192.168.222.1 netmask 255.255.255.0 8sleep 2 9 10dnsmasq -i wlx00a1b0817651 --dhcp-range=192.168.222.10,192.168.222.20,2h 11 12#Enable NAT 13sysctl -w net.ipv4.ip_forward=1 14iptables -F  15iptables -X  16iptables -t nat -A POSTROUTING -s 192.168.222.0/24 -j SNAT --to 192.168.41.15 17 18hostapd -B /etc/hostapd/hostapd.conf  19} 20 21stop() { 22iptables -P INPUT ACCEPT 23iptables -P FORWARD ACCEPT 24iptables -P OUTPUT ACCEPT 25iptables -F 26iptables -X 27iptables -t nat -F 28iptables -t nat -X 29iptables -t mangle -F 30iptables -t mangle -X 31systemctl stop dnsmasq 32pkill hostapd 33/sbin/ip link set down dev wlx00a1b0817651 34} 35 36case $1 in  37 start) 38 start 39 ;; 40 stop) 41 stop 42 ;; 43 *) 44 echo \u0026#34;Usage: $0 {start|stop}\u0026#34; 45 exit 2 46esac 四、用root身份执行即可 1sudo chmod 755 /usr/local/bin/initAP.sh 2sudo /usr/local/bin/initAP.sh start 这样就可以用自己的手机连上这个wifi热点，尽情冲浪啦。\n ","date":"2021-10-14","img":"","permalink":"/posts/20211014-linux_wifi/","series":null,"tags":null,"title":"Ubuntu下自建一个wifi热点供手机使用"},{"categories":null,"content":"某些场合，很有可能需要启动ISO或者USB盘，自带Linux系统，然后拯救当前损坏的系统\n或者直接启动一个LIVE CentOS系统，去做某些事，比如用MegaRaid划分Raid、测试系统等等\n这时候就需要制作出来一个LIVE CD的系统了\n制作步骤如下：\n一、安装live-tools 1yum -y install livecd-tools 二、准备Kickstart文件centos7-live-docker.ks 下载地址：centos7-live-docker.ks 1lang en_GB.UTF-8 2keyboard us 3timezone Asia/Shanghai --isUtc 4 5#selinux --enforcing 6selinux --disabled 7 8#firewall --enabled --service=cockpit 9firewall --disabled 10 11#xconfig --startxonboot 12part / --size 8192 --fstype ext4 13services --enabled=NetworkManager,sshd --disabled=network 14 15 16# Root password 17auth --useshadow --enablemd5 18rootpw --plaintext Kalaisadog2021 19 20repo --name=base --baseurl=http://mirror.centos.org/centos/7/os/x86_64/ 21repo --name=updates --baseurl=http://mirror.centos.org/centos/7/updates/x86_64/ 22repo --name=extras --baseurl=http://mirror.centos.org/centos/7/extras/x86_64/ 23repo --name=epel --baseurl=http://dl.fedoraproject.org/pub/epel/7/x86_64/ 24 25%packages  26@core 27kernel 28dracut 29bash 30firewalld 31NetworkManager 32e2fsprogs 33rootfiles 34docker 35openssh-server 36 37#By zhang ranrui 38unzip 39net-tools 40binutils 41wget 42bash-completion 43bc 44dmidecode 45dmraid 46dmraid-events 47lvm2 48lvm2-libs 49kpartx 50mdadm 51parted 52xfsdump 53xfsprogs 54gdisk 55bzip2 56extundelete 57libHX 58libHX-devel 59autoconf 60gcc 61gcc-c++ 62make 63screen 64telnet 65 66%end 67 68%post 69 70systemctl enable docker 71 72# By Zhang Ranrui, Add your custom script 73#wget http://www.rendoumi.com/soft/other/xfs_irecover -O /usr/local/bin/xfs_irecover 74#chmod 755 /usr/local/bin/xfs_irecover 75 76echo \u0026#34;Banner /etc/issue\u0026#34; \u0026gt;\u0026gt; /etc/ssh/sshd_config 77 78sed -i \u0026#34;s/After=network\\.target/After=network-online\\.target\\nWants=network-online\\.target/g\u0026#34; /usr/lib/systemd/system/rc-local.service 79 80chmod 755 /etc/systemd/system/rc.local.service.d 81chmod 644 /etc/systemd/system/rc.local.service.d/local.conf 82 83chmod 755 /etc/rc.d/rc.local 84systemctl enable rc-local 85systemctl start rc-local 86 87# FIXME: it\u0026#39;d be better to get this installed from a package 88cat \u0026gt; /etc/rc.d/init.d/livesys \u0026lt;\u0026lt; EOF 89#!/bin/bash 90# 91# live: Init script for live image 92# 93# chkconfig: 345 00 99 94# description: Init script for live image. 95### BEGIN INIT INFO 96# X-Start-Before: display-manager 97### END INIT INFO 98 99. /etc/init.d/functions 100 101if ! strstr \u0026#34;\\`cat /proc/cmdline\\`\u0026#34; rd.live.image || [ \u0026#34;\\$1\u0026#34; != \u0026#34;start\u0026#34; ]; then 102exit 0 103fi 104 105if [ -e /.liveimg-configured ] ; then 106 configdone=1 107fi 108 109exists() { 110 which \\$1 \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 || return 111 \\$* 112} 113 114# Make sure we don\u0026#39;t mangle the hardware clock on shutdown 115ln -sf /dev/null /etc/systemd/system/hwclock-save.service 116 117livedir=\u0026#34;LiveOS\u0026#34; 118for arg in \\`cat /proc/cmdline\\` ; do 119 if [ \u0026#34;\\${arg##rd.live.dir=}\u0026#34; != \u0026#34;\\${arg}\u0026#34; ]; then 120livedir=\\${arg##rd.live.dir=} 121return 122fi 123if [ \u0026#34;\\${arg##live_dir=}\u0026#34; != \u0026#34;\\${arg}\u0026#34; ]; then 124livedir=\\${arg##live_dir=} 125return 126fi 127done 128 129# enable swaps unless requested otherwise 130swaps=\\`blkid -t TYPE=swap -o device\\` 131if ! strstr \u0026#34;\\`cat /proc/cmdline\\`\u0026#34; noswap \u0026amp;\u0026amp; [ -n \u0026#34;\\$swaps\u0026#34; ] ; then 132 for s in \\$swaps ; do 133 action \u0026#34;Enabling swap partition \\$s\u0026#34; swapon \\$s 134 done 135fi 136if ! strstr \u0026#34;\\`cat /proc/cmdline\\`\u0026#34; noswap \u0026amp;\u0026amp; [ -f /run/initramfs/live/\\${livedir}/swap.img ] ; then 137 action \u0026#34;Enabling swap file\u0026#34; swapon /run/initramfs/live/\\${livedir}/swap.img 138fi 139 140mountDockerDisk() { 141 # support label/uuid 142 if [ \u0026#34;\\${dockerdev##LABEL=}\u0026#34; != \u0026#34;\\${dockerdev}\u0026#34; -o \u0026#34;\\${dockerdev##UUID=}\u0026#34; != \u0026#34;\\${dockerdev}\u0026#34; ]; then 143dockerdev=\\`/sbin/blkid -o device -t \u0026#34;\\$dockerdev\u0026#34;\\` 144fi 145 146 # if we\u0026#39;re given a file rather than a blockdev, loopback it 147 if [ \u0026#34;\\${dockerdev##mtd}\u0026#34; != \u0026#34;\\${dockerdev}\u0026#34; ]; then 148# mtd devs don\u0026#39;t have a block device but get magic-mounted with -t jffs2 149mountopts=\u0026#34;-t jffs2\u0026#34; 150elif [ ! -b \u0026#34;\\$dockerdev\u0026#34; ]; then 151loopdev=\\`losetup -f\\` 152if [ \u0026#34;\\${dockerdev##/run/initramfs/live}\u0026#34; != \u0026#34;\\${dockerdev}\u0026#34; ]; then 153action \u0026#34;Remounting live store r/w\u0026#34; mount -o remount,rw /run/initramfs/live 154fi 155losetup \\$loopdev \\$dockerdev 156dockerdev=\\$loopdev 157fi 158 159 # if it\u0026#39;s encrypted, we need to unlock it 160 if [ \u0026#34;\\$(/sbin/blkid -s TYPE -o value \\$dockerdev 2\u0026gt;/dev/null)\u0026#34; = \u0026#34;crypto_LUKS\u0026#34; ]; then 161echo 162echo \u0026#34;Setting up encrypted Docker device\u0026#34; 163plymouth ask-for-password --command=\u0026#34;cryptsetup luksOpen \\$dockerdev EncDocker\u0026#34; 164dockerdev=/dev/mapper/EncDocker 165fi 166 167 # and finally do the mount 168 mount \\$mountopts \\$dockerdev /var/lib/docker 169 # if we have /home under what\u0026#39;s passed for persistent home, then 170 # we should make that the real /home. useful for mtd device on olpc 171 if [ -d /var/lib/docker/docker ]; then mount --bind /var/lib/docker/docker /var/lib/docker ; fi 172 [ -x /sbin/restorecon ] \u0026amp;\u0026amp; /sbin/restorecon /var/lib/docker 173} 174 175findDockerDisk() { 176 for arg in \\`cat /proc/cmdline\\` ; do 177 if [ \u0026#34;\\${arg##dockerdisk=}\u0026#34; != \u0026#34;\\${arg}\u0026#34; ]; then 178dockerdev=\\${arg##dockerdisk=} 179return 180fi 181done 182} 183 184if strstr \u0026#34;\\`cat /proc/cmdline\\`\u0026#34; dockerdisk= ; then 185findDockerDisk 186elif [ -e /run/initramfs/live/\\${livedir}/docker.img ]; then 187 dockerdev=/run/initramfs/live/\\${livedir}/docker.img 188fi 189 190# if we have a persistent /home, then we want to go ahead and mount it 191if ! strstr \u0026#34;\\`cat /proc/cmdline\\`\u0026#34; nodockerdisk \u0026amp;\u0026amp; [ -n \u0026#34;\\$dockerdev\u0026#34; ] ; then 192 action \u0026#34;Mounting persistent /var/lib/docker\u0026#34; mountDockerDisk 193fi 194 195# make it so that we don\u0026#39;t do writing to the overlay for things which 196# are just tmpdirs/caches 197mount -t tmpfs -o mode=0755 varcacheyum /var/cache/yum 198mount -t tmpfs vartmp /var/tmp 199[ -x /sbin/restorecon ] \u0026amp;\u0026amp; /sbin/restorecon /var/cache/yum /var/tmp \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 200 201if [ -n \u0026#34;\\$configdone\u0026#34; ]; then 202 exit 0 203fi 204 205# add fedora user with no passwd 206action \u0026#34;Adding live user\u0026#34; useradd \\$USERADDARGS -c \u0026#34;Live System User\u0026#34; liveuser 207passwd -d liveuser \u0026gt; /dev/null 208usermod -aG wheel,docker liveuser \u0026gt; /dev/null 209 210# Remove root password lock 211passwd -d root \u0026gt; /dev/null 212(echo Kalaisadog2021; echo Kalaisadog2021)|passwd root --stdin 213 214# turn off firstboot for livecd boots 215systemctl --no-reload disable firstboot-text.service 2\u0026gt; /dev/null || : 216systemctl --no-reload disable firstboot-graphical.service 2\u0026gt; /dev/null || : 217systemctl stop firstboot-text.service 2\u0026gt; /dev/null || : 218systemctl stop firstboot-graphical.service 2\u0026gt; /dev/null || : 219 220# don\u0026#39;t use prelink on a running live image 221sed -i \u0026#39;s/PRELINKING=yes/PRELINKING=no/\u0026#39; /etc/sysconfig/prelink \u0026amp;\u0026gt;/dev/null || : 222 223# turn off mdmonitor by default 224systemctl --no-reload disable mdmonitor.service 2\u0026gt; /dev/null || : 225systemctl --no-reload disable mdmonitor-takeover.service 2\u0026gt; /dev/null || : 226systemctl stop mdmonitor.service 2\u0026gt; /dev/null || : 227systemctl stop mdmonitor-takeover.service 2\u0026gt; /dev/null || : 228 229# don\u0026#39;t enable the gnome-settings-daemon packagekit plugin 230gsettings set org.gnome.settings-daemon.plugins.updates active \u0026#39;false\u0026#39; || : 231 232# don\u0026#39;t start cron/at as they tend to spawn things which are 233# disk intensive that are painful on a live image 234systemctl --no-reload disable crond.service 2\u0026gt; /dev/null || : 235systemctl --no-reload disable atd.service 2\u0026gt; /dev/null || : 236systemctl stop crond.service 2\u0026gt; /dev/null || : 237systemctl stop atd.service 2\u0026gt; /dev/null || : 238 239# Mark things as configured 240touch /.liveimg-configured 241 242# add static hostname to work around xauth bug 243# https://bugzilla.redhat.com/show_bug.cgi?id=679486 244echo \u0026#34;localhost\u0026#34; \u0026gt; /etc/hostname 245 246# Fixing the lang install issue when other lang than English is selected . See http://bugs.centos.org/view.php?id=7217 247/usr/bin/cp /usr/lib/python2.7/site-packages/blivet/size.py /usr/lib/python2.7/site-packages/blivet/size.py.orig 248/usr/bin/sed -i \u0026#34;s#return self.humanReadable()#return self.humanReadable().encode(\u0026#39;utf-8\u0026#39;)#g\u0026#34; /usr/lib/python2.7/site-packages/blivet/size.py 249 250EOF 251 252# bah, hal starts way too late 253cat \u0026gt; /etc/rc.d/init.d/livesys-late \u0026lt;\u0026lt; EOF 254#!/bin/bash 255# 256# live: Late init script for live image 257# 258# chkconfig: 345 99 01 259# description: Late init script for live image. 260 261. /etc/init.d/functions 262 263if ! strstr \u0026#34;\\`cat /proc/cmdline\\`\u0026#34; rd.live.image || [ \u0026#34;\\$1\u0026#34; != \u0026#34;start\u0026#34; ] || [ -e /.liveimg-late-configured ] ; then 264exit 0 265fi 266 267exists() { 268 which \\$1 \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 || return 269 \\$* 270} 271 272touch /.liveimg-late-configured 273 274# read some variables out of /proc/cmdline 275for o in \\`cat /proc/cmdline\\` ; do 276 case \\$o in 277 ks=*) 278ks=\u0026#34;--kickstart=\\${o#ks=}\u0026#34; 279;; 280xdriver=*) 281xdriver=\u0026#34;\\${o#xdriver=}\u0026#34; 282;; 283esac 284done 285 286# if liveinst or textinst is given, start anaconda 287if strstr \u0026#34;\\`cat /proc/cmdline\\`\u0026#34; liveinst ; then 288 plymouth --quit 289 /usr/sbin/liveinst \\$ks 290fi 291if strstr \u0026#34;\\`cat /proc/cmdline\\`\u0026#34; textinst ; then 292 plymouth --quit 293 /usr/sbin/liveinst --text \\$ks 294fi 295 296# configure X, allowing user to override xdriver 297if [ -n \u0026#34;\\$xdriver\u0026#34; ]; then 298 cat \u0026gt; /etc/X11/xorg.conf.d/00-xdriver.conf \u0026lt;\u0026lt;FOE 299Section \u0026#34;Device\u0026#34; 300 Identifier \u0026#34;Videocard0\u0026#34; 301 Driver \u0026#34;\\$xdriver\u0026#34; 302EndSection 303FOE 304fi 305 306EOF 307 308chmod 755 /etc/rc.d/init.d/livesys 309/sbin/restorecon /etc/rc.d/init.d/livesys 310/sbin/chkconfig --add livesys 311 312chmod 755 /etc/rc.d/init.d/livesys-late 313/sbin/restorecon /etc/rc.d/init.d/livesys-late 314/sbin/chkconfig --add livesys-late 315 316# enable tmpfs for /tmp 317systemctl enable tmp.mount 318 319 320# enable docker 321systemctl enable docker.service 322 323# work around for poor key import UI in PackageKit 324rm -f /var/lib/rpm/__db* 325releasever=$(rpm -q --qf \u0026#39;%{version}\\n\u0026#39; --whatprovides system-release) 326basearch=$(uname -i) 327rpm --import /etc/pki/rpm-gpg/RPM-GPG-KEY-fedora-$releasever-$basearch 328echo \u0026#34;Packages within this LiveCD\u0026#34; 329rpm -qa 330# Note that running rpm recreates the rpm db files which aren\u0026#39;t needed or wanted 331rm -f /var/lib/rpm/__db* 332 333# go ahead and pre-make the man -k cache (#455968) 334/usr/bin/mandb 335 336# save a little bit of space at least... 337rm -f /boot/initramfs* 338# make sure there aren\u0026#39;t core files lying around 339rm -f /core* 340 341# convince readahead not to collect 342# FIXME: for systemd 343 344cat \u0026gt;\u0026gt; /etc/rc.d/init.d/livesys \u0026lt;\u0026lt; EOF 345 346 347# disable updates plugin 348cat \u0026gt;\u0026gt; /usr/share/glib-2.0/schemas/org.gnome.settings-daemon.plugins.updates.gschema.override \u0026lt;\u0026lt; FOE 349[org.gnome.settings-daemon.plugins.updates] 350active=false 351FOE 352 353# Show the system-config-keyboard tool on the desktop 354mkdir /home/liveuser/Desktop -p \u0026gt;/dev/null 355cat /usr/share/applications/system-config-keyboard.desktop | sed \u0026#39;/NotShowIn/d\u0026#39; |sed \u0026#39;s/Terminal=false/Terminal=true/\u0026#39; \u0026gt; /home/liveuser/Desktop/system-config-keyboard.desktop 356cat /usr/share/applications/liveinst.desktop | sed \u0026#39;/NoDisplay/d\u0026#39; \u0026gt; /home/liveuser/Desktop/liveinst.desktop  357chmod +x /home/liveuser/Desktop/*.desktop 358chown -R liveuser:liveuser /home/liveuser 359 360# Liveuser face 361if [ -e /usr/share/icons/hicolor/96x96/apps/fedora-logo-icon.png ] ; then 362 cp /usr/share/icons/hicolor/96x96/apps/fedora-logo-icon.png /home/liveuser/.face 363 chown liveuser:liveuser /home/liveuser/.face 364fi 365 366# make the installer show up 367if [ -f /usr/share/applications/liveinst.desktop ]; then 368 # Show harddisk install in shell dash 369 sed -i -e \u0026#39;s/NoDisplay=true/NoDisplay=false/\u0026#39; /usr/share/applications/liveinst.desktop 370# need to move it to anaconda.desktop to make shell happy 371#cp /usr/share/applications/liveinst.desktop /usr/share/applications/anaconda.desktop 372fi 373 cat \u0026gt;\u0026gt; /usr/share/glib-2.0/schemas/org.gnome.shell.gschema.override \u0026lt;\u0026lt; FOE 374[org.gnome.shell] 375favorite-apps=[\u0026#39;liveinst.desktop\u0026#39;,\u0026#39;firefox.desktop\u0026#39;, \u0026#39;evolution.desktop\u0026#39;, \u0026#39;empathy.desktop\u0026#39;, \u0026#39;rhythmbox.desktop\u0026#39;, \u0026#39;shotwell.desktop\u0026#39;, \u0026#39;libreoffice-writer.desktop\u0026#39;, \u0026#39;nautilus.desktop\u0026#39;, \u0026#39;gnome-documents.desktop\u0026#39;, \u0026#39;anaconda.desktop\u0026#39;] 376FOE 377 378 379# set up auto-login 380cat \u0026gt; /etc/gdm/custom.conf \u0026lt;\u0026lt; FOE 381[daemon] 382AutomaticLoginEnable=True 383AutomaticLogin=liveuser 384FOE 385 386# Turn off PackageKit-command-not-found while uninstalled 387if [ -f /etc/PackageKit/CommandNotFound.conf ]; then 388 sed -i -e \u0026#39;s/^SoftwareSourceSearch=true/SoftwareSourceSearch=false/\u0026#39; /etc/PackageKit/CommandNotFound.conf 389fi 390 391# make sure to set the right permissions and selinux contexts 392chown -R liveuser:liveuser /home/liveuser/ 393restorecon -R /home/liveuser/ 394 395# Fixing default locale to us 396localectl set-keymap us 397localectl set-x11-keymap us 398EOF 399 400 401# rebuild schema cache with any overrides we installed 402glib-compile-schemas /usr/share/glib-2.0/schemas 403 404 405%end 注意，上面注释了两个地方，都可以添加软件或者运行脚本\n三、build出iso文件 1livecd-creator --verbose -c centos7-live-docker.ks --cache=cache -f centos7-live-docker 然后就会得到centos7-live-docker.iso的文件，注意在build过程中的报错信息，多数是无法下载包导致的。\n直接加载ISO文件启动或者刻录到USB上启动，就可以进入这个自制的Live系统了\n千万注意，启动一定要选Bios legacy，不要用Uefi。\n相关一些有用的链接：\n https://github.com/minishift/minishift-centos-iso  https://github.com/livecd-tools/livecd-tools  https://blog.csdn.net/sharpbladepan/article/details/107423468   ","date":"2021-10-10","img":"","permalink":"/posts/20211010-live_cd/","series":null,"tags":null,"title":"CentOS 7 Live-CD 的制作"},{"categories":[],"content":"This article offers a sample of basic Markdown syntax that can be used in Hugo content files, also it shows whether basic HTML elements are decorated with CSS in a Hugo theme.\nHeadings The following HTML \u0026lt;h1\u0026gt;—\u0026lt;h6\u0026gt; elements represent six levels of section headings. \u0026lt;h1\u0026gt; is the highest section level while \u0026lt;h6\u0026gt; is the lowest.\nH1 H2 H3 H4 H5 H6 Paragraph Xerum, quo qui aut unt expliquam qui dolut labo. Aque venitatiusda cum, voluptionse latur sitiae dolessi aut parist aut dollo enim qui voluptate ma dolestendit peritin re plis aut quas inctum laceat est volestemque commosa as cus endigna tectur, offic to cor sequas etum rerum idem sintibus eiur? Quianimin porecus evelectur, cum que nis nust voloribus ratem aut omnimi, sitatur? Quiatem. Nam, omnis sum am facea corem alique molestrunt et eos evelece arcillit ut aut eos eos nus, sin conecerem erum fuga. Ri oditatquam, ad quibus unda veliamenimin cusam et facea ipsamus es exerum sitate dolores editium rerore eost, temped molorro ratiae volorro te reribus dolorer sperchicium faceata tiustia prat.\nItatur? Quiatae cullecum rem ent aut odis in re eossequodi nonsequ idebis ne sapicia is sinveli squiatum, core et que aut hariosam ex eat.\nBlockquotes The blockquote element represents content that is quoted from another source, optionally with a citation which must be within a footer or cite element, and optionally with in-line changes such as annotations and abbreviations.\nBlockquote without attribution  Tiam, ad mint andaepu dandae nostion secatur sequo quae. Note that you can use Markdown syntax within a blockquote.\n Blockquote with attribution  Don\u0026rsquo;t communicate by sharing memory, share memory by communicating.— Rob Pike1 Tables Tables aren\u0026rsquo;t part of the core Markdown spec, but Hugo supports supports them out-of-the-box.\n   Name Age     Bob 27   Alice 23    Inline Markdown within tables    Italics Bold Code     italics bold code    Code Blocks Code block with backticks 1\u0026lt;!doctype html\u0026gt; 2\u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; 3\u0026lt;head\u0026gt; 4 \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; 5 \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; 6\u0026lt;/head\u0026gt; 7\u0026lt;body\u0026gt; 8 \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; 9\u0026lt;/body\u0026gt; 10\u0026lt;/html\u0026gt; Code block with Hugo\u0026rsquo;s internal highlight shortcode 1\u0026lt;!doctype html\u0026gt; 2\u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; 3\u0026lt;head\u0026gt; 4 \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; 5 \u0026lt;title\u0026gt;Another Example HTML5 Document\u0026lt;/title\u0026gt; 6\u0026lt;/head\u0026gt; 7\u0026lt;body\u0026gt; 8 \u0026lt;p\u0026gt;A looooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooong text\u0026lt;/p\u0026gt; 9\u0026lt;/body\u0026gt; 10\u0026lt;/html\u0026gt; List Types Ordered List  First item Second item Third item  Unordered List  List item Another item And another item  Nested List  Fruit  Apple Orange Banana   Dairy  Milk Cheese    TODO List  Done WIP  Other Elements — abbr, sub, sup, kbd, mark GIFis a bitmap image format.\nH2O\nXn+ Yn= ZnPress CTRL+ALT+Deleteto end the session.\nMost salamandersare nocturnal, and hunt for insects, worms, and other small creatures.\n  The above quote is excerpted from Rob Pike\u0026rsquo;s talk during Gopherfest, November 18, 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n  ","date":"2020-11-09","img":"/images/markdown.png","permalink":"/posts/markdown-syntax/","series":["Manual"],"tags":["Markdown","CSS","HTML"],"title":"Markdown Syntax Guide"}]