[{"categories":null,"content":"这个需求也很有点意思，DBA 要求做 MySQL 的卸载从库，数据量会很大，硬盘空间后期需要扩容，但是 cpu 反倒占的不多。\n单独 MySQL 是无法限制其 CPU 核使用的，这样的话，最好就是做个虚机来控制 MySQL 总 CPU 核数的使用，然后硬盘扩容的话，比如要拉伸虚机的附加第二块硬盘，如果是 QEMU 格式，会花费很长时间，所以干脆把宿主机的目录直接透传进虚机，之后如果要扩容加硬盘，直接把新的大硬盘 mount 出来再透进去即可，新旧硬盘拷贝数据也比拉伸虚机硬盘快。\nCentOS7 下的做法如下：\n两个大前提：\n一、宿主机的 KVM qemu 系统需要使用新的 rpm 包，需要编译\n二、虚机的内核需要升级，mount 命令需要支持 -t p9 的新格式\n我们做好准备，就可以开始了\n一、编译宿主机的qemu新包 现在已经是2022年了，所以编译的方式也发生变化了，最佳编译方式是干脆启动一个 Docker 虚拟机，来编译出来 rpm 包，也不污染环境。\n首先克隆下来项目：\n1git clone https://github.com/AlekseyChudov/qemu-kvm-virtfs.git 2 3cd qemu-kvm-virtfs 4 看一下最后的 build 脚本，有一个地方需要修改：\n现在的 CentOS 最新版是 7.9.2009 ，这个版本树里是没有 qemu 的 Source Code 的，需要修改 baseurl，降低到 7.8.2003 才有 Source 的 repo\n1baseurl=https://mirrors.tripadvisor.com/centos-vault/centos/\\$releasever/virt/Source/kvm-common/ 2 3改成： 4 5baseurl=https://mirrors.tripadvisor.com/centos-vault/centos/7.8.2003/virt/Source/kvm-common/ 改好后 build 脚本如下\n1#!/bin/bash 2 3set -ex 4 5yum -y update 6 7yum -y install \\ 8 \u0026#34;@Development Tools\u0026#34; \\ 9 centos-release-qemu-ev \\ 10 glusterfs-api-devel \\ 11 glusterfs-devel \\ 12 iasl \\ 13 kernel-devel \\ 14 libcacard-devel \\ 15 libepoxy-devel \\ 16 mesa-libgbm-devel \\ 17 nss-devel \\ 18 spice-protocol \\ 19 spice-server-devel \\ 20 usbredir-devel 21 22cat \u0026lt;\u0026lt; EOF \u0026gt;\u0026gt; /etc/yum.repos.d/CentOS-QEMU-EV.repo 23[centos-qemu-ev-source] 24name=CentOS-\\$releasever - QEMU EV Sources 25baseurl=https://mirrors.tripadvisor.com/centos-vault/centos/7.8.2003/virt/Source/kvm-common/ 26gpgcheck=1 27enabled=0 28gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-SIG-Virtualization 29EOF 30 31yum-builddep -y qemu-kvm-ev 32 33yumdownloader --source qemu-kvm-ev 34 35rpm -Uvh qemu-kvm-ev-*.src.rpm 36 37sed -i \u0026#39;s/--disable-virtfs/--enable-virtfs/\u0026#39; /root/rpmbuild/SOURCES/build_configure.sh 38 39sed -i -e \u0026#39;/^%files -n qemu-kvm-common/,/^$/s/^$/%{_bindir}\\/virtfs-proxy-helper\\n%{_mandir}\\/man1\\/virtfs-proxy-helper.1.gz\\n/\u0026#39; \\ 40 -e \u0026#39;/^%if %{rhev}$/,/^%else$/s/pkgsuffix -ev/pkgsuffix -virtfs/\u0026#39; \\ 41 -e \u0026#39;/%define rhel_rhev_conflicts()/ a Provides: %1-ev = %{epoch}:%{version}-%{release} \\\\\\nObsoletes: %1-ev \u0026lt; %{obsoletes_version} \\\\\u0026#39; \\ 42 /root/rpmbuild/SPECS/qemu-kvm.spec 43 44rpmbuild -ba --clean /root/rpmbuild/SPECS/qemu-kvm.spec 然后一句话执行 Build：\n1cd qemu-kvm-virtfs 2 3docker run -it --privileged -v \u0026#34;$PWD/rpmbuild:/root/rpmbuild\u0026#34; \\ 4 docker.io/centos:7 /root/rpmbuild/build 5 6会得到如下的包，版本是 2.12.0-44.1.el7_8.1 ： 7 8rpmbuild/RPMS/x86_64/qemu-img-virtfs-2.12.0-44.1.el7_8.1.x86_64.rpm 9rpmbuild/RPMS/x86_64/qemu-kvm-common-virtfs-2.12.0-44.1.el7_8.1.x86_64.rpm 10rpmbuild/RPMS/x86_64/qemu-kvm-tools-virtfs-2.12.0-44.1.el7_8.1.x86_64.rpm 11rpmbuild/RPMS/x86_64/qemu-kvm-virtfs-2.12.0-44.1.el7_8.1.x86_64.rpm 12rpmbuild/RPMS/x86_64/qemu-kvm-virtfs-debuginfo-2.12.0-44.1.el7_8.1.x86_64.rpm 13rpmbuild/SRPMS/qemu-kvm-virtfs-2.12.0-44.1.el7_8.1.src.rpm 14 我们在宿主机上安装替换新的 qemu 包，并重启 libvirtd\n1yum -y install \\ 2 cloud-utils \\ 3 libvirt-client \\ 4 libvirt-daemon-config-network \\ 5 libvirt-daemon-config-nwfilter \\ 6 libvirt-daemon-driver-interface \\ 7 libvirt-daemon-driver-qemu \\ 8 virt-install \\ 9 rpmbuild/RPMS/x86_64/qemu-img-virtfs-2.12.0-44.1.el7_8.1.x86_64.rpm \\ 10 rpmbuild/RPMS/x86_64/qemu-kvm-common-virtfs-2.12.0-44.1.el7_8.1.x86_64.rpm \\ 11 rpmbuild/RPMS/x86_64/qemu-kvm-virtfs-2.12.0-44.1.el7_8.1.x86_64.rpm 12 13systemctl restart libvirtd 这样宿主机就准备好了。\n二、生产虚机，做好passthrough 生产虚机的时候，需要做好内外文件目录的直通，宿主机目录是 /opt/test，对应虚机目录是 /test\n1\u0026lt;domains ...\u0026gt; 2 ... 3 \u0026lt;devices ...\u0026gt; 4 \u0026lt;filesystem type=\u0026#39;mount\u0026#39; accessmode=\u0026#39;passthrough\u0026#39;\u0026gt; 5 \u0026lt;source dir=\u0026#39;/opt/test\u0026#39;/\u0026gt; 6 \u0026lt;target dir=\u0026#39;/test\u0026#39;/\u0026gt; 7 \u0026lt;/filesystem\u0026gt; 8 \u0026lt;/devices\u0026gt; 9\u0026lt;/domains\u0026gt; 虚机启动后，需要升级核心，支持新的 mount 选项\n1yum -y update 2yum -y --enablerepo centosplus install kernel-plus 3reboot 4 5# 把宿主机提供的 /test 点 mount 出来 6mount -t 9p -o trans=virtio,version=9p2000.L /test /opt/test 也可以把挂载点写进虚机的 fstab，下次就会自动 mount 了\n1/test /opt/test 9p trans=virtio,version=9p2000.L,nofail,_netdev,x-mount.mkdir 0 0 ","date":"2022-02-23","img":"","permalink":"https://zhangrr.github.io/posts/20220223-kvm_passthrouth/","series":null,"tags":null,"title":"KVM下宿主机的目录直通到虚机"},{"categories":null,"content":"这是个严肃话题，正经挖以太币的话，很多的矿厂都是如果直接挖到以太币地址，那么必须挖到完整一个才允许提现，手续费还特高。\n但是如果挖到以太链Polygon的地址的话，就可以 0.005 ETH提币了，如下图所示\n但是千万记住，Polygon提出的所谓 0.005 ETH，看下图，只是个ERC-20的Token，实际是wETH Token，必须经过转换才能提到 ETH 去。\n从 ERC-20 转换到其它代币呢，需要 MATIC 代币来付账，这玩意也必须得有啊！废话不多说，直接放上挖MATIC教程，我的机器是 CentoOS\n首先需要有个完全自主的的 ETH 钱包地址，这个我就不教大家了\n一、下载xmrig挖矿软件 下载地址：https://github.com/xmrig/xmrig/releases\n我们选择最近的下载就好\n二、做好加密通道 我们需要做好一条加密tcp通道\n用 ghostunnel, localhost:9012 \u0026mdash;\u0026gt; vps:9012 \u0026mdash;\u0026gt; rx.unmineable.com:3333\n三、用screen后台开挖 1screen 2 3#./xmrig -o localhost:9999 -a rx -k -u DOGE:MATIC地址.矿工名 4./xmrig -o localhost:9012 -u MATIC:0x64358C7ddC96001697aBbA7ed431BADB6ABAaec5.cpu01 -p x --cpu-no-yield 5 6ctrl+a+d 四、查看挖了多少 查看地址：https://unmineable.com/coins/MATIC/address/ 五、兑换 挖到了足够的 MATIC，就可以愉快的兑换了。\n","date":"2022-02-22","img":"","permalink":"https://zhangrr.github.io/posts/20220222-eth_matic/","series":null,"tags":null,"title":"如何用CPU挖Polygon网络的MATIC币"},{"categories":null,"content":"公司要做阿里的小程序接入，需要通过测试，测试呢需要提供硬盘的监控报告，比如 iops 。\n同事从网上找了一下，iops 监控原文如下：监控磁盘的 iops ，利用 linux 的 /proc/diskstats 的第四个字段和第八字段可监控读和写的 iops，第四个记录是记录所有读的次数，第八个字段是记录所有写的次数。通过 zabbix 上的差速率即可监控磁盘的 iops。\n文章链接：https://cloud.tencent.com/developer/article/1519113?ivk_sa=1024320u 仔细研究了一下上面的文章，看了它提供了两张监控图，分析一下：\n第一张图：\n有两个指标，绿色的是硬盘每秒的 io 读次数，红色的是硬盘每秒的 io 写次数。\n第二张图：\n同样两个指标，绿色的是硬盘每秒的 io 读 Bytes，红色的是硬盘每秒的 io 写 Bytes。\n知道了指标具体的含义，这样就好办了。\n我们用的是 prometheus 和 node_exporter\n首先去看看 node_exporter 暴露的指标，搜一搜 node_disk，会看到如下4个指标：\n1# HELP node_disk_reads_completed_total The total number of reads completed successfully. 2# TYPE node_disk_reads_completed_total counter 3node_disk_reads_completed_total{device=\u0026#34;sda\u0026#34;} 4.9530358e+07 4# HELP node_disk_writes_completed_total The total number of writes completed successfully. 5# TYPE node_disk_writes_completed_total counter 6node_disk_writes_completed_total{device=\u0026#34;sda\u0026#34;} 1.4449267304e+10 7 8# HELP node_disk_read_bytes_total The total number of bytes read successfully. 9# TYPE node_disk_read_bytes_total counter 10node_disk_read_bytes_total{device=\u0026#34;sda\u0026#34;} 6.4101677568e+11 11# HELP node_disk_written_bytes_total The total number of bytes written successfully. 12# TYPE node_disk_written_bytes_total counter 13node_disk_written_bytes_total{device=\u0026#34;sda\u0026#34;} 1.15483858333184e+14 14 可以看出是上面 4 个指标，这四个指标都是 counter 计数器类型的，都是只增不减的。\n然后去 prometheus ，画个图试试，query 分别如下（注意我们的 instance 即 node_exporter，是跑在了 50000 端口，是非标准的）：\n1node_disk_reads_completed_total{instance=\u0026#34;192.168.1.1:50000\u0026#34;} 2node_disk_writes_completed_total{instance=\u0026#34;192.168.1.1:50000\u0026#34;} 3 4node_disk_read_bytes_total{instance=\u0026#34;192.168.1.1:50000\u0026#34;} 5node_disk_written_bytes_total{instance=\u0026#34;192.18.1.1:50000\u0026#34;} 大家看到了 counter 类型，必然是一条斜线直冲天际。\n好了，我们然后去 grafana 里增加面板：\n选 Add Query\n先选数据源，选择系统中已经配好的 prometheus，怎么配这里就不说了：\n然后在 Query 的 Metrics 里填入 node_disk_written_bytes_total{instance=\u0026quot;192.168.1.1:50000\u0026quot;}：\n在 Legend 的空白处随便点一下，大折线出现了，而且给出了提示：Time series is monotonically increasing. Try applying a rate() function.\n听人劝，吃饱饭。我们改一下 Metrics 的查询语句，因为我们是5分钟抓一次数据，所以改成如下格式： rate(node_disk_written_bytes_total{instance=\u0026quot;192.18.1.1:50000\u0026quot;}[5m])\n再增加一个查询，Add Query 同时把 read bytes 和 write bytes 放进一张图去：\n最后修正一下：\n1A: 2Metrics: rate(node_disk_read_bytes_total{instance=\u0026#34;192.168.1.1:50000\u0026#34;}[5m]) 3Legend: sda read per second 4 5B: 6Metrics: rate(node_disk_written_bytes_total{instance=\u0026#34;192.168.1.1:50000\u0026#34;}[5m]) 7Legend: sda write per second 再把最大、最小、平均、当前给出来：\n这样图就做好了，最后完工的两张图：\n所有 prometheus 从各种 exporter 收上来的数据都可以这么图形化，以后画图就丰简由人了。\n","date":"2022-01-18","img":"","permalink":"https://zhangrr.github.io/posts/20220118-grafana_prometheus/","series":null,"tags":null,"title":"Grafana画出prometheus的图"},{"categories":null,"content":"最近开始弄hubot，顺带也开始学习 javascript ，首先从单独一个 nodejs 项目开始：\nnvm 和 node 的安装使用在前面已经说过了： Nodejs多版本的安装与管理 新建一个项目目录，build01\n然后在其下建立如下目录结构：\n1mkdir build01 2 3cd build01 4mkdir es6 dist 5 6mkdir public 7cd public 8mkdir es6 dist 解释一下，es6 下放的是服务器端的源代码，dist 下放的是服务器端处理过的源代码。\npublic/es6 放的是浏览器端的源代码，public/dist 放的是浏览器端处理过的源代码。\n一、安装gulp和babel babel 是用来做 js 格式转换的，注意，原有的 babel-preset-es2015 已经不适用了。\n1npm install --save-dev gulp gulp-babel babel-preset-env@next 生成一个.babelrc 文件，内容就一行\n1{ \u0026#34;presets\u0026#34;: [\u0026#34;env\u0026#34;] } 在 es6 目录下，建立一个测试的 test.js 文件，内容如下：\n1\u0026#39;use strict\u0026#39;; 2 3const sentences = [ 4{ subject: \u0026#39;JavaScript\u0026#39;, verb: \u0026#39;is\u0026#39;, object: \u0026#39;great\u0026#39; }, 5{ subject: \u0026#39;Elephants\u0026#39;, verb: \u0026#39;are\u0026#39;, object: \u0026#39;large\u0026#39; }, 6]; 7 8function say({ subject, verb, object }) { 9 console.log(`${subject} ${verb} ${object}`); 10} 11 12for ( let s of sentences) { 13 say(s); 14} 仔细看，上面这个文件用到了 es6 的语法，对象的解构，of语法。\n我们来用 babel 把它转换一下，变成 es5 的语法\n在build01目录下，生成gulpfile.js，内容如下：\n1const gulp = require(\u0026#39;gulp\u0026#39;); 2const babel = require(\u0026#39;gulp-babel\u0026#39;); 3 4 5gulp.task(\u0026#39;default\u0026#39;, done =\u0026gt;{ 6 7 gulp.src(\u0026#34;es6/**/*.js\u0026#34;) 8 .pipe(babel()) 9 .pipe(gulp.dest(\u0026#34;dist\u0026#34;)); 10 11 gulp.src(\u0026#34;public/es6/**/*.js\u0026#34;) 12 .pipe(babel()) 13 .pipe(gulp.dest(\u0026#34;public/dist\u0026#34;)); 14 15 done(); 16 17}); 注意上面，**表示洞穿所有子目录。运行 gulp ，就会在 dist 目录下生成新的 test.js ，内容如下：\n1\u0026#39;use strict\u0026#39;; 2 3var sentences = [{ 4subject: \u0026#39;JavaScript\u0026#39;, 5verb: \u0026#39;is\u0026#39;, 6object: \u0026#39;great\u0026#39; 7}, { 8 subject: \u0026#39;Elephants\u0026#39;, 9 verb: \u0026#39;are\u0026#39;, 10 object: \u0026#39;large\u0026#39; 11}]; 12 13function say(_ref) { 14 var subject = _ref.subject, 15verb = _ref.verb, 16object = _ref.object; 17console.log(\u0026#34;\u0026#34;.concat(subject, \u0026#34; \u0026#34;).concat(verb, \u0026#34; \u0026#34;).concat(object)); 18} 19 20for (var _i = 0; _i \u0026lt; sentences.length; _i++) { 21var s = sentences[_i]; 22say(s); 23} 仔细看，es6 的语法 变成 es5 的语法了。\n二、安装eslint babel 是对语法进行转换，那么 eslint 就是对语法作出规范，很多人吃 airbnb 那一套规则。\n1npm install --save-dev eslint gulp-eslint gulp-if 安装完成后执行命令进行初始化：\n1eslint --init 会提问一堆问题：\n1How would you like to use ESLint? … 2▸ To check syntax, find problems, and enforce code style 3 4What type of modules does your project use? … 5▸ JavaScript modules (import/export) 6 7Which framework does your project use? … 8 React 9 Vue.js 10▸ None of these 11 12Does your project use TypeScript? ‣ No / Yes 13 14#两个都选哦 15Where does your code run? … (Press \u0026lt;space\u0026gt; to select, \u0026lt;a\u0026gt; to toggle all, \u0026lt;i\u0026gt; to invert selection) 16✔ Browser 17✔ Node 18 19How would you like to define a style for your project? … 20▸ Answer questions about your style 21 22✔ What format do you want your config file to be in? · JSON 23✔ What style of indentation do you use? · 4 24✔ What quotes do you use for strings? · single 25✔ What line endings do you use? · unix 26✔ Do you require semicolons? · Yes 最后得到文件 .eslintrc.json\n1{ 2 \u0026#34;env\u0026#34;: { 3 \u0026#34;browser\u0026#34;: true, 4 \u0026#34;es2021\u0026#34;: true, 5 \u0026#34;node\u0026#34;: true 6 }, 7 \u0026#34;extends\u0026#34;: \u0026#34;eslint:recommended\u0026#34;, 8 \u0026#34;parserOptions\u0026#34;: { 9 \u0026#34;ecmaVersion\u0026#34;: 13, 10 \u0026#34;sourceType\u0026#34;: \u0026#34;module\u0026#34; 11 }, 12 \u0026#34;rules\u0026#34;: { 13 \u0026#34;indent\u0026#34;: [ 14 \u0026#34;error\u0026#34;, 15 4 16 ], 17 \u0026#34;linebreak-style\u0026#34;: [ 18 \u0026#34;error\u0026#34;, 19 \u0026#34;unix\u0026#34; 20 ], 21 \u0026#34;quotes\u0026#34;: [ 22 \u0026#34;error\u0026#34;, 23 \u0026#34;single\u0026#34; 24 ], 25 \u0026#34;semi\u0026#34;: [ 26 \u0026#34;error\u0026#34;, 27 \u0026#34;always\u0026#34; 28 ] 29 } 30} 这里注意，要改两个地方：\n1\u0026#34;es2021\u0026#34;: true, 2改成 3\u0026#34;es6\u0026#34;: true, 4 5\u0026#34;ecmaVersion\u0026#34;: 13, 6改成 7\u0026#34;ecmaVersion\u0026#34;: 10, 然后我们改写一下 gulpfile.js，增加 eslint 的部分\n1const gulp = require(\u0026#39;gulp\u0026#39;); 2const babel = require(\u0026#39;gulp-babel\u0026#39;); 3const eslint = require(\u0026#39;gulp-eslint\u0026#39;); 4const gulpIf = require(\u0026#39;gulp-if\u0026#39;); 5 6function isFixed(file) { 7 return file.eslint !== null \u0026amp;\u0026amp; file.eslint.fixed; 8} 9 10gulp.task(\u0026#39;default\u0026#39;, done =\u0026gt;{ 11 12 gulp.src([\u0026#34;es6/**/*.js\u0026#34;, \u0026#34;public/**/*.js\u0026#34;]) 13 .pipe(eslint({fix: true})) 14 .pipe(eslint.format()) 15 .pipe(gulpIf(isFixed, gulp.dest(file =\u0026gt; file.base))) 16 .pipe(eslint.failAfterError()); 17 18 19 gulp.src(\u0026#34;es6/**/*.js\u0026#34;) 20 .pipe(babel()) 21 .pipe(gulp.dest(\u0026#34;dist\u0026#34;)); 22 23 gulp.src(\u0026#34;public/es6/**/*.js\u0026#34;) 24 .pipe(babel()) 25 .pipe(gulp.dest(\u0026#34;public/dist\u0026#34;)); 26 27 done(); 28 29}); 再运行 gulp ，会把 es6/test.js 给格式化好，对比一下原文件就知道了，ident 被调整成4个空格了：\n1\u0026#39;use strict\u0026#39;; 2 3const sentences = [ 4 { subject: \u0026#39;JavaScript\u0026#39;, verb: \u0026#39;is\u0026#39;, object: \u0026#39;great\u0026#39; }, 5 { subject: \u0026#39;Elephants\u0026#39;, verb: \u0026#39;are\u0026#39;, object: \u0026#39;large\u0026#39; }, 6]; 7 8function say({ subject, verb, object }) { 9 console.log(`${subject} ${verb} ${object}`); 10} 11 12for ( let s of sentences) { 13 say(s); 14} ","date":"2022-01-04","img":"","permalink":"https://zhangrr.github.io/posts/20220104-javascript_babel_eslint/","series":null,"tags":null,"title":"Javascript的项目与babel和eslint"},{"categories":null,"content":"Hubot是一个通用的聊天机器人，能和很多聊天软件集成，比如slack、rockchat、telegram、企业微信、IRC等。\n公司用的是企业微信，之前用群机器人和 prometheus 集成，发送各种告警信息，但是群机器人无法接收消息；同时我们想集成进去很多自定义命令，比如发送指令就开始 jenkins build，发送 ansible 指令执行，发送流量图就自动把机房的流量图发过来，这个群机器人就用不成了。所以必须再直接一些，构建一个企业微信的应用。\n一、企业微信安装前准备 安装前需要在企业微信中拿到4个信息：\n WECHATWORK_CORP_ID WECHATWORK_APP_AGENT_ID WECHATWORK_APP_SECRET WECHATWORK_AES_KEY  首先联系企业微信管理员新增一个应用：\n在“管理后台-我的企业”中可以获得地一个信息，企业 ID 信息，CORP_ID ：\n拿拿 在“管理后台-应用管理-点击某具体应用-详情”页中可以获得二、三个信息， APP_AGENT_ID 和 APP_SECRET\n第四个 AES_KEY 就比较麻烦了，我们进入这个应用，点击配置的图标：\n然后进入，点击启用 API 接收\n然后设置一下\n url 填写 https://hubot.rendoumi.com/wechatwork/webhook  Token 点随机获取 EncodingAESKey 点随机获取，这就是具体的 AESKey 了  然后点击保存，这里必然会失败！没有关系，因为我们还没有装hubot呢。我们把这三个地方先记录下来，随后再来点击保存。\n二、Hubot 安装前准备 1、注意上面地址：URL 地址是 https ，然后 hubot 的缺省地址是 http://xxx:8080 ，这样如果前面没有证书卸载的设备，就得搭建一个Nginx（haproxy、traefik）来代理 https 443端口到8080。\n2、Hubot 是支持 Coffeescript 和 nodejs 的，从它的 Adapters 网页看\nhttps://hubot.github.com/docs/adapters/development/ 里面都是 coffee 味的单箭头函数，导致不太熟悉的八戒以为在 wechatwork module导出的时候也应该用单箭头函数，结果就悲剧了。\n所以务必清楚，Adapter 的语法是 js，普通自建自动回复script可以用coffeescript，后面会详细说这一点。\n3、安装Hubot必须新建一个普通用户，不能用 root 装\n用 root 会导致 nodejs 一堆报错，八戒新建了一个用户 bot，家目录/home/bot，然后再在这下面建立/home/bot/myhubot，在这个目录下安装。\n理由是这个如果直接装在这个用户的 home 目录下，会导致这个用户只能有这一个用途了，目录结构太浅，一堆文件\n4、Hubot的变量引入都是通过环境变量引入的\n所以无论什么wechatwork还是jenkins，都需要通过环境变量导入变量\n三、安装Hubot 首先安装nvm，参看文章 Nodejs多版本的安装与管理 1curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.0/install.sh | bash 安装的时候最好加个 https_proxy 代理，因为这个 shell 脚本不翻墙很难下下来。\n然后务必关了代理，退出终端并重新进入，安装 nodejs 的最新版本\n1nvm ls-remote 2nvm install v16.13.1 3nvm list 确认一下，nodejs 的版本缺省是 v16.13.1\n这里千万不要手欠把 npm 的源换成国内的腾讯等，就是国外源\n然后安装hubot，并初始化\n1su - bot 2npm install -g yo generator-hubot 3 4mkdir /home/bot/myhubot 5cd /home/bot/myhubot 6yo hubot 会问几个问题：\n Ower：写自己邮箱就行 Bot name: bot Description: rendoumi corp Bot adapter: campfire  注意最后一个，选campfire就好，我们稍后再装wechatwork\n1 _____________________________  2 / \\  3 //\\ | Extracting input for | 4 ////\\ _____ | self-replication process | 5 //////\\ /_____\\ \\ /  6 ======= |[^_/\\_]| /---------------------------- 7| | _|___@@__|__ 8+===+/ /// \\_\\ 9| |_\\ /// HUBOT/\\\\ 10|___/\\// / \\\\ 11\\ / +---+ 12\\____/ | | 13| //| +===+ 14\\// |xx|  15 16? Owner zhangranrui@rendoumi.com 17? Bot name bot 18? Description rendoumi corp 19? Bot adapter campfire 然后就会是漫长的等待：\n最后会报几个警告，不用管，忽略即可。\n1npm notice created a lockfile as package-lock.json. You should commit this file. 2+ hubot-help@1.0.1 3+ hubot-google-images@0.2.7 4+ hubot-rules@1.0.0 5+ hubot-shipit@0.2.1 6+ hubot-redis-brain@1.0.0 7+ hubot-google-translate@0.2.1 8+ hubot-diagnostics@1.0.0 9+ hubot-scripts@2.17.2 10+ hubot@3.3.2 11+ hubot-maps@0.0.3 12+ hubot-heroku-keepalive@1.0.3 13+ hubot-pugme@0.1.1 14added 99 packages from 53 contributors and audited 99 packages in 20.176s 15 162 packages are looking for funding 17 run `npm fund` for details 18 19found 1 low severity vulnerability 20 run `npm audit fix` to fix them, or `npm audit` for details 我们先跑一下，简单测一下功能，我们的bot name输入的是bot\n1cd /home/bot/myhubot 2 3./bin/hubot 4 5#进入后 6bot ping 7 8#得到回应 9PONG 这样就装好了 hubot，我们做一下修改，去掉 heroku 和 redis，否则会不停丢出报错信息，但是不要用 npm 去删除这两个包，放着那里备用也不占多少空间的\n1cd /home/bot/myhubot 2 3vi external-scripts.json 4[ 5 \u0026#34;hubot-diagnostics\u0026#34;, 6 \u0026#34;hubot-help\u0026#34;, 7 \u0026#34;hubot-heroku-keepalive\u0026#34;, 8 \u0026#34;hubot-google-images\u0026#34;, 9 \u0026#34;hubot-google-translate\u0026#34;, 10 \u0026#34;hubot-pugme\u0026#34;, 11 \u0026#34;hubot-maps\u0026#34;, 12 \u0026#34;hubot-redis-brain\u0026#34;, 13 \u0026#34;hubot-rules\u0026#34;, 14 \u0026#34;hubot-shipit\u0026#34; 15] 16 17#去掉下面两行 18hubot-heroku-keepalive 19hubot-redis-brain 三、安装微信 Adapter Adapter 插件，hubot是一套系统，通过各种插件跟各种聊天软件进行交互\n微信插件原始地址：https://github.com/billtt/hubot-wechatwork\n八戒稍微改了一下：https://github.com/zhangrr/hubot-wechatwork\n主要是添加了个自动回应和添加了一些调试信息，否则不知道加载成不成功。\n1#先装那个老的 2cd /home/bot/myhubot 3npm install hubot-wechatwork 4 5#覆盖掉老的模块 6cd /home/bot/myhubot/node_modules 7git clone https://github.com/zhangrr/hubot-wechatwork 启动之前普及一下 hubot 基本知识，hubot 中变量的传入都是通过环境变量导入的，启动测试一下wechatwork\n1export WECHATWORK_CORP_ID=aaaa 2export WECHATWORK_APP_AGENT_ID=bbbb 3export WECHATWORK_APP_SECRET=cccc 4export WECHATWORK_AES_KEY=dddd 5 6cd /home/bot/myhubot 7./bin/hubot -a wechatwork 看到信息，INFO Yunwei: loading wechatwork Adapter，说明启动没啥毛病\n然后我们回到企业微信这个应用的应用管理，点击保存接收消息服务器配置，这次就应该能成功了。\n测试一下，我们在企业微信中，对这个应用机器人私聊，发个 hi：\n再发个help，会冒出一堆信息\n然后我们可以试试指令，再发个ping，机器人回PONG，这就算正常工作了\n同时我们在终端也可以看到接收到的消息\n要注意的命令： wechatwork 有一条命令 chat create CHATID USER1,USER2,USER3,\u0026hellip;\n解释一下，是用来建立一个企业微信群聊的，群主默认是第一个人：\nchatid 是指一个群聊的 id 号，格式是字母+数字，比如yunwei001，这个一旦创建就无法销毁，会永远保存在企业微信那里，切切记住！！！\nuser1,user2 这种是员工企业微信的id，通常是用户名全称，比如 zhangrenren,liudaqiang\n所以说的时候务必要小心，万万记住 chatid 一旦建立就无法收回。\n之后可以调用 sendChatMessage 来给群聊发送消息。\n四、准备systemd启动hubot 这样不能每次用个终端启动 hubot 吧，写个 systemd 文件来处理吧\n由于我们用到了nvm，这里确实需要一些技巧，cat /etc/systemd/system/hubot.service\n1[Unit] 2Description=Hubot 3Requires=network.target 4After=network.target 5 6[Service] 7Type=simple 8WorkingDirectory=/home/bot/myhubot 9User=bot 10 11Restart=always 12TimeoutStartSec=10 13RestartSec=10 14 15; Configure Hubot environment variables, use quotes around vars with whitespace as shown below. 16; Environment=\u0026#34;HUBOT_SLACK_TOKEN=SLACK_TOKEN\u0026#34; 17; Environment=\u0026#34;HUBOT_JENKINS_AUTH=pe:x837491lkaflajksdf7\u0026#34; 18; Environment=\u0026#34;HUBOT_JENKINS_URL=http://192.168.1.90:8080/\u0026#34; 19Environment=\u0026#34;NODE_VERSION=default\u0026#34; 20Environment=\u0026#34;WECHATWORK_CORP_ID=aaaa\u0026#34; 21Environment=\u0026#34;WECHATWORK_APP_AGENT_ID=bbbb\u0026#34; 22Environment=\u0026#34;WECHATWORK_APP_SECRET=cccc\u0026#34; 23Environment=\u0026#34;WECHATWORK_AES_KEY=dddd\u0026#34; 24 25ExecStart=/home/bot/.nvm/nvm-exec /home/bot/myhubot/bin/hubot --adapter wechatwork 26 27[Install] 28WantedBy=multi-user.target 注意上面，我们指定了环境变量 NODE_VERSION=default，以及最下的 ExecStart，确定用 nvm-exec 引动 node\n另外把 wechatwork 所需的4个参数也放进环境变量中，这样重启就可以了\n1systemctl daemon-reload 2systemctl start hubot 五、集成jenkins 准备知识，我们必须在 jenkins 拿到用户名和 token（token是用来代替密码的），用来访问 jenkins\n首先登录 jenkins，访问网址：$JENKINS_URL/me/configure，新建一个 API Token，记录下来\n用法很简单：token替代密码用于 url 认证即可。\n1cd /home/bot/myhubot 2npm install hubot-jenkins-optimised 3 4#编辑external-scripts.json 5vi external-scripts.json 6[ 7 \u0026#34;hubot-diagnostics\u0026#34;, 8 \u0026#34;hubot-help\u0026#34;, 9 \u0026#34;hubot-google-images\u0026#34;, 10 \u0026#34;hubot-google-translate\u0026#34;, 11 \u0026#34;hubot-pugme\u0026#34;, 12 \u0026#34;hubot-maps\u0026#34;, 13 \u0026#34;hubot-rules\u0026#34;, 14 \u0026#34;hubot-shipit\u0026#34;, 15 \u0026#34;hubot-jenkins-optimised\u0026#34; 16] 17 18#上面增加了一行 19hubot-jenkins-optimised 20 21#编辑 /etc/systemd/system/hubot.service ，添加 jenkins 的环境变量 22#jenkins的用户名是pe，所有auth是 \u0026#34;pe:x837491lkaflajksdf7\u0026#34; 23...... 24Environment=\u0026#34;HUBOT_JENKINS_AUTH=pe:x837491lkaflajksdf7\u0026#34; 25Environment=\u0026#34;HUBOT_JENKINS_URL=http://192.168.1.90:8080/\u0026#34; 26...... 27 28# 重启 hubot 29systemctl daemon-reload 30systemctl restart hubot 然后我们私聊这个机器人，对它说: jenkins list，就可以看到项目了， jenkins b 3 就可以开始build第三个项目了，所有命令可以通过对机器人说 help 来获得\n六、集成ansible 集成 ansible，其实就是集成一个 shell，也很简单\n1cd /home/bot/myhubot 2npm install hubot-script-shellcmd 3 4#编辑external-scripts.json 5vi external-scripts.json 6[ 7 \u0026#34;hubot-diagnostics\u0026#34;, 8 \u0026#34;hubot-help\u0026#34;, 9 \u0026#34;hubot-google-images\u0026#34;, 10 \u0026#34;hubot-google-translate\u0026#34;, 11 \u0026#34;hubot-pugme\u0026#34;, 12 \u0026#34;hubot-maps\u0026#34;, 13 \u0026#34;hubot-rules\u0026#34;, 14 \u0026#34;hubot-shipit\u0026#34;, 15 \u0026#34;hubot-jenkins-optimised\u0026#34;, 16 \u0026#34;hubot-script-shellcmd\u0026#34; 17] 18 19#上面增加了一行 20hubot-script-shellcmd 21 22#把shellcmd目录下的bash目录，复制一份到/home/bot/myhubot/bash 23cp -R node_modules/hubot-script-shellcmd/bash ./ 24 25#我们把ansible的yml都放到/homebot/myhubot/bash/ansible目录下 26cd /home/bot/myhubot/bash 27mkdir ansible 28 29#把ansible-playbook的yml都放进去 30.... 31 32#然后编写命令脚本 33cd /home/bot/myhubot/bash/handles 34 35cat \u0026lt; EOF \u0026gt;\u0026gt; fuse 36#!/bin/bash 37 38if [ -n \u0026#34;$1\u0026#34; ] ; then 39 if [ -n \u0026#34;$2\u0026#34; ] ; then 40 ansible-playbook /home/bot/myhubot/bash/ansible/check-disk.yml -e \u0026#34;ip=$1fs=$2\u0026#34; 41 else 42 ansible-playbook /home/bot/myhubot/bash/ansible/check-disk.yml -e \u0026#34;ip=$1fs=/\u0026#34; 43 fi 44fi 45 46exit 0 47EOF 48 49#给执行权 50chmod 755 /home/bot/myhubot/bash/handles/fuse 51 52#重启hubot 53systemctl restart hubot check-disk.yml 的真身，是在远程机器上调用本机才有的rust dust命令，来算出远程机器的某个目录的使用情况\n1--- 2- name: check machine disk space 3 hosts: \u0026#34;{{ ip }}\u0026#34; 4 gather_facts: false 5 vars: 6 - ip: \u0026#34;{{ ip }}\u0026#34; 7 - fs: \u0026#34;{{ fs }}\u0026#34; 8 - ansible_ssh_user: \u0026#34;root\u0026#34; 9 - ansible_ssh_pass: \u0026#34;Fuck2021!\u0026#34; 10 - ansible_ssh_common_args: \u0026#34;-o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null\u0026#34; 11 - host_key_checking: False 12 tasks: 13 - name: Run a script with arguments (using \u0026#39;cmd\u0026#39; parameter) 14 ansible.builtin.script: 15 cmd: /usr/local/bin/dust -bcr {{ fs }} 16 register: output 17 18 - debug: var=output.stdout_lines 我们在企业微信对机器说 shell ，就会得到所有命令列表\n对它说 shell fuse 172.19.16.1 /export ，就会把本机才有的 dust 命令，拷贝到172.19.16.1的机器上，并对 /export 执行，得到占磁盘空间最大的文件和目录。\n参考资料： https://qydev.weixin.qq.com/wiki/index.php?title=%E5%8F%91%E9%80%81%E6%8E%A5%E5%8F%A3%E8%AF%B4%E6%98%8E http://sd.blackball.lv/library/Automation_and_Monitoring_with_Hubot_(2014).pdf ","date":"2021-12-22","img":"","permalink":"https://zhangrr.github.io/posts/20211222-hubot_wechat/","series":null,"tags":null,"title":"Hubot集成企业微信+jenkins+ansible"},{"categories":null,"content":"最近在搞 JavaScripts，nodejs的版本满天飞，之前用的 nvm 管理的方法突然不能用了。\n这里记录一下，比较新的 nvm 的安装使用方法：\n下载安装：\n1curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.0/install.sh | bash 以上的脚本安装到底做了什么呢？\n一、它 clone 了 nvm 到 $HOME/.nvm目录\n二、它在 .bashrc 中添加了三行\n事实上它是根据当前的 bash 选择在 .bash_profile、.zshrc、.profile、.bashrc中的某一个进行添加\n然后我们退出终端，并重新进入，看看\n1nvm list 2nvm ls-remote 然后选择最新的版本安装：\n1nvm install v16.13.1 如果想使用不同版本\n1nvm use 14.0.0 设置缺省版本或者取消\n1nvm alias defaut \u0026lt;your_nodejs_default_version\u0026gt; 2 3nvm alias default 14.0.0 4node -v # prints 14.0.0 参考地址：https://github.com/nvm-sh/nvm\n","date":"2021-12-17","img":"","permalink":"https://zhangrr.github.io/posts/20211217-nodejs_nvm/","series":null,"tags":null,"title":"Nodejs多版本的安装与管理"},{"categories":null,"content":"S3 的桶存储可以用 minio 来模拟。\n网上有个 Backblaze 的网站，提供 10GB 的免费空间，且如果是公网访问，1GB流量是免费的，如果前面套了 Cloudflare 的CDN，那基本是全免了。\n说下使用方式：\n首先去 https://www.backblaze.com/ 注册个帐号：\n然后直接建个桶（Create a Bucket）\n桶文件类型选择 public，否则无法公网访问：\n然后 gen 出 master app key 来并记录好：\n随后最大的问题就来了，如果你用它页面自带的上传下载工具，彻底完蛋，拖进去的文件夹会变成扁平的，完全丧失目录结构。\n必须要找一个好用的上传工具了，推荐 b2 。\nb2 工具下载：\nhttps://github.com/Backblaze/B2_Command_Line_Tool 下载后直接改名为 b2 ，然后放到 /usr/local/bin 中\n1wget -O /usr/local/bin/b2 https://github.com/Backblaze/B2_Command_Line_Tool/releases/download/v3.1.0/b2-linux 2chmod 755 /usr/lcoal/b2 运行一下，有很多命令参数：\n详细的使用文档：\nhttps://b2-command-line-tool.readthedocs.io/en/master/ 先去配置帐号，输入applicationKeyId和applicationKey\n1b2 authorize-account 然后就可以往桶里传东西了，把当前目录下的东西传到 rendoumi 这个桶里，并且不要传 .git 目录\n1b2 sync --excludeDirRegex .git . b2://rendoumi/ 套 Cloudflare 就随意了。full ssl，建个 CNAME 且用 Page rule 做 url 转发就可以了。\n","date":"2021-12-08","img":"","permalink":"https://zhangrr.github.io/posts/20211208-backblaze_usage/","series":null,"tags":null,"title":"Backblaze类S3免费免备案对象存储"},{"categories":null,"content":"kubernetes下定制 http 50x 以及 40x 服务器返回信息的话，如果用 Nginx 做 ingress，大家会很自然想到直接用 nginx 来定制：\n1 error_page 500 502 503 504 /50x.html; 2 location = /50x.html { 3root /export/html; 4} 这样做是不行的，因为 Nginx 作为 ingress 来使用，就不能落地，只能做转发；如果你落地了，访问了本地文件，就违背了初衷。而且在 ingress 的 pod 上放页面，会导致 ingress 的配置错乱。\n那么正确 50x 重定位的方法如下：首先建立一个 deploy 和对应的 svc，就是建立一个 web 服务器，能提供服务返回 50x 和 40x 的定制页面；然后在 ingress\n内指定 custom-http-errors 和 default-backend 指向它就可以了。\n一、建立miniserve的deploy和svc 我们选用 rust 的 miniserve 作为 web 服务器，准备好定制好的 index.html，写个 Dockerfile\n网址：https://github.com/svenstaro/miniserve\n1FROM alpine:3.12 2RUN apk add --update bash \u0026amp;\u0026amp; rm -rf /var/cache/apk/* 3 4COPY . /data/ 5 6RUN rm -rf /data/Dockerfile 7 8WORKDIR /data 9EXPOSE 8080 10 11CMD [\u0026#34;/data/miniserve\u0026#34;,\u0026#34;--index\u0026#34;,\u0026#34;index.html\u0026#34;] 12#CMD /bin/sh -c \u0026#34;while true; do echo hi; sleep 10; done\u0026#34; 作出镜像后推送到阿里镜像：registry.cn-shanghai.aliyuncs.com/rendoumi/miniserve:lastest\n然后准备好 deploy和 svc，部署到 kubernetes 里\n1---2apiVersion:apps/v13kind:Deployment4metadata:5name:miniserve-deploy6labels:7app:miniserve8spec:9replicas:110selector:11matchLabels:12app:miniserve13template:14metadata:15labels:16app:miniserve17spec:18containers:19- name:miniserve20image:registry.cn-shanghai.aliyuncs.com/rendoumi/miniserve:lastest21imagePullPolicy:Always22ports:23- containerPort:808024---25apiVersion:v126kind:Service27metadata:28name:miniserve-svc29labels:30app:miniserve31spec:32ports:33- name:http34protocol:TCP35port:8036targetPort:808037selector:38app:miniserve39type:ClusterIP二、修改ingress 修改的地方就是 annotations 的最后两行：\n1apiVersion:networking.k8s.io/v1beta1 2kind:Ingress 3metadata:4name:kala-com-ingress 5annotations:6kubernetes.io/ingress.class:\u0026#34;nginx\u0026#34;7nginx.ingress.kubernetes.io/limit-connections:\u0026#34;2\u0026#34;8nginx.ingress.kubernetes.io/limit-rpm:\u0026#34;2\u0026#34;9nginx.ingress.kubernetes.io/limit-rps:\u0026#34;1\u0026#34;10nginx.ingress.kubernetes.io/app-root:\u0026#34;/webui/\u0026#34;11nginx.ingress.kubernetes.io/auth-type:basic 12nginx.ingress.kubernetes.io/auth-secret:kala-auth 13nginx.ingress.kubernetes.io/auth-realm:\u0026#39;Authentication Required - Kala\u0026#39;14nginx.ingress.kubernetes.io/default-backend:\u0026#34;miniserve-svc\u0026#34;15nginx.ingress.kubernetes.io/custom-http-errors:\u0026#34;403,404,500,502,503,504\u0026#34;16spec:17rules:18- host:kala.rendoumi.com 19http:20paths:21- path:/ 22backend:23serviceName:kala-svc 24servicePort:80测试的话用 wrk 压住，然后curl再测就503转到miniserve-svc\n1wrk -c 3 -t 3 -d 10 http://kala.rendoumi.com/webui/ --latency 2 3curl -v -H \u0026#34;Host: kala.rendoumi.com\u0026#34; http://kala.rendoumi.com/webui/ ","date":"2021-11-29","img":"","permalink":"https://zhangrr.github.io/posts/20211129-kubernetes_custom_503/","series":null,"tags":null,"title":"Kubernetes下定制服务器503以及其他的403消息"},{"categories":null,"content":"在 kubernetes 中，ingress 负责转发、融断和限流。\n我们以 Nginx ingress 为例，讨论一下这方面的问题。\n一、Nginx ingress黑白名单 这个很简单了，丢进黑名单或者白名单，在入口前拦一刀。\n我们只要在 annotations 声明即可：\n1apiVersion:networking.k8s.io/v1beta1 2kind:Ingress 3metadata:4name:www-com-ingress 5annotations:6kubernetes.io/ingress.class:nginx 7nginx.ingress.kubernetes.io/ssl-redirect:\u0026#34;true\u0026#34;8nginx.ingress.kubernetes.io/block-cidrs:a.b.c.d/32 9#nginx.ingress.kubernetes.io/whitelist-source-range: a.b.c.d/32 10spec:11tls:12- hosts:13- www.huabbao.com 14secretName:www-huabbao-com-cert 15rules:16- host:www.huabbao.com 17http:18paths:19- path:/ 20backend:21serviceName:nginx-svc 注意，多个ip的话，之间用逗号隔开（该值是逗号分隔的CIDR列表） ：\n1nginx.ingress.kubernetes.io/whitelist-source-range:\u0026#39;58.246.36.130,180.167.74.98,114.85.176.38’ 二、Nginx ingress限速 限速这个问题比较复杂，首先我们需要考虑到用户体验，如果很鲁棒性的强制限制客户端流量是10KB，那么如果一开始的加载页面如果较大，客户直接就离场了。所以首先要考虑要有初始字节量，这个量需要能让客户顺畅的打开首页。然后我们再设定请求速率，再考虑单个IP地址的并发连接量（并发量×速率=客户端实际速率），第四个再考虑每分钟或每秒的最大请求数。\n以下注释定义按顺序排列，可以设置单个客户端IP地址打开的连接的限制，可用于缓解DDoS攻击。\n 一、nginx.ingress.kubernetes.io/limit-rate-after：设置初始字节量，在此之后，进一步传输将受到limit-rate速率限制。   二、nginx.ingress.kubernetes.io/limit-rate：每秒接受的请求速率（每秒字节数）。 三、nginx.ingress.kubernetes.io/limit-connections：来自单个IP地址的并发连接数。（可以建立最大连接数） 四、nginx.ingress.kubernetes.io/limit-rps：每秒可从给定IP接受的连接数。（每个源 IP 每个源 IP 每秒最大请求次数） 五、nginx.ingress.kubernetes.io/limit-rpm：每分钟可从给定IP接受的连接数。（每个源 IP 每分钟最大请求次数）  可以指定 nginx.ingress.kubernetes.io/limit-whitelist 设置从速率限制中排除的客户端IP源范围，该值也是逗号分隔的CIDR列表。\n如果在单个Ingress规则中设置了多个注释 limit-rpm，则 limit-rps 优先。\n1apiVersion:extensions/v1beta1 2kind:Ingress 3metadata:4name:my-ingress 5annotations:6nginx.ingress.kubernetes.io/limit-rate-after:500k7nginx.ingress.kubernetes.io/limit-rate:50k8nginx.ingress.kubernetes.io/limit-connections:\u0026#34;3\u0026#34;9nginx.ingress.kubernetes.io/limit-rps:\u0026#34;1\u0026#34;10nginx.ingress.kubernetes.io/limit-rpm:\u0026#34;3\u0026#34;11nginx.ingress.kubernetes.io/limit-whitelist:\u0026#34;192.168.7.0/24\u0026#34;12spec:13rules:14- host:my.test.com 15http:16paths:17- path:/ 18backend:19serviceName:nginx 20servicePort:80解释一下：\n首先设置了初始字节量是500k\n然后初始字节量用完之后，限制用户传输速率是50k/s\n随后并发连接设置为3，那么客户端如果同时打开3个连接，总速率是 50k×3=150k/s\n第四步进一步设置了每秒最大请求次数是1，下面又设置每分钟最大请求次数是3。\n第五步设置了 192.168.7.0/24 这段不受限速的影响\n设置好以后压测试一下：\n超限返回的代码是503\n1wrk -c 3 -t 3 -d 10 http://m.test.com --latency 2 3 4curl -v -H \u0026#34;Host: m.test.com\u0026#34; http://m.test.com 5\u0026lt; HTTP/1.1 503 Service Temporarily Unavailable 6\u0026lt; Date: Wed, 28 Jul 2021 04:32:03 GMT 7\u0026lt; Content-Type: text/html 8\u0026lt; Content-Length: 190 9\u0026lt; Connection: keep-alive 10HTTP/1.1 503 Service Temporarily Unavailable ","date":"2021-11-29","img":"","permalink":"https://zhangrr.github.io/posts/20211129-kubernetes_nginx_ingress_limit/","series":null,"tags":null,"title":"Kubernetes下nginx Ingress的限制"},{"categories":null,"content":"阿里的云的 OSS 并不是完全版本的 AWS S3 兼容。\n我们如果需要用 S3 协议访问 OSS，就比较麻烦了。\n所以搭建一个 minio 来做网关，代理OSS，minio 是基本兼容S3的，所以这样曲线救国，通过 S3 协议访问 minio 来访问最后端的 OSS\n这里还有一段故事：\nMinio 中间有一版是支持 oss 的，但是后来 oss 改了协议，所以现在的最新版本 minio 反而是不支持代理 oss 的，我们必须手动作出镜像，放到镜像库里，然后阿里 ACK 再使用\n首先去下载那一版直接支持oss的\n1wget http://dl.minio.org.cn/server/minio/release/linux-amd64/archive/minio.RELEASE.2020-04-15T19-42-18Z 2 3chmod 755 minio.RELEASE.2020-04-15T19-42-18Z 这个文件比较宝贵，给个本地备份链接下载：\nminio.RELEASE.2020-04-15T19-42-18Z 然后在当前目录编辑 Dockerfile ，因为 K8S 和 OSS 同一地域，所以用 OSS 私网域名：\n1FROM alpine:3.12  2 3RUN apk add --update bash \u0026amp;\u0026amp; rm -rf /var/cache/apk/*  4 5COPY minio.RELEASE.2020-04-15T19-42-18Z /data/minio.RELEASE.2020-04-15T19-42-18Z  6 7ENV MINIO_ACCESS_KEY=LTAI5tFFTbsxxxxxuLb  8ENV MINIO_SECRET_KEY=t78PyGnHZilxxxxxdxBCjvNgtVC5Y  9 10WORKDIR /data  11EXPOSE 9000  12 13CMD [\u0026#34;/data/minio.RELEASE.2020-04-15T19-42-18Z\u0026#34;,\u0026#34;gateway\u0026#34;,\u0026#34;oss\u0026#34;,\u0026#34;http://oss-cn-shanghai-internal.aliyuncs.com\u0026#34;]  14# CMD /bin/sh -c \u0026#34;while true; do echo hi; sleep 10; done\u0026#34; 注意上面，MINIO_ACCESS_KEY 和 MINIO_SECRET_KEY 对应的是阿里云 OSS的 AccessKey ID 和 AccessKey Secret\n打开阿里云网址，新建AccessKey ID 和 AccessKey Secret ，注意这东西只能看见一次，之后再也不能明文看了，所以第一次务必保存好！！！\nhttps://ram.console.aliyun.com/manage/ak 然后还有 Dockerfile 的最后一行，八戒的习惯是保留一个死循环 shell，如果镜像 CMD 有问题，无法启动，就换成这个先启动，然后再进去调试。（经常有什么库错、链接搞不好需要修改）\ndocker build -t registry.cn-shanghai.aliyuncs.com/rendoumi/minio . \n然后 push 上去\ndocker push registry.cn-shanghai.aliyuncs.com/rendoumi/minio \n编写好Deployment和svc，如果想公开还可以写 ingress 向外暴露，自己公司用还是 port-forward 更安全\n1--- 2apiVersion: apps/v1  3kind: Deployment  4metadata:  5 name: minio-deploy  6 labels:  7 app: minio  8spec:  9 replicas: 1  10 selector:  11 matchLabels:  12 app: minio  13 template:  14 metadata:  15 labels:  16 app: minio  17 spec:  18 containers:  19 - name: minio  20 image: registry.cn-shanghai.aliyuncs.com/rendoumi/minio:latest  21 ports:  22 - containerPort: 9000  23--- 24apiVersion: v1  25kind: Service  26metadata:  27 name: minio-svc  28 labels:  29 app: minio  30spec:  31 ports:  32 - name: http  33 protocol: TCP  34 port: 9000  35 targetPort: 9000  36 selector:  37 app: minio  38 type: ClusterIP  然后开转发：\n1kubectl port-forward svc/minio-svc 9000:9000 \u0026amp; 用浏览器访问 http://localhost:9000 就可以了，还得输入一遍密码\n这样就可以看到 OSS 的所有桶了\nMinio的官方命令行客户端是 mc，使用方法如下：\n1wget https://dl.min.io/client/mc/release/linux-amd64/mc 2chmod 755 mc 3 4./mc alias set minio http://minio-svc:9000 MINIO_ACCESS_KEY 和 MINIO_SECRET_KEY 5 6./mc ls minio minio 跑在容器外进行 OSS 代理的方法，注意不同地方，OSS 的域名改为公网的了：\n1#!/bin/sh 2export MINIO_ACCESS_KEY=LTAI5tFFTbsxxxxxuLb 3export MINIO_SECRET_KEY=t78PyGnHZilxxxxxdxBCjvNgtVC5Y 4nohup ./minio.RELEASE.2020-04-15T19-42-18Z gateway oss http://oss-cn-shanghai.aliyuncs.com \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 \u0026amp; ","date":"2021-11-26","img":"","permalink":"https://zhangrr.github.io/posts/20211126-kubernetes_minio_oss_gateway/","series":null,"tags":null,"title":"Kubernetes搭建minio作为阿里OSS的Gateway"},{"categories":null,"content":"本来 k8s 的 crontab 是启动一个容器来运行的，很简单，如下：\n1apiVersion:batch/v1beta12kind:CronJob3metadata:4name:curl-cron5spec:6schedule:\u0026#34;*/1 * * * *\u0026#34;7jobTemplate:8spec:9template:10spec:11containers:12- name:curl-cron13image:radial/busyboxplus:curl14imagePullPolicy:IfNotPresent15command:16- /bin/sh17- -c18- date;echo \u0026#34;run crontab\u0026#34;;curl http://www.baidu.com19restartPolicy:OnFailure20successfulJobsHistoryLimit:221failedJobsHistoryLimit:2上面就跑了一个 busybox 的 pod，每分钟去访问百度，然后在 stdout 输出结果\n这个没什么，注意上面文件的最后两行。限制成功以及失败 job 的 History 数量，如果不加限制，kubectl get pods 会看到无穷无尽的completed 状态的 curl-cron pod。\n  分割线，部署 crontab 的 yaml 很简单。但是，现在产品部有个需求，他们要在某一天进行促销活动，大概2天，期间会流量大增，于是想应用 hpa 来伸缩 pod，活动结束后关闭 hpa。之后他们还会有不断的促销，研发不知道什么时候开始，什么时候结束；但是研发又不想产品部看到 yaml，并且随时配合部署 hpa。\n还有个问题，如果用crontab，那么活动开始运行一次，活动结束一次，最后研发还得清理删除掉这两个 crontab 来恢复正常。\n这下麻烦了，有什么页面管理 crontab 的神器，能让产品部自己运行，然后权限分离，而且能指定再未来的某天运行一次或多次，且不用手工清理就好了。\n还真的有一个，kala 就是了！！！\nKala 是一个cron管理配置工具，项目地址：\nhttps://github.com/ajvb/kala 卡拉是基于 Airbnb 的 Chronos 翻写的 Go 程序。\n它比 crontab 更好一些的是，可以指定未来某天的一次性执行任务，有个使用界面\nKala 可以执行本地的命令，也可以发起 http 的请求。\n注意，kala 的源代码里有一个地方需要修改，否则参数bolt-path不生效，我们改掉它自己编译个2进制程序出来。\n1Modify cmd/server.go 2 3switch viper.GetString(\u0026#34;jobdb\u0026#34;) { 4case \u0026#34;boltdb\u0026#34;: 5db = boltdb.GetBoltDB(viper.GetString(\u0026#34;boltpath\u0026#34;)) 6 7Change to 8db = boltdb.GetBoltDB(viper.GetString(\u0026#34;bolt-path\u0026#34;)) 9 10Run it : 11./kala serve --jobdb=boltdb --bolt-path=/data/db 我们用 Dockerfile 造一个镜像出来：\n1FROM alpine:3.12 2RUN apk add --update bash \u0026amp;\u0026amp; rm -rf /var/cache/apk/* 3 4COPY . /data/ 5 6RUN mkdir /lib64 \u0026amp;\u0026amp; \\ 7 ln -s /lib/libc.musl-x86_64.so.1 /lib64/ld-linux-x86-64.so.2 \u0026amp;\u0026amp; \\ 8 mv /data/kubectl /bin \u0026amp;\u0026amp; \\ 9 mv /data/.kube /root \u0026amp;\u0026amp; \\ 10 rm -rf /data/Dockerfile 11 12WORKDIR /data 13EXPOSE 8000 14 15CMD [\u0026#34;/data/kala\u0026#34;, \u0026#34;serve\u0026#34;,\u0026#34;--jobdb=boltdb\u0026#34;,\u0026#34;--bolt-path=/data/db\u0026#34;] 16#CMD /bin/sh -c \u0026#34;while true; do echo hi; sleep 10; done\u0026#34; 注意上面我们把 kubectl 和 .kube 配置一起放进了镜像，当然一些 yaml 和脚本也可以都打进去，然后推到阿里云镜像仓库去（注意安全问题）。\n备注 ：kala默认的端口是8000，访问 url 是 /webui\n然后我们生成一个 secret 放进 k8s\n1htpasswd -c auth kala-auth 2New password: 3Re-type new password: 4Adding password for user kala 5 6kubectl create secret generic kala-auth --from-file=kala-auth 7secret \u0026#34;kala-auth\u0026#34; created 8 9kubectl get secret kala-auth -o yaml 然后我们定义一系列的资源文件，Deployment、SVC、Ingress，其中 ingress 引用了 kala-auth 的 secret，并且指定了 app-root 是 /webui/。\n另外 kala 使用了 k8s-kala-5g 的持久化卷，务必事先准备好 pv 和 pvc，不持久化的话 pod 一重启，之前保存的配置信息就全没了。\n1---2apiVersion:apps/v13kind:Deployment4metadata:5name:kala-deploy6labels:7app:kala8spec:9replicas:110selector:11matchLabels:12app:kala13template:14metadata:15labels:16app:kala17spec:18containers:19- name:kala20image:registry.cn-shanghai.aliyun.com/xxx/kala:latest21imagePullPolicy:Always22ports:23- containerPort:800024volumeMounts:25- mountPath:/data/db26name:kala-data27volumes:28- name:kala-data29persistentVolumeClaim:30claimName:k8s-kala-5g31---32apiVersion:v133kind:Service34metadata:35name:kala-svc36labels:37app:kala38spec:39ports:40- name:http41protocol:TCP42port:800043targetPort:800044selector:45app:kala46type:ClusterIP47---48apiVersion:networking.k8s.io/v1beta149kind:Ingress50metadata:51name:kala-com-ingress52annotations:53kubernetes.io/ingress.class:nginx54nginx.ingress.kubernetes.io/block-cidrs:111.201.134.93/3255nginx.ingress.kubernetes.io/auth-type:basic56nginx.ingress.kubernetes.io/auth-secret:kala-auth57nginx.ingress.kubernetes.io/auth-realm:\u0026#39;Authentication Required - Kala\u0026#39;58nginx.ingress.kubernetes.io/app-root:\u0026#39;/webui/\u0026#39;59spec:60rules:61- host:kala.rendoumi.com62http:63paths:64- path:/65backend:66serviceName:kala-svc67servicePort:80这样 kala 就可以从公网访问了，http://kala.rendoumi.com:8000/ ，且需要提供密码才能进入。\n下面说一下具体的用法，还是有一些技巧性的，登录进去后的页面：\n进入后左上角点击 Create 建立新任务\n要填写内容如下：\n  Job name：起个名字，叫：增大HPA\n  Command：要运行的脚本：/data/resize.sh 35 (固定扩大ECI的脚本)\n  Schedule：R0/2021-07-07T06:00:00+08:00/PT0S\n  ​ 这个最重要\n​ 这行的格式是这样的：Number of times to repeat/Start Datetime/Interval Between Runs\n​ R0 重复0次，即只执行一次，不重复\n​ 2021-07-06T06:00:00+8:00 时间戳，注意时区中国+8:00\n​ PT0S 无延迟0S启动\n​ 合起来就是\n​ R0/2021-07-07T06:00:00+08:00/PT0S 就是在2021年7月7日中国时间6点无任何延迟执行一次脚本 resize.sh\n Reset Form 这个一定要选掉，保持空，否则下次进来上次的数据都没了，还得手动输入，麻烦  其他都不填写，然后 Create 就行了\n然后我们可以再建一个恢复的任务\n我们去界面就可以看到多了两个任务，增大HPA和恢复HPA，这样就 OK 了\n注意：JOB一旦建立，以后就可以直接从界面上手动执行，Run Manually\n这东西真是个神器，产品部的同事可以随时修改hpa和恢复，而不用通过研发部了，善莫大焉。\n","date":"2021-11-25","img":"","permalink":"https://zhangrr.github.io/posts/20211125-kubernetes_crontab_kala/","series":null,"tags":null,"title":"替代kubernetes Crontab的神器kala"},{"categories":null,"content":"kubernetes 的动态伸缩 HPA 是非常有用的特性。\n我们的服务器托管在阿里云的 ACK 上，k8s 根据 cpu 或者 内存的使用情况，会自动伸缩关键 pod 的数量，以应对大流量的情形。而且更妙的是，动态扩展的 pod 并不是使用自己的固定服务器，而是使用阿里动态的 ECI 虚拟节点服务器，这样就真的是即开即用，用完即毁。有多大流量付多少钱，物尽其用。\n我们先明确一下概念：\nk8s 的资源指标获取是通过 api 接口来获得的，有两种 api，一种是核心指标，一种是自定义指标。\n  核心指标：Core metrics，由metrics-server提供API，即 metrics.k8s.io，仅提供Node和Pod的CPU和内存使用情况。api 是 metrics.k8s.io\n  自定义指标：Custom Metrics，由Prometheus Adapter提供API，即 custom.metrics.k8s.io，由此可支持任意Prometheus采集到的自定义指标。api 是 custom.metrics.k8s.io 和 external.metrics.k8s.io\n  一、核心指标metrics-server 阿里的 ACK 缺省是装了 metrics-server 的，看一下，系统里有一个metrics-server\n1kubectl get pods -n kube-system 再看看 api 的核心指标能拿到什么，先看 Node 的指标：\n1kubectl get --raw \u0026#34;/apis/metrics.k8s.io\u0026#34; | jq . 2kubectl get --raw \u0026#34;/apis/metrics.k8s.io/v1beta1\u0026#34; | jq . 3kubectl get --raw \u0026#34;/apis/metrics.k8s.io/v1beta1/nodes\u0026#34; | jq . 可以看到阿里 eci 虚拟节点的 cpu 和 memory 资源。\n再看看 Pod 的指标：\n1kubectl get --raw \u0026#34;/apis/metrics.k8s.io/v1beta1/pods\u0026#34; | jq . 可以清楚得看到调度到虚拟节点上的 Pod 的 cpu 和 memory 资源使用情况。大家看到只有 cpu 和 memory，这就够了。\n给个全部使用自己 Work Node 实体节点伸缩的例子：\nphp-hpa.yaml (这里定义了平均cpu使用到达80%或者内存平均使用到200M就伸缩）：\nphp-hpa 规范了 php-deploy 如何伸缩，最小2，最大10：\n1apiVersion: autoscaling/v2beta1  2kind: HorizontalPodAutoscaler  3metadata:  4 name: php-hpa 5 namespace: default 6spec:  7 scaleTargetRef:  8 apiVersion: extensions/v1beta1  9 kind: Deployment  10 name: php-deploy 11 minReplicas: 2  12 maxReplicas: 10  13 metrics:  14 - type: Resource  15 resource:  16 name: cpu  17 targetAverageUtilization: 80  18 - type: Resource  19 resource:  20 name: memory  21 targetAverageValue: 200Mi 再给个阿里 ACK 使用 ECI 虚拟节点伸缩的例子：\n我们首先定义一个正常的 deployment ，php-deploy，这个就正常定义跟别的没区别。\n然后再定义扩展到 eci 节点的 ElasticWorload，elastic-php，这个用来控制 php-deploy 拓展到 eci 虚拟节点去\n下面是固定6个，动态24个，合计30个\n1apiVersion: autoscaling.alibabacloud.com/v1beta1 2kind: ElasticWorkload 3metadata: 4 name: elastic-php 5spec: 6 sourceTarget: 7 name: php-deploy 8 kind: Deployment 9 apiVersion: apps/v1 10 min: 0 11 max: 6 12 elasticUnit: 13 - name: virtual-kubelet 14 labels: 15 virtual-kubelet: \u0026#34;true\u0026#34; 16 alibabacloud.com/eci: \u0026#34;true\u0026#34; 17 annotations: 18 virtual-kubelet: \u0026#34;true\u0026#34; 19 nodeSelector: 20 type: \u0026#34;virtual-kubelet\u0026#34; 21 tolerations: 22 - key: \u0026#34;virtual-kubelet.io/provider\u0026#34; 23 operator: \u0026#34;Exists\u0026#34; 24 min: 0 25 max: 24 26 replicas: 30 然后定义 HPA php-hpa 来控制 elastic-php 的自动伸缩\n1apiVersion: autoscaling/v2beta2  2kind: HorizontalPodAutoscaler  3metadata:  4 name: php-hpa  5 namespace: default  6spec:  7 scaleTargetRef:  8 apiVersion: autoscaling.alibabacloud.com/v1beta1  9 kind: ElasticWorkload  10 name: elastic-php  11 minReplicas: 6  12 maxReplicas: 30  13 metrics:  14 - type: Resource  15 resource:  16 name: cpu  17 target:  18 type: Utilization  19 averageUtilization: 90  20 behavior:  21 scaleUp:  22 policies:  23 #- type: percent  24 # value: 500%  25 - type: Pods  26 value: 5  27 periodSeconds: 180  28 scaleDown:  29 policies:  30 - type: Pods  31 value: 1  32 periodSeconds: 600  上面的 ElasticWorkload 需要仔细解释一下，php-deploy 定义的 pod 副本固定是6个，这6个都是在我们自己节点上不用再付费，然后 ECI 的 pod 副本数是0个到24个，那么总体pod数量就是 6+24 = 30 个，其中 24个是可以在虚拟节点上伸缩的。而 php-hpa 定义了伸缩范围是 6-30，那就意味着平时流量小的时候用的都是自己服务器上那6个固定 pod，如果流量大了，就会扩大到 eci 虚拟节点上，虚拟节点最大量是24个，如果流量降下来了，就会缩回去，缩到自己服务器的6个pod 上去。这样可以精确控制成本。\nphp-hpa 定义的最下面，扩大的时候如果 cpu 到了 90%，那么一次性扩大5个pod，缩小的时候一个一个缩，这样避免带来流量的毛刺。\n二、自定义指标Prometheus 如上其实已经可以满足大多数要求了，但是想更进一步，比如想从 Prometheus 拿到的指标来进行 hpa 伸缩。\n那就比较麻烦了\n看上图，Prometheus Operator 通过 http 拉取 pod 的 metric 指标，Prometheus Adaptor 再拉取 Prometheus Operator 存储的数据并且暴露给 Custom API 使用。为啥要弄这二个东西呢？因为 Prometheus 采集到的 metrics 数据并不能直接给 k8s 用，因为两者数据格式不兼容，还需要另外一个组件(kube-state-metrics)，将prometheus 的 metrics 数据格式转换成 k8s API 接口能识别的格式，转换以后，因为是自定义API，所以还需要用 Kubernetes aggregator 在主 API 服务器中注册，以便其他程序直接通过 /apis/ 来访问。\n我们首先来看看如何安装这二个东西：\n首先装 Prometheus Operator，这家伙会自动装一捆东西， Pormetheus、Grafana、Alert manager等，所以最好给它单独弄一个命名空间\n prometheus-operator  prometheus  alertmanager  node-exporter  kube-state-metrics  grafana   1#安装 2helm install --name prometheus --namespace monitoring stable/prometheus-operator 3 4#开个端口本地访问 prometheus 的面板，curl http://localhost:9090 5kubectl port-forward --namespace monitoring svc/prometheus-operator-prometheus 9090:9090 6 7 8#看看都有什么pod 9kubectl get pod -n monitoring 10NAME READY STATUS RESTARTS AGE 11pod/alertmanager-prometheus-operator-alertmanager-0 2/2 Running 0 98m 12pod/prometheus-operator-grafana-857dfc5fc8-vdnff 2/2 Running 0 99m 13pod/prometheus-operator-kube-state-metrics-66b4c95cd9-mz8nt 1/1 Running 0 99m 14pod/prometheus-operator-operator-56964458-8sspk 2/2 Running 0 99m 15pod/prometheus-operator-prometheus-node-exporter-dcf5p 1/1 Running 0 99m 16pod/prometheus-operator-prometheus-node-exporter-nv6ph 1/1 Running 0 99m 17pod/prometheus-prometheus-operator-prometheus-0 3/3 Running 1 98m 18 19#看看都有什么svc 20kubectl get svc -n monitoring 21NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE 22alertmanager-operated ClusterIP None \u0026lt;none\u0026gt; 9093/TCP,9094/TCP,9094/UDP 100m 23prometheus-operated ClusterIP None \u0026lt;none\u0026gt; 9090/TCP 100m 24prometheus-operator-alertmanager NodePort 10.1.238.78 \u0026lt;none\u0026gt; 9093:31765/TCP 102m 25prometheus-operator-grafana NodePort 10.1.125.228 \u0026lt;none\u0026gt; 80:30284/TCP 102m 26prometheus-operator-kube-state-metrics ClusterIP 10.1.187.129 \u0026lt;none\u0026gt; 8080/TCP 102m 27prometheus-operator-operator ClusterIP 10.1.242.61 \u0026lt;none\u0026gt; 8080/TCP,443/TCP 102m 28prometheus-operator-prometheus NodePort 10.1.156.181 \u0026lt;none\u0026gt; 9090:30268/TCP 102m 29prometheus-operator-prometheus-node-exporter ClusterIP 10.1.226.134 \u0026lt;none\u0026gt; 9100/TCP 102m 我们看到有 prometheus-operated 这个ClusterIP svc，注意 k8s 的 coredns 域名解析方式，集群的内部域名是 hbb.local，那么这个 svc 的全 hostname 就是 prometheus-operated.monitoring.svc.hbb.local，集群中可以取舍到 prometheus-operated.monitoring 或者 prometheus-operated.monitoring.svc 来访问。\n然后再装 Prometheus Adaptor，我们要根据上面的具体情况来设置 prometheus.url：\n1helm install --name prometheus-adapter stable/prometheus-adapter --set prometheus.url=\u0026#34;http://prometheus-operated.monitoring.svc\u0026#34;,prometheus.port=\u0026#34;9090\u0026#34; --set image.tag=\u0026#34;v0.4.1\u0026#34; --set rbac.create=\u0026#34;true\u0026#34; --namespace custom-metrics 2 访问 external 和 custom 验证一下：\n1kubectl get --raw \u0026#34;/apis/external.metrics.k8s.io/v1beta1\u0026#34; | jq 2{ 3 \u0026#34;kind\u0026#34;: \u0026#34;APIResourceList\u0026#34;, 4 \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, 5 \u0026#34;groupVersion\u0026#34;: \u0026#34;external.metrics.k8s.io/v1beta1\u0026#34;, 6 \u0026#34;resources\u0026#34;: [] 7} 8 9kubectl get --raw \u0026#34;/apis/custom.metrics.k8s.io/v1beta1\u0026#34; | jq 10{ 11 \u0026#34;kind\u0026#34;: \u0026#34;APIResourceList\u0026#34;, 12 \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, 13 \u0026#34;groupVersion\u0026#34;: \u0026#34;custom.metrics.k8s.io/v1beta1\u0026#34;, 14 \u0026#34;resources\u0026#34;: [ 15 { 16 \u0026#34;name\u0026#34;: \u0026#34;*/agent.googleapis.com|agent|api_request_count\u0026#34;, 17 \u0026#34;singularName\u0026#34;: \u0026#34;\u0026#34;, 18 \u0026#34;namespaced\u0026#34;: true, 19 \u0026#34;kind\u0026#34;: \u0026#34;MetricValueList\u0026#34;, 20 \u0026#34;verbs\u0026#34;: [ 21 \u0026#34;get\u0026#34; 22 ] 23 }, 24[...lots more metrics...] 25 { 26 \u0026#34;name\u0026#34;: \u0026#34;*/vpn.googleapis.com|tunnel_established\u0026#34;, 27 \u0026#34;singularName\u0026#34;: \u0026#34;\u0026#34;, 28 \u0026#34;namespaced\u0026#34;: true, 29 \u0026#34;kind\u0026#34;: \u0026#34;MetricValueList\u0026#34;, 30 \u0026#34;verbs\u0026#34;: [ 31 \u0026#34;get\u0026#34; 32 ] 33 } 34 ] 35} 重头戏在下面，其实 Prometheus Adaptor 从 Prometheus 拿的指标也是有限的，如果有自定义指标，或者想多拿些，就得继续拓展！！！\n最快的方式是编辑 namespace 是 custom-metrics 下的 configmap ，名字是 Prometheus-adapter，增加seriesQuery\nseriesQuery长这样子，下面是统计了所有 app=shopping-kart 的 pod 5分钟之内的变化速率总和：\n1apiVersion: v1 2kind: ConfigMap 3metadata: 4 labels: 5 app: prometheus-adapter 6 chart: prometheus-adapter-v1.2.0 7 heritage: Tiller 8 release: prometheus-adapter 9 name: prometheus-adapter 10data: 11 config.yaml: | 12- seriesQuery: \u0026#39;{app=\u0026#34;shopping-kart\u0026#34;,kubernetes_namespace!=\u0026#34;\u0026#34;,kubernetes_pod_name!=\u0026#34;\u0026#34;}\u0026#39; 13seriesFilters: [] 14resources: 15overrides: 16kubernetes_namespace: 17resource: namespace 18kubernetes_pod_name: 19resource: pod 20name: 21matches: \u0026#34;\u0026#34; 22as: \u0026#34;\u0026#34; 23metricsQuery: sum(rate(\u0026lt;\u0026lt;.Series\u0026gt;\u0026gt;{\u0026lt;\u0026lt;.LabelMatchers\u0026gt;\u0026gt;}[5m])) by (\u0026lt;\u0026lt;.GroupBy\u0026gt;\u0026gt;) 24 Adapter configmap的配置如下：\n  seriesQuery tells the Prometheus Metric name to the adapter（要去prometheus拿什么指标）\n  resources tells which Kubernetes resources each metric is associated with or which labels does the metric include, e.g., namespace, pod etc.（关联的资源，最常用的就是 pod 和 namespace）\n  metricsQuery is the actual Prometheus query that needs to be performed to calculate the actual values.（叠加 seriesQuery 后发送给prometheus的实际查询，用于得出最终的指标值）\n  name with which the metric should be exposed to the custom metrics API（暴露给API的指标名）\n  举个例子：如果我们要计算 container_network_receive_packets_total，在Prometheus UI里我们要输入以下行来查询:\nsum(rate(container_network_receive_packets_total{namespace=\u0026ldquo;default\u0026rdquo;,pod=~\u0026ldquo;php-deploy.*\u0026quot;}[10m])) by (pod)*\n转换成 Adapter 的 metricsQuery 就变成这样了，很难懂：\n*metricsQuery: \u0026lsquo;sum(rate(\u0026laquo;.series\u0026raquo;{\u0026laquo;.labelmatchers\u0026raquo;}10m])) by (\u0026laquo;.groupby\u0026raquo;)'\u0026lt;/.groupby\u0026gt;\u0026lt;/.labelmatchers\u0026gt;\u0026lt;/.series\u0026gt;*\n再给个例子：\n1rate(gorush_total_push_count{instance=\u0026#34;push.server.com:80\u0026#34;,job=\u0026#34;push-server\u0026#34;}[5m]) 变成 adapter 的 configmap\n1apiVersion: v1 2data: 3 config.yaml: | 4 rules: 5 - seriesQuery: \u0026#39;{__name__=~\u0026#34;gorush_total_push_count\u0026#34;}\u0026#39; 6seriesFilters: [] 7resources: 8overrides: 9namespace: 10resource: namespace 11pod: 12resource: pod 13name: 14matches: \u0026#34;\u0026#34; 15as: \u0026#34;gorush_push_per_second\u0026#34; 16metricsQuery: rate(\u0026lt;\u0026lt;.Series\u0026gt;\u0026gt;{\u0026lt;\u0026lt;.LabelMatchers\u0026gt;\u0026gt;}[5m]) 修改了configmap，必须重启prometheus-adapter的pod重新加载配置！！！\n在hpa中应用的例子：\n1apiVersion: autoscaling/v2beta1 2kind: HorizontalPodAutoscaler 3metadata: 4 name: gorush-hpa 5spec: 6 scaleTargetRef: 7 apiVersion: apps/v1 8 kind: Deployment 9 name: gorush 10 minReplicas: 1 11 maxReplicas: 5 12 metrics: 13 - type: Pods 14 pods: 15 metricName: gorush_push_per_second 16 targetAverageValue: 1m 再来一个，prometheus 函数名是 myapp_client_connected：\n1apiVersion: v1 2data: 3 config.yaml: | 4 rules: 5 - seriesQuery: \u0026#39;{__name__= \u0026#34;myapp_client_connected\u0026#34;}\u0026#39; 6seriesFilters: [] 7resources: 8overrides: 9k8s_namespace: 10resource: namespace 11k8s_pod_name: 12resource: pod 13name: 14matches: \u0026#34;myapp_client_connected\u0026#34; 15as: \u0026#34;\u0026#34; 16metricsQuery: \u0026lt;\u0026lt;.Series\u0026gt;\u0026gt;{\u0026lt;\u0026lt;.LabelMatchers\u0026gt;\u0026gt;,container_name!=\u0026#34;POD\u0026#34;} hpa的使用\n1apiVersion: autoscaling/v2beta1 2kind: HorizontalPodAutoscaler 3metadata: 4 name: hpa-sim 5 namespace: default 6spec: 7 scaleTargetRef: 8 apiVersion: apps/v1 9 kind: Deployment 10 name: hpa-sim 11 minReplicas: 1 12 maxReplicas: 10 13 metrics: 14 - type: Pods 15 pods: 16 metricName: myapp_client_connected 17 targetAverageValue: 20 很复杂吧。我们下面给个详细例子\n三、自定义指标全套例子 我们先定义一个 deployment，运行一个 nginx-vts 的 pod，这个镜像其实已经自己暴露出了 metric 指标\n1apiVersion: apps/v1 2kind: Deployment 3metadata: 4 name: nginx-deploy 5 annotations: 6 prometheus.io/scrape: \u0026#34;true\u0026#34; 7 prometheus.io/port: \u0026#34;80\u0026#34; 8 prometheus.io/path: \u0026#34;/status/format/prometheus\u0026#34; 9spec: 10 selector: 11 matchLabels: 12 app: nginx-deploy 13 template: 14 metadata: 15 labels: 16 app: nginx-deploy 17 spec: 18 containers: 19 - name: nginx-deploy 20 image: cnych/nginx-vts:v1.0 21 resources: 22 limits: 23 cpu: 50m 24 requests: 25 cpu: 50m 26 ports: 27 - containerPort: 80 28 name: http 然后定义个 svc，把80端口暴露出去\n1apiVersion: v1 2kind: Service 3metadata: 4 name: nginx-svc 5spec: 6 ports: 7 - port: 80 8 targetPort: 80 9 name: http 10 selector: 11 app: nginx-deploy 12 type: ClusterIP prometheus 是自动发现的，所以 annotations 就会触发 prometheus 自动开始收集这些 nginx metric指标\n集群内起个shell，访问看看\n1$ curl nginx-svc.default.svc.hbb.local/status/format/prometheus 2# HELP nginx_vts_info Nginx info 3# TYPE nginx_vts_info gauge 4nginx_vts_info{hostname=\u0026#34;nginx-deployment-65d8df7488-c578v\u0026#34;,version=\u0026#34;1.13.12\u0026#34;} 1 5# HELP nginx_vts_start_time_seconds Nginx start time 6# TYPE nginx_vts_start_time_seconds gauge 7nginx_vts_start_time_seconds 1574283147.043 8# HELP nginx_vts_main_connections Nginx connections 9# TYPE nginx_vts_main_connections gauge 10nginx_vts_main_connections{status=\u0026#34;accepted\u0026#34;} 215 11nginx_vts_main_connections{status=\u0026#34;active\u0026#34;} 4 12nginx_vts_main_connections{status=\u0026#34;handled\u0026#34;} 215 13nginx_vts_main_connections{status=\u0026#34;reading\u0026#34;} 0 14nginx_vts_main_connections{status=\u0026#34;requests\u0026#34;} 15577 15nginx_vts_main_connections{status=\u0026#34;waiting\u0026#34;} 3 16nginx_vts_main_connections{status=\u0026#34;writing\u0026#34;} 1 17# HELP nginx_vts_main_shm_usage_bytes Shared memory [ngx_http_vhost_traffic_status] info 18# TYPE nginx_vts_main_shm_usage_bytes gauge 19nginx_vts_main_shm_usage_bytes{shared=\u0026#34;max_size\u0026#34;} 1048575 20nginx_vts_main_shm_usage_bytes{shared=\u0026#34;used_size\u0026#34;} 3510 21nginx_vts_main_shm_usage_bytes{shared=\u0026#34;used_node\u0026#34;} 1 22# HELP nginx_vts_server_bytes_total The request/response bytes 23# TYPE nginx_vts_server_bytes_total counter 24# HELP nginx_vts_server_requests_total The requests counter 25# TYPE nginx_vts_server_requests_total counter 26# HELP nginx_vts_server_request_seconds_total The request processing time in seconds 27# TYPE nginx_vts_server_request_seconds_total counter 28# HELP nginx_vts_server_request_seconds The average of request processing times in seconds 29# TYPE nginx_vts_server_request_seconds gauge 30# HELP nginx_vts_server_request_duration_seconds The histogram of request processing time 31# TYPE nginx_vts_server_request_duration_seconds histogram 32# HELP nginx_vts_server_cache_total The requests cache counter 33# TYPE nginx_vts_server_cache_total counter 34nginx_vts_server_bytes_total{host=\u0026#34;_\u0026#34;,direction=\u0026#34;in\u0026#34;} 3303449 35nginx_vts_server_bytes_total{host=\u0026#34;_\u0026#34;,direction=\u0026#34;out\u0026#34;} 61641572 36nginx_vts_server_requests_total{host=\u0026#34;_\u0026#34;,code=\u0026#34;1xx\u0026#34;} 0 37nginx_vts_server_requests_total{host=\u0026#34;_\u0026#34;,code=\u0026#34;2xx\u0026#34;} 15574 38nginx_vts_server_requests_total{host=\u0026#34;_\u0026#34;,code=\u0026#34;3xx\u0026#34;} 0 39nginx_vts_server_requests_total{host=\u0026#34;_\u0026#34;,code=\u0026#34;4xx\u0026#34;} 2 40nginx_vts_server_requests_total{host=\u0026#34;_\u0026#34;,code=\u0026#34;5xx\u0026#34;} 0 41nginx_vts_server_requests_total{host=\u0026#34;_\u0026#34;,code=\u0026#34;total\u0026#34;} 15576 42nginx_vts_server_request_seconds_total{host=\u0026#34;_\u0026#34;} 0.000 43nginx_vts_server_request_seconds{host=\u0026#34;_\u0026#34;} 0.000 44nginx_vts_server_cache_total{host=\u0026#34;_\u0026#34;,status=\u0026#34;miss\u0026#34;} 0 45nginx_vts_server_cache_total{host=\u0026#34;_\u0026#34;,status=\u0026#34;bypass\u0026#34;} 0 46nginx_vts_server_cache_total{host=\u0026#34;_\u0026#34;,status=\u0026#34;expired\u0026#34;} 0 47nginx_vts_server_cache_total{host=\u0026#34;_\u0026#34;,status=\u0026#34;stale\u0026#34;} 0 48nginx_vts_server_cache_total{host=\u0026#34;_\u0026#34;,status=\u0026#34;updating\u0026#34;} 0 49nginx_vts_server_cache_total{host=\u0026#34;_\u0026#34;,status=\u0026#34;revalidated\u0026#34;} 0 50nginx_vts_server_cache_total{host=\u0026#34;_\u0026#34;,status=\u0026#34;hit\u0026#34;} 0 51nginx_vts_server_cache_total{host=\u0026#34;_\u0026#34;,status=\u0026#34;scarce\u0026#34;} 0 52nginx_vts_server_bytes_total{host=\u0026#34;*\u0026#34;,direction=\u0026#34;in\u0026#34;} 3303449 53nginx_vts_server_bytes_total{host=\u0026#34;*\u0026#34;,direction=\u0026#34;out\u0026#34;} 61641572 54nginx_vts_server_requests_total{host=\u0026#34;*\u0026#34;,code=\u0026#34;1xx\u0026#34;} 0 55nginx_vts_server_requests_total{host=\u0026#34;*\u0026#34;,code=\u0026#34;2xx\u0026#34;} 15574 56nginx_vts_server_requests_total{host=\u0026#34;*\u0026#34;,code=\u0026#34;3xx\u0026#34;} 0 57nginx_vts_server_requests_total{host=\u0026#34;*\u0026#34;,code=\u0026#34;4xx\u0026#34;} 2 58nginx_vts_server_requests_total{host=\u0026#34;*\u0026#34;,code=\u0026#34;5xx\u0026#34;} 0 59nginx_vts_server_requests_total{host=\u0026#34;*\u0026#34;,code=\u0026#34;total\u0026#34;} 15576 60nginx_vts_server_request_seconds_total{host=\u0026#34;*\u0026#34;} 0.000 61nginx_vts_server_request_seconds{host=\u0026#34;*\u0026#34;} 0.000 62nginx_vts_server_cache_total{host=\u0026#34;*\u0026#34;,status=\u0026#34;miss\u0026#34;} 0 63nginx_vts_server_cache_total{host=\u0026#34;*\u0026#34;,status=\u0026#34;bypass\u0026#34;} 0 64nginx_vts_server_cache_total{host=\u0026#34;*\u0026#34;,status=\u0026#34;expired\u0026#34;} 0 65nginx_vts_server_cache_total{host=\u0026#34;*\u0026#34;,status=\u0026#34;stale\u0026#34;} 0 66nginx_vts_server_cache_total{host=\u0026#34;*\u0026#34;,status=\u0026#34;updating\u0026#34;} 0 67nginx_vts_server_cache_total{host=\u0026#34;*\u0026#34;,status=\u0026#34;revalidated\u0026#34;} 0 68nginx_vts_server_cache_total{host=\u0026#34;*\u0026#34;,status=\u0026#34;hit\u0026#34;} 0 69nginx_vts_server_cache_total{host=\u0026#34;*\u0026#34;,status=\u0026#34;scarce\u0026#34;} 0 然后用 wrk 随机发狂发请求压一把，我们去 prometheus 的面板看看指标被收集到没有\n很疯狂啊。我们编辑 Prometheus-Adapter 的 configmap ，加上如下内容\n1rules: 2- seriesQuery: \u0026#39;nginx_vts_server_requests_total\u0026#39; 3 seriesFilters: [] 4 resources: 5 overrides: 6 kubernetes_namespace: 7 resource: namespace 8 kubernetes_pod_name: 9 resource: pod 10 name: 11 matches: \u0026#34;^(.*)_total\u0026#34; 12 as: \u0026#34;${1}_per_second\u0026#34; 13 metricsQuery: (sum(rate(\u0026lt;\u0026lt;.Series\u0026gt;\u0026gt;{\u0026lt;\u0026lt;.LabelMatchers\u0026gt;\u0026gt;}[1m])) by (\u0026lt;\u0026lt;.GroupBy\u0026gt;\u0026gt;)) 然后杀了 Prometheus-Adapter 的 Pod 让它重启重新加载配置，过段时间访问一下，看看值，是527m\n1kubectl get --raw \u0026#34;/apis/custom.metrics.k8s.io/v1beta1/namespaces/default/pods/*/nginx_vts_server_requests_per_second\u0026#34; | jq . 2{ 3 \u0026#34;kind\u0026#34;: \u0026#34;MetricValueList\u0026#34;, 4 \u0026#34;apiVersion\u0026#34;: \u0026#34;custom.metrics.k8s.io/v1beta1\u0026#34;, 5 \u0026#34;metadata\u0026#34;: { 6 \u0026#34;selfLink\u0026#34;: \u0026#34;/apis/custom.metrics.k8s.io/v1beta1/namespaces/default/pods/%2A/nginx_vts_server_requests_per_second\u0026#34; 7 }, 8 \u0026#34;items\u0026#34;: [ 9 { 10 \u0026#34;describedObject\u0026#34;: { 11 \u0026#34;kind\u0026#34;: \u0026#34;Pod\u0026#34;, 12 \u0026#34;namespace\u0026#34;: \u0026#34;default\u0026#34;, 13 \u0026#34;name\u0026#34;: \u0026#34;hpa-prom-demo-755bb56f85-lvksr\u0026#34;, 14 \u0026#34;apiVersion\u0026#34;: \u0026#34;/v1\u0026#34; 15 }, 16 \u0026#34;metricName\u0026#34;: \u0026#34;nginx_vts_server_requests_per_second\u0026#34;, 17 \u0026#34;timestamp\u0026#34;: \u0026#34;2020-04-07T09:45:45Z\u0026#34;, 18 \u0026#34;value\u0026#34;: \u0026#34;527m\u0026#34;, 19 \u0026#34;selector\u0026#34;: null 20 } 21 ] 22} ok，没问题，我们定义个hpa，根据这个指标来伸缩\n1apiVersion:autoscaling/v2beta12kind:HorizontalPodAutoscaler3metadata:4name:nginx-hpa5spec:6scaleTargetRef:7apiVersion:apps/v18kind:Deployment9name:nginx-deploy10minReplicas:211maxReplicas:512metrics:13- type:Pods14pods:15metricName:nginx_vts_server_requests_per_second16targetAverageValue:10这样就好了。\n如果 pod 本身不能暴露 metric ，我们可以在 sidecar 里安装 exporter 来收集数据并暴露出去就可以了。\n","date":"2021-11-25","img":"","permalink":"https://zhangrr.github.io/posts/20211125-kubernetes_hpa/","series":null,"tags":null,"title":"Kubernetes的hpa和自定义指标hpa"},{"categories":null,"content":"公司已经由 saltstack 全面转向了 ansible 。\n用 ansible-playbook 执行各种任务的时候，需要登录主机，就必然涉及到主机 ssh 密码的输入。\n最早我们是在 inventory 里做了定义：\n1[deqin:vars] 2ansible_ssh_common_args=\u0026#34;-o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null\u0026#34; 3host_key_checking=False 4ansible_ssh_user=\u0026#34;peadmin\u0026#34; 5ansible_ssh_pass=\u0026#34;Fuck2021!\u0026#34; 6 7[deqin] 8192.168.1.19 太直白了，所有看到这文件内容的人都会知道密码了。完全没有安全性，这样行不通啊！\n好在 ansible-vault 提供了一种方法来解决：那就是生成一个密文放进去，然后解开它必须再输入一个密码。这样看到的人也不知道实际的密码到底是什么\n具体的做法如下，首先生成 key \u0026ndash;\u0026gt; 加密字符串的键值对：\n1ansible-vault encrypt_string \u0026#39;Fuck2021!\u0026#39; --name \u0026#39;ansible_ssh_pass\u0026#39; 输入密码，会得到下面一串字符\n1ansible_ssh_pass: !vault | 2 $ANSIBLE_VAULT;1.1;AES256 3 37393235646234613332646366306233346330656666623862313339313861393239646261366237 4 6663343263363161643634653266343466356634656539650a393834663938636165336431656433 5 66333761643538623434363334316661653035313166333137373562363436613636366162353239 6 3661623733323933350a373164626131646235616361356638653733646534616163393362373135 7 6139 这个就是密文了，必须用输入的密码才能解开。\n注意：这里的键值 name 不可改变，如果你想把字符串拷贝下来，改掉 ansible_ssh_pass 的名字，改成别的，想改名引用，是不行的。\n这一大长串密文有以下两种用法：\n一、ini格式的inventory引用 最原始的 inventory.ini 内容如下：\n1[deqin:vars] 2ansible_ssh_common_args=\u0026#34;-o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null\u0026#34; 3host_key_checking=False 4ansible_ssh_user=\u0026#34;peadmin\u0026#34; 5 6[deqin] 7192.168.1.19 8 我们定义 playbook 文件 shenji.yml：\n1- hosts:deqin2become:yes3vars_files:4- pass.yml5vars:6ansible_ssh_pass:\u0026#39;{{ ansible_ssh_pass }}\u0026#39;78tasks:9- name:mkdirs10file:path=\u0026#34;{{ item }}\u0026#34; state=directory11with_items:12- \u0026#34;OS.05\u0026#34;13- \u0026#34;OS.06\u0026#34;把密文放进 pass.yml 文件\n1cat \u0026lt;\u0026lt; EOF \u0026gt;\u0026gt; pass.yml 2ansible_ssh_pass: !vault | 3$ANSIBLE_VAULT;1.1;AES256 437393235646234613332646366306233346330656666623862313339313861393239646261366237 56663343263363161643634653266343466356634656539650a393834663938636165336431656433 666333761643538623434363334316661653035313166333137373562363436613636366162353239 73661623733323933350a373164626131646235616361356638653733646534616163393362373135 86139 9EOF 运行 playbook：\n1ansible-playbook --ask-vault-pass -i inventory.ini shenji.yml -vvv 为什么这样呢？因为 ansible-vault 加密过的字符串是 yaml 格式的，在 ini 里无法直接引用。\n所以在 playbook 的 yaml 文件中引入它，然后再跟从 inventory.ini 中获取的变量合作一起。\n二、yaml格式的inventory引用 上面我们看到了必须间接引用才可以，为了避免掉 pass.yml 文件，那么干脆把 inventroy 用 yaml 格式来写，那不就可以了么\n如下即可：\n1cat \u0026lt;\u0026lt; EOF \u0026gt;\u0026gt; inventory.yml2---3all:4hosts:5deqin:6ansible_host:192.168.1.197vars:8host_key_checking:\u0026#34;False\u0026#34;9ansible_ssh_common_args:\u0026#34;-o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null\u0026#34;10ansible_ssh_pass:!vault |11$ANSIBLE_VAULT;1.1;AES2561237393235646234613332646366306233346330656666623862313339313861393239646261366237136663343263363161643634653266343466356634656539650a3938346639386361653364316564331466333761643538623434363334316661653035313166333137373562363436613636366162353239153661623733323933350a37316462613164623561636135663865373364653461616339336237313516613917EOF然后运行就可以了：\n1ansible-playbook --ask-vault-pass -i inventory.yml shenji.yml -vvv ","date":"2021-11-24","img":"","permalink":"https://zhangrr.github.io/posts/20211124-ansible_vault/","series":null,"tags":null,"title":"Ansible Vault加密的使用"},{"categories":null,"content":"审计啊审计，公司使用的华为防火墙需要配置双因子登录认证，这下麻烦了。\n查了一下华为手册，支持 Radius 认证，那么没办法，最省钱的办法就是用 FreeIPA 和 FreeRadius 搭一套 OTP 双因子认证了。\n系统是 CentOS 7 ，已关闭防火墙服务，方法如下：\n一、搭建FreeIPA 首先设置 hostname\n1hostnamectl set-hostname freeipa.rendoumi.local 2 3echo \u0026#34;192.168.1.5 freeipa.rendoumi.local\u0026#34; \u0026gt;\u0026gt; /etc/hosts 然后安装 FreeIPA，注意要回答的几个问题\n 不装bind，无论是 dnsmasq 或 coredns，都比 bind 轻，要装也装那两个。 server hostname 是 freeipa.rendoumi.local domian name 是 rendoumi.local realm name 是大写的 RENDOUMI.LOCAL 有两个密码，第一个是 LDAP 的密码，第二个是 IPA 的密码  1yum -y install deltarpm 2yum update 3 4yum -y install freeipa-server 5ipa-server-install 6 7This program will set up the IPA Server. 8 9This includes: 10 * Configure a stand-alone CA (dogtag) for certificate management 11 * Configure the Network Time Daemon (ntpd) 12 * Create and configure an instance of Directory Server 13 * Create and configure a Kerberos Key Distribution Center (KDC) 14 * Configure Apache (httpd) 15 16To accept the default shown in brackets, press the Enter key. 17 18WARNING: conflicting time\u0026amp;date synchronization service \u0026#39;chronyd\u0026#39; will be disabled in favor of ntpd 19 20Do you want to configure integrated DNS (BIND)? [no]:no 21 22Server host name [freeipa.rendoumi.local]: 23 24Please confirm the domain name [rendoumi.local]: 25 26Please provide a realm name [RENDOUMI.LOCAL]: 27 28Directory Manager password: 29Password (confirm): 30... 31IPA admin password: 32Password (confirm): 33 34The IPA Master Server will be configured with: 35Hostname: freeipa.rendoumi.local 36IP address(es): 192.168.1.5 37Domain name: rendoumi.local 38Realm name: RENDOUMI.LOCAL 39 40Continue to configure the system with these values? [no]: yes 41 42The following operations may take some minutes to complete. 43Please wait until the prompt is returned. 44 45Configuring NTP daemon (ntpd) 46 47... 48 49Setup complete 50 51Next steps: 52 1. You must make sure these network ports are open: 53 TCP Ports: 54 * 80, 443: HTTP/HTTPS 55 * 389, 636: LDAP/LDAPS 56 * 88, 464: kerberos 57 UDP Ports: 58 * 88, 464: kerberos 59 * 123: ntp 60 61 2. You can now obtain a kerberos ticket using the command: \u0026#39;kinit admin\u0026#39; 62 This ticket will allow you to use the IPA tools (e.g., ipa user-add) 63 and the web user interface. 64 65Be sure to back up the CA certificate stored in /root/cacert.p12 66This file is required to create replicas. The password for this file is the Directory Manager password 67 以上，就装好了 FreeIPA，配置文件在 /etc/ipa/default.conf\n验证一下：\n1# 输入ipa密码 2kinit admin 3klist 4 5ipactl status 6# sn 输入 01 7ipa cert-show 登录： http://freeipa.rendoumi.com ，（注意你访问的机器必须能解析到这个域名）用户名 admin ，密码是上面填入的 ipa 密码，建立一个新用户\n然后给这个用户添加 OTP Token：\n缺省什么都不用填，直接选 Add：\n会蹦出来一个二维码，建议是用 FreeOTP 扫描：\n我们在手机上装上 FreeOTP 软件，扫描添加：\n这样就ok了。下次登录的时候密码就是预设密码+FreeOTP密码合在一起。中间没有加号哦\n比如预设密码是 Fuck，otp密码是762405，合在一起就是 Fuck762405，一起输入即可。\n那 FreeIPA 的部分就完成了。\n二、搭建FreeRadius 上面的部分其实是 FreeIPA 充当了用户数据库，用 LDAP 存放数据，而 Radius 需要从 IPA 拿到用户信息。\n安装：\n1yum -y install freeradius freeradius-utils freeradius-ldap freeradius-krb5 Radius 的配置都在 /etc/raddb 目录下：\n编辑 /etc/raddb/client.conf ，增加一个网段的认证，允许 172.0.0.0/8 访问\n1client localnet { 2 ipaddr = 172.0.0.0/8 3proto = * 4secret = Fuck2021 5nas_type = other 6limit { 7max_connections = 16 8lifetime = 0 9idle_timeout = 30 10} 11} 同时修改下面的 clinet localhost 部分，修改 secret，之后我们要从本地登录做测试\n1client localhost { 2 secret = ChinaBank2021 3 再修改 /etc/raddb/sites-enabled/default and /etc/raddb/sites-enabled/inner-tunnel ，支持 LDAP，有二处地方\n把\n1 # 2 # The ldap module reads passwords from the LDAP database. 3 -ldap 换成：\n1 # 2 # The ldap module reads passwords from the LDAP database. 3 ldap 4 if ((ok || updated) \u0026amp;\u0026amp; User-Password) { 5 update { 6 control:Auth-Type := ldap 7} 8} 把\n1# Auth-Type LDAP { 2# ldap 3# } 换成：\n1 Auth-Type LDAP { 2 ldap 3 } 然后 ldap 模块配置一下\n1ln -s /etc/raddb/mods-available/ldap /etc/raddb/mods-enabled/ 我们先用 ldapsearch 搜索一下，看看具体的 dn 信息，这里输入之前设置的 ldap 密码\n1ldapsearch -x -v -W -D \u0026#39;cn=Directory Manager\u0026#39; uid=test|grep test 2ldap_initialize( \u0026lt;DEFAULT\u0026gt; ) 3Enter LDAP Password: 4filter: uid=test 5requesting: All userApplication attributes 6memberOf: cn=test,cn=groups,cn=accounts,dc=rendoumi,dc=local 得到 cn=accounts,dc=rendoumi,dc=local\n再去修改 /etc/raddb/mods-enabled/ldap 文件，修改 server 和 base_dn 与之对应：\n1 server = \u0026#39;freeipa.rendoumi.local\u0026#39; 2base_dn = \u0026#39;cn=accounts,dc=rendoumi,dc=local\u0026#39; 注意，上面我们没装 bind，所以必须在 /etc/hosts 存在记录，否则本地就访问不到了\n启动 radiusd 的调试模式：\n1radiusd –X 2... 3Listening on auth address * port 1812 as server default 4Listening on acct address * port 1813 as server default 5Listening on auth address :: port 1812 as server default 6Listening on acct address :: port 1813 as server default 7Listening on auth address 127.0.0.1 port 18120 as server inner-tunnel 8Opening new proxy socket \u0026#39;proxy address * port 0\u0026#39; 9Listening on proxy address * port 36752 10Ready to process requests 再开一个终端测试一下，注意，我们是从本地(127.0.0.1)发起测试的，所以对应要用到上面设置的 secret，用 admin 登录，就避免要用到 freeotp 的口令，这里 xxxxxxxx 是 admin 的密码：\n1radtest admin xxxxxxxx freeipa.rendoumi.local 1812 ChinaBank2021 2Sent Access-Request Id 57 from 0.0.0.0:45247 to 172.18.31.41:1812 length 75 3 User-Name = \u0026#34;admin\u0026#34; 4 User-Password = \u0026#34;xxxxxxxx\u0026#34; 5 NAS-IP-Address = 172.18.31.41 6 NAS-Port = 1812 7 Message-Authenticator = 0x00 8 Cleartext-Password = \u0026#34;xxxxxxxx\u0026#34; 9Received Access-Accept Id 57 from 172.18.31.41:1812 to 0.0.0.0:0 length 20 看到上面 Access-Accept 就ok了，ctrl-c 终止 radiusd 的运行，开启 radiusd 服务。\n1systemctl enable --now radiusd 然后在华为防火墙设置这个 radiusd 服务器就可以了。\n参考资料：\nhttps://www.freeipa.org/page/Using_FreeIPA_and_FreeRadius_as_a_RADIUS_based_software_token_OTP_system_with_CentOS/RedHat_7 ","date":"2021-11-23","img":"","permalink":"https://zhangrr.github.io/posts/20211123-freeipa_freeradius/","series":null,"tags":null,"title":"使用FreeIPA和FreeRadius搭建双因子认证服务器"},{"categories":null,"content":"这个比较有意思，同事要存放 2TB 的数据，但是系统是 1.7TB 的 4 块 600G 盘组成的 Raid10。\n很明显盘空间不够了，去库房找两块 10TB 的大盘组成 Raid1 给他用好了。\n问题来了，系统是 KVM 虚机，怎样把这个 10TB 的大盘给送进虚机呢？\n这里面还真有要注意的问题：\n Important\nGuest virtual machines should not be given write access to whole disks or block devices (for example, /dev/sdb). Guest virtual machines with access to whole block devices may be able to modify volume labels, which can be used to compromise the host physical machine system. Use partitions (for example, /dev/sdb1) or LVM volumes to prevent this issue.\n 注意上面的说明，是不建议将整个硬盘送进虚机里面去的，建议是送分区进去，避免会写坏整个盘。\n所以先把盘的分区做好，格式化并挂载，以 /dev/sdb 为例\n1parted -s /dev/sdb mklabel gpt mkpart primary 0% 100% 2mkfs.xfs /dev/sdb1 3mount -t xfs /dev/sdb1 /mnt/sdb1 然后到实体机的磁盘目录\n1# cd /dev/disk/ 2# ls 3by-id by-path by-uuid 4 5# cd by-id 6# ll 7total 0 8lrwxrwxrwx 1 root root 9 3月 26 2018 scsi-36b083fe0e4f71a001c36c36716543d4d -\u0026gt; ../../sda 9lrwxrwxrwx 1 root root 10 3月 26 2018 scsi-36b083fe0e4f71a001c36c36716543d4d-part1 -\u0026gt; ../../sda1 10lrwxrwxrwx 1 root root 10 3月 26 2018 scsi-36b083fe0e4f71a001c36c36716543d4d-part2 -\u0026gt; ../../sda2 11lrwxrwxrwx 1 root root 10 3月 26 2018 scsi-36b083fe0e4f71a001c36c36716543d4d-part3 -\u0026gt; ../../sda3 12lrwxrwxrwx 1 root root 9 11月 18 16:02 scsi-36b083fe0e4f71a002928bf63ef284827 -\u0026gt; ../../sdb 13lrwxrwxrwx 1 root root 10 11月 18 16:24 scsi-36b083fe0e4f71a002928bf63ef284827-part1 -\u0026gt; ../../sdb1 14lrwxrwxrwx 1 root root 9 3月 26 2018 wwn-0x6b083fe0e4f71a001c36c36716543d4d -\u0026gt; ../../sda 15lrwxrwxrwx 1 root root 10 3月 26 2018 wwn-0x6b083fe0e4f71a001c36c36716543d4d-part1 -\u0026gt; ../../sda1 16lrwxrwxrwx 1 root root 10 3月 26 2018 wwn-0x6b083fe0e4f71a001c36c36716543d4d-part2 -\u0026gt; ../../sda2 17lrwxrwxrwx 1 root root 10 3月 26 2018 wwn-0x6b083fe0e4f71a001c36c36716543d4d-part3 -\u0026gt; ../../sda3 18lrwxrwxrwx 1 root root 9 11月 18 16:02 wwn-0x6b083fe0e4f71a002928bf63ef284827 -\u0026gt; ../../sdb 19lrwxrwxrwx 1 root root 10 11月 18 16:24 wwn-0x6b083fe0e4f71a002928bf63ef284827-part1 -\u0026gt; ../../sdb1 看中间一行，scsi-36b083fe0e4f71a002928bf63ef284827-part1 指向 /dev/sdb1，记录下来\n然后 virsh-edit 编辑 KVM 虚机文件，增加硬盘部分：\n1 \u0026lt;disk type=\u0026#39;block\u0026#39; device=\u0026#39;disk\u0026#39;\u0026gt; 2\u0026lt;driver name=\u0026#39;qemu\u0026#39; type=\u0026#39;raw\u0026#39;/\u0026gt; 3\u0026lt;source dev=\u0026#39;/dev/disk/by-id/scsi-36b083fe0e4f71a002928bf63ef284827-part1\u0026#39;/\u0026gt; 4\u0026lt;target dev=\u0026#39;vda\u0026#39; bus=\u0026#39;virtio\u0026#39;/\u0026gt; 5\u0026lt;/disk\u0026gt; 注：上面我们是使用了 by-id，当然也可以使用 by-path 和 by-uuid 来指定源盘\n重启虚机并进入查看，我们能看到这块盘盘符是 /dev/vda\n1fdisk -l 2 3Disk /dev/vda: 11755.9 GB, 11755860262912 bytes, 22960664576 sectors 4Units = sectors of 1 * 512 = 512 bytes 5Sector size (logical/physical): 512 bytes / 512 bytes 6I/O size (minimum/optimal): 512 bytes / 512 bytes 接下来就比较怪异了，这个盘分明没有分区，我们却可以直接 mount\n1mount -t xfs /dev/vda /material 然后 df -h 再看\n能认出来，正常使用就行了。\n非常古怪是吧。\n大家还要注意实体机的挂载路径是 /mnt/sdb1，虚机内的挂载路径是 /material，这两个路径都是指向同一块盘，可以共通。\n","date":"2021-11-19","img":"","permalink":"https://zhangrr.github.io/posts/20211119-kvm_disk_passthrough/","series":null,"tags":null,"title":"KVM下附加硬盘的passthrough直通"},{"categories":null,"content":"说到 CentOS7 的紧急模式与救援模式，网上可以搜到漫天飞的帖子，说一下区别\nRESCUE 救援模式： 救援模式启动的系统没有挂载硬盘，可以将硬盘 mount 出然后拷出数据。\nEMERGENCY 紧急模式： 紧急模式启动的系统是一个最小的环境。根目录档案系统将会被挂载为仅能读取，而且将不会做任何的设定。\n当然进入的方法也很简单，进入系统的时候按 e 修改 grub 菜单参数，就可以进入不同的模式\n本文讨论的重点不是怎么进去，而是那两句命令，在紧急状态下反正我是记不住的\n1systemd.unit=rescue.target 2systemd.unit=emergency.target 都没有之前的 single 简单，也完全记不住，既然记不住，那就干脆做到菜单里好了，这才是本文的重点。\n现在都是使用 grub2 了，而不是 grub，这很重要。grub2的配置文件是 /boot/grub2/grub.cfg。\n修改 grub2 有两个工具，grub2-mkconfig 和 grubby，不要同时使用这两个工具修改，会覆盖的\n grub2-mkconfig 会去搜索 /boot 目录下的内核文件，有多少个内核就会生成多少个启动项。那么如果是同一个内核，想修改不同的启动参数，做多个启动项就完蛋，他不能自动生成单内核的多个启动项 grubby 很灵活，可以根据当前 grub2 的配置，生成一个内核，多个不同启动参数的多个启动项。  那么我们要加进去两个只是启动参数不同，内核其实一样的启动项，用 grubby 就好了\n1grubby --add-kernel=\\$(ls -1cat /boot/vmlinuz*|grep rescue) --title=\u0026#34;RESCUE BOOT\u0026#34; --initrd=\\$(ls -1cat /boot/initramfs*|grep rescue) --args=\u0026#34;systemd.unit=rescue.target\u0026#34; --copy-default 2 3grubby --add-kernel=\\$(ls -1cat /boot/vmlinuz*|grep rescue) --title=\u0026#34;EMERGENCY BOOT\u0026#34; --initrd=\\$(ls -1cat /boot/initramfs*|grep rescue) --args=\u0026#34;systemd.unit=emergency.target\u0026#34; --copy-default 切忌我们之后不能运行\n1grub2-mkconfig -o /boot/grub2/grub.cfg 否则上面的两个启动项菜单会消失，因为 grub2-mkconfig 配置的话一个内核只能有一个启动项\ngrub2-mkconfig 也有自己的强项，如果要修改缺省的菜单超时时间，grubby 就做不到了\n1sed -i \u0026#39;/^GRUB_TIMEOUT=/s/^.*$/GRUB_TIMEOUT=10/\u0026#39; /etc/default/grub 2grub2-mkconfig -o /boot/grub2/grub.cfg ","date":"2021-11-19","img":"","permalink":"https://zhangrr.github.io/posts/20211119-linux_rescue/","series":null,"tags":null,"title":"CentOS7的救援模式和紧急模式"},{"categories":null,"content":"说实在话，这个场景非常怪异，客户在 linux 下要动态根据 url 选择代理：\n看图，中间的是前端代理，地址是 192.168.1.1:8080，然后客户设置使用这个代理\n1export http_proxy=http://192.168.1.1:8080 2export https_proxy=http://192.168.1.1:8080 然后对应后端有三个代理，两个 http 代理，一个 socks 代理\n1http 192.168.2.1:3128 2socks 192.168.2.2:1080 3http 192.168.2.3:3128 我们要根据客户的请求 URL 来决定具体要使用后端的哪个代理\n这个如果在浏览器上设置非常容易，设置 PAC 即可。但是偏偏客户端不是浏览器，而是一个程序，那么麻烦就来了。怎么设置呢？\n步骤很简单：\n一、安装 pacproxy 网址：https://github.com/williambailey/pacproxy\n1wget https://github.com/williambailey/pacproxy/releases/download/v.2.0.4/pacproxy_2.0.4_linux_amd64.tar.gz 2tar zxvf pacproxy_2.0.4_linux_amd64.tar.gz 拷出来 pacproxy 备用\n二、生成配置文件 最主要的就是 PAC 文件的生成\n我们给个例子：\n1cat \u0026lt;\u0026lt; EOF \u0026gt;\u0026gt; pac 2function FindProxyForURL(url, host) 3{ 4if (host == \u0026#34;www.baidu.com\u0026#34;) { 5return \u0026#34;PROXY 192.168.2.1:3128\u0026#34;; 6} 7else if (host == \u0026#34;www.sina.com.cn\u0026#34;) { 8return \u0026#34;SOCKS 192.168.2.2:1080\u0026#34;; 9} 10else if (host == \u0026#34;www.sohu.com\u0026#34;) { 11return \u0026#34;SOCKS 192.168.2.3:1080\u0026#34;; 12} 13else { 14return \u0026#34;DIRECT\u0026#34;; 15} 16} 17EOF 其实 PAC 文件的内容就是一段 javascript，用来返回代理的地址\n运行并测试：\n1pacproxy -c pac -l 0.0.0.0:8080 -v 2 3 4export http_proxy=http://192.168.1.1:8080 5export https_proxy=http://192.168.1.1:8080 6curl -I www.baidu.com 这样就在 Linux 搭建了一个动态的 PAC 代理\nPAC文件的参考资料：\n  https://web.archive.org/web/20070602031929/http://wp.netscape.com/eng/mozilla/2.0/relnotes/demo/proxy-live.html   https://developer.mozilla.org/zh-CN/docs/Web/HTTP/Proxy_servers_and_tunneling/Proxy_Auto-Configuration_PAC_file   https://www.websense.com/content/support/library/web/v76/pac_file_best_practices/PAC_file_sample.aspx   ","date":"2021-11-16","img":"","permalink":"https://zhangrr.github.io/posts/20211116-pacproxy/","series":null,"tags":null,"title":"Linux下web Pacproxy的用法"},{"categories":null,"content":"Tomcat 是一个 HTTP server。同时，它也是一个 serverlet 容器，可以执行 java 的 Servlet，也可以把 JavaServer Pages（JSP）和 JavaServerFaces（JSF）编译成 Java Servlet。它的模型图如下：\n给个生产环境 server.xml 的例子：\n1\u0026lt;?xml version=\u0026#39;1.0\u0026#39; encoding=\u0026#39;utf-8\u0026#39;?\u0026gt; 2\u0026lt;Server port=\u0026#34;-1\u0026#34; shutdown=\u0026#34;SHUTDOWN\u0026#34;\u0026gt; 3\u0026lt;Listener className=\u0026#34;org.apache.catalina.startup.VersionLoggerListener\u0026#34; /\u0026gt; 4\u0026lt;Listener className=\u0026#34;org.apache.catalina.core.AprLifecycleListener\u0026#34; SSLEngine=\u0026#34;on\u0026#34; /\u0026gt; 5\u0026lt;Listener className=\u0026#34;org.apache.catalina.core.JreMemoryLeakPreventionListener\u0026#34; /\u0026gt; 6\u0026lt;Listener className=\u0026#34;org.apache.catalina.mbeans.GlobalResourcesLifecycleListener\u0026#34; /\u0026gt; 7\u0026lt;Listener className=\u0026#34;org.apache.catalina.core.ThreadLocalLeakPreventionListener\u0026#34; /\u0026gt; 8 9 \u0026lt;GlobalNamingResources\u0026gt; 10 \u0026lt;Resource name=\u0026#34;UserDatabase\u0026#34; auth=\u0026#34;Container\u0026#34; 11type=\u0026#34;org.apache.catalina.UserDatabase\u0026#34; 12description=\u0026#34;User database that can be updated and saved\u0026#34; 13factory=\u0026#34;org.apache.catalina.users.MemoryUserDatabaseFactory\u0026#34; 14pathname=\u0026#34;conf/tomcat-users.xml\u0026#34; /\u0026gt; 15\u0026lt;/GlobalNamingResources\u0026gt; 16 17 \u0026lt;Service name=\u0026#34;Catalina\u0026#34;\u0026gt; 18\u0026lt;Connector port=\u0026#34;8080\u0026#34; 19server=\u0026#34;www.rendoumi.com\u0026#34; 20protocol=\u0026#34;org.apache.coyote.http11.Http11NioProtocol\u0026#34; 21maxHttpHeaderSize=\u0026#34;8192\u0026#34; 22acceptCount=\u0026#34;500\u0026#34; 23maxThreads=\u0026#34;1000\u0026#34; 24minSpareThreads=\u0026#34;200\u0026#34; 25enableLookups=\u0026#34;false\u0026#34; 26redirectPort=\u0026#34;8443\u0026#34; 27connectionTimeout=\u0026#34;20000\u0026#34; 28relaxedQueryChars=\u0026#34;[]|{}@!$*()+\u0026#39;.,;^\\`\u0026amp;quot;\u0026amp;lt;\u0026amp;gt;\u0026#34; 29disableUploadTimeout=\u0026#34;true\u0026#34; 30allowTrace=\u0026#34;false\u0026#34; 31URIEncoding=\u0026#34;UTF-8\u0026#34; 32useBodyEncodingForURI=\u0026#34;true\u0026#34; /\u0026gt;  33 34 \u0026lt;Engine name=\u0026#34;Catalina\u0026#34; defaultHost=\u0026#34;localhost\u0026#34;\u0026gt; 35\u0026lt;Realm className=\u0026#34;org.apache.catalina.realm.LockOutRealm\u0026#34;\u0026gt; 36\u0026lt;Realm className=\u0026#34;org.apache.catalina.realm.UserDatabaseRealm\u0026#34; 37resourceName=\u0026#34;UserDatabase\u0026#34;/\u0026gt; 38\u0026lt;/Realm\u0026gt; 39\u0026lt;Host name=\u0026#34;localhost\u0026#34; appBase=\u0026#34;webapps\u0026#34; unpackWARs=\u0026#34;false\u0026#34; autoDeploy=\u0026#34;false\u0026#34;\u0026gt; 40\u0026lt;Valve className=\u0026#34;org.apache.catalina.valves.AccessLogValve\u0026#34; directory=\u0026#34;logs\u0026#34; prefix=\u0026#34;access_log.\u0026#34; suffix=\u0026#34;.txt\u0026#34; 41fileDateFormat=\u0026#34;yyyy-MM-dd\u0026#34; 42pattern=\u0026#34;%a|%A|%T|%{X-Forwarded-For}i|%l|%u|%t|%r|%s|%b|%{Referer}i|%{User-Agent}i \u0026#34; resolveHosts=\u0026#34;false\u0026#34;/\u0026gt; 43\u0026lt;Context path=\u0026#34;\u0026#34; docBase=\u0026#34;/export/servers/tomcat/webapps/web\u0026#34; /\u0026gt; 44\u0026lt;/Host\u0026gt; 45 46 \u0026lt;/Engine\u0026gt; 47 \u0026lt;/Service\u0026gt; 48\u0026lt;/Server\u0026gt; 分解开来一部分一部分的看：\nServer Server 第2行，是最大且必须唯一的组件。表示一个 Tomcat 的实例，它可以包含多个 Services，而每个 Service 都可以有自己的 Engine 和 connectors。\n1\u0026lt;Server port=\u0026#34;8005\u0026#34; shutdown=\u0026#34;SHUTDOWN\u0026#34;\u0026gt; ...... \u0026lt;/Server\u0026gt; 注意上面，我们把缺省的 port=\u0026ldquo;8005\u0026rdquo; 改成了 “-1”，这样防止从8005重启，避免安全问题。\nListeners 一个 Server 容器拥有若干 Listeners (行 3-7)。一个 Listener 监听并回应特定的事件.\n  例如 GlobalResourcesLifecycleListener 就设置了 global resources，这样就可以使用 JNDI 来存取像数据库这种资源。\n1\u0026lt;Listener className=\u0026#34;org.apache.catalina.mbeans.GlobalResourcesLifecycleListener\u0026#34; /\u0026gt;   Global Naming Resources \u0026lt;GlobalNamingResources\u0026gt; (行 9-15) 定义了 JNDI (Java Naming and Directory Interface) 资源，这个是 LDAP 的东西，允许 java 软件通过目录发现对象和属性值。\n缺省定义了一个 UserDatabase 的 JNDI 资源（行 10-14），这个对象其实是一个内存数据库，保存着从 conf/tomcat-users.xml 中加载上来的用户认证数据。\n1\u0026lt;GlobalNamingResources\u0026gt; 2 \u0026lt;Resource name=\u0026#34;UserDatabase\u0026#34; auth=\u0026#34;Container\u0026#34; 3type=\u0026#34;org.apache.catalina.UserDatabase\u0026#34; 4description=\u0026#34;User database that can be updated and saved\u0026#34; 5factory=\u0026#34;org.apache.catalina.users.MemoryUserDatabaseFactory\u0026#34; 6pathname=\u0026#34;conf/tomcat-users.xml\u0026#34; /\u0026gt; 7\u0026lt;/GlobalNamingResources\u0026gt; 我们可以再定义一个 Mysql 之类的 JNDI 来保存 mysql 数据库连接信息，用来实现连接池\nServices Service 用来关联多个 Connectors 到 Engine。缺省的配置是配了一个 叫做 Catalina 的 Service ，Catalinna 缺省配了两个 Connectors，一个是8080的HTTP，一个是8009的AJP。\n1\u0026lt;Service name=\u0026#34;Catalina\u0026#34;\u0026gt; 2\u0026lt;Connector port=\u0026#34;8080\u0026#34; protocol=\u0026#34;HTTP/1.1\u0026#34; 3connectionTimeout=\u0026#34;20000\u0026#34; 4redirectPort=\u0026#34;8443\u0026#34; /\u0026gt; 5\u0026lt;Connector port=\u0026#34;8009\u0026#34; protocol=\u0026#34;AJP/1.3\u0026#34; redirectPort=\u0026#34;8443\u0026#34; /\u0026gt; 6\u0026lt;/Service\u0026gt; 注意最上面的配置，我们取消了8009的AJP，这个协议现在基本没人用，取消掉也避免安全问题。\n下面给出一个 SSL 8443 的 connectors 的例子\n1 \u0026lt;Connector port=\u0026#34;8443\u0026#34; maxHttpHeaderSize=\u0026#34;8192\u0026#34; SSLEnabled=\u0026#34;true\u0026#34; server=\u0026#34;Rendoumi\u0026#34; 2maxThreads=\u0026#34;150\u0026#34; minSpareThreads=\u0026#34;25\u0026#34; maxSpareThreads=\u0026#34;75\u0026#34; 3enableLookups=\u0026#34;false\u0026#34; disableUploadTimeout=\u0026#34;true\u0026#34; 4acceptCount=\u0026#34;100\u0026#34; scheme=\u0026#34;https\u0026#34; secure=\u0026#34;true\u0026#34; clientAuth=\u0026#34;false\u0026#34; 5URIEncoding=\u0026#34;UTF-8\u0026#34; 6sslProtocol=\u0026#34;SSL\u0026#34; 7ciphers=\u0026#34;SSL_RSA_WITH_RC4_128_MD5, SSL_RSA_WITH_RC4_128_SHA\u0026#34; 8keystoreFile=\u0026#34;/export/servers/tomcat/rendoumi.keystore\u0026#34; 9keystorePass=\u0026#34;Fuck2020\u0026#34; /\u0026gt; Containers 概念 Tomcat 的容器概念，Tomcat认为 Engine, Host, Context 和 Cluster处在同一个容器中. 最顶层的是 Engine; 最底层是Context。\n另外像 Realm 和 Valve，也可以放在容器中\nEngine Engine 是容器的顶端，可以包含有若干 Hosts。我们可以配置 tomcat 服务多个虚拟主机。下面配置就是服务本机的一个服务。\n1\u0026lt;Engine name=\u0026#34;Catalina\u0026#34; defaultHost=\u0026#34;localhost\u0026#34;\u0026gt; Catalina Engine 从 HTTP connector 处接收到客户端的 HTTP 请求，根据请求头中 Host: xxx 的内容，把请求路由到某个具体的 Host上。\nRealm Realm 本质是一个数据库，是用来保存用户名、密码、认证角色的东西。\n1\u0026lt;Realm className=\u0026#34;org.apache.catalina.realm.LockOutRealm\u0026#34;\u0026gt; 2\u0026lt;Realm className=\u0026#34;org.apache.catalina.realm.UserDatabaseRealm\u0026#34; resourceName=\u0026#34;UserDatabase\u0026#34;/\u0026gt; 3\u0026lt;/Realm\u0026gt; Hosts Host 定义了虚拟主机，可以定义多个虚拟主机。其中 appBase 千万不能为空，否则就可以读到 tomcat 的所有文件，就出线上事故大 Bug 了！！！\n1\u0026lt;Host name=\u0026#34;localhost\u0026#34; appBase=\u0026#34;webapps\u0026#34; unpackWARs=\u0026#34;true\u0026#34; autoDeploy=\u0026#34;true\u0026#34;\u0026gt; 我们最上面的生产配置，不自解压，也不自动部署。\nValve Valve 是用来拦截 HTTP requests的，做预处理后再交给下一个组件，它能在 Engine, Host, Context 中被定义。下面的例子就是在 Engine 中做了拦截，定义了日志格式，交给下一个日志处理组件做处理。\n缺省配置\n1\u0026lt;Valve className=\u0026#34;org.apache.catalina.valves.AccessLogValve\u0026#34; directory=\u0026#34;logs\u0026#34; 2prefix=\u0026#34;localhost_access_log.\u0026#34; suffix=\u0026#34;.txt\u0026#34; 3pattern=\u0026#34;%h %l %u %t \u0026amp;quot;%r\u0026amp;quot; %s %b\u0026#34; /\u0026gt; 我们的配置修改了缺省的日志格式，多了很多明细的内容，改变了分割符：\n1\u0026lt;Valve className=\u0026#34;org.apache.catalina.valves.AccessLogValve\u0026#34; directory=\u0026#34;logs\u0026#34; 2prefix=\u0026#34;access_log.\u0026#34; suffix=\u0026#34;.txt\u0026#34; 3fileDateFormat=\u0026#34;yyyy-MM-dd\u0026#34; 4pattern=\u0026#34;%a|%A|%T|%{X-Forwarded-For}i|%l|%u|%t|%r|%s|%b|%{Referer}i|%{User-Agent}i \u0026#34; resolveHosts=\u0026#34;false\u0026#34;/\u0026gt;  Context Context定义了具体的访问路径，一定要指定 docBase ！！！大家一定要记住 appBase 和 docBase 不同为空！！！\n1\u0026lt;Context path=\u0026#34;\u0026#34; docBase=\u0026#34;/export/servers/tomcat/webapps/web\u0026#34; /\u0026gt;  如上，我们就配好了一个生产环境的 server.xml ，注意，这个 xml 文件是可以放在解开的tomcat压缩包 apache-tomcat-8.5.64 目录之外的。\n我们再另外编写一个启动文件 start.sh：\n1#!/bin/sh 2export JAVA_OPTS=\u0026#34;-server -Xms1024m -Xmx2048m -Djava.awt.headless=true -Dsun.net.client.defaultConnectTimeout=60000 -Dsun.net.client.defaultReadTimeout=60000 -Djmagick.systemclassloader=no -Dnetworkaddress.cache.ttl=300 -Dsun.net.inetaddr.ttl=300\u0026#34; 3export CATALINA_HOME=/export/servers/tomcat/apache-tomcat-8.5.64 4 5cd /export/servers/tomcat/ 6$CATALINA_HOME/bin/catalina.sh start -config \u0026#34;/export/servers/tomcat/server.xml\u0026#34; 这样，我们的启动文件和配置文件，就和 tomcat 目录分离了，这样做的好处是如果 tomcat 需要升级，那么直接解压换目录即可。\n我们可以做个软链接 tomcat 链接到 apache-tomcat-8.5.64，这样更方便操作。\n","date":"2021-11-15","img":"","permalink":"https://zhangrr.github.io/posts/20211115-tomcat_config/","series":null,"tags":null,"title":"Tomcat Server.xml配置详细解释"},{"categories":null,"content":"公司用了6年的GlusterFS终于到了要调整卷参数的地步了。\n小文件已经多到要影响 IO 的地步了。\n首先说一下结论，GlusterFS 安装完成后，基本不需要调整任何参数。生产系统千万不可盲目！\n然后我们这是有特殊情况，所以调优步骤如下（以卷名为 esign-vol 为例）：\n1#必须关掉NFS 2gluster volume set esign-vol nfs.disable on 3 4#必须保留10%的空间，避免塞爆卷空间 5gluster volume set esign-vol cluster.min-free-disk 10% 6 7#本机有256G的内存，所以设置25G的读缓存 8gluster volume set esign-vol performance.cache-size 25GB 9#读缓存中，单个文件的缓存，最大文件size是128MB，大于128MB的单个文件不缓存 10gluster volume set esign-vol performance.cache-max-file-size 128MB 11 12#设置每个客户端都允许多线程，缺省是2，多个小文件增加为4 13gluster volume set esign-vol client.event-threads 4 14#设置服务器端对特定的卷允许多线程，缺省是1,多个小文件增加为4 15gluster volume set esign-vol server.event-threads 4 16 17#分割线，以下参数不要调整，除非明确知道后果 18 19#设置 io 线程数量，这个值缺省是16，已经很大了，足够用 20gluster volume set esign-vol performance.io-thread-count 16 21#设置写缓冲区，这个值缺省是1M，弄大了如果停电什么的，会丢数据 22gluster volume set esign-vol performance.write-behind-window-size: 1M 以上就可以了。还有个参数 global-threading ，缺省是 off，不要设置为 on，有使用条件的，弄错了反而会导致性能降低。\n","date":"2021-11-15","img":"","permalink":"https://zhangrr.github.io/posts/20211115-gluster_tuning/","series":null,"tags":null,"title":"GlusterFS文件系统的优化"},{"categories":null,"content":"在实际中遇到这样一个问题，公司软件发布上线自动化。\n说简单点，就是需要去登录一个上线的内部网站，然后爬下所有的上线数据。\n然后根据爬下来的数据整理好，可以一起上线的，就并发多线程，其实就是去传参数点击一个链接等返回。\n不能并发的就单线程点链接。\n那这个事情必须更有效率，单线程的没问题，用 python 的 request 就可以实现了。\n我们仔细研究一下协程，先讲一下历史：\n使用Python的人往往纠结在多线程、多进程，哪个效率更高？到底用哪个好呢？\n其实 Python 的多进程和多线程，相对于别家的协程和异步处理机制，都不行，线程之间切换耗费 CPU 和寄存器，OS 的调度不可控，多进程之间通讯也不便。性能根本不行。\n后来呢 Python 改进了语法，出现了 yiled from 充当协程调度，有人就根据这个特性开发了第三方的协程框架，Tornado，Gevent等。\n官方也不能坐视不理啊，任凭别人出风头，于是 Python 之父深入简出3年，苦心钻研自家的协程，async/await 和 asyncio 库，并放到 Python3.5 后成为官方原生的协程。\n对于 http请求、读写文件、读写数据库这种高延时的 IO 操作，协程是个大杀器，优点非常多；它可以在预料到一个阻塞将发生时，挂起当前协程，跑去执行其它协程，同时把事件注册到循环中，实现了多协程并发，其实这玩意是跟 Nodejs 的回调学的。\n看下图，详细解释下，左边我们有100个网页请求，并发100个协程请求（其实也是1个1个发），当需要等待长时间回应回应时，挂起当前协程，并注册一个回调函数到事件循环（Event Loop）中，执行下一个协程，当有协程事件完成再通过回调函数唤醒挂起的协程，然后返回结果。\n这个跟 nodejs 的回调函数基本一样，我们必须注意主进程和协程的关系，如果我在一个主进程中，触发协程函数，有100个协程，那么必须等待100个协程都结束后，才能回到正常的那个主进程中。当然，主进程也可能也是一个协程。\n那么协程的基本用法\n  async f(n) 声明一个函数是协程的\n  await f(n) 挂起当前协程，把控制权交回 event loop，并且执行f(n)和注册之后的f(n)回调。\n举个例子：如果在 g() 这个函数中执行了 await f()，那么g()函数会被挂起，并等待 f() 函数有结果结束，然后返回 g() 继续执行。\n  1async def get(url): 2 async with aiohttp.ClientSession() as session: 3 async with session.get(url) as response: 4 return await response.text() 最后一行 await 是挂起命令，挂起当前函数 get() ，并执行 response.text() 和注册回调，等待 response.text() 执行完成后重新激活当前函數get()继续执行，返回。\n所以 await 只叫做挂起是不太对的，感觉应该叫做 挂起并注册回调 比较合适。\n看以下程序，在 Python 3.7 之前，协程是这么用的：\n1import time 2import asyncio 3 4now = lambda : time.time() 5 6async def do_some_work(x): 7 print(\u0026#39;Waiting: \u0026#39;, x) 8 9start = now() 10coroutine = do_some_work(2) 11loop = asyncio.get_event_loop() 12loop.run_until_complete(coroutine) 13print(\u0026#39;TIME: \u0026#39;, now() - start) 我们指定了一个协程 coroutine ，然后定义了一个事件循环 loop，loop 是需要 run_until_complete 所有的协程，然后交出控制权，返回正常的主进程。\n跟上图完全匹配。\n在 Python 3.7 之后，简化了用法，一句 asyncio.run 就可以了：\n1asyncio.run(do_some_work(2)) 上面程序就变了，省了好多，但是副作用是第一次看到的人会不明白它是怎么进化过来的：\n1import time 2import asyncio 3 4now = lambda : time.time() 5 6async def do_some_work(x): 7 print(\u0026#39;Waiting: \u0026#39;, x) 8 9start = now() 10asyncio.run(do_some_work(2)) 11print(\u0026#39;TIME: \u0026#39;, now() - start) 如果我们要访问一个网站的100个网页，单线程的做法是：请求一次，回来一次，然后进行下一个\n1for url in urls： 2 response=get(url) 3 results=parse(response) 这样效率很低，协程呢，做法就不同了，一次发起100个请求（准确的说也是一个一个发），不同的是协程不会死等返回，而是发一个请求，挂起，再发一个再挂起，发起100个，就挂起100个，然后注册并等待100个返回，效率提升了100倍。可以理解为同时做了100件事，做到由自己调度而不是交给CPU，程序的并发由自己来控制，而不是交由 OS 去调度，效率极大的提高了。\n进化到协程，我们把费 IO 的 get 函数抽出来放到协程里：\n1async def get(url:str): 2 my_conn = aiohttp.TCPConnector(limit=10) 3 async with aiohttp.ClientSession(connector=my_conn) as session: 4 async with session.get(url) as resp: 5 return await resp.text() 6 7 8for url in urls： 9 response=asyncio.run(get(url)) 10 results=parse(response) 11 具体到我们的项目，我们首先要登录一个网页拿到 cookie，这个过程其实就一个协程，没人会登录个几百次吧。然后把放了 cookie 的 session 取出来，供后面的协程再复用就可以了，示例代码如下：\n1import aiohttp 2import asyncio 3 4async def login(): 5 my_conn = aiohttp.TCPConnector(limit=10) 6 async with aiohttp.ClientSession(connector=my_conn) as session: 7 data = {\u0026#39;loginname\u0026#39;:\u0026#39;wangbadan\u0026#39;,\u0026#39;password\u0026#39;:\u0026#39;Fuckyouall\u0026#39;} 8 async with session.post(\u0026#39;http://192.168.1.3/user/login\u0026#39;,data=data) as resp: 9 print(resp.url) 10 print(resp.status) 11 print(await resp.text()) 12 return session 13 14session = asyncio.run(login()) 15print(f\u0026#34;{session}\u0026#34;) 再给一个完全版的主函数是进程，下载是协程的例子，注意里面的 aiohttp.TCPConnector(limit=10)，限制一下并发是10个，否则会被服务器 Ban 掉：\n1import asyncio 2import time 3import aiohttp 4from aiohttp.client import ClientSession 5 6async def download_link(url:str,session:ClientSession): 7 async with session.get(url) as response: 8 result = await response.text() 9 print(f\u0026#39;Read {len(result)}from {url}\u0026#39;) 10 11async def download_all(urls:list): 12 my_conn = aiohttp.TCPConnector(limit=10) 13 async with aiohttp.ClientSession(connector=my_conn) as session: 14 tasks = [] 15 for url in urls: 16 task = asyncio.ensure_future(download_link(url=url,session=session)) 17 tasks.append(task) 18 await asyncio.gather(*tasks,return_exceptions=True) # the await must be nest inside of the session 19 20url_list = [\u0026#34;https://www.google.com\u0026#34;,\u0026#34;https://www.bing.com\u0026#34;]*50 21print(url_list) 22start = time.time() 23asyncio.run(download_all(url_list)) 24end = time.time() 25print(f\u0026#39;download {len(url_list)}links in {end - start}seconds\u0026#39;) 协程里的 session 也有很多种用法，参考下面的链接就好：\nhttps://blog.csdn.net/weixin_39643613/article/details/109171090 我们也给出简单易用的线程池版，说不定以后会用上：\n1import requests 2from requests.sessions import Session 3import time 4from concurrent.futures import ThreadPoolExecutor 5from threading import Thread,local 6 7url_list = [\u0026#34;https://www.google.com/\u0026#34;,\u0026#34;https://www.bing.com\u0026#34;]*50 8thread_local = local() 9 10def get_session() -\u0026gt; Session: 11 if not hasattr(thread_local,\u0026#39;session\u0026#39;): 12 thread_local.session = requests.Session() 13 return thread_local.session 14 15def download_link(url:str): 16 session = get_session() 17 with session.get(url) as response: 18 print(f\u0026#39;Read {len(response.content)}from {url}\u0026#39;) 19 20def download_all(urls:list) -\u0026gt; None: 21 with ThreadPoolExecutor(max_workers=10) as executor: 22 executor.map(download_link,url_list) 23 24start = time.time() 25download_all(url_list) 26end = time.time() 27print(f\u0026#39;download {len(url_list)}links in {end - start}seconds\u0026#39;) ","date":"2021-11-12","img":"","permalink":"https://zhangrr.github.io/posts/20211112-python_aiohttp/","series":null,"tags":null,"title":"Python的协程详细解释"},{"categories":null,"content":"我们介绍了如何在 kubernetes 环境中使用 filebeat sidecar 方式收集日志\n使用的是 filebeat 的 moudle 模块，但凡是常用的软件，基本都有对应的模块可用，所以我们首先应该使用模块来收集日志。\n那对于一些我们自己写的 Go 软件呢，或者根本不是标准的，那该怎么办呢？\n那就自定义 filebeat.inputs，网上的 filebeat 例子，其实大多都是这样的\n很简单，给个例子\n1[beat-logstash-some-name-832-2015.11.28] IndexNotFoundException[no such index] 2 at org.elasticsearch.cluster.metadata.(IndexNameExpressionResolver.java:566) 3 at org.elasticsearch.cluster.metadata.(IndexNameExpressionResolver.java:133) 4 at org.elasticsearch.cluster.metadata.(IndexNameExpressionResolver.java:77) 5 at org.elasticsearch.action.admin..checkBlock(TransportDeleteIndexAction.java:75) 看如上日志，如果不用模式匹配的话，那么会送5条记录到 ES， Kibana 看起来就十分割裂了。\n所以要用正则把底下的4行和上面的第1行合在一起，合并成一条就记录推送到 ES 去\n仔细观察，开头一行是以 [ 开始的，所以正则就是 \u0026lsquo;^\\['，我们要匹配的是底下的4行，必须反转一下模式。\nmultiline 的 negate 属性，这个是指定是否反转匹配到的内容，这里如果是 true 的话，反转，那就是选择不以 [ 开头的行。 netgate 属性的缺省值是 false\nmultiline 的 match 属性，after 指追加到上一条事件（向上合并），before 指合并到下一条（向下合并）\n于是，我们就可以写出以下的匹配模式，这样就把匹配到条目追加到上一条，合并在一起了：\n1filebeat.inputs: 2 - type: log 3 enabled: true 4 paths: 5 - /Users/liuxg/data/multiline/multiline.log 6 multiline.type: pattern 7 multiline.pattern: \u0026#39;^\\[\u0026#39; 8 multiline.negate: true 9 multiline.match: after 10 11output.elasticsearch: 12 hosts: [\u0026#34;localhost:9200\u0026#34;] 13 index: \u0026#34;multiline\u0026#34; 14 再给个例子，java 的 stack 调用，日志格式如下：\n1Exception in thread \u0026#34;main\u0026#34; java.lang.NullPointerException 2 at com.example.myproject.Book.getTitle(Book.java:16) 3 at com.example.myproject.Author.getBookTitles(Author.java:25) 4 at com.example.myproject.Bootstrap.main(Bootstrap.java:14) 分析一下：\n 匹配到开头是空格的那些行，正则 \u0026lsquo;^[[:space]]\u0026rsquo; 匹配的行不反转，就是要那些空格开头的行，所以 negate 是缺省的 false 匹配的空格行追加到上一个事件，向上合并，所以 match 是 after  1filebeat.inputs: 2 - type: log 3 enabled: true 4 paths: 5 - /Users/liuxg/data/multiline/multiline.log 6 multiline.type: pattern 7 multiline.pattern: \u0026#39;^[[:space:]]\u0026#39; 8 multiline.negate: false 9 multiline.match: after 再给个例子，假如你在 Go 程序里定义了输出日志的格式，以 Start new event 开头，以 End event 结尾\n1[2015-08-24 11:49:14,389] Start new event 2[2015-08-24 11:49:14,395] Content of processing something 3[2015-08-24 11:49:14,399] End event 4[2015-08-24 11:50:14,389] Some other log 5[2015-08-24 11:50:14,395] Some other log 6[2015-08-24 11:50:14,399] Some other log 7[2015-08-24 11:51:14,389] Start new event 8[2015-08-24 11:51:14,395] Content of processing something 9[2015-08-24 11:51:14,399] End event 我们就用到 filebeat multiline 的另一个开关，flush_pattern，来控制如何结束，注意 negate 是 true ，匹配到的是 Start 和 End 中间的部分：\n1multiline.type: pattern 2multiline.pattern: \u0026#39;Start new event\u0026#39; 3multiline.negate: true 4multiline.match: after 5multiline.flush_pattern: \u0026#39;End event\u0026#39; 6 7filebeat.inputs: 8 - type: log 9 enabled: true 10 paths: 11 - /Users/liuxg/data/multiline/multiline.log 12 multiline.type: pattern 13 multiline.pattern: \u0026#39;Start new event\u0026#39; 14 multiline.negate: true 15 multiline.match: after 16 multiline.flush_pattern: \u0026#39;End event\u0026#39; 给两篇参考，大家一定要先看那篇原版英文的，中文的那篇翻得不太好；最好两篇对照着看。\n参考：\n https://www.elastic.co/guide/en/beats/filebeat/current/multiline-examples.html  https://developer.aliyun.com/article/764544   ","date":"2021-11-11","img":"","permalink":"https://zhangrr.github.io/posts/20211111-k8s_filebeat/","series":null,"tags":null,"title":"Kubernetes使用filebeat Multiline自定义收集日志"},{"categories":null,"content":"在生产环境使用 Kubernetes ，绕不过去的一个问题就是持久化卷。\n如果是使用阿里 ACK 托管平台的话，可以用 OSS 来持久化卷，如果是自搭的 kubernetes，那么存储就需要仔细考虑了。\nceph比较复杂，容易出故障。nfs 也不可用，毛病多多。minio倒是可以。\n这种情况下使用双副本的 GlusterFS 就是不错的选择。\n生产环境就不能随意了，最好不要使用 Heketi，因为凡是要持久化的东西，都是比较重要的东西，最好都有 yaml 记录。\nGlusterFS 的搭建就不说了。说说实际使用过程：\n一、装GFS，生产新卷 安装就不说了，我们的GFS有两个节点，172.19.20.18 和 172.19.20.36，我们强制建立一个两副本的卷： kuaijian-vol\n1gluster volume create kuaijian-vol replica 2 transport tcp 172.19.20.18:/glusterfs/kuaijian-vol 172.19.20.36:/glusterfs/kuaijian-vol force 二、为k8s产生GFS的endingpoint和service 1cat \u0026lt;\u0026lt; EOF \u0026gt;\u0026gt; ep-svc.yaml 2--- 3apiVersion: v1 4kind: Service 5metadata: 6name: gfs-cluster_svc 7spec: 8ports: 9- port: 1 10--- 11apiVersion: v1 12kind: Endpoints 13metadata: 14name: gfs-cluster_svc 15subsets: 16- addresses: 17- ip: 172.19.20.18 18ports: 19- port: 1 20- addresses: 21- ip: 172.19.20.36 22ports: 23- port: 1 24EOF 25 26kubectl apply -f ep-svc.yaml 这里要提一个概念，通常情况下 service 是通过 selector 标签来选择对应的 pod 来增加 endingpoint 的。如下：\n1apiVersion: v1 2kind: Service 3metadata: 4 name: go-api_svc 5spec: 6 ports: 7 - port: 8080 8 protocol: TCP 9 targetPort: 8080 10 selector: 11 app: go-api 12 type: ClusterIP 而上面，我们没有通过标签，而是让 endingpoint 和 svc 同名而手动增加 endingpoint 到 svc 的。\n三、为k8s生产创建 PV和PVC 静态环境不使用 Storageclass 持久化卷的图解如下，pod做pvc声明，pvc连接到pv，pv从GFS中拿到卷。\n首先声明一个 pv：\n1cat \u0026lt;\u0026lt; EOF \u0026gt;\u0026gt; kuaijian-pv.yaml 2apiVersion: v1 3kind: PersistentVolume 4metadata: 5name: gfs-kuaijian-50G_pv 6labels: 7name: gfs-kuaijian-50G_pv 8spec: 9capacity: 10storage: 50Gi 11accessModes: 12- ReadWriteMany 13glusterfs: 14endpoints: gfs-cluster_svc 15path: kuaijian-vol 16readOnly: false 17persistentVolumeReclaimPolicy: Retain 18EOF 19 20kubectl apply -f kuaijian-pv.yaml 然后声明一个 pvc 通过 matchLabels 来跟之前的 pv 绑定。\n1cat \u0026lt;\u0026lt; EOF \u0026gt;\u0026gt; kuaijian-pvc.yaml 2kind: PersistentVolumeClaim 3apiVersion: v1 4metadata: 5name: gfs-kuaijian-50G_pvc 6spec: 7accessModes: 8- ReadWriteMany 9resources: 10requests: 11storage: 50Gi 12selector: 13matchLabels: 14name: gfs-kuaijian-50G_pv 15EOF 16 17kubectl apply -f kuaijian-pvc.yaml 注意上面的 pvc，我们一下子申请了50G，把整个 pv 空间全用光了；当然我们也可以只申请个 10Gi，下个 pvc 再 10Gi，这样也是行的通的。\n四、POD使用PVC 声明一个 Nginx 的 Deployment 来使用这个 pvc\n1apiVersion: apps/v1 2kind: Deployment 3metadata: 4 name: nginx-deployment 5 labels: 6 app: nginx 7spec: 8 replicas: 2 9 selector: 10 matchLabels: 11 app: nginx 12 template: 13 metadata: 14 labels: 15 app: nginx 16 spec: 17 containers: 18 - name: nginx 19 image: nginx 20 ports: 21 - containerPort: 80 22 volumeMounts: 23 - name: data-www 24 mountPath: /data/www 25 volumes: 26 - name: data-www 27 persistentVolumeClaim: 28 claimName: gfs-kuaijian-50G_pvc 这样 kubernetes 的存储部分就搞定了。GFS 用于生产非常稳定，基本跑了 7 年了没有大毛病。\n","date":"2021-11-10","img":"","permalink":"https://zhangrr.github.io/posts/20211110-k8s_gfs/","series":null,"tags":null,"title":"生产环境kubernetes使用持久化卷GlusterFS"},{"categories":null,"content":"在生产环境中，ES 通常是不会在 k8s 集群中存在的，一般 MySQL 和 Elasticsearch 都是独立在 k8s 之外。\n那么无论哪种 pod，要甩日志到 ES，最轻量的方案肯定是用 filebeat 甩过去了。\n当然，如果是阿里的 ACK，logtail 和 logstore 配搭已经非常不错了，根本用不到 filebeat 和 ES。\n可但是，我们不想为阿里 sls、logstore 出钱买单，就只能用 filebeat + ES 了\n说一下 filebeat 的 sidecar 边车（僚机）用法：\n如上图所示，简单说就是起一个 filebeat 的 logging-agent 边车（僚机），边车和主应用之间共享某个文件夹（mountPath），达到收集主应用日志并发送到 ES，而不用动 app-container 分毫。\n我们以部署一个 Tomcat 应用为例来说明：\n一、打造 filebeat 边车镜像 首先准备 Dockerfile\n1FROM alpine:3.12  2 3ARG VERSION=7.15.1  4 5COPY docker-entrypoint.sh /  6 7RUN set -x \\  8 \u0026amp;\u0026amp; cd /tmp \\  9 \u0026amp;\u0026amp; wget https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-${VERSION}-linux-x86_64.tar.gz \\  10 \u0026amp;\u0026amp; tar xzvf filebeat-${VERSION}-linux-x86_64.tar.gz \\  11 \u0026amp;\u0026amp; mv filebeat-${VERSION}-linux-x86_64 /opt \\  12 \u0026amp;\u0026amp; rm /tmp/* \\  13 \u0026amp;\u0026amp; chmod +x /docker-entrypoint.sh  14 15 16ENV PATH $PATH:/opt/filebeat-${VERSION}-linux-x86_64  17 18WORKDIR /opt/filebeat-${VERSION}-linux-x86_64  19 20ENTRYPOINT [\u0026#34;/docker-entrypoint.sh\u0026#34;]  我们以 alphine:3.12 为底版，然后下载 filebeat 7.15.1的二进制包并释放到 /opt 下，最后指定入口文件 /docker-entrypoint.sh\n奥妙全在这个 docker-entrypoint.sh 中了\n1#!/bin/bash 2 3cat \u0026gt; /etc/filebeat.yaml \u0026lt;\u0026lt; EOF 4filebeat.config.modules: 5path: /opt/filebeat-7.15.1-linux-x86_64/modules.d/*.yml 6reload.enabled: true 78# 加入自定义的字段 9fields_under_root: true 10fields: 11project: kuaijian-tomcat 12pod_ip: ${POD_IP} 13pod_name: ${POD_NAME} 14node_name: ${NODE_NAME} 15pod_namespace: ${POD_NAMESPACE} 1617# 收集云厂商的数据和docker的变量 18processors: 19- add_cloud_metadata: ~ 20- add_docker_metadata: ~ 2122filebeat.modules: 23- module: apache 24access: 25enabled: true 26var.paths: 27- \u0026#34;/usr/local/tomcat/logs/localhost_access_log.*.txt\u0026#34; 28error: 29enabled: true 30var.paths: 31- \u0026#34;/usr/local/tomcat/logs/application.log*\u0026#34; 32- \u0026#34;/usr/local/tomcat/logs/catalina.*.log\u0026#34; 33- \u0026#34;/usr/local/tomcat/logs/host-manager.*.log\u0026#34; 34- \u0026#34;/usr/local/tomcat/logs/localhost.*.log\u0026#34; 35- \u0026#34;/usr/local/tomcat/logs/manager.*.log\u0026#34; 3637setup.template.name: \u0026#34;tomcat-logs\u0026#34; 38setup.template.pattern: \u0026#34;tomcat-logs-*\u0026#34; 39output.elasticsearch: 40hosts: [\u0026#34;172.19.20.xxx:9200\u0026#34;,\u0026#34;172.19.20.xxx:9200\u0026#34;] 41index: \u0026#34;tomcat-logs-%{+yyyy.MM}\u0026#34; 42EOF 43 44set -xe 45 46# If user don\u0026#39;t provide any command  47# Run filebeat  48if [[ \u0026#34;$1\u0026#34; == \u0026#34;\u0026#34; ]]; then 49 exec /opt/filebeat-7.15.1-linux-x86_64/filebeat -c /etc/filebeat.yaml 50else 51 # Else allow the user to run arbitrarily commands like bash  52 exec \u0026#34;$@\u0026#34; 53fi 我们为什么不在 k8s 里用 configmap 来配置 filebeat.yml 呢？\n理由是收集日志文件多且路径、类型各不相同，这么一堆的配置都放在 configmap 里会让人癫狂的。所以干脆放到镜像里，便于调试也便于修改。\n上面我们也充分利用了 filebeat 的 module，有 module 可用就必须用 module，而不是手动指定 filebeat.inputs ，可用的 mudole 实在太多了，一定要善用！！！另外 tomcat 和 apache 的日志格式是一样的。\n我们在最后执行的时候，也加了 exec $@ 便于调试，如果没有指定 CMD，就启动 filebeat，如果指定了比如 /bin/bash，就进入调试状态。\n我们打好镜像就 push 到 harbor 里待用\n附录：https://www.elastic.co/guide/en/beats/filebeat/current/configuration-general-options.html filebeat的配置列表\n二、sidecar部署 我们写一个 k8s 的 tomcat deployment文件：\n1apiVersion: apps/v1 2kind: Deployment 3metadata: 4 name: tomcat 5 labels: 6 app: tomcat 7spec: 8 replicas: 1 9 selector: 10 matchLabels: 11 app: tomcat 12 template: 13 metadata: 14 labels: 15 app: tomcat 16 spec: 17 containers: 18 - name: filebeat-sidecar 19 image: xxxx.xxxx.xxx/filebeat:xxx 20 env: 21 - name: POD_NAMESPACE 22 valueFrom: 23 fieldRef: 24 apiVersion: v1 25 fieldPath: metadata.namespace 26 - name: NODE_NAME 27 valueFrom: 28 fieldRef: 29 apiVersion: v1 30 fieldPath: spec.nodeName 31 - name: POD_IP 32 valueFrom: 33 fieldRef: 34 apiVersion: v1 35 fieldPath: status.podIP 36 - name: POD_NAME 37 valueFrom: 38 fieldRef: 39 apiVersion: v1 40 fieldPath: metadata.name  41 volumeMounts: 42 - name: logs-volume 43 mountPath: /usr/local/tomcat/logs 44 - name: tomcat 45 image: tomcat 46 ports: 47 - containerPort: 8080 48 volumeMounts: 49 - name: logs-volume 50 mountPath: /usr/local/tomcat/logs 51 volumes: 52 - name: logs-volume 53 emptyDir: {} 可以看到我们在这个 deployment 里定义了 pod 是单副本，里面跑了两个 container，一个是 filebeat，一个是 tocmat，两者通过同一个 volume 连接在一起，这样就可以做到不修改 tomcat container 而拿到里面的日志了。\n这样就把 tomcat 应用的日志收到 ES 去了。\n","date":"2021-11-10","img":"","permalink":"https://zhangrr.github.io/posts/20211110-k8s_sidecar/","series":null,"tags":null,"title":"Kubernetes生产环境使用filebeat Sidecar收集日志"},{"categories":null,"content":"我们选择 haproxy 1.8 版本以上的，编译安装到路径 /export/servers/haproxy\n1make TARGET=linux2628 PREFIX=/export/servers/haproxy USE_GETADDRINFO=1 USE_ZLIB=1 USE_REGPARM=1 USE_OPENSSL=1 \\ 2 USE_SYSTEMD=1 USE_PCRE=1 USE_PCRE_JIT=1 USE_NS=1 3 4make install PREFIX=/export/servers/haproxy 编辑 haproxy.conf 配置文件：\n1global  2 maxconn 5120  3 chroot /export/servers/haproxy  4 daemon  5 quiet  6 nbproc 2  7 pidfile /tmp/haproxy.pid 8 9defaults  10 timeout connect 5s  11 timeout client 50s  12 timeout server 20s 13 14listen http  15 bind :80  16 timeout client 1h  17 tcp-request inspect-delay 2s  18 acl is_http req_proto_http  19 tcp-request content accept if is_http  20 server server-http :8080 21 use_backend ssh if !is_http 22 23backend ssh  24 mode tcp  25 timeout server 1h  26 server server-ssh :22 解释一下：我们在 8080 端口开了 http 服务，在 22 端口开了 ssh 服务，80端口由 haproxy 做代理转发，首先判断客户端请求是否是 http 请求，如果是就转发到 8080 端口，如果不是，就转发到 22 端口，这样就实现了 80 端口同时跑 http 和 ssh 两个服务。\n","date":"2021-11-10","img":"","permalink":"https://zhangrr.github.io/posts/20211110-haproxy_multiple_port/","series":null,"tags":null,"title":"Haproxy一个端口跑多个服务"},{"categories":null,"content":"Ucloud的机器在两会期间干脆端口全灭，firewall设置进来的端口全关闭！！！！！！\n还好有个Global ssh的服务，可以 ssh ubuntu@111.129.37.89.ipssh.net 登录上去，注意，直接ssh ubuntu@111.129.37.89 是不通的。\n那我们就搭建一个SSLH服务，可以把ssh和openvpn以及ssl服务统统塞到一个端口22里\n动手吧\n1、修改openssh的端口 从22端口改成2222，千万别重启，22这会先得归ssh用\n2、安装sslh\n1sudo apt install sslh 2vi /etc/default/sslh 3 4找到Run=no 5改成Run=yes 6 7然后到下面，按需配置（不跑443的话可以不配） 8DAEMON_OPTS=\u0026#34;--user sslh --listen 0.0.0.0:22 --ssh 127.0.0.1:2222 --ssl 127.0.0.1:443 --openvpn 127.0.0.1:1194 --pidfile /var/run/sslh/sslh.pid --timeout 5\u0026#34; 3、配置sslh并且重启服务器\n1sudo systemctl enable sslh 2sudo reboot 就搞定了\n","date":"2021-11-10","img":"","permalink":"https://zhangrr.github.io/posts/20211110-sslh_multiple_port/","series":null,"tags":null,"title":"Sslh的一个端口同时跑多个服务"},{"categories":null,"content":"这个要求挺古怪的，背景是防火墙只开了 nginx 443 端口。我也想同时 ssh 登录进去，但是F5没开IP\n就只能这么干了，让 Nginx 一个端口跑多个服务\n在 nginx.conf 加一段，stream 配置，nginx 的 ip 是 192.168.8.110：\n1 2#Multi Ports 3stream { 4 upstream ssh { 5 server 192.168.8.112:22; 6 } 7 8 upstream https { 9 server 192.168.8.111:443; 10 } 11 12 map $ssl_preread_protocol $upstream { 13 default ssh; 14 \u0026#34;TLSv1.2\u0026#34; https; 15 \u0026#34;TLSv1.3\u0026#34; https; 16 \u0026#34;TLSv1.1\u0026#34; https; 17 \u0026#34;TLSv1.0\u0026#34; https; 18 } 19 20 # SSH and SSL on the same port 21 server { 22 listen 443; 23 24 proxy_pass $upstream; 25 ssl_preread on; 26 } 27} 28 测试一下：\n1curl -v https://192.168.8.110 2 3ssh 192.168.8.110 -p 443 4 这样就可以了。\n","date":"2021-11-09","img":"","permalink":"https://zhangrr.github.io/posts/20211109-nginx_multiple_port/","series":null,"tags":null,"title":"Nginx的一个端口同时跑SSH和HTTPS服务"},{"categories":null,"content":"kubernetes 装好正常运行一段时间后，会出现要把研发和运维权限分开的场景：\n比如：\n 给某个用户某一指定名称空间下的管理权限 给用户赋予集群的只读权限 …  非常麻烦，我们这里不讨论过多的概念，从运维的角度出发，简单实用化\n我们需要明确三个RBAC最基本的概念\n Role: 角色，它定义了一组规则，定义了一组对Kubernetes API对象的操作权限 RoleBinding: 定义了\u0026quot;被作用者\u0026quot;和\u0026quot;角色\u0026quot;的绑定关系 Subject: 被作用者，既可以是\u0026quot;人\u0026quot;，也可以是机器，当然也可以是 Kubernetes 中定义的用户(ServiceAccount主要负责kubernetes内置用户)  我们的操作过程流程如下，首先创建客户端证书；其次创建Role角色；再创建RoleBinding，把Subject和Role绑定，就完事了；最后一步是生成 kubectl 的配置文件。\n一、创建客户端证书 我们以已建好的阿里 ACK 为例，或者自建好的 Kubernetes 也行；确定已经有了 .kube/config 配置文件，拥有集群最高权限，并且可以正常执行 kubectl 命令。\n首先是生成证书，并向集群提出证书请求并签发，脚本如下：\n1#!/bin/sh 2 3useraccount=reader 4 5openssl req -new -newkey rsa:4096 -nodes -keyout $useraccount-k8s.key -out $useraccount-k8s.csr -subj \u0026#34;/CN=$useraccount/O=devops\u0026#34; 6csr=$(cat $useraccount-k8s.csr | base64 | tr -d \u0026#39;\\n\u0026#39;) 7cat \u0026lt;\u0026lt; EOF \u0026gt; k8s-csr.yaml 8apiVersion: certificates.k8s.io/v1beta1 9kind: CertificateSigningRequest 10metadata: 11name: $useraccount-k8s-access 12spec: 13groups: 14- system:authenticated 15request: $csr 16usages: 17- client auth 18EOF 19 20 21kubectl create -f k8s-csr.yaml 22kubectl certificate approve $useraccount-k8s-access 23 24kubectl get csr $useraccount-k8s-access -o jsonpath=\u0026#39;{.status.certificate}\u0026#39; | base64 --decode \u0026gt; $useraccount-k8s-access.crt 25kubectl config view -o jsonpath=\u0026#39;{.clusters[0].cluster.certificate-authority-data}\u0026#39; --raw | base64 --decode - \u0026gt; k8s-ca.crt 解释一下：我们定义了一个用户CN=reader，然后向集群发送了证书请求并签发，最终从集群获得了 reader-k8s-access.crt 的客户端证书和 k8s-ca.crt 的 CA 证书。\nk8s-csr.yaml 和 reader-k8s.csr 都是中间产物，最终我们有了客户端私钥 reader-k8s.key ，客户端证书 reader-k8s-access.crt ，CA 证书 k8s-ca.crt 这三个有用的文件。\n二、创建 Role 角色 1cat \u0026lt;\u0026lt; EOF \u0026gt;\u0026gt; role.yaml 2apiVersion: rbac.authorization.k8s.io/v1 3kind: Role 4metadata: 5name: role-pods_reader 6namespace: default 7rules: 8- apiGroups: 9- \u0026#34;\u0026#34; 10resources: 11- pods 12- pods/log 13verbs: 14- get 15- list 16- watch 17EOF 18 19kubectl apply -f role.yaml 解释：我们创建了一个 role 角色，名字叫做 role-pods_reader，所属命名空间是 default ，它对 pods 和 pods/log 有 get 、list、watch的权限，也就是说role-pods_reader 可以查看 default 空间的 pods 和 pods 的日志。\n三、创建 Rolebinding 1cat \u0026lt;\u0026lt; EOF \u0026gt;\u0026gt; rolebinding.yaml 2apiVersion: rbac.authorization.k8s.io/v1 3kind: RoleBinding 4metadata: 5name: rolebinding-default_pods_reader 6namespace: default 7roleRef: 8apiGroup: rbac.authorization.k8s.io 9kind: Role 10name: role-pods_reader 11subjects: 12- apiGroup: rbac.authorization.k8s.io 13kind: User 14name: reader 15EOF 16 17kubectl apply -f rolebinding.yaml 解释：我们创建了一个 rolebinding，名字叫做 rolebinding-default_pods_reader，同样所属命名空间是 default，它绑了两个东西，一个是 role，就是上面第二步创建的 role-pods_reader；另一个是 subject，对应了一个用户，就是我们第一步创建的那个 reader。\n四、生成 kubectl 配置文件 1#!/bin/sh 2useraccount=reader 3namespace=default 4 5kubectl config set-cluster $(kubectl config view -o jsonpath=\u0026#39;{.clusters[0].name}\u0026#39;) --server=$(kubectl config view -o jsonpath=\u0026#39;{.clusters[0].cluster.server}\u0026#39;) --certificate-authority=k8s-ca.crt --embed-certs --kubeconfig=$useraccount-k8s-config 6 7kubectl config set-credentials $useraccount --client-certificate=$useraccount-k8s-access.crt --client-key=$useraccount-k8s.key --embed-certs --kubeconfig=$useraccount-k8s-config 8 9kubectl config set-context $useraccount --cluster=$(kubectl config view -o jsonpath=\u0026#39;{.clusters[0].name}\u0026#39;) --namespace=$namespace --user=$useraccount --kubeconfig=$useraccount-k8s-config 10 11kubectl config use-context $useraccount --kubeconfig=$useraccount-k8s-config 解释：上面看起来很复杂，其实就是四步，在配置文件里设置 cluster 、设置 credentials 证书、设置 context 上下文、设置当前上下文。\n完事后会产生一个完整的 reader-k8s-config 文件，如下：\n五、测试 我们验证一下：\n1KUBECONFIG=reader-k8s-config kubectl get pods 2 3KUBECONFIG=reader-k8s-config kubectl auth can-i delete pods 4 5KUBECONFIG=reader-k8s-config kubectl auth can-i delete svc 6 这样一个对 default 空间的只读用户就建立好了\n六、集群只读用户 我们这里给出集群只读用户的 role，命名 role-cluster_reader\n1kubectl apply -f - \u0026lt;\u0026lt;EOF 2apiVersion: rbac.authorization.k8s.io/v1 3kind: ClusterRole 4metadata: 5name: role-cluster_reader 6rules: 7- apiGroups: 8- \u0026#34;\u0026#34; 9resources: 10- nodes 11- pods 12- pods/exec 13- pods/log 14- services 15- configmaps 16- secrets 17- serviceaccounts 18- endpoints 19verbs: 20- get 21- list 22- watch 23- apiGroups: 24- apps 25resources: 26- deployments 27- replicasets 28- daemonsets 29- statefulsets 30verbs: 31- get 32- list 33- watch 34- apiGroups: 35- batch 36resources: 37- jobs 38- cronjobs 39verbs: 40- get 41- list 42- watch 43EOF 以及 rolebinding，还是绑到第一步的用户 reader 的例子\n1kubectl apply -f - \u0026lt;\u0026lt;EOF 2apiVersion: rbac.authorization.k8s.io/v1 3kind: ClusterRoleBinding 4metadata: 5name: rolebinding-cluster_reader 6roleRef: 7apiGroup: rbac.authorization.k8s.io 8kind: ClusterRole 9name: role-cluster_reader 10subjects: 11- apiGroup: rbac.authorization.k8s.io 12kind: User 13name: reader 14EOF 生成配置文件的步骤跟上一步是一样的。\n从上面大家可以看到，其实最主要的就是 role 的 yaml 文件，里面控制着到底有怎么样的权限。\n","date":"2021-11-09","img":"","permalink":"https://zhangrr.github.io/posts/20211109-k8s_rbac/","series":null,"tags":null,"title":"Kubernetes创建普通账号"},{"categories":null,"content":"没啊办法，翻墙翻墙还是翻墙。\n上游有若干 trojian 、v2ray 、sock5 、http各种各样的代理，这样多种的选择，那么就装一个 clash 客户端就可以全接管了。\n说下我们的做法：找个小Linux做旁路由，DNS和网关都设置在这台机器上，局域网内的机器都通过这台上网。\n我们用到的是 clash 的 Tproxy redir-host 和 udp-proxy 模式，这种模式比较强大。用就用最强大的。\n安装很简单，操作系统 centos 或者 ubuntu 都行，项目地址：\nhttps://github.com/Dreamacro/clash 说明书：\nhttps://lancellc.gitbook.io/clash/clash-config-file/proxy-groups/load-balance 首先下载二进制文件，现在版本是 v1.7.1，解压后放到 /usr/local/bin 目录下\n1wget https://github.com/Dreamacro/clash/releases/download/v1.7.1/clash-linux-amd64-v1.7.1.gz 2gzip -d clash-linux-amd64-v1.7.1.gz 3chmod 755 clash-linux-amd64-v1.7.1 4mv clash-linux-amd64-v1.7.1 /usr/local/bin 然后生成 clash.service\n1cat \u0026lt;\u0026lt; EOF \u0026gt;\u0026gt; /etc/systemd/system/clash.service 2[Unit] 3Description=clash service 4After=network.target 56[Service] 7Type=simple 8User=root 9ExecStart=/usr/local/bin/clash-linux-amd64-v1.7.1 10Restart=on-failure # or always, on-abort, etc 1112[Install] 13WantedBy=multi-user.target 14EOF 然后最重要的，就是配置文件了\n我这里这个旁路由的设备 IP 地址是 192.168.2.2，网卡设备是 enp2s0\n下卖弄\n1# http的代理端口 2port: 7890 3# sock5的代理端口 4socks-port: 7891 5# 重定向的端口 6redir-port: 7892 7 8ipv6: false 9 10allow-lan: true 11bind-address: \u0026#39;192.168.2.2\u0026#39; 12interface-name: enp2s0 13 14mode: rule 15log-level: info 16external-controller: 0.0.0.0:9090 17secret: \u0026#34;Fuck2021\u0026#34; 18external-ui: dashboard 19 20profile: 21 store-selected: false 22 tracing: true 23 24hosts: 25 # 把cantv的域名解析屏蔽掉，禁止它自动升级 26 \u0026#39;tms.can.cibntv.net\u0026#39;: 0.0.0.0 27 28dns: 29 enable: true 30 listen: 0.0.0.0:1053 31 enhanced-mode: redir-host 32 nameserver: 33 - 114.114.114.114 34 - tls://dns.rubyfish.cn:853 # dns over tls 35 - https://1.1.1.1/dns-query # dns over https 36 - tcp://1.1.1.1 37 38proxies: 39 - name: \u0026#34;trojan1\u0026#34; 40 type: trojan 41 server: www.linuxboy.net 42 port: 443 43 password: Fuck2021 44 sni: www.linuxboy.net 45 skip-cert-verify: true 46 47 - name: \u0026#34;vmess1\u0026#34; 48 type: vmess 49 server: 101.59.201.93 50 port: 41555 51 uuid: 7a17ae5e-fb86-42e2-abd4-b8c33cfabcd 52 alterId: 64 53 cipher: auto 54 55proxy-groups: 56 - name: Proxy 57 type: select 58 proxies: 59 - trojan 60 61 - name: \u0026#34;auto\u0026#34; 62 type: url-test 63 proxies: 64 - vmess1 65 - trojan1 66 url: \u0026#39;http://www.gstatic.com/generate_204\u0026#39; 67 interval: 300 68 69rules: 70 - DOMAIN-SUFFIX,v2ex.com,Proxy 71 - DOMAIN-SUFFIX,t66y.com,auto 72 - DOMAIN-SUFFIX,reddit.com,Proxy 73 - SRC-IP-CIDR,192.168.1.0/32,DIRECT 74 - SRC-IP-CIDR,192.168.2.0/32,DIRECT 75 - IP-CIDR,127.0.0.0/8,DIRECT 76 - GEOIP,CN,DIRECT 77 - MATCH,Proxy 解释一下：proxy 定义了两个代理，一个是 trojan，一个是 v2ray。然后再集合成组，一个组叫 Proxy， 显式指定用 trojan；另一个组叫 auto，根据 vmess1 和 trojan1 访问 http://www.gstatic.com/generate_204 的页面速度，谁快就用谁，缺省300秒会访问一次这个页面来决定哪个代理快。\n剩下的 rules 就很简单，把自己知道要访问的域名放到代理中去，然后把局域网的 IP 段放进 DIRECT 直接访问，最后 GEO IP 不是中国的由 Proxy 兜底。\n网上有一大堆规则，八戒的建议是不要去学，规则越多越慢，你自己知道要访问什么网站需要翻墙，加进去就好了。弄一堆，自己看着都头蒙\n最后我们在 rc.local 放入以下 iptable 内容，就可以了\n1#clash 2#tcp 3iptables -t nat -N clash 4iptables -t nat -A clash -d 0.0.0.0/8 -j RETURN 5iptables -t nat -A clash -d 10.0.0.0/8 -j RETURN 6iptables -t nat -A clash -d 127.0.0.0/8 -j RETURN 7iptables -t nat -A clash -d 169.254.0.0/16 -j RETURN 8iptables -t nat -A clash -d 172.16.0.0/12 -j RETURN 9iptables -t nat -A clash -d 192.168.0.0/16 -j RETURN 10iptables -t nat -A clash -d 224.0.0.0/4 -j RETURN 11iptables -t nat -A clash -d 240.0.0.0/4 -j RETURN 12iptables -t nat -A clash -d 192.168.2.2 -j RETURN 13iptables -t nat -A clash -p tcp -j REDIRECT --to-port 7892 14iptables -t nat -I PREROUTING -p tcp -d 8.8.8.8 -j REDIRECT --to-port 7892 15iptables -t nat -I PREROUTING -p tcp -d 8.8.4.4 -j REDIRECT --to-port 7892 16iptables -t nat -A PREROUTING -p tcp -j clash 17 18#udp 19ip rule add fwmark 1 table 100 20ip route add local default dev lo table 100 21iptables -t mangle -N clash 22iptables -t mangle -A clash -d 0.0.0.0/8 -j RETURN 23iptables -t mangle -A clash -d 10.0.0.0/8 -j RETURN 24iptables -t mangle -A clash -d 127.0.0.0/8 -j RETURN 25iptables -t mangle -A clash -d 169.254.0.0/16 -j RETURN 26iptables -t mangle -A clash -d 172.16.0.0/12 -j RETURN 27iptables -t mangle -A clash -d 192.168.0.0/16 -j RETURN 28iptables -t mangle -A clash -d 224.0.0.0/4 -j RETURN 29iptables -t mangle -A clash -d 240.0.0.0/4 -j RETURN 30iptables -t mangle -A clash -d 192.168.2.2 -j RETURN 31iptables -t mangle -A clash -p udp -j TPROXY --on-port 7892 --tproxy-mark 1 32iptables -t mangle -A PREROUTING -p udp -j clash 33iptables -t nat -N CLASH_DNS 34iptables -t nat -F CLASH_DNS 35iptables -t nat -A CLASH_DNS -p udp -j REDIRECT --to-port 1053 36iptables -t nat -I OUTPUT -p udp --dport 53 -j CLASH_DNS 37iptables -t nat -I PREROUTING -p udp --dport 53 -j REDIRECT --to 1053 最后启动clash\n1systemctl start clash ","date":"2021-11-08","img":"","permalink":"https://zhangrr.github.io/posts/20211108-clash/","series":null,"tags":null,"title":"Clash的搭建教程"},{"categories":null,"content":"上篇简单介绍了 onedev ，这篇我们具体拿个 java spring 的项目来实际编译一下\n首先必须确认环境：\nonedev 和 agent 都是用 root 安装运行的，然后已经安装了 docker，且 selinux 设置为 disabled，否则会出权限麻烦。\n我们用的例子是 spring 的 petclinic，正常的 build 的步骤如下：\n1git clone https://github.com/spring-projects/spring-petclinic.git 2cd spring-petclinic 3./mvnw package 我们首先在 onedev 的 projects 新建一个项目 spring-boot\n然后到 clone 下来的源代码目录下\n1cd spring-petclinic 2 3git init 4git add . 5git commit -m \u0026#34;Spring boot demo project\u0026#34; 6 7git remote add origin http://192.168.86.101:6610/spring-boot 8git push --set-upstream origin master 这样就可以在 spring-boot 里看到代码了\n然后看上图，有个紫色灯泡，Enable build support by adding.onedev-buildspec.yml，点那个链接\n我们增加一个 build 的步骤，里面再增加两个 job，一个是 Get code ，一个是 build\n第一步肯定是把代码拿下来，选择 Checkout Code ，命名为 Get code ，然后其他保持缺省配置，保存\n第二步就是 build 代码，选择 Execute Shell/Batch Commands，然后实际是启动了一个 docker 镜像来执行 build 的过程\n  Image 填入：maven:3.5-jdk-8-alphine\n  Commands 填入：\n1unset MAVEN_CONFIG 2cat \u0026lt;\u0026lt; EOF \u0026gt;\u0026gt; /root/.m2/settings.xml 3\u0026lt;settings\u0026gt; 4\u0026lt;proxies\u0026gt; 5\u0026lt;proxy\u0026gt; 6\u0026lt;id\u0026gt;default\u0026lt;/id\u0026gt; 7\u0026lt;active\u0026gt;true\u0026lt;/active\u0026gt; 8\u0026lt;protocol\u0026gt;http\u0026lt;/protocol\u0026gt; 9\u0026lt;host\u0026gt;192.168.1.10\u0026lt;/host\u0026gt; 10\u0026lt;port\u0026gt;3128\u0026lt;/port\u0026gt; 11\u0026lt;/proxy\u0026gt; 12\u0026lt;/proxies\u0026gt; 13\u0026lt;/settings\u0026gt; 14EOF 15./mvnw package 解释一下，maven:3.5-jdk-8-alphine 这个镜像，如果在里面执行 mvnw package ，会报错 repository 的错，所以必须把 MAVEN_CONFIG 的环境变量给删除，另外整个 build 过程会去拉 N 多包和配置，大概200多兆，如果不翻墙，基本是失败。所以这里强制 mvn 使用了代理！！！否则 build 一天都不会成功。\n  第三步是配置 mvn repository的缓存，大家不想每次build都去下一遍依赖包吧，在 More Settings 设置\n在Caches里填入：\n  key: maven-cache\n  path: /root/.m2/repository\n  然后执行 Build , 然后等待，第一次时间会很长，终于 Successful 了\n接下来的步骤就可以用 Dockerfile 生成镜像，然后推到 Harbor，再下载或者 gitops下载来各种 yaml 文件，拉 kubectl 和配置文件，就可以推送到 kubernetes 了。\n补充一下，在别的地方突然看到有 maven 的阿里镜像地址，记录一下：\n1 \u0026lt;mirror\u0026gt; 2 \u0026lt;id\u0026gt;alimaven\u0026lt;/id\u0026gt; 3 \u0026lt;name\u0026gt;aliyun maven\u0026lt;/name\u0026gt; 4 \u0026lt;url\u0026gt;http://maven.aliyun.com/nexus/content/groups/public/\u0026lt;/url\u0026gt; 5 \u0026lt;mirrorOf\u0026gt;*\u0026lt;/mirrorOf\u0026gt; 6 \u0026lt;/mirror\u0026gt; ","date":"2021-11-08","img":"","permalink":"https://zhangrr.github.io/posts/20211108-onedev_maven/","series":null,"tags":null,"title":"Onedev构建一个实际java Spring应用"},{"categories":null,"content":"vpn的搭建一直是一个难解的题目，openvpn、ipsec、tinc 都在用，都不够简洁。\nn2n 是一种 peer to peer 端到端的 vpn，搭建起来非常简单方便。必备利器\n首先了解一下概念\nn2n 的 vpn 分为两种角色，Supernode 超级节点和 Edgenode 边缘节点。\n很简单，Supernode 最好是公网地址，需要开放端口供其它节点连接上来；剩余的 Edgenode 可以没有公网地址，各个 Edgenode 边缘节点之间会尝试绕过 Supernode 直接连接。这大大提高了节点之间通讯的效率。通讯是加密的，非常安全。\n安装的话完全不用安装，直接一个命令就搞定了。\nhttps://github.com/ntop/n2n 弄出两个可执行文件 supernode 和 edge 就好了\n超级节点：\n1/usr/local/bin/supernode -p 11111 -v 边缘节点：\n1/usr/local/bin/edge -d n2n0 -c ThisisaSecret2012 -k Fuck2020 -a 192.168.0.2 -l 41.22.59.112:11111 -f 参数解释：\n -d 生成的虚拟网卡的命名 -c 某组vpn节点的口令。大家看到启动 supernode 的时候没有任何参数，supdernode 只负责转发。supernode 支持多组不同的 vpn。这里就是用来区分不同的 vpn 组的。 -k 节点之间通讯是用的 twofish 加密算法，这里指定的是密钥 -a 节点的ip -l supernode的ip和端口 -f 放到后台 daemon 执行  这样就建立好了，系统中会多出一张 n2n0 的网卡。\n","date":"2021-11-05","img":"","permalink":"https://zhangrr.github.io/posts/20211105-n2n_vpn/","series":null,"tags":null,"title":"N2n一种peer to Peer的VPN的使用"},{"categories":null,"content":"其实一直在用 github、gitlab、jenkins，但是 github 时不时的抽风， gitlab 的 runner 套 Docker in docker 的方法很难用。\n所以 CI/CD 这一块挺喜欢阿里云效这种简便易行的。但确实找不到合适类似的\n从 V2EX 上看到一个老哥发的 onedev，就是一站式的工具，这不就试试先\n以 centos7 为例，安装过程如下：\n一、安装 java 1.8 版本 1rpm -ivh jdk-8u201-linux-x64.rpm 二、安装 git 高版本 缺省 centos7 和 epel 带的 git 版本太低，不符合要求，得加个新的源装新版本\n1yum -y install https://packages.endpoint.com/rhel/7/os/x86_64/endpoint-repo-1.7-1.x86_64.rpm 2yum -y install git curl 三、安装配置 onedev https://github.com/theonedev/onedev 下载压缩包，然后解压运行 bin/server.sh start ，很简洁，不错！\n运行完打开 http://192.168.86.101:6610 进行初始配置，就两步就 ok 了。\n三、例子 我们是要用的，所以在 projects 新建一个项目 spring-boot\n然后到源代码目录下\n1git init 2git add . 3git commit -m \u0026#34;Spring boot demo project\u0026#34; 4 5git remote add origin http://192.168.86.101:6610/spring-boot 6git push --set-upstream origin master 这样就可以在 spring-boot 里看到代码了\n然后看上图，有个紫色灯泡，Enable build support by adding.onedev-buildspec.yml，点那个链接就可以进入一系列 build 、push、deploy 的 pipeline 了。\n友情提示，单机的话，机器需要装 onedev 的 agent，然后还有 docker，就可以build了。用起来感觉跟云效差不多，很不错。\n","date":"2021-11-05","img":"","permalink":"https://zhangrr.github.io/posts/20211105-onedev/","series":null,"tags":null,"title":"一站式Git软件onedev的安装使用"},{"categories":null,"content":"Nginx 可以用 kill -HUP 来重启，不会丢失已建连接。Haproxy 如何做才能做到 zero downtime 无缝重载呢？\n做法如下：\n一、配置 编译harpoxy的时候带上参数 USE_SYSTEMD，选择 Haproxy 1.8 以上版本\n1make TARGET=linux2628 USE_GETADDRINFO=1 USE_ZLIB=1 USE_REGPARM=1 USE_OPENSSL=1 \\ 2 USE_SYSTEMD=1 USE_PCRE=1 USE_PCRE_JIT=1 USE_NS=1 3make install Haproxy无缝重载技术：\n  旧进程当前管理的连接根据 file descriptor 文件描述符通过 socket 套接字传输到新进程。\n  在这个过程中，文件socket（unix socket）的连接没有断开。\n  新进程在充当 master-worker 主工作者的同时执行此任务。\n  综上所述，通过使用unix socket来维护连接状态并在旧进程和新进程之间传递，防止了连接丢失。\nhaproxy 的运行使用 Systemd 重载，使用 -Ws 方式。 （此外，在构建时必须启用 USE_SYSTEMD）\n1 -D : start as a daemon. The process detaches from the current terminal after 2 forking, and errors are not reported anymore in the terminal. It is 3 equivalent to the \u0026#34;daemon\u0026#34; keyword in the \u0026#34;global\u0026#34; section of the 4 configuration. It is recommended to always force it in any init script so 5 that a faulty configuration doesn\u0026#39;t prevent the system from booting. 6 7 -W : master-worker mode. It is equivalent to the \u0026#34;master-worker\u0026#34; keyword in 8 the \u0026#34;global\u0026#34; section of the configuration. This mode will launch a \u0026#34;master\u0026#34; 9 which will monitor the \u0026#34;workers\u0026#34;. Using this mode, you can reload HAProxy 10 directly by sending a SIGUSR2 signal to the master. The master-worker mode 11 is compatible either with the foreground or daemon mode. It is 12 recommended to use this mode with multiprocess and systemd. 13 14 -Ws : master-worker mode with support of `notify` type of systemd service. 15 This option is only available when HAProxy was built with `USE_SYSTEMD` 16 build option enabled. 17 18 具体的启动脚本：/etc/systemd/system/haproxy.service\n1[Unit] 2Description=HAProxy Load Balancer 3After=network-online.target 4Wants=network-online.target 5 6[Service] 7Environment=\u0026#34;CONFIG=/etc/haproxy/haproxy.cfg\u0026#34; \u0026#34;PIDFILE=/run/haproxy.pid\u0026#34; 8ExecStartPre=/usr/sbin/haproxy -f $CONFIG -c -q 9ExecStart=/usr/sbin/haproxy -Ws -f $CONFIG -p $PIDFILE 10ExecReload=/usr/sbin/haproxy -f $CONFIG -c -q 11ExecReload=/bin/kill -USR2 $MAINPID 12KillMode=mixed 13Restart=always 14SuccessExitStatus=143 15Type=notify 16 17# The following lines leverage SystemD\u0026#39;s sandboxing options to provide 18# defense in depth protection at the expense of restricting some flexibility 19# in your setup (e.g. placement of your configuration files) or possibly 20# reduced performance. See systemd.service(5) and systemd.exec(5) for further 21# information. 22 23# NoNewPrivileges=true 24# ProtectHome=true 25# If you want to use \u0026#39;ProtectSystem=strict\u0026#39; you should whitelist the PIDFILE, 26# any state files and any other files written using \u0026#39;ReadWritePaths\u0026#39; or 27# \u0026#39;RuntimeDirectory\u0026#39;. 28# ProtectSystem=true 29# ProtectKernelTunables=true 30# ProtectKernelModules=true 31# ProtectControlGroups=true 32# If your SystemD version supports them, you can add: @reboot, @swap, @sync 33# SystemCallFilter=~@cpu-emulation @keyring @module @obsolete @raw-io 34 35[Install] 36WantedBy=multi-user.target 二、验证 验证是否真的是无缝重载的步骤如下：\n在haproxy.cfg的global段落中加入stat的配置：\n1stats socket /var/run/haproxy.sock level admin expose-fd listeners process 1 运行一个不断循环重启的脚本：\n1while true ; do systemctl reload haproxy ; sleep 3 ; done 用 apache 的压测工具 ab 来压一下。\nSend request while service is reloading:\n1ab -r -c 20 -n 100000 http://127.0.0.1/ 最后检查结果中 \u0026ldquo;Failed requests\u0026rdquo; 是否为零就可以了\n","date":"2021-11-04","img":"","permalink":"https://zhangrr.github.io/posts/20211104-haproxy_restart/","series":null,"tags":null,"title":"Haproxy的Zero Downtime重启如何做"},{"categories":null,"content":"用 alphine 镜像的一些常用技巧：\n会随时增加：\n一、修改源，用国外的源非常慢，替换成国内的中科大源 1sed -i \u0026#39;s/dl-cdn.alpinelinux.org/mirrors.ustc.edu.cn/g\u0026#39; /etc/apk/repositories 二、更新apk库 1apk update 三、安装软件 1#安装telnet 2apk add busybox-extras 3 4#安装curl 5apk add curl 6 7#安装时间组件 8apk add tzdata 9 10#更新并且安装软件 11apk add --update tzdata 四、进入容器一步执行换源、更新、安装 1sed -i \u0026#39;s/dl-cdn.alpinelinux.org/mirrors.ustc.edu.cn/g\u0026#39; /etc/apk/repositories \u0026amp;\u0026amp; apk update \u0026amp;\u0026amp; apk add curl \u0026amp;\u0026amp; apk add busybox-extras 五、解决缺少glibc库的问题 如果不ln会报错，原因是缺少glibc库！！！解决方法如下：\n1RUN mkdir /lib64 \u0026amp;\u0026amp; \\  2 ln -s /lib/libc.musl-x86_64.so.1 /lib64/ld-linux-x86-64.so.2 六、一些调试的CMD 1CMD [\u0026#34;/bin/bash\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;while true; do echo hi; sleep 10; done\u0026#34;] 2 3kubectl run curlpod --image=radial/busyboxplus:curl --command -- /bin/sh -c \u0026#34;while true; do echo hi; sleep 10; done\u0026#34; 七、pod的等待技巧 这儿里启动正式的pod之前，先临时起了两个容器等待其他服务的完成\n1 containers: 2 - name: myapp-container 3 image: busybox 4 command: [\u0026#39;sh\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#39;echo The app is running! \u0026amp;\u0026amp; sleep 3600\u0026#39;] 5 initContainers: 6 - name: init-myservice 7 image: busybox 8 command: [\u0026#39;sh\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#39;until nslookup myservice; do echo waiting for myservice; sleep 2; done;\u0026#39;] 9 - name: init-mydb 10 image: busybox 11 command: [\u0026#39;sh\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#39;until nslookup mydb; do echo waiting for mydb; sleep 2; done;\u0026#39;] ","date":"2021-11-04","img":"","permalink":"https://zhangrr.github.io/posts/20211104-alphine_usage/","series":null,"tags":null,"title":"Alphine镜像的使用技巧"},{"categories":null,"content":"我们都喜欢用 alphine 的镜像做底包，来生产自己的镜像\nalphine 的底包的时间设定就非常重要了\n直接给出 Dockerfile\n1FROM alpine:3.12 2 3# latest certs 4RUN apk add ca-certificates --no-cache \u0026amp;\u0026amp; update-ca-certificates 5 6# timezone support 7ENV TZ=Asia/Shanghai 8RUN apk add --update tzdata --no-cache \u0026amp;\u0026amp;\\ 9 cp /usr/share/zoneinfo/${TZ} /etc/localtime \u0026amp;\u0026amp;\\ 10 echo $TZ \u0026gt; /etc/timezone 11 12# install chrony and place default conf which can be overridden with volume 13RUN apk add --no-cache chrony \u0026amp;\u0026amp; mkdir -p /etc/chrony 14COPY chrony.conf /etc/chrony/. 15 16# port exposed 17EXPOSE 123/udp 18 19# start 20CMD [ \u0026#34;/usr/sbin/chronyd\u0026#34;, \u0026#34;-d\u0026#34;, \u0026#34;-s\u0026#34;] 21 时间设定重要的就是下面这几行\n1# timezone support 2ENV TZ=Asia/Shanghai 3RUN apk add --update tzdata --no-cache \u0026amp;\u0026amp;\\ 4 cp /usr/share/zoneinfo/${TZ} /etc/localtime \u0026amp;\u0026amp;\\ 5 echo $TZ \u0026gt; /etc/timezone 还有如果在 image: maven:3.5-jdk-8-alpine 这种镜像里，无论怎么改时间都不是东八区，可以试试下面的命令：\n1export COMMIT_TIME=$(TZ=CST-8 date +%F-%H-%M) ","date":"2021-11-03","img":"","permalink":"https://zhangrr.github.io/posts/20211103-alphine_timezone/","series":null,"tags":null,"title":"Alphine镜像中timezone的设定"},{"categories":null,"content":"其实我们的生产环境一直是 KVM ，然后用 shell 脚本控制虚机的生成，也是用到了 Cloud-init 的标准镜像。\n听说 Proxmox 也很不错，于是想看看能否也在生产环境中用上\n如果在生产环境中用，必须要让 proxmox 支持 cloud-init ，否则无意义，下面也说一下跑在生产的注意事项\n首先我们用光盘安装：\n然后第一个注意的地方就是硬盘，选 Options 后：\n会冒出一堆选项，公司的生产环境，服务器如果没有 raid 卡是很奇怪的，所以 zfs 反而不是标配，因为我们会事先在 raid 卡上划分好硬盘，生产环境基本必然是 raid10 ，接下来就是 ext4 和 xfs 二选一了，八戒选 ext4 ，因为坏了好修理，xfs_repair 用起来相当龟毛：\n那么，选定了 ext4 ，接下来就比较重要了\n  hdsize 1116.0 ，单位是G，这个是自动收集上来的，不用改\n  swapsize，交换分区大小，这个给 8 G（最大8G）\n  maxroot，这个分区是第一个分区，存放 iso 和 template 的，需要给够，100 G\n  minfree，第一个分区最小留多大，给 10 G（缺省16G）\n  maxvz，这个分区是第二个分区，存放实际的虚机文件，全都用上，什么也不填写\n  然后继续，国家选 china，Hostname 填写 proxmox-168-86-103.local，再填写好其他信息，就安装成功了。\n打开网页，我们可以看到一个 local，100G，对应上面的 maxroot\n然后 local-lvm ，就是剩余放虚机的空间\nssh登录系统，首先换成中科大的 apt 源，并升级一下系统：\n1sed -i \u0026#39;s|^deb http://ftp.debian.org|deb https://mirrors.ustc.edu.cn|g\u0026#39; /etc/apt/sources.list 2sed -i \u0026#39;s|^deb http://security.debian.org|deb https://mirrors.ustc.edu.cn/debian-security|g\u0026#39; /etc/apt/sources.list 3CODENAME=`cat /etc/os-release |grep CODENAME |cut -f 2 -d \u0026#34;=\u0026#34;` 4echo \u0026#34;deb https://mirrors.ustc.edu.cn/proxmox/debian $CODENAMEpve-no-subscription\u0026#34; \u0026gt; /etc/apt/sources.list.d/pve-no-subscription.list 5cat /etc/apt/sources.list.d/pve-no-subscription.list 6rm /etc/apt/sources.list.d/pve-enterprise.list 7apt update 8apt upgrade 那生产使用，是必须用 Cloud-init 的标准化镜像的。我们需要造出一个 template 。\n以 Centos7 为例子\n1wget http://cloud.centos.org/centos/7/images/CentOS-7-x86_64-GenericCloud.qcow2 2apt-get install libguestfs-tools 然后准备脚本 modify.sh ：\n1#!/bin/sh 2image_name=CentOS-7-x86_64-GenericCloud.qcow2 3# virt-edit -a ${image_name} /etc/cloud/cloud.cfg 4virt-edit -a ${image_name} /etc/cloud/cloud.cfg -e \u0026#39;s/disable_root: [Tt]rue/disable_root: False/\u0026#39; 5virt-edit -a ${image_name} /etc/cloud/cloud.cfg -e \u0026#39;s/disable_root: 1/disable_root: 0/\u0026#39; 6virt-edit -a ${image_name} /etc/cloud/cloud.cfg -e \u0026#39;s/lock_passwd: [Tt]rue/lock_passwd: False/\u0026#39; 7virt-edit -a ${image_name} /etc/cloud/cloud.cfg -e \u0026#39;s/lock_passwd: 1/lock_passwd: 0/\u0026#39; 8virt-edit -a ${image_name} /etc/cloud/cloud.cfg -e \u0026#39;s/ssh_pwauth: 0/ssh_pwauth: 1/\u0026#39; 9virt-edit -a ${image_name} /etc/ssh/sshd_config -e \u0026#39;s/PasswordAuthentication no/PasswordAuthentication yes/\u0026#39; 10virt-edit -a ${image_name} /etc/ssh/sshd_config -e \u0026#39;s/PermitRootLogin [Nn]o/PermitRootLogin yes/\u0026#39; 11virt-edit -a ${image_name} /etc/ssh/sshd_config -e \u0026#39;s/#PermitRootLogin [Yy]es/PermitRootLogin yes/\u0026#39; 12virt-edit -a ${image_name} /etc/ssh/sshd_config -e \u0026#39;s/#PermitRootLogin prohibit-password/PermitRootLogin yes/\u0026#39; 13virt-edit -a ${image_name} /etc/ssh/sshd_config -e \u0026#39;s/[#M]axAuthTries 6/MaxAuthTries 20/\u0026#39; 14virt-customize --install cloud-init,atop,htop,nano,vim,qemu-guest-agent,curl,wget,telnet,lsof,screen -a ${image_name} 运行它，以上命令其实是侵入镜像，修改 sshd_config 允许 root 用 password 登录，然后又安了几个常用软件，大家可以按需修改。\n最后生成 template , 脚本： vm.sh 1#!/bin/sh 2vm_id=9999 3image_name=CentOS-7-x86_64-GenericCloud.qcow2 4 5qm create ${vm_id} --memory 8196 --net0 virtio,bridge=vmbr0 6qm importdisk ${vm_id} ${image_name} local-lvm 7qm set ${vm_id} --scsihw virtio-scsi-pci --scsi0 local-lvm:vm-${vm_id}-disk-0 8qm set ${vm_id} --ide2 local-lvm:cloudinit 9qm set ${vm_id} --boot c --bootdisk scsi0 10qm set ${vm_id} --serial0 socket --vga serial0 11qm template ${vm_id} cloud-init 技术的核心其实就是用配置文件，在虚机启动的时候动态修改，这里把配置放到了 ide2 的一个虚拟 cdrom 中\n最终会生成一个 id 为 9999 的 template\n我们还需要改两处：\n一是 CPU、MEMORY、硬盘大小，缺省是 8G，我们生产的镜像标配是80G，需要 resize , 加 72G，合计80G\n二是 cloud-init 部分，用户名、密码、DNS、IP、MASK、GATEWAY\n这样这个 template 就做好了，在生产的时候，只需要 clone 这个模板（模式要选 Full Clone），然后记得修改为不同的IP，就可以了。\n总体来说，这个东西偏小白，对于习惯了 KVM 的人来说，反而不如脚本来的快。\n相关文档：https://whattheserver.com/proxmox-cloud-init-os-template-creation/\n","date":"2021-11-03","img":"","permalink":"https://zhangrr.github.io/posts/20211103-proxmox/","series":null,"tags":null,"title":"生产环境Proxmox 7.02的安装和配置"},{"categories":null,"content":"kubernetes 中 nginx ingress 的优化分两部分\n一、系统sysctl部分优化 首先是对nginx启动前的系统性能进行优化，这部分调整网络的缓冲区，减小闲置 socket 关闭的时间\n以阿里 ACK 为例，我们可以编辑 deployments 的 nginx-ingress-controller\n1 initContainers: 2 - command: 3 - /bin/sh 4 - -c 5 - | 6 mount -o remount rw /proc/sys 7 sysctl -w net.core.somaxconn=65535 8sysctl -w net.ipv4.ip_local_port_range=\u0026#34;1024 65535\u0026#34; 9sysctl -w net.ipv4.tcp_tw_reuse=1 10sysctl -w fs.file-max=1048576 11sysctl -w net.ipv4.tcp_keepalive_time = 300 12sysctl -w net.ipv4.tcp_keepalive_probes = 5 13sysctl -w net.ipv4.tcp_keepalive_intvl = 15 14 二、nginx ingress 参数优化 大家制动，nginx ingree 其实是做为一个中间代理，所以上下游的socket参数也需要优化\n同样以阿里ACK为例，我们可以编辑 configmaps 的 nginx-configuration\n1apiVersion: v1 2data: 3 allow-backend-server-header: \u0026#34;true\u0026#34; 4 enable-underscores-in-headers: \u0026#34;true\u0026#34; 5 generate-request-id: \u0026#34;true\u0026#34; 6 ignore-invalid-headers: \u0026#34;true\u0026#34; 7 log-format-upstream: $remote_addr - [$remote_addr] - $remote_user [$time_local] 8 \u0026#34;$request\u0026#34; $status $body_bytes_sent \u0026#34;$http_referer\u0026#34; \u0026#34;$http_user_agent\u0026#34; $request_length 9 $request_time [$proxy_upstream_name] $upstream_addr $upstream_response_length 10 $upstream_response_time $upstream_status $req_id $host [$proxy_alternative_upstream_name] 11 proxy-body-size: 20m 12 proxy-connect-timeout: \u0026#34;10\u0026#34; 13 reuse-port: \u0026#34;true\u0026#34; 14 server-tokens: \u0026#34;false\u0026#34; 15 ssl-redirect: \u0026#34;false\u0026#34; 16 17 upstream-keepalive-timeout: \u0026#34;900\u0026#34; 18 keep-alive-requests: \u0026#34;10000\u0026#34; 19 upstream-keepalive-connections: \u0026#34;500\u0026#34; 20 max-worker-connections: \u0026#34;65536\u0026#34; 21 22 worker-cpu-affinity: auto 23kind: ConfigMap ","date":"2021-11-02","img":"","permalink":"https://zhangrr.github.io/posts/20211102-ingress_nginx/","series":null,"tags":null,"title":"K8s中nginx Ingress的性能优化"},{"categories":null,"content":"这篇是纯配置篇，解释都在配置里了，是生产服务器 sysctl.conf 的配置\n1### KERNEL ### 2 3# Reboot after 10sec. on kernel panic 4kernel.panic = 10 5 6### IMPROVE SYSTEM MEMORY MANAGEMENT ### 7 8# Increase size of file handles and inode cache 9fs.file-max = 2097152 10 11# Insure we always have enough memory 12vm.min_free_kbytes = 8192 13 14# Do less swapping 15vm.swappiness = 10 16vm.dirty_ratio = 10 17vm.dirty_background_ratio = 2 18 19 20### GENERAL NETWORK SECURITY OPTIONS ### 21 22# Avoid a smurf attack 23net.ipv4.icmp_echo_ignore_broadcasts = 1 24 25# Turn on protection for bad icmp error messages 26net.ipv4.icmp_ignore_bogus_error_responses = 1 27 28# Turn on syncookies for SYN flood attack protection 29net.ipv4.tcp_syncookies = 1 30net.ipv4.tcp_max_syn_backlog = 8192 31 32 33# Turn on timestamping 34net.ipv4.tcp_timestamps = 1 35 36# Turn on and log spoofed, source routed, and redirect packets 37net.ipv4.conf.all.log_martians = 1 38net.ipv4.conf.default.log_martians = 1 39 40# No source routed packets here 41net.ipv4.conf.all.accept_source_route = 0 42net.ipv4.conf.default.accept_source_route = 0 43 44# Turn on reverse path filtering 45net.ipv4.conf.all.rp_filter = 1 46net.ipv4.conf.default.rp_filter = 1 47 48# Make sure no one can alter the routing tables 49net.ipv4.conf.all.accept_redirects = 0 50net.ipv4.conf.default.accept_redirects = 0 51net.ipv4.conf.all.secure_redirects = 0 52net.ipv4.conf.default.secure_redirects = 0 53 54# Don\u0026#39;t act as a router 55net.ipv4.ip_forward = 0 56net.ipv4.conf.all.send_redirects = 0 57net.ipv4.conf.default.send_redirects = 0 58 59# Number of times SYNACKs for passive TCP connection. 60net.ipv4.tcp_synack_retries = 2 61 62# Allowed local port range 63net.ipv4.ip_local_port_range = 1024 65000 64 65# Protect Against TCP Time-Wait 66net.ipv4.tcp_rfc1337 = 1 67 68# Decrease the time default value for tcp_fin_timeout connection 69net.ipv4.tcp_fin_timeout = 15 70 71# Decrease the time default value for connections to keep alive 72net.ipv4.tcp_keepalive_time = 300 73net.ipv4.tcp_keepalive_probes = 5 74net.ipv4.tcp_keepalive_intvl = 15 75# This means that the keepalive process waits 300 seconds for socket  76# activity before sending the first keepalive probe, and then resend 77# it every 15 seconds. If no ACK response is received for 5 consecutive  78# times (75s in this case), the connection is marked as broken. 79 80### TUNING NETWORK PERFORMANCE ### 81 82# Disable IPv6 83net.ipv6.conf.all.disable_ipv6 = 1 84net.ipv6.conf.default.disable_ipv6 = 1 85net.ipv6.conf.lo.disable_ipv6 = 1 86 87# Default Socket Receive Buffer 88net.core.rmem_default = 31457280 89 90# Maximum Socket Receive Buffer 91net.core.rmem_max = 12582912 92 93# Default Socket Send Buffer 94net.core.wmem_default = 31457280 95 96# Maximum Socket Send Buffer 97net.core.wmem_max = 12582912 98 99# Increase number of incoming connections 100net.core.somaxconn = 5000 101 102# Increase number of incoming connections backlog 103net.core.netdev_max_backlog = 65536 104 105# Enable TCP window scaling 106net.ipv4.tcp_window_scaling = 1 107 108# Increase the maximum amount of option memory buffers 109net.core.optmem_max = 25165824 110 111 112# Increase the maximum total buffer-space allocatable 113# This is measured in units of pages (4096 bytes) 114net.ipv4.tcp_mem = 65536 131072 262144 115net.ipv4.udp_mem = 65536 131072 262144 116 117# Increase the read-buffer space allocatable 118net.ipv4.tcp_rmem = 8192 87380 16777216 119net.ipv4.udp_rmem_min = 16384 120 121# Increase the write-buffer-space allocatable 122net.ipv4.tcp_wmem = 8192 65536 16777216 123net.ipv4.udp_wmem_min = 16384 124 125 126# Increase the tcp-time-wait buckets pool size to prevent simple DOS attacks 127net.ipv4.tcp_max_tw_buckets = 1800000 128 129# TIME_WAIT socket policy 130# Note: if both enabled then disable 131# net.ipv4.tcp_timestamps for servers  132# behind NAT to prevent dropped incoming connections 133#net.ipv4.tcp_tw_recycle = 1 134net.ipv4.tcp_tw_reuse = 1 135 136# Enable TCP MTU probing (in case of Jumbo Frames enabled) 137#net.ipv4.tcp_mtu_probing = 1 138 139# Speedup retrans (Google recommended) 140net.ipv4.tcp_slow_start_after_idle = 0 141net.ipv4.tcp_early_retrans = 1 142 143# Conntrack 144# 288bytes x 131072 = 37748736 (~38MB) max memory usage 145#net.netfilter.nf_conntrack_max = 131072 146#net.netfilter.nf_conntrack_tcp_loose = 1 147 148#TCP的直接拥塞通告(tcp_ecn)关掉 149net.ipv4.tcp_ecn = 0 150 151#路由缓存刷新频率，当一个路由失败后多长时间跳到另一个路由，默认是300。 152net.ipv4.route.gc_timeout = 100 153 154#设定系统中最多允许在多少TCP套接字不被关联到任何一个用户文件句柄上。 155#如果超过这个数字，没有与用户文件句柄关联的TCP 套接字将立即被复位 156#防简单Dos 157net.ipv4.tcp_max_orphans = 655360 158 159# NOTE: Enable this if machine support it 160# -- 10gbe tuning from Intel ixgb driver README -- # 161# turn off selective ACK and timestamps 162#net.ipv4.tcp_sack = 0 163#net.ipv4.tcp_timestamps = 1 ** 注意，net.ipv4.tcp_tw_recycle 不要打开，在 NAT 环境中会出错，而且在 K8S 中也会因 NAT 导致 pod 出错，切记！！！**\n","date":"2021-11-02","img":"","permalink":"https://zhangrr.github.io/posts/20211102-sysctl_conf/","series":null,"tags":null,"title":"Linux内核sysctl内核参数优化"},{"categories":null,"content":"Custom Configuration of TCP Socket Keep-Alive Timeouts 这是个古老的话题，我们在机器的优化中，需要设置 TCP Socket 的 Timeout 参数\n用来加快 TCP 关闭无用闲置连接的时间\nLinux 内核中有三个缺省参数:\n  1 tcp_keepalive_time  缺省是 7200 秒    1 tcp_keepalive_probes  缺省是 9    1 tcp_keepalive_intvl  缺省是 75 秒    处理流程如下：\n一、客户端打开一个 TCP socket 连接，开始跟服务器通讯\n二、如果这条 socket 连接空闲没有任何数据传输，静默了 tcp_keepalive_time 秒后，那么客户端会主动发送一个空的 ACK 包到服务器\n三、那么，根据服务器是否回应了一个相应的 ACK 包来判断\n1ACK  未回应  等待 tcp_keepalive_intvl 秒，然后再发一个 ACK 包 重复以上等待并发送 ACK 包的过程，直到次数等于 tcp_keepalive_probes 如果第2步做完还收不到任何回应，发送一个 RST 包并关闭连接   回应了: 回到上述第二步  那么缺省情况下，7200+75×9，一个没有任何数据传输的 socket 才会被关闭，大概是2小时11分钟。\n这个时间太长了。需要优化一下：\n1net.ipv4.tcp_keepalive_time = 300 2net.ipv4.tcp_keepalive_probes = 5 3net.ipv4.tcp_keepalive_intvl = 15 上面的时间是 300 + 5x15，大概是6分钟，大大缩短释放空闲 socket 的时间\n","date":"2021-11-02","img":"","permalink":"https://zhangrr.github.io/posts/20211102-tcp_keealive/","series":null,"tags":null,"title":"Linux内核TCP连接Keep-Alive Timeout的配置"},{"categories":null,"content":"这属于Shell的高级技巧了，我们可能需要在 bash 中并发 wget rsync 文件，下面就讨论一下这个问题。\n首先从简单的单线程开始：\n1$ for i in $(seq 1 2); do echo $i; done 21 32 可以看到是顺序执行的，下面变多线程：\n1$ for i in $(seq 1 2); do echo $i \u0026amp; done 2[1] 245505 31 4[2] 245506 52 6[1] Done echo $i 7[2] Done echo $i 可以看到我们只把 ; 号改成了 \u0026amp; 号，程序就变成了多线程执行。\n区别在于 ; 号会等待之前的命令执行完毕再执行下一条，而 \u0026amp; 不等待，直接继续执行下一条；相当于后台运行了前一条命令。\n下面说说 find 的单线程和多线程：\nfind 的 exec 用法\n1$ find /path [args] -exec [cmd] {} \\;  {} 占位符号，存放find找到的记录 ; 对于每一条找到的单独记录，执行的cmd是一条一条单独执行的 执行的顺序如下: cmd result1; cmd result2; \u0026hellip;; cmd result N  1$ find /path [args] -exec [cmd] {} \\+  {} 占位符号，存放find找到的记录 + 对于找到的所有记录，执行的cmd是合并了所有记录集执行的 执行顺序如下: cmd result1 result2 \u0026hellip; result N  多个exec可以串起来：\n1$ find /tmp/dir1/ -type f -exec grep howtouselinux {} \\; -exec echo {} \\; | sed \u0026#39;s/howtouselinux/deep/g\u0026#39; 至此，find 也还是单线程执行的，并没有并发。\nfind 要并发，就只能跟 xargs 结合在一起：\nxargs 通常配合管道使用，将前面命令产生的参数，逐个传入后续命令，作为参数。xargs 传来的参数，默认位于 xargs 后面命令的最后，如果要改变位置，需要用**-I**参数。xargs 如果不带命令，缺省是 echo\n  -d 分隔符\n 1$ echo -e \u0026#34;a\\tb\\tc\u0026#34; | xargs -d \u0026#34;\\t\u0026#34; echo 2a b c    -I{} 指定占位符，-I %那就是 % 替代从之前管道取得的参数\n 1$ find . -type d | xargs -I % -0 rsync -auvPR % 192.168.1.38::new/    -0 跟find的-print0配合，find命令有一个特别的参数-print0，指定输出的文件列表以null分隔。然后，xargs命令的-0参数表示用null当作分隔符。\n 1$ find ./new -mindepth 6 -maxdepth 6 -type d -print0 | xargs -I % -0 rsync -auvPR % 172.18.34.38::new/    -P 最大并发线程数，下面是并发30线程\n 1$ find ./new -mindepth 6 -maxdepth 6 -type d -print0 | xargs -P 30 -I % -0 rsync -auvPR % 172.18.34.38::new/    -n 选项限制单个命令行的参数个数，下面是 rsync 一行命令传带60个文件，30个进程那就是30个 rsync，每个 rsync 同时传60个文件。\n 1$ find ./new -mindepth 6 -maxdepth 6 -type d -print0 | xargs -P 30 -n 60 -I % -0 rsync -auvPR % 172.18.34.38::new/ 2 3$ echo {0..10} | xargs -I{} -n 2 40 1 52 3 64 5 76 7 88 9 910    使用 bash -c 并发的例子：\n1$ time for i in $(seq 1 5); do echo $[$RANDOM % 5 + 1]; done | xargs -I{} echo \u0026#34;sleep {}; echo \u0026#39;Done! {}\u0026#39;\u0026#34; | xargs -P5 -I{} bash -c \u0026#34;{}\u0026#34; ","date":"2021-10-29","img":"","permalink":"https://zhangrr.github.io/posts/20211029-bash_multithread/","series":null,"tags":null,"title":"Shell以及find的多线程执行"},{"categories":null,"content":"之前讲过如何对 opnvpn 总体限速，这次来了一个更严格的程序限速需求：\n场景如下：\n  两个机房间有一条专线 100M\n  两个机房间需要同步数据，同步需要限制到60M，给别的程序留出带宽空间\n  传输是多个文件，用 rsync 并发传送\n  分析了一下脚本的核心部分\n1find . -type f | grep $(date -d\u0026#34;-1 day\u0026#34; +\u0026#39;%Y%m%d\u0026#39;) |xargs -I % -P 30 rsync -auvPR % 192.168.9.17::mysql 发现是利用 xargs 的并发，-P 30 最大并发30个，启动了 rsync 同步\nrsync 没有限速，这就麻烦了。\n一、单文件单独限速 首先是使用 rsync 的 \u0026ndash;bwlimit=600 参数，把速度限制为 600KB/s ，600×8=4800，单进程基本是5M的速度，最多只能跑12个了，就会跑到60M。\n这样也不太对，尤其是 rsync 并发进程逐渐减少，少于12个的时候，这样就出现跑不满60M的现象。\n二、多文件整体限速 那么 rsync 支持多文件传输 ，使用如下格式整体限速\n1rsync --bwlimit=7200 -auvPR 文件1 文件2 文件3 192.168.9.17::mysql 问题又来了，文件1 文件2 文件3 的路径非常长，而文件个数不定，有撑爆命令行单行长度限制的可能，也不可行\n三、tc 使用 tc 可以控制源ip或者目的ip的带宽，但是本机网卡是万兆光卡，生产环境，每时每刻都有数据读写。\n一旦错了，就直接完蛋了。也不太可行\n四、杀器trickle 寻找了半天，终于找到了个大杀器trickle，可以对程序单独限速，也可以对一堆程序整体限速\n安装：\n1$ yum install -y epel-release 2$ yum install trickle 参数解释：\n -u 上载速度 KB/s ，乘以8换算成网络速度 -d 下载速度 KB/s ，乘以8换算成网络速度 -s standalone独立模式，不参与 trickled 的整体模式  如果要对一个程序单独限速，10KB\n1trickle -s -u 10 -d 10 wget http://mirrors.163.com/rocky/8/isos/x86_64/Rocky-8.4-x86_64-dvd1.iso wget 的下载完全被限制在了10KB\n如过要对使用trickle的所有程序做整体限速\n1# 首先启动守护进程 trickled 2$ trickled -u 7200 -d 7200 3 4# 然后所有用trickle执行的命令就会整理限速在7200KB/s （网络速度60M） 5find . -type f | grep $(date -d\u0026#34;-1 day\u0026#34; +\u0026#39;%Y%m%d\u0026#39;) |xargs -I % -P 30 trickle rsync -auvPR % 192.168.9.17::mysql ","date":"2021-10-28","img":"","permalink":"https://zhangrr.github.io/posts/20211028-trickle/","series":null,"tags":null,"title":"Linux下的程序限速软件Trickle"},{"categories":null,"content":"Dockerfile 是造出镜像的基础，是必须熟知并了解的知识：\n一、编写Dockerfile 先给个例子，是 minio 代理访问阿里的 OSS\n1FROM alpine:3.12  2 3RUN apk add --update bash \u0026amp;\u0026amp; rm -rf /var/cache/apk/*  4 5COPY minio.RELEASE.2020-04-15T19-42-18Z /data/minio.RELEASE.2020-04-15T19-42-18Z  6 7ENV MINIO_ACCESS_KEY=LTAI5tFFTbsxxxxxuLb  8ENV MINIO_SECRET_KEY=t78PyGnHZilxxxxxdxBCjvNgtVC5Y  9 10WORKDIR /data  11EXPOSE 9000  12 13CMD [\u0026#34;/data/minio.RELEASE.2020-04-15T19-42-18Z\u0026#34;,\u0026#34;gateway\u0026#34;,\u0026#34;oss\u0026#34;,\u0026#34;http://oss-cn-shanghai-internal.aliyuncs.com\u0026#34;]  14# CMD /bin/sh -c \u0026#34;while true; do echo hi; sleep 10; done\u0026#34; 15  详细解释每一条语句：\n  FROM\n基板，alpine 3.12 是个比较微小的版本，注意它的毛病，/bin/sh其实是busybox，没有/bin/bash，某些bash的函数功能支持不全，比如for循环\n  RUN\n在容器中运行命令，上例中我们添加了 bash ，并清理了缓存。命令间用 \u0026amp;\u0026amp; 可以避免镜像过多分层。\nRUN分两种模式shell和exec模式:\n我们只用 exec 模式，因为在 image 里装入多个 shell，没什么意义。\n  COPY 和 ADD\n作用都是将文件或目录复制到 Dockerfile 构建的镜像中\n我们只用COPY，如果遇到要把URL的文件放进去，可以先wget，然后放；如果要解压tarr包放进去，那就先解压再放。\n注意源文件路径都使用相对路径，目标路径使用绝对路径。\n如果dest不指定绝对路径，则是想对于WORDIR的相对路径\n  CMD 入口\n用 [] 分割， 把所有 \u0026quot;\u0026quot; 的部分合并为一行，中间用空格隔开执行；或者直接一行没任何分割符。\n所以上面的例子就是执行了一句：\n1/data/minio.RELEASE.2020-04-15T19-42-18Z gateway oss http://oss-cn-shanghai-internal.aliyuncs.com 技巧：\n把几个命令合在一起执行\n()表示在当前shell合并执行\n{}表示派生出一个子shell，在子shell中合并执行，{ echo \u0026ldquo;aaa\u0026rdquo; }必须有空格\n\u0026amp;表示后台运行\n命令之间使用 \u0026amp;\u0026amp; 连接，实现逻辑与的功能。\n 只有在 \u0026amp;\u0026amp; 左边的命令返回真（命令返回值 $? == 0），\u0026amp;\u0026amp; 右边的命令才会被执行。 只要有一个命令返回假（命令返回值 $? == 1），后面的命令就不会被执行。  \u0026amp;\u0026amp;左边的命令（命令1）返回真(即返回0，成功被执行）后，\u0026amp;\u0026amp;右边的命令（命令2）才能够被执行；换句话说，“如果这个命令执行成功\u0026amp;\u0026amp;那么执行第二个命令”。\n最下的语法用了seq而不是for循环，是因为busybox的sh不支持for语法\n所以才有如此怪异的语法，在容器中后台跑10个php think queue，1个crond，前台跑一个php-fpm：\n1CMD for i in $(seq 10); do (php think queue \u0026amp;) ; done \u0026amp; crond \u0026amp;\u0026amp; php-fpm   ARG 参数\nARG VERSION=7.6.1\n定义后可以用${VERSION}引用，build的时候可以加\u0026ndash;build-arg 传参数进去\n1docker build --build-arg VERSION=${LATEST} -t $(ORG)/$(NAME):$(BUILD) .   还有很多 Build 的技巧，如果造一个 go 语言编译环境的中间层镜像，然后造最终镜像。\n但是八戒还是推荐直接造出二进制可执行文件，然后直接拷贝进去就好，不要弄的过于麻烦，中间层那种适用于用源码 CI/CD 中无编译环境的情况。\n二、调试容器 很多情况下我们造好了 image ，一跑就掉下来了，也不知道是什么情况\n这个时候，我们把 CMD 换成一个 sh 执行一个死循环\n1CMD /bin/sh -c \u0026#34;while true; do echo hi; sleep 10; done\u0026#34; 然后进入容器，然后再执行之前的 CMD 命令，看看报错信息是什么，就可以调试了\n1$ docker exec -it 89174asklja /bin/sh 三、pod的等待技巧 这里启动正式的pod之前，先临时起了两个容器等待其他服务的完成\n1 containers: 2 - name: myapp-container 3 image: busybox 4 command: [\u0026#39;sh\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#39;echo The app is running! \u0026amp;\u0026amp; sleep 3600\u0026#39;] 5 initContainers: 6 - name: init-myservice 7 image: busybox 8 command: [\u0026#39;sh\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#39;until nslookup myservice; do echo waiting for myservice; sleep 2; done;\u0026#39;] 9 - name: init-mydb 10 image: busybox 11 command: [\u0026#39;sh\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#39;until nslookup mydb; do echo waiting for mydb; sleep 2; done;\u0026#39;] ","date":"2021-10-28","img":"","permalink":"https://zhangrr.github.io/posts/20211028-dockerfile/","series":null,"tags":null,"title":"Dockerfile的编写与调试技巧"},{"categories":null,"content":"给同事做了个 PHP 接口，转发发送短信的请求，同时要把发送记录发送到远程的 cacti 的 syslog 去\n很简单，但是也不简单\n首先是 PHP 服务器，是最简化编译的，php -m 查了一下\n1php -m 2[PHP Modules] 3Core 4ctype 5curl 6date 7dom 8fileinfo 9filter 10gettext 11hash 12iconv 13json 14libxml 15openssl 16pcre 17PDO 18pdo_sqlite 19Phar 20posix 21Reflection 22session 23SimpleXML 24SPL 25sqlite3 26standard 27tokenizer 28xml 29xmlreader 30xmlwriter 31 32[Zend Modules] 居然没有 socket 模块，没办法，找到源代码，编译一个安装，原有的 php 安装路径是 /export/servers/php\n1$ tar zxvf php-7.4.0.tar.gz 2$ cd php-7.4.0/sockets 3$ /export/servers/php/bin/phpize 4$ ./configure --enable-sockets --with-php-config=/export/servers/php/bin/php-config 5$ make 6$ make install 又看了一眼，是 php-fpm，居然没有 php.ini ，得，再生成一个，放在 /export/servers/php/lib/php.ini\n1extension_dir = \u0026#34;/export/servers/php740/lib/php/extensions/no-debug-non-zts-20190902/\u0026#34; 2extension = sockets.so 然后重启 php-fpm ，重新 php -m 检查，发现有 socket 模块就 ok 了。\n接下来就是 php 源代码了\n1\u0026lt;?php 2date_default_timezone_set(\u0026#39;Asia/Shanghai\u0026#39;); 3 4function send_remote_syslog($message) { 5 $sock = socket_create(AF_INET, SOCK_DGRAM, SOL_UDP); 6 foreach(explode(\u0026#34;\\n\u0026#34;, $message) as $line) { 7 $syslog_message = \u0026#34;\u0026lt;22\u0026gt;\u0026#34; . date(\u0026#39;M d H:i:s \u0026#39;) . \u0026#39;Qi_an_xin sms_log: \u0026#39; . $line; 8 socket_sendto($sock, $syslog_message, strlen($syslog_message), 0, \u0026#39;172.18.31.6\u0026#39;, 514); 9 } 10 socket_close($sock); 11} 12 13 send_remote_syslog(\u0026#34;ABC 验证码: 39792192\u0026#34;); 14?\u0026gt;这样就可以了，上面代码比较难理解的是\u0026lt;22\u0026gt;，那是报错级别的计算方法，Facility + Severity：\n1 * Facility values: 2 * 0 kernel messages 3 * 1 user-level messages 4 * 2 mail system 5 * 3 system daemons 6 * 4 security/authorization messages 7 * 5 messages generated internally by syslogd 8 * 6 line printer subsystem 9 * 7 network news subsystem 10 * 8 UUCP subsystem 11 * 9 clock daemon 12 * 10 security/authorization messages 13 * 11 FTP daemon 14 * 12 NTP subsystem 15 * 13 log audit 16 * 14 log alert 17 * 15 clock daemon 18 * 16 local user 0 (local0) (default value) 19 * 17 local user 1 (local1) 20 * 18 local user 2 (local2) 21 * 19 local user 3 (local3) 22 * 20 local user 4 (local4) 23 * 21 local user 5 (local5) 24 * 22 local user 6 (local6) 25 * 23 local user 7 (local7) 26 * 27 * Severity values: 28 * 0 Emergency: system is unusable 29 * 1 Alert: action must be taken immediately 30 * 2 Critical: critical conditions 31 * 3 Error: error conditions 32 * 4 Warning: warning conditions 33 * 5 Notice: normal but significant condition (default value) 34 * 6 Informational: informational messages 35 * 7 Debug: debug-level messages 计算方法就是 （facility*8 + severity），这里的22+0，可以理解成 local6 ，就是级别6\n如果发了一个 \u0026ldquo;local use 4\u0026rdquo; 和 Serverity = 5 的消息，那么就是 20×8+5=165 ，包头就是 \u0026lt;165\u0026gt;\n","date":"2021-10-28","img":"","permalink":"https://zhangrr.github.io/posts/20211028-php_syslog/","series":null,"tags":null,"title":"PHP程序如何发送syslog到远程服务器"},{"categories":null,"content":"用 kubernetes 越多，用 docker 越多，就愈发感觉到好处多多。\n简简单单的一个可执行文件，用 docker 基板 alphine 封装，就可以运行起一个 pod ，然后指定 deployment、svc、ingress，就可以将服务暴露出去。\n其实很多情况下单可执行文件 + systemd也是不错的选择。\n这不就遇到个问题，ghostunnel 这个软件，github 只释放出了源代码以及 windows 、linux 和 mac 的三个可执行版本。\n可我的执行环境是 nanopi ，是个 arm7 的架构，就无计可施了。\n无奈下，在 nanopi 上装了 go ，编译了个 arm7 的出来。\n但是遇到 vaultwanden ，rust 的，就没法弄了，vps 太弱，根本无法用 cargo 编译。\n那怎么办呢？方法如下，不安装 Docker ，也可以把镜像中的文件抽取出来\n1$ mkdir vm 2$ wget https://raw.githubusercontent.com/jjlin/docker-image-extract/main/docker-image-extract 3$ chmod +x docker-image-extract 4$ ./docker-image-extract vaultwarden/server:alpine 5Getting API token... 6Getting image manifest for vaultwarden/server:alpine... 7Fetching and extracting layer a0d0a0d46f8b52473982a3c466318f479767577551a53ffc9074c9fa7035982e... 8Fetching and extracting layer 3a9a529931676767ec84d35ab19774b24bd94e20f6fff7e6bda57ef5f2a66cfc... 9Fetching and extracting layer f9dcfa9aefe67ce52ab2a73e515ea715d242348b8fc338dbe4ca72a853ea0318... 10Fetching and extracting layer 4249d8cece35148b5faca2c6a98d566a271a1996b127b14480793ee8825e43c0... 11Fetching and extracting layer 72f4873a62cc82eaf28905077df3791e3b235bf5d17670e7aff6d5fb5e280739... 12Fetching and extracting layer 8eb772c524f9d998c8c7c92acc5ba96e3e9ebfb175dbb2441fe6e7b7598874f5... 13Fetching and extracting layer 663794f103b44abb8a90e1376dce14735905e2f938b4ca7e0ff379b09cbf6148... 14Image contents extracted into ./output. 15 这样我们就可以在 output 目录下得到 vaultwarden 和 web-vault\n如果我们要拿到 arm7 的镜像，还需要再费点劲，以 ghostunnel 为例：\n访问： https://hub.docker.com/r/ghostunnel/ghostunnel/tags 找到 linux/arm/v7 的 tag，40247f4b49c3 点开后，找到 DIGEST: ，复制sha256以及后面的字串，sha256:40247f4b49c364046ebf47696dbd31752b4db2ae2b9edf1fd4c52c2573f06e04\n按如下方法释放即可：\n1$ ./docker-image-extract ghostunnel/ghostunnel:sha256:40247f4b49c364046ebf47696dbd31752b4db2ae2b9edf1fd4c52c2573f06e04 不用担心目录结构，释放出来的文件会放到当前 output 目录下。\n","date":"2021-10-27","img":"","permalink":"https://zhangrr.github.io/posts/20211027-docker_extract_file/","series":null,"tags":null,"title":"没有装Docker如何从镜像中释放出文件"},{"categories":null,"content":"为什么会有非 Docker 环境这个怪字眼呢？\n无他，因为满网搜索到的教程都是在 Docker 环境下安装使用。\n但是穷啊，八戒的 vps 是个单核 500m 的 justhost 机器，便宜的很，这种廉价机器来跑 Docker，基本要占100M，跑不太动。\n这种一穷二白的环境，就只能把 Bitwarden 从容器里拆出来用。\n好在 Bitwarden_rs 是一个 rust 程序，占内存(16M左右)和cpu极少，本身就适合在 systemd 环境下跑。\n这里就利用 vaultwarden 和 traefik，在一台老破小服务器上运行。\n系统环境是 CentOS 7.9\n步骤如下：\n一、下载bitwarden(vaultwarden) 1wget https://github.com/dani-garcia/vaultwarden/archive/refs/tags/1.23.0.tar.gz 二、安装cargo并编译（可选） 1yum install -y epel-release 2yum install -y openssl-devel cargo 3 4cd vaultwarden-1.23.0 5cargo build --release --features sqlite 直接爆错啊，小小的 vps 连编译都过不去，编译进程都被 kill 掉了\n三、下载vaultwarden主文件 编译不通，就只能想别的办法了。Faint\n找一台有 docker 机器，从里面把文件都解析出来好了\n1docker pull vaultwarden/server:alpine 2docker create --name vw vaultwarden/server:alpine 3docker cp vw:/vaultwarden . 4docker cp vw:/web-vault . 5docker rm vw 这样会得到一个可执行文件 vaultwarden 和一个目录 web-vault\n我们把这两个东西都挪到 /opt/vaultwarden 目录下，并且建立 data 文件夹，用来存放要生成的 sqlite3 数据文件。\n1mkdir -p /opt/vaultwarden/data 2mv vaultwarden /opt/vaultwarden 3mv web-vault /opt/vaultwarden 四、生成systemd启动文件 注意，下面我们设置了 vaultwarden ROCKET_ADDRESS 的监听地址是 127.0.0.1 ，一是为了安全，二是为了下一步我们搭建 traefik，来反代 vaultwarden 用的；因为访问 vaultwarden 必须要加证书，而它本身是没有这个功能的，必须前置一个 nginx 或者 haproxy 或者 traefik 或者 carddy。\n1cat \u0026lt;\u0026lt; EOF \u0026gt;\u0026gt; /etc/systemd/system/vaultwarden.service  2[Unit] 3Description=Bitwarden 4 5[Service] 6Type=simple 7Restart=always 8Environment=\u0026#34;ROCKET_ADDRESS=127.0.0.1\u0026#34; 9WorkingDirectory=/opt/vaultwarden 10ExecStart=/opt/vaultwarden/vaultwarden 11 12[Install] 13WantedBy=local.target 14EOF 五、配置traefik 1wget https://github.com/traefik/traefik/releases/download/v2.4.8/traefik_v2.4.8_linux_amd64.tar.gz 2tar zxvf traefik_v2.4.8_linux_amd64.tar.gz 3 4mkdir -p /opt/traefik/dynamic 5mv traefik /opt/traefik 生成traefik配置文件，利用 traefik 自动申请 Let\u0026rsquo;s encrypt 证书\n1cat \u0026lt;\u0026lt; EOF \u0026gt;\u0026gt; /opt/traefik/traefik.yml 2log: 3 level: DEBUG 4 5api: 6 insecure: false 7 dashboard: true 8 9entryPoints: 10 http: 11 address: \u0026#34;:80\u0026#34; 12 http: 13 redirections: 14 entryPoint: 15 to: https 16 scheme: https 17 https: 18 address: \u0026#34;:443\u0026#34; 19 20certificatesResolvers: 21 letsEncrypt: 22 acme: 23 storage: /opt/traefik/acme.json 24 email: zhangranrui@gmail.com 25 tlsChallenge: {} 26 httpChallenge: 27 entryPoint: http 28 29providers: 30 file: 31 directory: /opt/traefik/dynamic 32 watch: true 配置 vaultwarden 代理\n1cat \u0026lt;\u0026lt; EOF \u0026gt;\u0026gt; /opt/traefik/dynamic/pass.yml 2http: 3 routers: 4 https_01: 5 rule: \u0026#34;Host(`xxx.rendoumi.com`)\u0026#34; 6 service: svc_01 7 tls: 8 certresolver: letsEncrypt 9 http_01: 10 rule: \u0026#34;Host(`xxx.rendoumi.com`)\u0026#34; 11 service: svc_01 12 entryPoints: 13 - http 14 services: 15 svc_01: 16 loadBalancer: 17 servers: 18 - url: \u0026#34;http://localhost:8000\u0026#34; 19EOF 设置 traefik 的 systemd 启动文件\n1cat \u0026lt;\u0026lt; EOF \u0026gt;\u0026gt; /etc/systemd/system/traefik.service  2[Unit] 3Description=traefik 4 5[Service] 6Type=simple 7Restart=always 8WorkingDirectory=/export/servers/traefik 9ExecStart=/export/servers/traefik/traefik 10 11[Install] 12WantedBy=local.target 13EOF 五、启动vaultwarden和traefik 1systemctl daemon-reload 2systemctl enable --now vaultwarden 3systemctl enable --now traefik 打开页面，我们就成功的用一台老破小搭建了自己的密码管理服务器！！！\n","date":"2021-10-27","img":"","permalink":"https://zhangrr.github.io/posts/20211027-bitwarden/","series":null,"tags":null,"title":"Bitwarden（vaultwarden）如何在非Docker环境下安装使用"},{"categories":null,"content":"在生产环境来创建阿里ACK托管k8s集群的过程：\n完全用于生产，不是搭建来做测试用的。\n授公司委托，给的RAM用户，所以阿里云RAM第一次登录后，强制修改密码\n然后授权资源管理， 正式开始建立过程\n一、准备条件   两台及以上ecs服务器\n  阿里云账户余额100元以上（阿里云要求）\n  阿里云oss一个（oss和ecs在一个区域最好）\n  首先阿里云创建k8s集群要求至少有两台ecs服务器，可以创建集群的时候再购买，不要预先购买。\n二、下面开始创建： 阿里云最左上角的菜单（新版本首页）-\u0026gt;产品与服务-\u0026gt;容器服务kubernetes版本\n第一次创建会让开启ram授权，正常点击授权就可以\n点击创建集群\n点击后如下\n各个选项的详细说明：\n第一部分：\n集群版本： 最上面可以选择ACK托管版，和其他4个版本，着重说一下专有版和托管版的区别\n专有版本：master和worker都需要自己创建，如果需要高可用，那么master需要至少三个，也就是说，如果你不想把master和worker放在同一台服务器上，就要多使用三台服务器。\nACK托管版：master由阿里云给创建，自己只需要购买worker服务器。\n集群名称： k8s-hbb\n地域： 请选择自己ecs和rds等资源所在区域，这里是华东2（上海）\nKubernetes版本：阿里云已经做好充分的测试了，所以选择默认的即可。这里是 1.18.8-aliyun.1\n容器运行时： Docker 19.03.5\n第二部分：\n专有网络: 专有网络选择和ecs，rds同一个专有网络，这里是vpc-uf6pcr7nvp3dqmx86yyk0，网段是172.19.0.0/16\n虚拟交换机： 同一个专有网络下面的交换机是可以互通的，这里新建一个虚拟交换机，网段是172.19.240.0/20\n网络插件： 选Flannel，除去阿里云自己的区别描述，还有一点 如果使用flannel插件，则worker端对外，访问外网（比如短信接口等）使用的是worder所在ecs自己的eip或者如果使用的是snat模式，就是snat绑定的eip。如果使用的terway插件则走的就是snat的eip。注意，创建集群成功后，会为集群创建一个对外服务的ingress的slb，worker内部的容器直接对外访问，使用的不是这个slb的ip。slb只是进来的通道。\npod网络 CIDR：为统一起见，10.240.0.0/20\nservice CIDR：为统一起见，192.168.240.0/20\n注意以上 建立好了三个网段，三个网段中均有240字段，便于记忆\nECS(2台)：172.19.240.0/20\nPOD网段：10.240.0.0/20\nService网段：192.168.240.0/20 ​\n节点IP数量：256，指单个节点可运行 Pod 数量的上限。 一定要拉到最大量256 ，弄到16的话，一个节点本身要跑10多个system的pod，就无法跑应用pod了。\n第三部分:\n配置SNAT：必选配置SNAT，对外主动访问的时候IP需要一致。解释：如果ecs没有访问外网能力，则必须使用snat，snat就是把vpc绑定一个eip，然后给内部的ecs使用nat方式主动外出访问用的，比如主动反问第三方的接口等。如果ecs自己已经绑定了eip或者自带ip带宽，可以不选择。\nAPISERVER访问：必选公网EIP暴露，这个绑定以后ip不收费，可以使用流量包，管理master用的。如果要使用『云效』必选。\n默认不选中使用EIP暴露API Server。 API Server提供了各类资源对象（Pod，Service等）的增删改查及watch等HTTP Rest接口。 - 如果选择开放，会创建一个EIP，并挂载到内网SLB上。此时，Master节点的6443端口（对应API Server）暴露出来，用户可以在外网通过kubeconfig连接并操作集群。 - 如果选择不开放，则不会创建EIP，您只能在VPC内部用kubeconfig连接并操作集群。\nRDS白名单：这个注意选择，目前阿里云显示出来的只有普通的mysql-rds，redis的和polardb的都不显示\n安全组：选择企业类型就可以，后期可以修改规则。\n第四部分\nKube-proxy模式：选IPVS，比IPTABLES性能高\n集群本地域名： hbb.local，.local结尾的统统是本地域名\n下一步，增加worker\n必须选新增实例，不要选择现有实例\n新增实例：就是新购买ecs，要注意自己选择vpc和交换机\n选择已有实例：可以选择现有的服务器，注意：现有服务器会被更换硬盘，硬盘内容会被清空。\n企业级实例规格族\n实例规格族名称格式为ecs.\u0026lt;规格族\u0026gt;，实例规格名称为ecs.\u0026lt;规格族\u0026gt;.large。\n ecs：云服务器ECS的产品代号。 \u0026lt;规格族\u0026gt;：由小写字母加数字组成。  小写字母为某个单词的缩写，并标志着规格族的性能领域。部分小写字母的含义如下所示。  c：一般表示计算型（computational） g：一般表示通用型（general） r：一般表示内存型（ram） ne：一般表示网络增强型（network enhanced）   数字一般区别同类型规格族间的发布时间。更大的数字代表新一代规格族，拥有更高的性价比，价格低性能好。   large：n越大，vCPU核数越多。  例如，ecs.g6.2xlarge表示通用型g6规格族中的一个实例规格，拥有8个vCPU核。相比于g5规格族，g6为新一代通用型实例规格族。\n按上面选ecs.c6.large，费用0.95/时，似乎比ecs.n1.medium 1.34/时好。\n节点数量：最少是2个，无法减到0，没办法。\n系统盘：选ESSD，速度快，40G即可。不要开云盘备份，会要求设置snapshot策略，要收钱。\n操作系统：选CentOS 7.9，不要选aliyun的自定义版本。更加标准化，便于升级。\n这里选择操作系统的时候，只有两个系统可以选择，一个是centos7，一个是阿里云linux。为什么操作系统不能选很多种？因为阿里云要使用Cloud-init自动安装docker的各种工具包进作为worker角色的ecs，所以他对系统的要求更高，否则很可能出现各种各样的问题。这里才会有这种限制。\n密钥对选则新建一个k8s-ssh。\n下一步组件配置\n安装ingress组件：需要对外服务，这个必选，会给分配一个slb负载均衡\n如果想K8S集群的服务直接提供服务给用户访问，可以选择『公网』，它会创建一个SLB并用EIP暴露公网，后端是k8s-ingress入口。\n负载类型： 公网，对外服务就要写公网。\n存储插件： 必选CSI，对之后创建数据卷语法没影响\n监控插件： 基础版是免费的，可以放心使用\n日志服务： 日志服务可以加，尤其是以后如果想采集内部doker里面的日志，这里还是推荐加一下最好，他会自动创建标记采集，后面使用这个标记可以方便的自动添加日志节点。\nslb费用： 0.66/小时,带宽费用0.8/g\n下一步确认配置\n核对一下是否和自己选择的一样\n核对无误后点击创建集群。注意：创建集群的时候，会检测一些权限，如果权限未开通，可以令开页面进行开通授权，比如ess弹性伸缩，开通后点小按钮刷新状态， 状态都ok以后，点击创建集群。\n等待十分钟左右集群创建成功。到此创建集群已经完成。\n然后到控制台可以查看，这样ACK集群就创建好了。\n后记：\n毁掉ACK的时候，切忌去删除arms-prom的helm，再删除ack集群，否则会清不干净东西。\n下次重建的时候会装不上prometheus\n","date":"2021-10-26","img":"","permalink":"https://zhangrr.github.io/posts/20211026-ack_build/","series":null,"tags":null,"title":"阿里云ACK完全生产环境规划和搭建"},{"categories":null,"content":"之前介绍过如何制作一个 centos live cdrom 系统\n那么，某些情况下我们可能无法弄一个 pxe 系统，而只能通过 idrac 挂载 iso 的方式安装系统\n该如何去做呢？\n步骤如下：\n一、下载Centos的minimal安装光盘 1wget http://mirrors.163.com/centos/7.9.2009/isos/x86_64/CentOS-7-x86_64-Minimal-2009.iso 2yum install -y mkisofs 二、准备kickstart安装文件 下载： centos7.ks 1text 2skipx 3install 4 5auth --useshadow --enablemd5 6authconfig --enableshadow --passalgo=sha512 7 8firstboot --disable 9keyboard us 10lang en_US.UTF-8 11reboot 12cdrom 13 14firewall --disable 15selinux --disabled 16 17services --enabled=\u0026#34;chronyd\u0026#34; 18logging level=info 19 20 21#ignoredisk --only-use=vda 22ignoredisk --only-use=sda 23#bootloader --location=mbr --append=\u0026#34;net.ifnames=0 biosdevname=0 crashkernel=auto\u0026#34; 24bootloader --location=mbr --append=\u0026#34;crashkernel=auto\u0026#34; 25 26rootpw --plaintext Renren2021! 27timezone Asia/Shanghai --isUtc 28 29network --device=lo --hostname=localhost.localdomain 30user --name=supdev --gid=511 --groups=\u0026#34;supdev\u0026#34; --uid=511 --password=\u0026#34;Renren2021!\u0026#34; 31 32zerombr 33clearpart --all --initlabel  34 35part biosboot --fstype=biosboot --size=1 36part /boot --fstype ext4 --size=2048  37part swap --asprimary --size=8192 38part / --fstype ext4 --size=1 --grow 39 40#part biosboot --fstype=biosboot --size=1 41#part /boot --fstype ext2 --size 250 42#part pv.01 --size 1 --grow 43#volgroup vg pv.01 44#logvol / --vgname=vg --size=1 --grow --fstype ext4 --fsoptions=discard,noatime --name=root 45#logvol /tmp --vgname=vg --size=1024 --fstype ext4 --fsoptions=discard,noatime --name=tmp 46#logvol swap --vgname=vg --recommended --name=swap 47 48#uefi 49#partition /boot/efi --asprimary --fstype=vfat --label EFI --size=200 50#partition /boot --asprimary --fstype=ext4 --label BOOT --size=500 51#partition / --asprimary --fstype=ext4 --label ROOT --size=4096 --grow 52 53 54services --enabled=network 55 56reboot 57 58%pre 59parted -s /dev/sda mklabel gpt 60%end 61 62%packages 63@core 64@system-admin-tools 65@additional-devel 66@virtualization-client 67@virtualization-platform 68@virtualization-tools 69libguestfs-tools-c 70perl-Sys-Virt 71qemu-guest-agent 72qemu-kvm-tools 73curl 74dstat 75expect 76openssl 77initscripts 78ipmitool 79lrzsz 80lsof 81mtools 82nc 83nmap 84perl 85perl-CPAN 86procps 87python 88screen 89sysstat 90systemtap 91systemtap-client 92systemtap-devel 93tcpdump 94telnet 95vim 96wget 97wsmancli 98zip 99chrony 100kexec-tools 101net-tools 102ntp 103ntpdate 104man 105acpid 106chrony 107telnet 108%end 三、准备生成iso的脚本 下载： makeiso.sh 1#!/bin/bash 2rm -rf /tmp/bootiso /tmp/bootcustom /tmp/boot.iso 3mkdir /tmp/bootiso 4mount -o loop CentOS-7-x86_64-Minimal-2009.iso /tmp/bootiso 5 6mkdir /tmp/bootcustom 7cp -r /tmp/bootiso/* /tmp/bootcustom 8umount /tmp/bootiso 9rmdir /tmp/bootiso 10 11 12chmod -R u+w /tmp/bootcustom 13 14cp centos7.ks /tmp/bootcustom/isolinux/ks.cfg 15 16sed -i \u0026#39;/menu\\ default/d\u0026#39; /tmp/bootcustom/isolinux/isolinux.cfg 17sed -i \u0026#39;s/^timeout\\ .*/timeout 10/g\u0026#39; /tmp/bootcustom/isolinux/isolinux.cfg 18sed -i \u0026#39;/^label\\ linux/i label\\ kickstart\u0026#39; /tmp/bootcustom/isolinux/isolinux.cfg 19sed -i \u0026#39;/^label\\ linux/i \\ \\ menu\\ label\\ ^Install\\ Using\\ Kickstart\\ CentOS 7\u0026#39; /tmp/bootcustom/isolinux/isolinux.cfg 20sed -i \u0026#39;/^label\\ linux/i \\ \\ menu\\ default\u0026#39; /tmp/bootcustom/isolinux/isolinux.cfg 21sed -i \u0026#39;/^label\\ linux/i \\ \\ kernel\\ vmlinuz\\ biosdevname=0\u0026#39; /tmp/bootcustom/isolinux/isolinux.cfg 22sed -i \u0026#39;/^label\\ linux/i \\ \\ append\\ initrd=initrd.img\\ ks=cdrom:\\/ks.cfg\u0026#39; /tmp/bootcustom/isolinux/isolinux.cfg 23sed -i \u0026#39;/^label\\ linux/i \\\\n\u0026#39; /tmp/bootcustom/isolinux/isolinux.cfg 24 25cd /tmp/bootcustom 26mkisofs -o /tmp/boot.iso -b isolinux.bin -c boot.cat -no-emul-boot -boot-load-size 4 -boot-info-table -V \u0026#34;CentOS 7 x86_64\u0026#34; -R -J -v -T isolinux/. . 生成的自动安装光盘文件在 /tmp/boot.iso ，在 idrac 中 mount 出来，就可以用 virtual CD-ROM 自动安装了\n","date":"2021-10-25","img":"","permalink":"https://zhangrr.github.io/posts/20211025-autoinstall_cd/","series":null,"tags":null,"title":"Centos Auto Install Cdrom自动安装cdrom的制作"},{"categories":null,"content":"为了研发方便就给他们在内网开通了 vsftpd 的服务。\n结果 java 直接有封好的 ftp library 可用，大家就直接用了。\n导致任何单独的一个文件上传都会起一个 ftp 实例，没有复用 ftp 的 socket 链接 。系统挤压了大量的socket连接。\n烦恼啊，出了事就麻烦。需要把日志都详细记下来\n做法如下：\n1vi /etc/vsftp/vsftpd.conf 2...... 3dual_log_enable=YES 4log_ftp_protocol=YES 5xferlog_enable=YES 6xferlog_std_format=NO 7...... 解释一下：\n  dual_log_enable \u0026mdash; 和 xferlog_enable 协同，会写两份日志，一份到/var/log/xferlog，一份到/var/log/vsftpd.log\n  log_ftp_protocol \u0026mdash; 和 xferlog_enable 协同，同时xferlog_std_format需要设置为NO，这样所有的 FTP 命令都会记录下来。\n  这样所有人的操作都会被记录下来，就后顾无忧了。\n","date":"2021-10-25","img":"","permalink":"https://zhangrr.github.io/posts/20211025-vsftpd/","series":null,"tags":null,"title":"Vsftpd的日志设置"},{"categories":null,"content":"一般来说，我们要搭建一个正式的pxe自动装机系统，需要装 dnsmasq 做 dhcp + tftp ，需要编译 ipxe 来获得 undionly.kpxe ，需要 http 服务器来提供资源下载，repo 同步服务来提供 repo。组件非常多，也比较麻烦。\n当然，这么多也是有必要的，因为可以持续提供一个稳定的装机系统。\n场景一换，如果我们在本地机房里，什么都没有，想搭一套环境的步骤就比较繁复了。\nPyPXE 就是非常简单的一个程序，居然自己实现了用于 PXE 的 dhcp、tftp 和 http 全部的功能，而且支持 iPXE。\n太牛逼了，前提啊，PyPXE 是基于 Python 2.7 的，Python 3.x是运行不了的。\n想让它跑起来还必须做一定的修改，步骤如下：\n一、下载PyPXE 1git clone https://github.com/pypxe/PyPXE.git 2cd PyPXE 下载就行了，不用安装。\n二、手动生成config.json配置文件 1{ 2 \u0026#34;DHCP_SERVER_IP\u0026#34;: \u0026#34;192.168.85.27\u0026#34;, 3 \u0026#34;DHCP_FILESERVER\u0026#34;: \u0026#34;192.168.85.27\u0026#34;, 4 5 \u0026#34;DHCP_OFFER_BEGIN\u0026#34;: \u0026#34;192.168.85.200\u0026#34;, 6 \u0026#34;DHCP_OFFER_END\u0026#34;: \u0026#34;192.168.85.250\u0026#34;, 7 \u0026#34;DHCP_SUBNET\u0026#34;: \u0026#34;255.255.255.0\u0026#34;, 8 \u0026#34;DHCP_ROUTER\u0026#34;: \u0026#34;192.168.85.1\u0026#34;, 9 \u0026#34;DHCP_DNS\u0026#34;: \u0026#34;114.114.114.114\u0026#34;, 10 11 \u0026#34;DHCP_SERVER_PORT\u0026#34;: 67, 12 \u0026#34;DHCP_BROADCAST\u0026#34;: \u0026#34;\u0026#34;, 13 \u0026#34;DHCP_MODE_PROXY\u0026#34;: false, 14 \u0026#34;DHCP_WHITELIST\u0026#34;: false, 15 \u0026#34;HTTP_PORT\u0026#34;: 80, 16 \u0026#34;LEASES_FILE\u0026#34;: \u0026#34;\u0026#34;, 17 \u0026#34;MODE_DEBUG\u0026#34;: \u0026#34;dhcp\u0026#34;, 18 \u0026#34;MODE_VERBOSE\u0026#34;: \u0026#34;\u0026#34;, 19 \u0026#34;NBD_BLOCK_DEVICE\u0026#34;: \u0026#34;\u0026#34;, 20 \u0026#34;NBD_COPY_TO_RAM\u0026#34;: false, 21 \u0026#34;NBD_COW\u0026#34;: true, 22 \u0026#34;NBD_COW_IN_MEM\u0026#34;: false, 23 \u0026#34;NBD_PORT\u0026#34;: 10809, 24 \u0026#34;NBD_SERVER_IP\u0026#34;: \u0026#34;0.0.0.0\u0026#34;, 25 \u0026#34;NBD_WRITE\u0026#34;: false, 26 \u0026#34;NETBOOT_DIR\u0026#34;: \u0026#34;netboot\u0026#34;, 27 \u0026#34;NETBOOT_FILE\u0026#34;: \u0026#34;boot.http.ipxe\u0026#34;, 28 \u0026#34;STATIC_CONFIG\u0026#34;: \u0026#34;\u0026#34;, 29 \u0026#34;SYSLOG_PORT\u0026#34;: 514, 30 \u0026#34;SYSLOG_SERVER\u0026#34;: null, 31 \u0026#34;USE_DHCP\u0026#34;: true, 32 \u0026#34;USE_HTTP\u0026#34;: true, 33 \u0026#34;USE_IPXE\u0026#34;: true, 34 \u0026#34;USE_TFTP\u0026#34;: true 35} 上面json文件无法加注解，我们把它分三部分\n  本机配置，本机的地址都是 192.168.85.27\n  dhcp 的配置，开始192.168.85.200，结束192.68.85.250，掩码255.255.255.0，网关192.168.85.1，DNS114.114.114.114\n  第三部分不用动\n  三、下载ISO并修改ipxe脚本 1cd netboot 2wget http://mirrors.163.com/rocky/8/isos/x86_64/Rocky-8.4-x86_64-dvd1.iso 3mkdir rocky8.iso 4mount -o loop Rocky-8.4-x86_64-dvd1.iso rocky8.iso 5 6cat \u0026lt;\u0026lt; EOF \u0026gt;\u0026gt; boot.http.ipxe 7#!ipxe 89:start 10menu PXE Boot Options 11item shell iPXE shell 12item Rocky8 Install rocky8 13item exit Exit to BIOS 1415choose --default rocky8 --timeout 5000 option \u0026amp;\u0026amp; goto ${option} 16:shell 17shell 181920:rocky8 21set root http://192.168.85.27/rocky8.iso 22initrd ${root}/images/pxeboot/initrd.img 23kernel ${root}/images/pxeboot/vmlinuz inst.repo=${root}/ initrd=initrd.img ip=dhcp 24boot 252627:exit 28exit 29EOF 三、修改源代码 运行一下：\n1python -m pypxe.server --config config.json --debug all --verbose all 如果我们起一台机器或者虚机，会报第一个错：\nUnicodeDecodeError: \u0026lsquo;ascii\u0026rsquo; codec can\u0026rsquo;t decode byte 0xc0 in position 0: ordinal not in range(128)\n这个是代码报错，我们需要修改一下\n1vi pypxe/dhcp.py 2 3 def tlv_encode(self, tag, value): 4 \u0026#39;\u0026#39;\u0026#39;Encode a TLV option.\u0026#39;\u0026#39;\u0026#39; 5 6 # 注释掉下面的两行，我们不需要打印出我们一定能看懂的字符，都按bytes处理即可 7 #if type(value) is str: 8 # value = value.encode(\u0026#39;ascii\u0026#39;) 9 value = bytes(value) 10return struct.pack(\u0026#39;BB\u0026#39;, tag, len(value)) + value 然后我们需要修改第二个地方，理由是这个 PyPXE 会判断 Client 发过来的 dhcp 请求，它只实现了针对PXE-Client的 Vendor-class：\n所以我们也要屏蔽一下，否则按照正常过程\n客户端dhcp \u0026ndash;\u0026gt; PyPXE 后，PyPXE 送回客户 ipxe 脚本，然后客户安装，当加载了vmlinuz和initrd之后会进入anaconda-linux进行系统安装，过程中会再次向DHCP服务器申请IP地址， 这个时候他向DHCP Server发出的discover申请是得不到回复的，因此安装过程将被打断。\n1vi pypxe/dhcp.py 2 3 def validate_req(self, client_mac): 4 # client request is valid only if contains Vendor-Class = PXEClient 5 \u0026#39;\u0026#39;\u0026#39;代码整个注释掉，直接返回 True 6 if self.whitelist and self.get_mac(client_mac) not in self.get_namespaced_static(\u0026#39;dhcp.binding\u0026#39;): 7 self.logger.info(\u0026#39;Non-whitelisted client request received from {0}\u0026#39;.format(self.get_mac(client_mac))) 8 return False 9 if 60 in self.options[client_mac] and \u0026#39;PXEClient\u0026#39;.encode() in self.options[client_mac][60][0]: 10 self.logger.info(\u0026#39;PXE client request received from {0}\u0026#39;.format(self.get_mac(client_mac))) 11 return True 12 self.logger.info(\u0026#39;Non-PXE client request received from {0}\u0026#39;.format(self.get_mac(client_mac))) 13 return False 14 \u0026#39;\u0026#39;\u0026#39; 15 return True 这样修改后，就可以正常安装了。\n服务器启动：\n客户端启动pxe开始安装，看下面，系统的ipxe dhcp一次，然后chainload.kpxe 又一次，anaconda 又一次，最少会发三次或更多的dhcp请求。\n用 VNC 连进去可以看到安装画面，如果是 kickstart 就是全自动安装了。\n","date":"2021-10-22","img":"","permalink":"https://zhangrr.github.io/posts/20211022-pypxe/","series":null,"tags":null,"title":"PyPXE-一个牛逼的一站式PXE安装包"},{"categories":null,"content":"上一篇文章我们介绍了 ETCD 的容器化，搞这件事情的主要目的其实是要动态更新 Nginx 的配置\n这一章我们就来配置 confd 和 Nginx，来达到动态更新 Nginx 配置的目的\n一、安装配置confd 下载并安装：\n1wget https://github.com/kelseyhightower/confd/releases/download/v0.16.0/confd-0.16.0-linux-amd64 2mv confd-0.16.0-linux-amd64 /usr/sbin/confd 3chmod +x /usr/sbin/confd 生成配置文件：\n我们在 etcd 中存放的格式如下\n1etcdctl set /nginx/app01/subdomain app1 2etcdctl set /nginx/app01/upstream/app01_1 \u0026#34;192.168.0.1:5601\u0026#34; 3 4/nginx/app01/subdomain \u0026#34;app01\u0026#34; 5/nginx/app01/upstream/app01_1 \u0026#34;192.168.0.1:5601\u0026#34; 6/nginx/app01/upstream/app01_2 \u0026#34;192.168.0.2:5601\u0026#34; 那么，我们先生成 confd 的配置文件：\n1mkdir -p /etc/confd/{conf.d,templates} 2 3cat \u0026lt;\u0026lt;EOF\u0026gt;\u0026gt;/etc/confd/conf.d/nginx.toml 4[template] 5src = \u0026#34;nginx.conf.tmpl\u0026#34; 6dest = \u0026#34;/etc/nginx/conf.d/nginx-auto.conf\u0026#34; 7keys = [ 8\u0026#34;/nginx/app01/subdomain\u0026#34;, 9\u0026#34;/nginx/app01/upstream\u0026#34;, 10] 11check_cmd = \u0026#34;/usr/sbin/nginx -t\u0026#34; 12reload_cmd = \u0026#34;/usr/sbin/nginx -s reload\u0026#34; 13EOF 14 15cat \u0026lt;\u0026lt;EOT\u0026gt;\u0026gt;/etc/confd/templates/nginx.conf.tmpl 16upstream {{getv \u0026#34;/nginx/app01/subdomain\u0026#34;}} { 17{{range getvs \u0026#34;/nginx/app01/upstream/*\u0026#34;}} 18server {{.}}; 19{{end}} 20} 2122server { 23server_name {{getv \u0026#34;/nginx/app01/subdomain\u0026#34;}}.example.com; 24location / { 25proxy_pass http://{{getv \u0026#34;/nginx/app01/subdomain\u0026#34;}}; 26proxy_redirect off; 27proxy_set_header Host $host; 28proxy_set_header X-Real-IP $remote_addr; 29proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; 30} 31} 32EOT 33 confd 会根据 etcd 的值，结合 nginx.conf.tmpl ，生成 nginx-auto.conf，然后 nginx -t 验证通过后，执行 nginx -s rolad。\n注意：nginx的配置中必须有 include /etc/nginx/conf.d/*.conf;\n二、运行confd 1# 只处理一次 2confd -onetime -backend etcd -node http://etcd-svc.default:2379 3 4# 按时间轮询 5confd -interval=60 -backend etcd -node http://etcd-svc.default:2379 \u0026amp; 6 这样就可以动态更新 Nginx 了。\n","date":"2021-10-21","img":"","permalink":"https://zhangrr.github.io/posts/20211021-etcd_confd_nginx/","series":null,"tags":null,"title":"ETCD + CONFD + NGINX的配置"},{"categories":null,"content":"由于使用到了阿里的 K8S 托管集群 ACK，于是想占便宜。想用到托管 master node 的 etcd 来保存数据。\n结果是，未遂！！无法使用。\n阿里有单独的配置管理服务，复杂化了，不想用。\n那么解决方案就是，启动只有一个节点副本的 etcd pod，然后数据持久化到 OSS 的 S3 桶中。\n一、实现etcd的单节点docker化 首先我们只想在测试环境中跑一个单节点的 etcd，还没有用到 k8s，做法如下：\n1#!/bin/bash 2 3NODE1=172.18.31.33 4REGISTRY=quay.io/coreos/etcd 5# available from v3.2.5 6#REGISTRY=gcr.io/etcd-development/etcd 7 8docker run \\ 9 -p 2379:2379 \\ 10 -p 2380:2380 \\ 11 --volume=/data/etcd:/etcd-data \\ 12 --name etcd ${REGISTRY}:latest \\ 13 /usr/local/bin/etcd \\ 14 --data-dir=/etcd-data --name node1 \\ 15 --initial-advertise-peer-urls http://${NODE1}:2380 --listen-peer-urls http://0.0.0.0:2380 \\ 16 --advertise-client-urls http://${NODE1}:2379 --listen-client-urls http://0.0.0.0:2379 \\ 17 --initial-cluster node1=http://${NODE1}:2380 18 19 如上就可以了，容器跑起来以后进入容器测试一下：\n1docker exec -it 425f26903466 /bin/sh 2 3etcdctl -C http://127.0.0.1:2379 member list 4c3511611548b7c7c: name=node1 peerURLs=http://172.18.31.33:2380 clientURLs=http://172.18.31.33:2379 isLeader=true 5 6etcdctl ls --recursive / 这样一个单节点的 etcd 就弄好了，对外暴露的是 2379 和 2380 端口\n二、实现 etcd 的单节点 k8s 化 首先编写一个deployment文件etcd-deploy.yaml：\n下载：etcd-deploy.yaml 1apiVersion: apps/v1  2kind: Deployment  3metadata:  4 name: etcd-deploy 5 labels:  6 app: etcd 7spec:  8 replicas: 1  9 selector:  10 matchLabels:  11 app: etcd 12 template:  13 metadata:  14 labels:  15 app: etcd 16 spec:  17 containers:  18 - name: etcd 19 image: quay.io/coreos/etcd:latest 20 ports: 21 - containerPort: 2379 22 name: client 23 protocol: TCP 24 - containerPort: 2380 25 name: server 26 protocol: TCP 27 command: 28 - /usr/local/bin/etcd 29 - --name 30 - etcd 31 - --initial-advertise-peer-urls 32 - http://etcd:2380 33 - --listen-peer-urls 34 - http://0.0.0.0:2380 35 - --listen-client-urls 36 - http://0.0.0.0:2379 37 - --advertise-client-urls 38 - http://etcd:2379 39 - --initial-cluster 40 - etcd=http://etcd:2380 41- --data-dir 42- /etcd-data 43volumeMounts: 44- mountPath: /etcd-data 45name: etcd-data 46lifecycle: 47postStart: 48exec: 49command: 50- \u0026#34;sh\u0026#34; 51- \u0026#34;-c\u0026#34; 52- \u0026gt; 53echo \u0026#34;127.0.0.1 etcd\u0026#34; \u0026gt;\u0026gt; /etc/hosts; 54volumes: 55- name: etcd-data 56persistentVolumeClaim: 57claimName: k8s-etcd-20g 58restartPolicy: Always 注意上面，我们使用了一个 pvc 卷 k8s-etcd-20g，这个卷挂在 /etcd-data，是由 OSS 建立的，用于持久话数据，省得重启 etcd 的 pod，数据消失不见了。\n然后，我们需要把这个 deployment 作为 svc 服务暴露在集群中，再编写一个etcd-svc.yaml\n下载：etcd-svc.yaml 1apiVersion: v1 2kind: Service 3metadata: 4 name: etcd-svc 5spec: 6 ports: 7 - port: 2379 8 name: tcp2379 9 protocol: TCP 10 targetPort: 2379 11 - port: 2380 12 name: tcp2380 13 protocol: TCP 14 targetPort: 2380 15 selector: 16 app: etcd 17 type: ClusterIP kubectl apply 部署到 k8s 中，这样就可以了。\nk8s测试方法，随便启动一个 busybox pod，进去测试一下：\n1kubectl run curl --image=radial/busyboxplus:curl -i --tty --rm 2 3curl http://etcd-svc:2379/version 4 5curl http://etcd-svc.default:2379/version 6 7curl http://etcd-svc.default:2379/v2/keys 8 9curl http://etcd-svc.default:2379/v2/keys/?recursive=true 10 11curl http://etcd-svc.default:2379/v2/keys/service/nginx 12 13curl http://etcd-svc.default:2379/v2/keys/service/nginx/127.0.0.1 14 15curl --location --request PUT \u0026#39;http://etcd-svc:2379/v2/keys/service/nginx/10.240.0.41\u0026#39; --header \u0026#39;Content-Type: application/x-www-form-urlencoded\u0026#39; --data-urlencode \u0026#39;value=10.240.0.41:9000\u0026#39; 16 17curl http://etcd-svc.default:2379/v2/keys/service/nginx/ ","date":"2021-10-21","img":"","permalink":"https://zhangrr.github.io/posts/20211021-etcd_docker/","series":null,"tags":null,"title":"Etcd单节点应用"},{"categories":null,"content":"在生产环境中我们大量使用了 kvm 的虚拟技术，虚拟机的镜像系统使用的是 Cloud-init 的技术\n不可避免的，虚机会遭到各种损坏，维护的手段就十分必要了\n假设我们有一个虚机文件 vis-16-41-18.qcow2 坏了\n一、安装支持包 1yum install libguestfs libguestfs-tools 二、查看日志 1virt-log -a vis-16-41-18.qcow2 没有什么特殊的报错信息\n三、分析文件系统组成 virt-filesystems和virt-df都可以，用virt-df看的更多一些\n1virt-filesystems -l -a vis-16-41-18.qcow2 2Name Type VFS Label Size Parent 3/dev/sda1 filesystem ext4 - 209715200 - 4/dev/sda2 filesystem ext4 - 214536355840 - 5 6virt-df -a vis-16-41-18.qcow2 7Filesystem 1K-blocks Used Available Use% 8vis-16-41-18.qcow2:/dev/sda1 194241 31706 152295 17% 9vis-16-41-18.qcow2:/dev/sda2 206088704 5639856 189973444 3% 10 四、挂载文件系统开始修复（方法1） 从上面可以看到 vis-16-41-18.qcow2 里面有两个分区，/dev/sda1 和/dev/sda2\n第一个应该是/boot，第二个是/\n把 / mount 出来\n1mkdir 18 2guestmount -a vis-16-41-18.qcow2 -m /dev/sda2 --rw ./18 或者全自动mount\n1guestmount -a vis-16-41-18.qcow2 -i --rw ./18 这样就可以直接进18目录进行修复操作了\n1cd 18/lib64 2ls libc*.* 发现同事胡乱升级glibc，把libc的基础库弄坏了，少libc.so.6的软链接，建立一个修复即可\n1ln -s libc-2.15.so libc.so.6 五、挂载文件系统开始修复（方法2） 我们可以用 guestmount，也可以直接用 guestfish 。\nguestfish 是个命令行工具。它使用 libguestfs 的所有功能。\n1guestfish 2 3Welcome to guestfish, the libguestfs filesystem interactive shell for 4editing virtual machine filesystems. 5 6Type: \u0026#39;help\u0026#39; for help on commands 7 \u0026#39;man\u0026#39; to read the manual 8 \u0026#39;quit\u0026#39; to quit the shell 9 10\u0026gt;\u0026lt;fs\u0026gt; add vis-16-41-18.qcow2 11\u0026gt;\u0026lt;fs\u0026gt; run 12\u0026gt;\u0026lt;fs\u0026gt; list-filesystems 13/dev/sda1: ext4 14/dev/sda2: ext4 15\u0026gt;\u0026lt;fs\u0026gt; mount /dev/sda2 / 16\u0026gt;\u0026lt;fs\u0026gt; cat /etc/fstab 17 18# 19# /etc/fstab 20# Created by anaconda on Mon Dec 29 15:24:53 2014 21# 22# Accessible filesystems, by reference, are maintained under \u0026#39;/dev/disk\u0026#39; 23# See man pages fstab(5), findfs(8), mount(8) and/or blkid(8) for more info 24# 25UUID=9fdc111c-3042-4527-b3f8-a2961e55077e / ext4 defaults 1 1 26UUID=1855d5e1-18f8-48ea-8c3b-c52cdd512a5e /boot ext4 defaults 1 2 27tmpfs /dev/shm tmpfs defaults 0 0 28devpts /dev/pts devpts gid=5,mode=620 0 0 29sysfs /sys sysfs defaults 0 0 30proc /proc proc defaults 0 0 31 32\u0026gt;\u0026lt;fs\u0026gt; guestfish的常用命令：\n1add vis-16-41-18.qcow2 2run 3list-filesystems 4 5ll / 6ls / 7cat /etc/fstab 8write-append /etc/rc.d/rc.local \u0026#34;service sshd start\u0026#34; 9edit /etc/fstab. 10less /var/log/messages 11mkdir /tmp/a 12touch /tmp/a/b.txt 13write /tmp/a/b.txt 14rm /tmp/a/b.txt 15 16upload Upload a local file to the disk. ###注意：是上载本地文件到镜像文件去！！！ 17 六、virt对应guestfish的一些命令 1virt-cat vis-16-41-18.qcow2 /home/supdev/.bash_history 2 3virt-copy-in Copy files and directories into a guest. 4virt-copy-out Copy files and directories out of a guest. 5 6virt-edit Edit a file in a guest. 7virt-ls List files and directories in a guest 七、virt-rescue救援模式 如果虚机系统起不来，可以先尝试进入 rescue 救援模式\nvirt-rescue 类似于救援 CD，但用于虚拟机，且无需提供 CD。\nvirt-rescue 为用户提供救援外壳和一些简单的恢复工具，可用于检查和更正虚拟机或磁盘映像中的问题。\n1virt-rescue -a vis-16-41-18.qcow2 2Welcome to virt-rescue, the libguestfs rescue shell. 3 4Note: The contents of / are the rescue appliance. 5You need to mount the guest\u0026#39;s partitions under /sysroot 6before you can examine them. A helper script for that exists: 7mount-rootfs-and-do-chroot.sh /dev/sda2 8 9\u0026gt;\u0026lt;rescue\u0026gt; 10[ 67.194384] EXT4-fs (sda1): mounting ext3 file system 11using the ext4 subsystem 12[ 67.199292] EXT4-fs (sda1): mounted filesystem with ordered data 13mode. Opts: (null) 14mount: /dev/sda1 mounted on /sysroot. 15mount: /dev bound on /sysroot/dev. 16mount: /dev/pts bound on /sysroot/dev/pts. 17mount: /proc bound on /sysroot/proc. 18mount: /sys bound on /sysroot/sys. 19Directory: /root 20Thu Jun 5 13:20:51 UTC 2014 21(none):~ # ","date":"2021-10-21","img":"","permalink":"https://zhangrr.github.io/posts/20211021-libguestfs/","series":null,"tags":null,"title":"Libguestfs的救援手段"},{"categories":null,"content":"这是个娱乐话题，Dogecoin 狗币在马斯克的吹捧鼓动下，冲上云霄\n其实真的用CPU挖币，应该是挖 xmb 门罗币才是对的选择，挖狗币只是娱乐一下\n废话不多说，直接放上教程，我的机器是 CentoOS\n首先需要有个狗币钱包地址，这个我就不教大家了\n一、下载xmrig挖矿软件 下载地址：https://github.com/xmrig/xmrig/releases\n我们选择最近的下载就好\n二、做好加密通道 我们需要做好一条加密tcp通道\n用 ghostunnel, localhost:9999 \u0026mdash;\u0026gt; vps:9999 \u0026mdash;\u0026gt; rx.unmineable.com:3333\n三、用screen后台开挖 1screen 2 3#./xmrig -o localhost:9999 -a rx -k -u DOGE:狗币地址.矿工名#heyt-3711 4./xmrig -o localhost:9999 -a rx -k -u DOGE:DLR3DZGucJiSdahARW1vV5B1h3WYiw454a.work01 5 6ctrl+a+d 四、查看挖了多少 查看地址：https://unmineable.com/coins/DOGE/address/\n","date":"2021-10-21","img":"","permalink":"https://zhangrr.github.io/posts/20211021-dogecoin/","series":null,"tags":null,"title":"如何用CPU挖狗币Dogecoin"},{"categories":null,"content":"chrony 已经成了事实标准，替代了ntp。\n但是，有几个细节，需要非常注意。\n给出我们的配置，/etc/chrony.conf\n1 2# Use public servers from the pool.ntp.org project. 3# Please consider joining the pool (http://www.pool.ntp.org/join.html). 4server 172.10.1.1 iburst prefer minpoll 6 maxpoll 10 5server 172.10.1.2 iburst 6 7# Record the rate at which the system clock gains/losses time. 8driftfile /var/lib/chrony/drift 9 10# Allow the system clock to be stepped in the first three updates 11# if its offset is larger than 1 second. 12makestep 1.0 3 13 14# Enable kernel synchronization of the real-time clock (RTC). 15rtcsync 16 17# Enable hardware timestamping on all interfaces that support it. 18#hwtimestamp * 19 20# Increase the minimum number of selectable sources required to adjust 21# the system clock. 22#minsources 2 23 24# Allow NTP client access from local network. 25#allow 192.168.0.0/16 26 27# Serve time even if not synchronized to a time source. 28#local stratum 10 29 30# Specify file containing keys for NTP authentication. 31#keyfile /etc/chrony.keys 32 33# Specify directory for log files. 34logdir /var/log/chrony 35logchange 0.5 36# Select which information is logged. 37#log measurements statistics tracking rtc 里面有好几个细节，下面逐一解释一下：\n一、server 这里可以添加很多时间服务器，172.10.1.1 和 172.10.1.2 是两台自建的时间服务器。\nibust 会在 chrony 启动的2秒内，去快速poll服务器4次来快速矫正当前系统时间\nprefer 优先使用指定的服务器\nminpoll 6，缺省是6，意思是2的6次方，也就是64秒，最小轮询时间服务器的时间间隔是64秒\nmaxpoll 10，缺省是10，同上，2的10次方，也就是1024秒，最大轮询时间间隔是1024秒\n通常情况下一过minpoll的时间周期，就会触发一次时间同步询问。\n二、makestep 正常情况下如果系统时钟跟时间服务器不一致，chrony调整的方式是慢慢增加，或慢慢减少，不会一步到位，直接去跟时间服务器对齐。\nmakestep 1.0 3，意思就是如果时间服务器跟系统时间相差1秒，那么就在下3个时钟更新中追上时间服务器。\n这样就会立刻快速追平了，这样会带来时间跳跃。\n三、rtcsync 这个是把系统时钟同步到主板的硬件时钟去。\n缺省情况下是11分钟同步一次\n四、logchange logchange 0.5，意思是如果chrony调整的系统时间，超过了0.5秒的时长，就会发一条消息到syslog，这样我们就能在/var/log/messages里看到这条消息了。\n五、验证调试 开发人员会问，什么时候同步的服务器啊，多长时间同步一次，时间到底准不准啊，有没有发生跳跃啊\n我们就用chronyc sources来验证，配置中\n1server 172.10.1.1 iburst prefer minpoll 4 maxpoll 5 2server 172.10.1.2 iburst 解释一下，minpoll 4 maxpoll 5 ，那么最小轮询时间16秒，最大32秒。\n我们可以看到上图 LastRx 就是上次询问时间服务器的间隔时间，14秒、15秒、16秒，然后就变1了，最小间隔16秒后，立即就询问时间服务器，同步时间。\n同样可以看到第二台时间服务器就不受这个限制，缺省minpoll 6，就是64秒。所以上图第二台，63秒、64秒、65秒，变0，才去询问时间服务器。\n总结一下，chrony 调整时间偏差是匀速的，缓慢的。它询问时间服务器的间隔由minpoll来控制。\n我们用logchange来记录大的时间调整，以备追溯和查询。\n","date":"2021-10-20","img":"","permalink":"https://zhangrr.github.io/posts/20211020-chrony/","series":null,"tags":null,"title":"Chrony的几个详细配置细节"},{"categories":null,"content":"很现实的问题，局域网内有态势感知和网络流量分析，这很讨厌！\n那么，如何把某段流量隐藏起来，让态势感知无法分析呢？\n前提条件，你需要有国外的一台 VPS 作为外援，把 TCP 流量通过 TLS 加密送到国外的服务器，然后再转发到正确的目标服务器上，这样就不会被人追踪了。\n这里推荐 Ghostunnel ，这是个 Go 的程序，只有一个执行文件。配合 certik 证书生成，就完美了。\n项目地址：https://github.com/ghostunnel/ghostunnel\n首先我们使用 certik 生成三个证书，ca.pem server.pem 和 client1.pem\n然后ghostunnel以及三个证书文件都放在/usr/local/bin下\n一、在VPS上运行ghostunnel模式server 1/usr/local/bin/ghostunnel server --listen 0.0.0.0:9999 --target tr.dero.herominers.com:1117 --keystore server.pem --cacert ca.pem --allow-cn client1 --unsafe-target 上面监听了端口0.0.0.0:9999（监听0.0.0.0必须加参数\u0026ndash;unsafe-target），远程转发到dero的矿池端口1117，只允许验证过的cn client1连接。\n二、在本地机器上运行ghostunnel模式client 1/usr/local/bin/ghostunnel client --listen localhost:9999 --target 193.42.114.129:9999 --keystore client.pem --cacert ca.pem 本地监听localhost:9999，所以不用加\u0026ndash;unsafe-target参数，然后连接到远程 vps 服务器，ip地址是193.42.114.129，端口是9999\n这样就完成了。可以放心的启动程序，连接到本地端口localhost:9999，TCP流量就会被隐藏起来，不会被分析到.\n","date":"2021-10-19","img":"","permalink":"https://zhangrr.github.io/posts/20211019-ghostunnel/","series":null,"tags":null,"title":"Ghostunnel使用TLS加密TCP流量"},{"categories":null,"content":"我们会有很多时候需要用到TLS证书，一个非常方便、小众的工具就是 certik 。\n这个软件纯由 Go 组成，就一个可执行文件，使用了 etcd 的 boltdb 格式存放所有的证书。\n软件地址：https://github.com/opencoff/certik\n使用：\n一、初始化证书库 1./certik -v tls.db init CA 二、签发一张server证书 1#./certik -v tls.db server -i IP.ADDR.ES server.domain.name 2./certik -v tls.db server -i 193.42.114.129 server 上面我们 server 的 ip 是193.42.114.129，域名简洁起见就叫 server 了 。\n三、签发一张client1证书 1./certik -v tls.db client client1 四、查看证书库 1./certik -v tls.db list 我们就可以看到三个证书了，一个 CA ，一个 server ，一个 client1\n五、导出各个证书 1#导出CA 2./certik -v tls.db export --root-ca 3 4#导出server 5./certik -v tls.db export server 6 7#导出client1 8./certik -v tls.db export client1 以上就可以得到各个东西了。\n那么我们 gen 出证书做什么用呢？当然用作 ghostunnel 的验证用\n","date":"2021-10-19","img":"","permalink":"https://zhangrr.github.io/posts/20211019-certik/","series":null,"tags":null,"title":"Certik 证书签发软件"},{"categories":null,"content":"现在国内都禁止挖币，什么币安、火币、cnspark之类的都不允许国内 IP 访问了。\n如何实现的呢？\n首先需要得到国家IP段，下载地址：http://www.ipdeny.com/ipblocks/。这里以我们国家 cn 为例\n步骤如下：\n一、安装ipset 1#Debian/Ubuntu系统 2apt-get -y install ipset 3 4#CentOS系统 5yum -y install ipset 二、清空iptable规则 1#防止设置不生效，清空之前的防火墙规则 2iptables -P INPUT ACCEPT 3iptables -F 三、创建ipset规则集 1#创建一个名为cnip的规则 2ipset -N cnip hash:net 3 4#下载国家IP段，这里以中国为例 5wget -P . http://www.ipdeny.com/ipblocks/data/countries/cn.zone 6 7#将IP段添加到cnip规则集中 8for i in $(cat /root/cn.zone ); do ipset -A cnip $i; done 四、创建iptable黑名单 1#扔黑名单 2iptables -A INPUT -p tcp --dport 80 -m set --match-set cnip src -j DROP 3iptables -A INPUT -p tcp --dport 443 -m set --match-set cnip src -j DROP 4 5#然后放行其他的 6iptables -P INPUT ACCEPT ","date":"2021-10-18","img":"","permalink":"https://zhangrr.github.io/posts/20211018-ipset_block_cn/","series":null,"tags":null,"title":"使用IPSET封掉某个国家整个的访问"},{"categories":null,"content":"公司安装了 openvpn ，带来方便，但是也有很多不便的地方，机房的总带宽就那么多。\n很多人共用 vpn 的时候，就会抢占带宽。\n那么，我们需要限制一下，限制 openvpn 所能使用的带宽，避免抢占 WEB 的带宽\n做法如下：\n由于我们不是要单独限制某一个 openvpn 用户，而是限制整体，所以简单用 TC 就可以了\n1#!/bin/sh 2tc qdisc del dev tun0 root 3tc qdisc add dev tun0 root handle 1: htb default 1 4tc class add dev tun0 parent 1: classid 1:1 htb rate 30Mbit ceil 30Mbit 解释一下：\n 我们 openvpn 启的是 tun0 ，所以限制的对象就是 dev tun0 首先第一行清除 tun0 的根队列 然后第二行建立 tun0 的 root 根队列为 1:0 htb ，缺省是1:1的子队列 最后一行，第三行建立 1:1 的子队列，带宽限制是 30Mbit ，注意这里是大B，就是网络术语中的带宽，换算成小b的话，需要除以8  效果很明显，直接被限制住（41兆而不是30M是因为这台机器是虚机，实体机上还有别的流量）：\n","date":"2021-10-18","img":"","permalink":"https://zhangrr.github.io/posts/20211018-openvpn_limit_bandwidth/","series":null,"tags":null,"title":"OpenVPN 限制流量带宽"},{"categories":null,"content":"本站这个博客的由来：\n源自于 wiredcraft 的面试，这个公司让老八很是向往，可以远程工作，第一次面试是 devops ，老外见面聊了后，由代理中国人面。因为面的是K8S的东西，正好刚给画包包公司做了整体迁往阿里ACK的工程，以为没问题，实际是直接问倒了我。就好比master node上面都跑了什么进程，唉，一言难尽啊，又问到ansible的变量，回答估计也不满意，结果就挂了\n知耻而后勇，后面去恶补了一下ansible和k8s的东西，实际也是实际操作居多，然后第二次面的是 sysadmin，全程老外面。还出了几道题，应该是没问题。但是工资要高了，也被刷了。\n这个博客就是其中一道题，那既然搭建出来了，干脆就好好用吧。\n本身自己对静态的 Blog 系统也比较感兴趣，自己主站 www.rendoumi.com 的 Blog 是由 journey 搭建的，是一个精巧的 go 程序，最妙的是它兼容 Ghost 博客系统，也能使用 Ghost 的 theme ，这样就完美的把自己以前 Ghost 的博客迁移了过去，但是，里面文章有很多过时了，但也不想清理，干脆借着这个机会，从新开始。搭一个自己喜欢的系统继续写新博客\n流行的 Markdown 写作平台有Hexo和Hugo，选Hugo是因为实在是不喜欢node，弄一堆npm，迁移麻烦死。\n这个博客程序基于Hugo，托管在 github，众所周知，github 是托管静态文件的地方，Markdown的最大毛病是图片。图片放在图床也不是好办法，所以图片和静态文件要在一起，下面就说一下搭建过程，我的主机是Ubuntu：\n一、下载Hugo 下载地址： https://github.com/gohugoio/hugo/releases/download/v0.88.1/hugo_0.88.1_Linux-64bit.tar.gz 1wget https://github.com/gohugoio/hugo/releases/download/v0.88.1/hugo_0.88.1_Linux-64bit.tar.gz 2tar zxvf hugo_0.88.1_Linux-64bit.tar.gz 二、初始一个博客写作目录 1./hugo new site MyBlog ​\n三、下载theme 1cd MyBlog 2git clone https://github.com/halogenica/beautifulhugo.git themes/beautifulhugo 3echo theme = \\\u0026#34;beautifulhugo\\\u0026#34; \u0026gt;\u0026gt; config.toml ​\n四、写一篇新文章 1cd MyBlog 2../hugo new posts/my-first-post.md echo \u0026#34;#### This is another Blog\u0026#34; \u0026gt;\u0026gt; content/posts/my-first-post.md 五、运行server，build草稿 1cd MyBlog 2../hugo server --buildDrafts ​\n六、测试一下 1curl http://localhost:1313 ​\n七、推送到github 首先我们要去github开一个xxx.github.io的repo仓库，然后 git 把生成的静态内容推上去就好了\n1cd MyBlog 2 3#生成静态文件 4../hugo --buildDrafts 5 6#文件生成的目录是public 7cd public 8 9#正常git操作就可以了 10git init 11git add . 12git commit -m \u0026#34;first commit\u0026#34; 13git branch -M main 14git remote add origin git@github.com:zhangrr/zhangrr.github.io.git 15git push -u origin main 八、看下结果 打开网页 http://zhangrr.github.io 就能看到网页了\n​\n九、选择写作软件 其实现在开始才是最重要的，用什么软件来写，就用大家推荐的 Typora 来就好了\n1# or use 2# sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys BA300B7755AFCFAE 3wget -qO - https://typora.io/linux/public-key.asc | sudo apt-key add - 4 5# add Typora\u0026#39;s repository 6sudo add-apt-repository \u0026#39;deb https://typora.io/linux ./\u0026#39; 7sudo apt-get update 8 9# install typora 10sudo apt-get install typora 十、选择目录存放格式 这个才是最主要的问题，看下图，post目录是所有文章，下面按目录存放，目录名是日期和文章名，目录里面是index.md和文章附带的图片。\n我觉得这个模式才是符合我的要求的。\n1. 2├── post 3│ ├── 2018-01-11-关联了两款小程序.md 4│ └── index.md 5│ ├── 2018-02-05-一款小小的物流数据产品.md 6│ └── index.md 7│ ├── 2018-03-19-现已加入 Algolia 搜索服务.md 8│ └── index.md 9│ ├── 2018-04-13-我是如何搞砸了本站搜索服务的.md 10│ └── index.md 11│ ├── 2018-04-18-小站构建工具已成功切换到 Hugo.md 12│ └── index.md 13│ ├── 2018-04-19-开始翻译一个文档：Saleor.md 14│ └── index.md 15│ ├── 2018-04-22-Saleor 初稿已翻译完成.md 16│ └── index.md 17│ ├── 2018-04-26-今天全是干货 18│ │ ├── IMG_5991-4755089.jpg 19│ │ ├── IMG_5997-4755064.jpg 20│ │ ├── IMG_5998-4755103.jpg 21│ │ ├── IMG_5999-4755051.jpg 22│ │ ├── IMG_6001-4755073.jpg 23│ │ ├── IMG_6002-4755080.jpg 24│ │ ├── IMG_6003-4755030.jpg 25│ │ ├── IMG_6004-4755009.jpg 26│ │ └── index.md 27│ ├── 2018-05-02-从 Jekyll 到 Hugo 的一些细节.md 28│ └── index.md 29│ └── 2018-05-03-Hugo 的文件管理方案.md 30│ └── index.md 31 十一、微调 Typora   为了能显示目录结构Outline，所以所有副标题需要用 ctrl+2 的标题文本，这样就能自动生成Outline\n     为了能让剪贴板自动把ctrl+v贴上的图片放到目录里面，需要设置Image\n  这样就完美了，以后就在这里写工作博客了。\n  十二、更新， Typora 的降级 最新消息，Typora 升到了 1.0 ，开始收费了。\n免费的最后一个版本是 typora 0.11.18 版本，还好能下到：\n下载地址：https://download.typora.io/linux/typora_0.11.18_amd64.deb\n回退方法：\n1sudo apt autoremove typora 2sudo dpkg -i typora_0.11.18_amd64.deb 3 4sudo add-apt-repository --remove \u0026#39;deb https://typora.io/linux ./\u0026#39; ","date":"2021-10-15","img":"","permalink":"https://zhangrr.github.io/posts/20211015-hugo_blog/","series":null,"tags":null,"title":"本站博客的由来以及搭建使用教程"},{"categories":null,"content":"上一篇文章，我们实施了Ubuntu下wifi热点的搭建，那么，其实我是想抓我iphone手机的https明文流量包来着。\n怎么抓取呢？\n方法也很简单\n一、安装 mitmproxy 有wifi热点那台机器的wlan网卡地址是192.168.222.1，就在那台上安装，便于抓取\n1pip install mitmproxy 二、运行mitmproxy 1mitmproxy -p 8080 三、配置iphone安装mitm证书 打开手机，用Safari浏览器打开网址：http://mitm.it\n 找到IOS那行，仔细看一看说明\n 点击 Get mitmproxy-ca-cert.pem 安装描述文件\n安装完以后，在设置 \u0026ndash;\u0026gt; 已下载描述文件，安装描述文件\n 安装好以后会显示绿色的已验证\n然后在手机上用 safari 访问网址：https://ip138.com\n回到 Ubuntu 的命令行窗口，上下选中抓到的包，然后按回车查看，左右光标键移动，可以看到response是明文的，q键是返回上一级\n四、包的拦截修改 上面演示的是常规的查看操作，下面介绍一下 mitmproxy 的另一强大功能，拦截修改 request 和 response。\n输入 i，然后输入 ~s 再按回车键，这时候就进入了 response 拦截模式。如果输入  ~q 则进入 request 的拦截模式，更多的命令可以输入 ？ 查看。拦截模式下的页面显示如下图所示：\n其中红色的表示请求正被拦截，这时 Enter 进入后 再按 e 就可以修改 request 或者 response。修改时是用 vim 进行编辑的，修改完成后按 a 将请求放行，如果要放行所有请求输入 A 即可。\n","date":"2021-10-14","img":"","permalink":"https://zhangrr.github.io/posts/20211014-iphone_hijack/","series":null,"tags":null,"title":"Iphone手机的https抓包"},{"categories":null,"content":"公司的wifi信号很弱，也不保险。省事起见，还是自己建立一个好\nusb无线网卡设备必须是一个 nl80211 兼容的无线设备，所以驱动就是这个：nl80211\n我的操作系统是Ubuntu，如果是CentOS命令基本一样\n插上wifi usb卡后 ip a 看一下网卡的名称，我这里是：wlx00a1b0817651，够长\n一、安装hostapd软件 1sudo apt install -y hostapd 二、建立hostapd.conf文件 1vi /etc/hostapd/hostapd.conf 2driver=nl80211 3ssid=Fast_8188  4channel=10 5interface=wlx00a1b0817651 6wpa=2 7wpa_passphrase=GreatWall2021! 8wpa_key_mgmt=WPA-PSK 9wpa_pairwise=TKIP 三、建立启动脚本/usr/local/bin/initAP.sh 1cat /usr/local/bin/initAP.sh 2 3#!/bin/bash 4 5start() {  6rfkill unblock all 7ifconfig wlx00a1b0817651 up 192.168.222.1 netmask 255.255.255.0 8sleep 2 9 10dnsmasq -i wlx00a1b0817651 --dhcp-range=192.168.222.10,192.168.222.20,2h 11 12#Enable NAT 13sysctl -w net.ipv4.ip_forward=1 14iptables -F  15iptables -X  16iptables -t nat -A POSTROUTING -s 192.168.222.0/24 -j SNAT --to 192.168.41.15 17 18hostapd -B /etc/hostapd/hostapd.conf  19} 20 21stop() { 22iptables -P INPUT ACCEPT 23iptables -P FORWARD ACCEPT 24iptables -P OUTPUT ACCEPT 25iptables -F 26iptables -X 27iptables -t nat -F 28iptables -t nat -X 29iptables -t mangle -F 30iptables -t mangle -X 31systemctl stop dnsmasq 32pkill hostapd 33/sbin/ip link set down dev wlx00a1b0817651 34} 35 36case $1 in  37 start) 38 start 39 ;; 40 stop) 41 stop 42 ;; 43 *) 44 echo \u0026#34;Usage: $0 {start|stop}\u0026#34; 45 exit 2 46esac 四、用root身份执行即可 1sudo chmod 755 /usr/local/bin/initAP.sh 2sudo /usr/local/bin/initAP.sh start 这样就可以用自己的手机连上这个wifi热点，尽情冲浪啦。\n ","date":"2021-10-14","img":"","permalink":"https://zhangrr.github.io/posts/20211014-linux_wifi/","series":null,"tags":null,"title":"Ubuntu下自建一个wifi热点供手机使用"},{"categories":null,"content":"某些场合，很有可能需要启动ISO或者USB盘，自带Linux系统，然后拯救当前损坏的系统\n或者直接启动一个LIVE CentOS系统，去做某些事，比如用MegaRaid划分Raid、测试系统等等\n这时候就需要制作出来一个LIVE CD的系统了\n制作步骤如下：\n一、安装live-tools 1yum -y install livecd-tools 二、准备Kickstart文件centos7-live-docker.ks 下载地址：centos7-live-docker.ks 1lang en_GB.UTF-8 2keyboard us 3timezone Asia/Shanghai --isUtc 4 5#selinux --enforcing 6selinux --disabled 7 8#firewall --enabled --service=cockpit 9firewall --disabled 10 11#xconfig --startxonboot 12part / --size 8192 --fstype ext4 13services --enabled=NetworkManager,sshd --disabled=network 14 15 16# Root password 17auth --useshadow --enablemd5 18rootpw --plaintext Kalaisadog2021 19 20repo --name=base --baseurl=http://mirror.centos.org/centos/7/os/x86_64/ 21repo --name=updates --baseurl=http://mirror.centos.org/centos/7/updates/x86_64/ 22repo --name=extras --baseurl=http://mirror.centos.org/centos/7/extras/x86_64/ 23repo --name=epel --baseurl=http://dl.fedoraproject.org/pub/epel/7/x86_64/ 24 25%packages  26@core 27kernel 28dracut 29bash 30firewalld 31NetworkManager 32e2fsprogs 33rootfiles 34docker 35openssh-server 36 37#By zhang ranrui 38unzip 39net-tools 40binutils 41wget 42bash-completion 43bc 44dmidecode 45dmraid 46dmraid-events 47lvm2 48lvm2-libs 49kpartx 50mdadm 51parted 52xfsdump 53xfsprogs 54gdisk 55bzip2 56extundelete 57libHX 58libHX-devel 59autoconf 60gcc 61gcc-c++ 62make 63screen 64telnet 65 66%end 67 68%post 69 70systemctl enable docker 71 72# By Zhang Ranrui, Add your custom script 73#wget http://www.rendoumi.com/soft/other/xfs_irecover -O /usr/local/bin/xfs_irecover 74#chmod 755 /usr/local/bin/xfs_irecover 75 76echo \u0026#34;Banner /etc/issue\u0026#34; \u0026gt;\u0026gt; /etc/ssh/sshd_config 77 78sed -i \u0026#34;s/After=network\\.target/After=network-online\\.target\\nWants=network-online\\.target/g\u0026#34; /usr/lib/systemd/system/rc-local.service 79 80chmod 755 /etc/systemd/system/rc.local.service.d 81chmod 644 /etc/systemd/system/rc.local.service.d/local.conf 82 83chmod 755 /etc/rc.d/rc.local 84systemctl enable rc-local 85systemctl start rc-local 86 87# FIXME: it\u0026#39;d be better to get this installed from a package 88cat \u0026gt; /etc/rc.d/init.d/livesys \u0026lt;\u0026lt; EOF 89#!/bin/bash 90# 91# live: Init script for live image 92# 93# chkconfig: 345 00 99 94# description: Init script for live image. 95### BEGIN INIT INFO 96# X-Start-Before: display-manager 97### END INIT INFO 98 99. /etc/init.d/functions 100 101if ! strstr \u0026#34;\\`cat /proc/cmdline\\`\u0026#34; rd.live.image || [ \u0026#34;\\$1\u0026#34; != \u0026#34;start\u0026#34; ]; then 102exit 0 103fi 104 105if [ -e /.liveimg-configured ] ; then 106 configdone=1 107fi 108 109exists() { 110 which \\$1 \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 || return 111 \\$* 112} 113 114# Make sure we don\u0026#39;t mangle the hardware clock on shutdown 115ln -sf /dev/null /etc/systemd/system/hwclock-save.service 116 117livedir=\u0026#34;LiveOS\u0026#34; 118for arg in \\`cat /proc/cmdline\\` ; do 119 if [ \u0026#34;\\${arg##rd.live.dir=}\u0026#34; != \u0026#34;\\${arg}\u0026#34; ]; then 120livedir=\\${arg##rd.live.dir=} 121return 122fi 123if [ \u0026#34;\\${arg##live_dir=}\u0026#34; != \u0026#34;\\${arg}\u0026#34; ]; then 124livedir=\\${arg##live_dir=} 125return 126fi 127done 128 129# enable swaps unless requested otherwise 130swaps=\\`blkid -t TYPE=swap -o device\\` 131if ! strstr \u0026#34;\\`cat /proc/cmdline\\`\u0026#34; noswap \u0026amp;\u0026amp; [ -n \u0026#34;\\$swaps\u0026#34; ] ; then 132 for s in \\$swaps ; do 133 action \u0026#34;Enabling swap partition \\$s\u0026#34; swapon \\$s 134 done 135fi 136if ! strstr \u0026#34;\\`cat /proc/cmdline\\`\u0026#34; noswap \u0026amp;\u0026amp; [ -f /run/initramfs/live/\\${livedir}/swap.img ] ; then 137 action \u0026#34;Enabling swap file\u0026#34; swapon /run/initramfs/live/\\${livedir}/swap.img 138fi 139 140mountDockerDisk() { 141 # support label/uuid 142 if [ \u0026#34;\\${dockerdev##LABEL=}\u0026#34; != \u0026#34;\\${dockerdev}\u0026#34; -o \u0026#34;\\${dockerdev##UUID=}\u0026#34; != \u0026#34;\\${dockerdev}\u0026#34; ]; then 143dockerdev=\\`/sbin/blkid -o device -t \u0026#34;\\$dockerdev\u0026#34;\\` 144fi 145 146 # if we\u0026#39;re given a file rather than a blockdev, loopback it 147 if [ \u0026#34;\\${dockerdev##mtd}\u0026#34; != \u0026#34;\\${dockerdev}\u0026#34; ]; then 148# mtd devs don\u0026#39;t have a block device but get magic-mounted with -t jffs2 149mountopts=\u0026#34;-t jffs2\u0026#34; 150elif [ ! -b \u0026#34;\\$dockerdev\u0026#34; ]; then 151loopdev=\\`losetup -f\\` 152if [ \u0026#34;\\${dockerdev##/run/initramfs/live}\u0026#34; != \u0026#34;\\${dockerdev}\u0026#34; ]; then 153action \u0026#34;Remounting live store r/w\u0026#34; mount -o remount,rw /run/initramfs/live 154fi 155losetup \\$loopdev \\$dockerdev 156dockerdev=\\$loopdev 157fi 158 159 # if it\u0026#39;s encrypted, we need to unlock it 160 if [ \u0026#34;\\$(/sbin/blkid -s TYPE -o value \\$dockerdev 2\u0026gt;/dev/null)\u0026#34; = \u0026#34;crypto_LUKS\u0026#34; ]; then 161echo 162echo \u0026#34;Setting up encrypted Docker device\u0026#34; 163plymouth ask-for-password --command=\u0026#34;cryptsetup luksOpen \\$dockerdev EncDocker\u0026#34; 164dockerdev=/dev/mapper/EncDocker 165fi 166 167 # and finally do the mount 168 mount \\$mountopts \\$dockerdev /var/lib/docker 169 # if we have /home under what\u0026#39;s passed for persistent home, then 170 # we should make that the real /home. useful for mtd device on olpc 171 if [ -d /var/lib/docker/docker ]; then mount --bind /var/lib/docker/docker /var/lib/docker ; fi 172 [ -x /sbin/restorecon ] \u0026amp;\u0026amp; /sbin/restorecon /var/lib/docker 173} 174 175findDockerDisk() { 176 for arg in \\`cat /proc/cmdline\\` ; do 177 if [ \u0026#34;\\${arg##dockerdisk=}\u0026#34; != \u0026#34;\\${arg}\u0026#34; ]; then 178dockerdev=\\${arg##dockerdisk=} 179return 180fi 181done 182} 183 184if strstr \u0026#34;\\`cat /proc/cmdline\\`\u0026#34; dockerdisk= ; then 185findDockerDisk 186elif [ -e /run/initramfs/live/\\${livedir}/docker.img ]; then 187 dockerdev=/run/initramfs/live/\\${livedir}/docker.img 188fi 189 190# if we have a persistent /home, then we want to go ahead and mount it 191if ! strstr \u0026#34;\\`cat /proc/cmdline\\`\u0026#34; nodockerdisk \u0026amp;\u0026amp; [ -n \u0026#34;\\$dockerdev\u0026#34; ] ; then 192 action \u0026#34;Mounting persistent /var/lib/docker\u0026#34; mountDockerDisk 193fi 194 195# make it so that we don\u0026#39;t do writing to the overlay for things which 196# are just tmpdirs/caches 197mount -t tmpfs -o mode=0755 varcacheyum /var/cache/yum 198mount -t tmpfs vartmp /var/tmp 199[ -x /sbin/restorecon ] \u0026amp;\u0026amp; /sbin/restorecon /var/cache/yum /var/tmp \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 200 201if [ -n \u0026#34;\\$configdone\u0026#34; ]; then 202 exit 0 203fi 204 205# add fedora user with no passwd 206action \u0026#34;Adding live user\u0026#34; useradd \\$USERADDARGS -c \u0026#34;Live System User\u0026#34; liveuser 207passwd -d liveuser \u0026gt; /dev/null 208usermod -aG wheel,docker liveuser \u0026gt; /dev/null 209 210# Remove root password lock 211passwd -d root \u0026gt; /dev/null 212(echo Kalaisadog2021; echo Kalaisadog2021)|passwd root --stdin 213 214# turn off firstboot for livecd boots 215systemctl --no-reload disable firstboot-text.service 2\u0026gt; /dev/null || : 216systemctl --no-reload disable firstboot-graphical.service 2\u0026gt; /dev/null || : 217systemctl stop firstboot-text.service 2\u0026gt; /dev/null || : 218systemctl stop firstboot-graphical.service 2\u0026gt; /dev/null || : 219 220# don\u0026#39;t use prelink on a running live image 221sed -i \u0026#39;s/PRELINKING=yes/PRELINKING=no/\u0026#39; /etc/sysconfig/prelink \u0026amp;\u0026gt;/dev/null || : 222 223# turn off mdmonitor by default 224systemctl --no-reload disable mdmonitor.service 2\u0026gt; /dev/null || : 225systemctl --no-reload disable mdmonitor-takeover.service 2\u0026gt; /dev/null || : 226systemctl stop mdmonitor.service 2\u0026gt; /dev/null || : 227systemctl stop mdmonitor-takeover.service 2\u0026gt; /dev/null || : 228 229# don\u0026#39;t enable the gnome-settings-daemon packagekit plugin 230gsettings set org.gnome.settings-daemon.plugins.updates active \u0026#39;false\u0026#39; || : 231 232# don\u0026#39;t start cron/at as they tend to spawn things which are 233# disk intensive that are painful on a live image 234systemctl --no-reload disable crond.service 2\u0026gt; /dev/null || : 235systemctl --no-reload disable atd.service 2\u0026gt; /dev/null || : 236systemctl stop crond.service 2\u0026gt; /dev/null || : 237systemctl stop atd.service 2\u0026gt; /dev/null || : 238 239# Mark things as configured 240touch /.liveimg-configured 241 242# add static hostname to work around xauth bug 243# https://bugzilla.redhat.com/show_bug.cgi?id=679486 244echo \u0026#34;localhost\u0026#34; \u0026gt; /etc/hostname 245 246# Fixing the lang install issue when other lang than English is selected . See http://bugs.centos.org/view.php?id=7217 247/usr/bin/cp /usr/lib/python2.7/site-packages/blivet/size.py /usr/lib/python2.7/site-packages/blivet/size.py.orig 248/usr/bin/sed -i \u0026#34;s#return self.humanReadable()#return self.humanReadable().encode(\u0026#39;utf-8\u0026#39;)#g\u0026#34; /usr/lib/python2.7/site-packages/blivet/size.py 249 250EOF 251 252# bah, hal starts way too late 253cat \u0026gt; /etc/rc.d/init.d/livesys-late \u0026lt;\u0026lt; EOF 254#!/bin/bash 255# 256# live: Late init script for live image 257# 258# chkconfig: 345 99 01 259# description: Late init script for live image. 260 261. /etc/init.d/functions 262 263if ! strstr \u0026#34;\\`cat /proc/cmdline\\`\u0026#34; rd.live.image || [ \u0026#34;\\$1\u0026#34; != \u0026#34;start\u0026#34; ] || [ -e /.liveimg-late-configured ] ; then 264exit 0 265fi 266 267exists() { 268 which \\$1 \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 || return 269 \\$* 270} 271 272touch /.liveimg-late-configured 273 274# read some variables out of /proc/cmdline 275for o in \\`cat /proc/cmdline\\` ; do 276 case \\$o in 277 ks=*) 278ks=\u0026#34;--kickstart=\\${o#ks=}\u0026#34; 279;; 280xdriver=*) 281xdriver=\u0026#34;\\${o#xdriver=}\u0026#34; 282;; 283esac 284done 285 286# if liveinst or textinst is given, start anaconda 287if strstr \u0026#34;\\`cat /proc/cmdline\\`\u0026#34; liveinst ; then 288 plymouth --quit 289 /usr/sbin/liveinst \\$ks 290fi 291if strstr \u0026#34;\\`cat /proc/cmdline\\`\u0026#34; textinst ; then 292 plymouth --quit 293 /usr/sbin/liveinst --text \\$ks 294fi 295 296# configure X, allowing user to override xdriver 297if [ -n \u0026#34;\\$xdriver\u0026#34; ]; then 298 cat \u0026gt; /etc/X11/xorg.conf.d/00-xdriver.conf \u0026lt;\u0026lt;FOE 299Section \u0026#34;Device\u0026#34; 300 Identifier \u0026#34;Videocard0\u0026#34; 301 Driver \u0026#34;\\$xdriver\u0026#34; 302EndSection 303FOE 304fi 305 306EOF 307 308chmod 755 /etc/rc.d/init.d/livesys 309/sbin/restorecon /etc/rc.d/init.d/livesys 310/sbin/chkconfig --add livesys 311 312chmod 755 /etc/rc.d/init.d/livesys-late 313/sbin/restorecon /etc/rc.d/init.d/livesys-late 314/sbin/chkconfig --add livesys-late 315 316# enable tmpfs for /tmp 317systemctl enable tmp.mount 318 319 320# enable docker 321systemctl enable docker.service 322 323# work around for poor key import UI in PackageKit 324rm -f /var/lib/rpm/__db* 325releasever=$(rpm -q --qf \u0026#39;%{version}\\n\u0026#39; --whatprovides system-release) 326basearch=$(uname -i) 327rpm --import /etc/pki/rpm-gpg/RPM-GPG-KEY-fedora-$releasever-$basearch 328echo \u0026#34;Packages within this LiveCD\u0026#34; 329rpm -qa 330# Note that running rpm recreates the rpm db files which aren\u0026#39;t needed or wanted 331rm -f /var/lib/rpm/__db* 332 333# go ahead and pre-make the man -k cache (#455968) 334/usr/bin/mandb 335 336# save a little bit of space at least... 337rm -f /boot/initramfs* 338# make sure there aren\u0026#39;t core files lying around 339rm -f /core* 340 341# convince readahead not to collect 342# FIXME: for systemd 343 344cat \u0026gt;\u0026gt; /etc/rc.d/init.d/livesys \u0026lt;\u0026lt; EOF 345 346 347# disable updates plugin 348cat \u0026gt;\u0026gt; /usr/share/glib-2.0/schemas/org.gnome.settings-daemon.plugins.updates.gschema.override \u0026lt;\u0026lt; FOE 349[org.gnome.settings-daemon.plugins.updates] 350active=false 351FOE 352 353# Show the system-config-keyboard tool on the desktop 354mkdir /home/liveuser/Desktop -p \u0026gt;/dev/null 355cat /usr/share/applications/system-config-keyboard.desktop | sed \u0026#39;/NotShowIn/d\u0026#39; |sed \u0026#39;s/Terminal=false/Terminal=true/\u0026#39; \u0026gt; /home/liveuser/Desktop/system-config-keyboard.desktop 356cat /usr/share/applications/liveinst.desktop | sed \u0026#39;/NoDisplay/d\u0026#39; \u0026gt; /home/liveuser/Desktop/liveinst.desktop  357chmod +x /home/liveuser/Desktop/*.desktop 358chown -R liveuser:liveuser /home/liveuser 359 360# Liveuser face 361if [ -e /usr/share/icons/hicolor/96x96/apps/fedora-logo-icon.png ] ; then 362 cp /usr/share/icons/hicolor/96x96/apps/fedora-logo-icon.png /home/liveuser/.face 363 chown liveuser:liveuser /home/liveuser/.face 364fi 365 366# make the installer show up 367if [ -f /usr/share/applications/liveinst.desktop ]; then 368 # Show harddisk install in shell dash 369 sed -i -e \u0026#39;s/NoDisplay=true/NoDisplay=false/\u0026#39; /usr/share/applications/liveinst.desktop 370# need to move it to anaconda.desktop to make shell happy 371#cp /usr/share/applications/liveinst.desktop /usr/share/applications/anaconda.desktop 372fi 373 cat \u0026gt;\u0026gt; /usr/share/glib-2.0/schemas/org.gnome.shell.gschema.override \u0026lt;\u0026lt; FOE 374[org.gnome.shell] 375favorite-apps=[\u0026#39;liveinst.desktop\u0026#39;,\u0026#39;firefox.desktop\u0026#39;, \u0026#39;evolution.desktop\u0026#39;, \u0026#39;empathy.desktop\u0026#39;, \u0026#39;rhythmbox.desktop\u0026#39;, \u0026#39;shotwell.desktop\u0026#39;, \u0026#39;libreoffice-writer.desktop\u0026#39;, \u0026#39;nautilus.desktop\u0026#39;, \u0026#39;gnome-documents.desktop\u0026#39;, \u0026#39;anaconda.desktop\u0026#39;] 376FOE 377 378 379# set up auto-login 380cat \u0026gt; /etc/gdm/custom.conf \u0026lt;\u0026lt; FOE 381[daemon] 382AutomaticLoginEnable=True 383AutomaticLogin=liveuser 384FOE 385 386# Turn off PackageKit-command-not-found while uninstalled 387if [ -f /etc/PackageKit/CommandNotFound.conf ]; then 388 sed -i -e \u0026#39;s/^SoftwareSourceSearch=true/SoftwareSourceSearch=false/\u0026#39; /etc/PackageKit/CommandNotFound.conf 389fi 390 391# make sure to set the right permissions and selinux contexts 392chown -R liveuser:liveuser /home/liveuser/ 393restorecon -R /home/liveuser/ 394 395# Fixing default locale to us 396localectl set-keymap us 397localectl set-x11-keymap us 398EOF 399 400 401# rebuild schema cache with any overrides we installed 402glib-compile-schemas /usr/share/glib-2.0/schemas 403 404 405%end 注意，上面注释了两个地方，都可以添加软件或者运行脚本\n三、build出iso文件 1livecd-creator --verbose -c centos7-live-docker.ks --cache=cache -f centos7-live-docker 然后就会得到centos7-live-docker.iso的文件，注意在build过程中的报错信息，多数是无法下载包导致的。\n直接加载ISO文件启动或者刻录到USB上启动，就可以进入这个自制的Live系统了\n千万注意，启动一定要选Bios legacy，不要用Uefi。\n相关一些有用的链接：\n https://github.com/minishift/minishift-centos-iso  https://github.com/livecd-tools/livecd-tools  https://blog.csdn.net/sharpbladepan/article/details/107423468   ","date":"2021-10-10","img":"","permalink":"https://zhangrr.github.io/posts/20211010-live_cd/","series":null,"tags":null,"title":"CentOS 7 Live-CD 的制作"},{"categories":[],"content":"This article offers a sample of basic Markdown syntax that can be used in Hugo content files, also it shows whether basic HTML elements are decorated with CSS in a Hugo theme.\nHeadings The following HTML \u0026lt;h1\u0026gt;—\u0026lt;h6\u0026gt; elements represent six levels of section headings. \u0026lt;h1\u0026gt; is the highest section level while \u0026lt;h6\u0026gt; is the lowest.\nH1 H2 H3 H4 H5 H6 Paragraph Xerum, quo qui aut unt expliquam qui dolut labo. Aque venitatiusda cum, voluptionse latur sitiae dolessi aut parist aut dollo enim qui voluptate ma dolestendit peritin re plis aut quas inctum laceat est volestemque commosa as cus endigna tectur, offic to cor sequas etum rerum idem sintibus eiur? Quianimin porecus evelectur, cum que nis nust voloribus ratem aut omnimi, sitatur? Quiatem. Nam, omnis sum am facea corem alique molestrunt et eos evelece arcillit ut aut eos eos nus, sin conecerem erum fuga. Ri oditatquam, ad quibus unda veliamenimin cusam et facea ipsamus es exerum sitate dolores editium rerore eost, temped molorro ratiae volorro te reribus dolorer sperchicium faceata tiustia prat.\nItatur? Quiatae cullecum rem ent aut odis in re eossequodi nonsequ idebis ne sapicia is sinveli squiatum, core et que aut hariosam ex eat.\nBlockquotes The blockquote element represents content that is quoted from another source, optionally with a citation which must be within a footer or cite element, and optionally with in-line changes such as annotations and abbreviations.\nBlockquote without attribution  Tiam, ad mint andaepu dandae nostion secatur sequo quae. Note that you can use Markdown syntax within a blockquote.\n Blockquote with attribution  Don\u0026rsquo;t communicate by sharing memory, share memory by communicating.— Rob Pike1 Tables Tables aren\u0026rsquo;t part of the core Markdown spec, but Hugo supports supports them out-of-the-box.\n   Name Age     Bob 27   Alice 23    Inline Markdown within tables    Italics Bold Code     italics bold code    Code Blocks Code block with backticks 1\u0026lt;!doctype html\u0026gt; 2\u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; 3\u0026lt;head\u0026gt; 4 \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; 5 \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; 6\u0026lt;/head\u0026gt; 7\u0026lt;body\u0026gt; 8 \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; 9\u0026lt;/body\u0026gt; 10\u0026lt;/html\u0026gt; Code block with Hugo\u0026rsquo;s internal highlight shortcode 1\u0026lt;!doctype html\u0026gt; 2\u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; 3\u0026lt;head\u0026gt; 4 \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; 5 \u0026lt;title\u0026gt;Another Example HTML5 Document\u0026lt;/title\u0026gt; 6\u0026lt;/head\u0026gt; 7\u0026lt;body\u0026gt; 8 \u0026lt;p\u0026gt;A looooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooong text\u0026lt;/p\u0026gt; 9\u0026lt;/body\u0026gt; 10\u0026lt;/html\u0026gt; List Types Ordered List  First item Second item Third item  Unordered List  List item Another item And another item  Nested List  Fruit  Apple Orange Banana   Dairy  Milk Cheese    TODO List  Done WIP  Other Elements — abbr, sub, sup, kbd, mark GIFis a bitmap image format.\nH2O\nXn+ Yn= ZnPress CTRL+ALT+Deleteto end the session.\nMost salamandersare nocturnal, and hunt for insects, worms, and other small creatures.\n  The above quote is excerpted from Rob Pike\u0026rsquo;s talk during Gopherfest, November 18, 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n  ","date":"2020-11-09","img":"https://zhangrr.github.io/images/markdown.png","permalink":"https://zhangrr.github.io/posts/markdown-syntax/","series":["Manual"],"tags":["Markdown","CSS","HTML"],"title":"Markdown Syntax Guide"}]