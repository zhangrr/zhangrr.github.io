[{"categories":null,"content":"想自己画图来监控交换机的流量，第一步就是通过snmpwalk来得到交换机的进出流量：\n我们用得是Cisco 3750的24口交换机，给个OID表：\n先看一下交换机的端口情况：\n1snmpwalk -v 2c -c xxxxxxxx 172.16.1.1 .1.3.6.1.2.1.2.2.1.2 得到一堆信息，问网络工程师得到1/0/5和2/0/5这两个端口是双上连线路：\n1IF-MIB::ifDescr.10105 = STRING: GigabitEthernet1/0/5 2IF-MIB::ifDescr.10605 = STRING: GigabitEthernet2/0/5 记住两个端口的描述符，10105和10605.\n继续，我们看上表，.1.3.6.1.2.1.2.2.1.10(接口收到的字节数)，我们要得到某个具体端口收到的字节数，就必须加上端口的描述符了。所以要得到1/0/5的收到字节数，就是.1.3.6.1.2.1.2.2.1.10.10105\n1#snmpwalk -v 2c -c xxxxxxxx 172.16.1.1 .1.3.6.1.2.1.2.2.1.10.10105 2IF-MIB::ifInOctets.10105 = Counter32: 2198534228 ok，拿到字节数了，然后单位是bytes，换算成bit(*8)，然后除若干1024，得到单位m，g, t等，入库，然后调hichart画图即可。\n然而，还有一个问题，看清上面的单位，Counter32，呵呵，所以这里是不对的。现如今的网络多是千兆网卡了，所以counter32是不对的, 100Mbps以上必须使用Counter64的单位，给个对照表：\n高速网卡(100兆以上）:\nifHCInOctets: 1.3.6.1.2.1.31.1.1.1.6 (64-bit Octets in counter) ifHCOutOctets: 1.3.6.1.2.1.31.1.1.1.10 (64-bit Octets out counter) ifHCInUcastPkts: 1.3.6.1.2.1.31.1.1.1.7 (64-bit Packets in counter) ifHCOutUcastPkts: 1.3.6.1.2.1.31.1.1.1.11 (64-bit Packets out counter) ifHighSpeed: 1.3.6.1.2.1.31.1.1.1.15 (An estimate of the interface\u0026rsquo;s current bandwidth in units of 1Mbps) 低速网卡：\nifInOctets: 1.3.6.1.2.1.2.2.1.10 (32-bit Octets in counter) ifOutOctets: 1.3.6.1.2.1.2.2.1.16 (32-bit Octets out counter) ifInUcastPkts: 1.3.6.1.2.1.2.2.1.11 (32-bit Packets in counter) ifOutUcastPkts: 1.3.6.1.2.1.2.2.1.17 (32-bit Packets out counter) ifSpeed: 1.3.6.1.2.1.2.2.1.5 (Currently negotiated speed of the interface - Max: 4.294 Gbps) 详细解释下：ifInOctets是32-bit无符整数，2的32次方是4,294,967,296。去掉0，所以范围是0-4,294,967,295，大概是34g的bit. 如果按网速74-92Mbps来算, 大概6分钟就会填满，92Mbps * 60seconds *6minutes = 33gigabits.所以填满是很快的，如果你正好在这期间读取数据，很有可能会读错。\n所以命令应该是：\n1snmpwalk -v 2c -c xxxxxxxx 172.16.1.1 1.3.6.1.2.1.31.1.1.1.6.10105 最后附上计算流量的公式：\n1(ifInOctetsCurrent - ifInOctetsPrevious) * 8 / pollingSeconds ","date":"2024-01-08","img":"","permalink":"https://bajie.dev/posts/20240108-snmp_network/","series":null,"tags":null,"title":"SNMP OID来监控网络设备流量"},{"categories":null,"content":"公司为了跟建设银行建立专线，首先用电话线拨号进行测试。 买回来一个外置的Modem，接在服务器的com1口上，先用电话测试一下，OK没问题，那就继续配modem拨号\nLinux下配置Modem拨号有两种方式，传统的pppd方式和简单的wvdial方式。\n一、wvdial配置 wvdial的配置方法超级简单， 执行命令：wvdialconf /etc/wvdial.conf 它会自动测出系统的Modem，稍微修改一下，加几个参数：\n1vi /etc/wvdial.conf 2 3[Dialer Defaults] 4Modem = /dev/ttyS0 5Baud = 115200 6Init1 = ATZ 7Init2 = ATQ0 V1 E1 S0=0 \u0026amp;C1 \u0026amp;D2 +FCLASS=0 8ISDN = 0 9Auto Reconnect = on 10Modem Type = Analog Modem 11Phone =0,28929191 12Username = ttt 13Password = qqq 这就弄好了，执行wvdail \u0026amp;就可以拨号了，结束也很简单，kill -9 杀掉wvdial和ppd进程即可。\n注意：wvdial 拨通后系统多了一块网卡ppp0，路由信息都未修改，为了能到达建行的服务器，需要编辑/etc/ppp/ip-up文件，加一句：\n1route add -host 12.0.98.150 gw 12.0.98.236 然后重拨ping一下，ping 12.0.98.150，能ping通就说明ok了。\n二、pppd配置 wvdial隐藏了很多信息，我们下面用pppd来看看真实的拨号过程吧：\n其实modem拨号认证的方式有两种，一种是显示明文的login:(username:)方式，另一种是显示乱码的pap(chap)方式。\n①首先：\n1vi /etc/ppp/options 2lock 3crtscts 4defaultroute 5noauth ②清理一下老的连接：\n1killall pppd 2rm /var/lock/LCK..ttyS0 ③找出认证的方式：\n1/usr/sbin/pppd /dev/ttyS0 115200 debug connect 2 \u0026#34;/usr/sbin/chat -v \u0026#39;\u0026#39; \u0026#39;AT\u0026amp;F0\u0026#39; OK 3ATD0,28929191 CONNECT \u0026#39;dc\u0026#39; \u0026#34; less /var/log/messages\n如果看见什么login username之流的那就是明文认证，如果类似下面的，看见了，那就是pap(chap)方式。\n1pppd 2.4.2 started by root, uid 0 2Removed stale lock on ttyS0 (pid 3561) 3send (AT^M) 4expect (OK) 5AT^M^M 6OK 7-- got it 8send (ATD0,28929191^M) 9expect (CONNECT) 10^M 11ATD0,28929191^M^M 12CONNECT 13-- got it 14send (d) 15Serial connection established. 16using channel 2 17Using interface ppp0 18Connect: ppp0 \u0026lt;--\u0026gt; /dev/ttyS0 19sent [LCP ConfReq id=0x1 \u0026lt;mru 552\u0026gt; \u0026lt;asyncmap 0x0\u0026gt; \u0026lt;magic 0x3947d7a8\u0026gt; \u0026lt;pcomp\u0026gt; \u0026lt;accomp\u0026gt;] 20rcvd [LCP ConfReq id=0x1 \u0026lt;mru 1500\u0026gt; \u0026lt;asyncmap 0x0\u0026gt; \u0026lt;auth chap MD5\u0026gt; \u0026lt;magic 0xebe7666\u0026gt;] 21sent [LCP ConfAck id=0x1 \u0026lt;mru 1500\u0026gt; \u0026lt;asyncmap 0x0\u0026gt; \u0026lt;auth chap MD5\u0026gt; \u0026lt;magic 0xebe7666\u0026gt;] 22sent [LCP ConfReq id=0x1 \u0026lt;mru 552\u0026gt; \u0026lt;asyncmap 0x0\u0026gt; \u0026lt;magic 0x3947d7a8\u0026gt; \u0026lt;pcomp\u0026gt; \u0026lt;accomp\u0026gt;] 23sent [LCP ConfReq id=0x1 \u0026lt;mru 552\u0026gt; \u0026lt;asyncmap 0x0\u0026gt; \u0026lt;magic 0x3947d7a8\u0026gt; \u0026lt;pcomp\u0026gt; \u0026lt;accomp\u0026gt;] 24rcvd [LCP ConfReq id=0x2 \u0026lt;mru 1500\u0026gt; \u0026lt;asyncmap 0x0\u0026gt; \u0026lt;auth chap MD5\u0026gt; \u0026lt;magic 0x125e3a73\u0026gt;] 25sent [LCP ConfAck id=0x2 \u0026lt;mru 1500\u0026gt; \u0026lt;asyncmap 0x0\u0026gt; \u0026lt;auth chap MD5\u0026gt; \u0026lt;magic 0x125e3a73\u0026gt;] 如果是明文认证：\n1/usr/sbin/pppd /dev/ttyS0 115200 debug connect 2\u0026#34;/usr/sbin/chat -v \u0026#39;\u0026#39; ATDT0,28929191 CONNECT 3\u0026#39;\u0026#39; ogin: ttt assword: qqq\u0026#34; 如果是pap(chap)认证：\n1/usr/sbin/pppd /dev/ttyS0 115200 debug 2user ttt connect \u0026#34;/usr/sbin/chat -v \u0026#39;\u0026#39; ATDT0,28929191 CONNECT \u0026#39;dc\u0026#39; \u0026#34; 看出区别了吧，pap(chap)跟明文的区别就在于多了一个 user ttt 的参数。 这就是pppd拨号的详细过程了，断掉拨号连接就用killall pppd即可。 大家可以把这么长的一行命令写到sh里，就好看了。\n","date":"2024-01-08","img":"","permalink":"https://bajie.dev/posts/20240108-modem_dial/","series":null,"tags":null,"title":"Linux下配置Modem拨号"},{"categories":null,"content":"我们的容器采用的是lxc类型，一是为了固定ip，二是避开iptable类型转发的方式。因为我们是强网络管理环境，网络工程师对网络的规划和管控非常强。\n这种方式下启动lxc容器倒是很好办，直接编辑/etc/rc.d/rc.local\n1docker start jko2o-16-13-49 2docker start vis-16-13-51 3docker start vis-16-13-50 4docker start vis-16-13-48 5docker start vis-16-13-47 就可以开机自动启动容器了。\n容器内我们是采用supervisord主进程来保持容器不自动销毁的，那么其他命令如果都加入supervisord，那会非常麻烦。\n简单化，用lxc-execute来执行，比较麻烦的就是需要查出来lxc容器的名字，用以下命令查出jko2o-16-13-49的全名：\n1docker inspect -f \u0026#39;{{.Id}}\u0026#39; jko2o-16-13-49 2ebe2e6a1249f6f22334014b0072186dfaee52173f3df06cd826f9e64e7d4c51f 然后编辑/etc/rc.d/rc.local，用lxc-execute执行命令就可以了：\n1#47 2lxc-execute -n e223d81ea73551460b82e1977b92ef07e24437cd1f4494470feafd4ff455104b -- service mysqld start 3lxc-execute -n e223d81ea73551460b82e1977b92ef07e24437cd1f4494470feafd4ff455104b -- service httpd start 4 5#48 6lxc-execute -n 1075971953b9c79494509bf89131e03bbd5ab0b9a388217bd9b6de7a553016a5 -- //www/wdlinux/init.d/mysqld start 7lxc-execute -n 1075971953b9c79494509bf89131e03bbd5ab0b9a388217bd9b6de7a553016a5 -- //www/wdlinux/init.d/httpd start 8lxc-execute -n 1075971953b9c79494509bf89131e03bbd5ab0b9a388217bd9b6de7a553016a5 -- //www/wdlinux/init.d/wdapache start 9lxc-execute -n 1075971953b9c79494509bf89131e03bbd5ab0b9a388217bd9b6de7a553016a5 -- //www/wdlinux/init.d/nginxd start 10lxc-execute -n 1075971953b9c79494509bf89131e03bbd5ab0b9a388217bd9b6de7a553016a5 -- //www/wdlinux/init.d/pureftpd start 11 12#49 13lxc-execute -n ebe2e6a1249f6f22334014b0072186dfaee52173f3df06cd826f9e64e7d4c51f -- /export/servers/nginx178/sbin/nginx 14 15#50 16lxc-execute -n 5b7e8cd3130d31bae5b089202c7e2092b9daca5f19cc055ce4ee50cf8b789504 -- /export/servers/tomcat/tomcat.sh ","date":"2024-01-08","img":"","permalink":"https://bajie.dev/posts/20240108-docker_lxc_startup/","series":null,"tags":null,"title":"Docker Lxc类型容器自启动以及自动执行命令"},{"categories":null,"content":"openvpn server can\u0026rsquo;t use udp port 这个问题非常古怪，我们有两个机房。分别在两边假设了openvpn服务器，一个机房的ovpn就完全正常，而另一个机房的openvpn如果是tcp就没问题，如果是udp就报错，无法连接。把正常机房的openvpn完全复制一份拿过去，也是一样报错，太郁闷了！\n错误提示是：\n1TLS Error: TLS key negotiation failed to occur within 60 seconds (check your network connectivity) 非常古怪啊： 在内网用nc来试验udp是否正常：\n1nc -ul 1194 在内网随便找一台机器测试：\n1nc -u xxxx 1194 然后两边随便输入字符测试，没问题。\n然后从公网测试，就出问题了。nc连接上以后，只能发送一次数据，然后管道就断裂了，无法再传数据。\n1nc -u 公网ip 1194 2aaa 3nc: Write error: Connection refused 百思不得其解啊，试了若干次，才发现问题的症结所在。\n错误的那台服务器，有两个网络地址： 172.16.8.4和172.16.8.1 其中8.4的地址是缺省地址，8.1的地址是用ifconfig手动后添加的。 而前端防火墙做得公网映射是映射到了8.1的1194端口\ntcp由于双方是要校验来往src地址的，所以没问题，到了udp，只管发不管收，所以udp包就从8.4的缺省地址发出去，导致双方不一致，无法正常接收了。\n解决方法也很简单，在openvpn服务器端的配置文件中强行指定local端的ip即可：\n1local 172.16.8.1 ","date":"2024-01-08","img":"","permalink":"https://bajie.dev/posts/20240108-opnevpn_udp_port/","series":null,"tags":null,"title":"Openvpn服务器端无法开通udp端口的故障排除"},{"categories":null,"content":"Cisco ASA5520 VPN线路的监控和重启 公司从事第三方支付工作，跟很多银行都有合作关系，拉了很多专线直通银行，双方建立VPN，两端都是Cisco的设备，但是，这些线路有时候会莫名其妙的断掉，关键是程序不知道啊，老是重连，一直等到客服反映客户投诉，查一圈程序后才知道。这在生产环境上可是行不通的，找来了所谓Cisco高手，也搞不明白为什么老断，没办法，于是写了两个监控脚本，使用Ping检测VPN对端的状况，一旦Ping不通，就用脚本登陆防火墙，自动重启VPN。\n在安装之前，先安装一个能从命令行发送邮件的软件Email来发送报警邮件，否则每台机器都起sendmail，没什么必要:\n1https://github.com/muquit/mailsend-go 2 3 4#发送实例 5mailsend-go -smtp smtp.126.com -port 25 \\ 6auth \\ 7 -user xxx@126.com -pass xxx \\ 8 -from xxx@126.com -to \u0026#34;xxx@126.com\u0026#34; \\ 9-sub \u0026#34;Test\u0026#34; \\ 10body -msg \u0026#39;hello world\u0026#39; 11 12126邮箱这里的密码用的是授权码 授权密码，不是邮箱密码 说明一下场景：\n210.210.210.3是对端Cisco vpn设备的公网IP 192.168.101.99是建立了VPN后，对端服务器的私网IP地址； 192.168.1.1是己方Cisco ASA5520的私网地址。 1#!/bin/sh 2 3while [ \u0026#34;1\u0026#34; -eq \u0026#34;1\u0026#34; ] 4do 5 live=`ping -c4 \u0026#34;192.168.101.99\u0026#34;|wc -l` 6 if [ $live -eq 5 ] ; then 7 /usr/local/bin/mailsend-go -debug -to \u0026#34;ranrui.zhang@rendoumi.com\u0026#34; -from monit@ddky.com -ssl -port 465 -smtp smtp.qiye.aliyun.com \\ 8 auth -user \u0026#34;monit@ddky.com\u0026#34; -pass \u0026#34;xxxxxxxx\u0026#34; \\ 9 -sub \u0026#34;vpn断了\u0026#34; body -msg \u0026#34;`date +%Y`年`date +%m`月`date +%d`日 vpn断了！！！\u0026#34; \\ 10 -cs \u0026#34;utf-8\u0026#34; 11 /usr/local/bin/revpn.sh 12 echo \u0026#34;`date +%Y`年`date +%m`月`date +%d`日 `date +%H`点`date +%M`分 13 线路不能到达王府井机房，重启VPN。\u0026#34; 14 sleep 60 15 fi; 16 sleep 60 17done 以下是用expect自动登录Cisco路由器重启vpn的脚本 revpn.sh\n1#!/usr/bin/expect 2 3set timeout 30 4spawn ssh pix@192.168.1.1 5expect \u0026#34;password:\u0026#34; 6send \u0026#34;xxxxxxxx\\n\u0026#34; 7expect \u0026#34;ASAtoTelecom\u0026gt;\u0026#34; 8send \u0026#34;en\\n\u0026#34; 9expect \u0026#34;Password:\u0026#34; 10send \u0026#34;xxxxxxxx\\n\u0026#34; 11expect \u0026#34;ASAtoTelecom#\u0026#34; 12send \u0026#34;clear isakmp sa\\n\u0026#34; 13expect \u0026#34;ASAtoTelecom#\u0026#34; 14send \u0026#34;clear ipsec sa peer 210.210.210.3\\n\u0026#34; 15expect \u0026#34;ASAtoTelecom#\u0026#34; 16send \u0026#34;exit\\n\u0026#34; 17expect eof 18exit 0 ","date":"2024-01-08","img":"","permalink":"https://bajie.dev/posts/20240107-vpn_monitor/","series":null,"tags":null,"title":"Cisco ASA5520 VPN线路的监控和自动重启"},{"categories":null,"content":"新的一年依始，开始定好了旅行计划，但是有个东西却依然心神不宁。\n那就是万一在国外有什么变动，如何往国内打电话呢？！\n说到这里，不得不提个东西，那就是旅顺的App，这个东西已经消失了，但是绝对值得被记住啊。\n这个东西在塞班岛旅游的时候曾经救了自己两次\n首先是它的第一代产品：\n第一代的原理似乎是把手机的信号直接通过网络转移到了国内，随身携带然后配合app使用，这点非常牛吧！国内的窜出点会随机，忽而天津，忽而别的地方，所以呼出电话搞不好是长途计费。第一次呢八戒是在塞班岛中部那个麦当劳附近，用这个接到了国内的电话，说是出大事了，有人把公司数据库的数据给改了，当时约定下午6：00再打，然后下午在台湾牛肉面的店里，又用这个往国内通了电话，打了半个多小时，领导告知了具体的情况，要是接不起来那可真就麻烦大了。\n这个是它第二代的产品：\n这个是升级版pro，就更厉害了，直接放家里，手机上下个app，有网络就能用了，不用像第一代一样随身携带了。\n这个更是救命了，话说在塞班的时候正是疫情刚发作的时候，塞班直接封岛了，然后好多航班都延期或终止了，就是用这个旅顺打国内电话改签了回国的机票，足足打了两个小时啊，到处占线等待，如果打国际长途的话费用不堪设想，另外打塞班大韩航空的电话改签两程机票也是不可想像的，所以，关键时刻是真的救命啊。\n这么好的软件却直接倒闭消失了，真是万分可惜啊。\n哎，现在没有这个了，只能自己搭一个voip电话自用了。\n刚开始是考虑RasPBX，其实自己用树莓派搭过，费事不说，主要树莓派容易死机，务必要稳定，所以干脆用FreePBX搭建在kvm服务器虚拟机上的方式，来保持绝对稳定。\n以下教程只适用于 FreePBX 16 的官方ISO，具体做法如下：\n一、安装KVM虚机： 1#下载FreePBX16的iso光盘 2wget https://downloads.freepbxdistro.org/ISO/SNG7-PBX16-64bit-2302-1.iso 3 4#创建qcow2虚机文件 5qemu-img create -f qcow2 freepbx-8-2-60/freepbx-8-2-60.qcow2 20G 6 7virt-install \\ 8--name=freepbx-8-2-60 \\ 9--vcpu=4 \\ 10--ram=8096 \\ 11--disk path=/export/kvm/freepbx-8-2-60/freepbs-8-2-60.qcow2,format=qcow2,size=20 \\ 12--cdrom=/export/kvm/SNG7-PBX16-64bit-2302-1.iso \\ 13--network bridge=br0 \\ 14--os-type=linux \\ 15--vnc --vnclisten=0.0.0.0 --vncport=5916 用vnc连接5916端口安装，设置一下root的密码安装。\n这样安装好后，系统其实是dhcp动态获得ip的，进入看了看，是Centos，我们不需要dhcp，把IP给固定死\n1vi /etc/sysconfig/network-scripts/ifcfg-eth0 2 3TYPE=\u0026#34;Ethernet\u0026#34; 4BOOTPROTO=\u0026#34;static\u0026#34; 5IPADDR=10.8.2.60 6NETMASK=255.255.255.0 7GATEWAY=10.8.2.1 8DNS1=114.114.114.114 9DEFROUTE=\u0026#34;yes\u0026#34; 10NAME=\u0026#34;eth0\u0026#34; 11DEVICE=\u0026#34;eth0\u0026#34; 12ONBOOT=\u0026#34;yes\u0026#34; SSH登录一下没问题，看到绿框就ok了\n二、配置FreePBX的分机 打开网址 http://10.8.2.60 , 登录 FreePBX Administration\n登上去后是这个样子\n上面我没有开System Firewall，这个看个人需求，绝对不要像公网开放80端口，只开5060的udp和1000-10004的UDP。\n开了公网80的话，那就是另外一个悲伤的故事了！！！\n然后建立第一个分机888：\n直接在Applications-\u0026gt;Extensions-\u0026gt;Add new Chan_SIP Extension新建一个分机，在Generaltab页里面填好第一个SIP账户的信息\nGeneral填写:\nUser Extension 分机号为888 Display Name 为 ba jie，最好自己名字，好区分 Secret 是密码，要填写个非常复杂的，绝对不能简单。 然后最底下，Submit 提交一下\n最后点击右上角的Apply Config应用刚才的配置。\n三、配置dongle无线卡 内部分机有了，我们需要配置外部线路\n去淘宝上买个dongle卡，插到服务器上\n登录服务器，lsusb 查看一下usb的地址\n得到 vendori d: 12d1 和 product id: 1436 这两个字串，编辑一个文件 usb-attach.xml 文件：\n1vi usb-attach.xml 2\u0026lt;hostdev mode=\u0026#39;subsystem\u0026#39; type=\u0026#39;usb\u0026#39;\u0026gt; 3 \u0026lt;source\u0026gt; 4 \u0026lt;vendor id=\u0026#39;0x12d1\u0026#39;/\u0026gt; 5 \u0026lt;product id=\u0026#39;0x1436\u0026#39;/\u0026gt; 6 \u0026lt;/source\u0026gt; 7\u0026lt;/hostdev\u0026gt; 然后挂接上\n1virsh attach-device freepbx-8-2-60 --file usb_huawei.xml --persistent ssh登录进入freepbx，lsusb查看一下，同样能看到就对了\n然后就是大工程了，编译驱动程序，非常的复杂\n不想编译的办法：\n1# 解压包 2unzip -x dongle.zip 3 4cp chan_dongle.so /usr/lib64/asterisk/modules 5chmod 644 /usr/lib64/asterisk/modules/chan_dongle.so 完整的编译的方法：\n1#解压包 2unzip asterisk-chan-dongle-master* 3 4#安装准备包编译 5yum install automake autotools-dev autoconf 6yum -y install tcl asterisk18-devel make automake binutils 7 8#编译安装 9cd asterisk-chan-dongle-master/ 10autoupdate 11aclocal 12autoconf 13automake -a 14./bootstrap 15./configure --with-astversion=18.19.0 16make clean 17make 18make install 19 20cd /usr/lib64/asterisk/modules 21ls -lcat|more 看到有dongle字样就ok了\n没完呢啊，还需要把配置文件都弄进去\n1#解压 2unzip -x dongle.zip 3 4#install dongles driver 5cd 6echo \u0026#39;KERNEL==\u0026#34;ttyUSB*\u0026#34;, MODE=\u0026#34;0666\u0026#34;, OWNER=\u0026#34;asterisk\u0026#34;, GROUP=\u0026#34;uucp\u0026#34;\u0026#39;\u0026gt;/etc/udev/rules.d/92-dongle.rules 7echo \u0026#39;rungroup = dialout\u0026#39;\u0026gt;\u0026gt;/etc/asterisk/asterisk.conf 8mv dongle.conf /etc/asterisk/dongle.conf 9chown asterisk.asterisk /etc/asterisk/dongle.conf 10chmod 664 /etc/asterisk/dongle.conf 11mv extensions_custom.conf /etc/asterisk/extensions_custom.conf 12chown asterisk.asterisk /etc/asterisk/extensions_custom.conf 13chmod 664 /etc/asterisk/extensions_custom.conf 下载地址：dongle.zip 然后把虚机reboot一下，进入asterisk查看\n1asterisk -rvvv 2dongle show devices 看到自己dongle设备的id，是dongle1，记下来，就ok\n四、配置FreePBX的进出线路 1、增加trunk 回到 FreePBX 的配置界面，先配一下 Trunk 线路，增加一个trunk:connectivity-\u0026gt;Trunks\n增加一个 Custom Trunk:\nGeneral填写：\nTrunk Name : china_unicom Outbound CallerID: 空 （联通手机号，最好空，后面有提示忽略，这个接打的时候会自动替你填上的） Maximum Channels: 1 (只有一个联通sim卡，就是1，2个就是2) 然后点到custom Settings，填入如下内容，注意，我的设备是dongle1，按查出来的设备填写啊。\n1dongle/dongle1/$OUTNUM$ 然后同样 Submit， 然后 Apply config. 会提示CallerID，ok忽略即可。\n2、配置拨入线路Inbound route connectivity-\u0026gt;Inbound Routes\n增加一个新的Inbound，名字就叫gsm_in，可以随便改。DID和CallerID都是ANY，意味着所有达到这个sim卡内的电话都接下来，Destination选择咱们一开始的888分机，这样呼入的电话就可以被888分机接听了\nDescription: gsm_in DID Number: ANY CallerID Number: ANY Set Destination: Extensions ​ 888 zhang ranrui 最后同样 Submit , Apply Config配置生效\n3、配置拨出线路Outbound Routes connectivity-\u0026gt;Outbound Routes\n增加一条新的拨出线路，名字叫gsm_out好了，可以随便改，Trunk选咱们建立的 china_unicom\nRoute Name : gsm_out\nTrunk Sequence for Matched Routes: china_unicom\n注意，点到拨号盘，还需要配一下 Dail Patterns\nMatch Pattern: ZX. ZX. 的意思是拨出去的号码最少有3个数字那么长，且第一位不是0，那手机11位，起始是1就满足了。很多教程教配个1和9什么的，有毛病啊，拨出去还要加拨1和9\n最后仍然是Submit，Apply Config配置生效。\n这样基本上就完工了\n五、配置手机端voip软件 我只有iphone手机，首先要翻墙，下载zoiper软件，这个就不多说了\n打开后添加一个SIP账号：\nAccount name: zhang ranrui 可以随便填 Domain: freepbx的ip地址，这个不能胡填 User name: 888 分机号码 Password: 密码，就是FreePBX分机的Secret值，一定要设置的够复杂 Enable video FMTP: disable不选 然后在Account的netwok settings中，STUN一定要禁掉，直通。\nok，这样就拨打那个sim的电话就会转到这个888的分机上了。\n六、路由器设置 有公网设置，需要做端口映射\n1路由器Port 5060 udp --\u0026gt; FreePBX Port 5060 udp 2 3路由器Port 10000-\u0026gt;10004 udp --\u0026gt; FreePBX Port 10000-10004 udp 10000到10004的端口映射出去是因为语音一路，数据一路，我们只有一个sim卡，4路足够了。\n","date":"2024-01-05","img":"","permalink":"https://bajie.dev/posts/20240105-voip_iphone/","series":null,"tags":null,"title":"出国旅行安装一个FreePBX的voip电话自用"},{"categories":null,"content":"同事有个需求，想开放个域名，给别的同事们下载文件用。\n这个其实很建单，问题就是文件怎么放上去，sftp、ftp什么的都需要搭个服务器，其实最简单应该是webdav，但是放公网又不太安全。\n本来考虑是用 oss 的桶，前面直接套个CDN，但是如果桶文件泄露被疯狂下载，也会付出银两。\n最后做法是用 Minio 弄一个模拟的桶环境，前面配置上 Nginx，带宽限制到1M，这样就无所谓了。同事用另一台机器模拟S3的API往上面放文件供下载。\n一、minio的安装\n下载二进制文件，简单粗暴启动：\n1#!/bin/bash 2nohup /app/minio/minio server --address \u0026#39;0.0.0.0:9000\u0026#39; --console-address \u0026#39;0.0.0.0:9001\u0026#39; /app/bucket \u0026gt; /app/minio/minio.log 2\u0026gt;\u0026amp;1 \u0026amp; 这样就启动了，然后网页打开 http://10.10.247.211:9000 ，就可以看到登录界面了\n登录后，我们先建立一个Bucket，就叫做pub桶，把pub桶的策略改成public，这样nginx代理的时候才能直接访问到\n然后我们上传个图片，BOMS3.0.png，然后去/app/bucket目录下看看\n结果发现BOMS3.0.png居然是个目录，进去继续看\nmeta信息和若干的part，看来这是为多版本准备的，这就是为什么要策略是public的原因，不是就无法从9000访问。\n二、安装配置nginx\n1server { 2 listen 80; 3 listen [::]:80; 4 server_name supervisor-task.ddky.com; 5 6 location / { 7 return 301 https://supervisor-task.ddky.com$request_uri; 8 } 9} 10 11server { 12 listen 443 ssl; 13 server_name supervisor_task.ddky.com; 14 15 ssl_certificate /etc/nginx/cert/_.ddky.com.crt; 16 ssl_certificate_key /etc/nginx/cert/_.ddky.com.key; 17 ssl_session_timeout 5m; 18 19 location / { 20 rewrite ^/$ /pub/index.html break; 21 proxy_pass http://10.10.247.211:9000/pub/; 22 proxy_redirect off; 23 } 24 25 access_log /app/logs/access.log ; 26 error_log /app/logs/error.log; 27 28} 主要就是要配置代理，代理到minio的9000端口\n这样就好了。\n","date":"2023-12-28","img":"","permalink":"https://bajie.dev/posts/20231228-minio_nginx/","series":null,"tags":null,"title":"配置Minio+nginx的代理来开放桶内内容"},{"categories":null,"content":"NFS的SERVER分两部分： 1、RPC 主程序：rpcbind NFS 本质是一个 RPC 服务，而要启动任何一个 RPC 服务之前，都需要做好 port 的对应 (mapping) 的工作才行，这个工作其实就是『 rpcbind 』这个服务所负责的！也就是说， 在启动任何一个 RPC 服务之前，我们都需要启动 rpcbind 才行！ (在 CentOS 5.x 以前这个软件称为 portmap，在 CentOS 6.x 之后才称为 rpcbind 的！)\n2、NFS 主程序：nfs-utils 就是提供 rpc.nfsd 及 rpc.mountd 这两个 NFS daemons 与其他相关 documents 与说明文件、执行文件等的软件！这个就是 NFS 服务所需要的主要软件！\n安装服务端NFS Server 1yum install nfs-utils rpcbind -y 2sudo apt install nfs-kernel-server 客户端不提供服务，所以不用装rpcbind 1yum install nfs-utils 2sudo apt install nfs-common 将NFS和rpcbind加入开机启动 1systemctl enable --now rpcbind 2systemctl enable --now nfs 客户端不用启用任何服务 服务端检查是否安装nfs： 1rpm -qa | grep nfs与 rpm -qa | grep rpcbind即可 停止服务端的nfs server的方法： 1systemctl stop nfs 2systemctl stop rpcbind 配置共享目录 在服务端配置一个共享目录\n1$ mkdir /data 2$ chmod 755 /data 根据这个目录，相应配置导出目录\n1$ vi /etc/exports 添加如下配置 /data/ 192.168.0.0/24(rw,sync,no_root_squash,no_all_squash)\n1/data: 共享目录位置。 2192.168.0.0/24: 客户端 IP 范围，* 代表所有，即没有限制。 3rw: 权限设置，可读可写。 4sync: 同步共享目录。 5no_root_squash: 可以使用 root 授权。 6no_all_squash: 可以使用普通用户授权。 在客户端上查询server： 1showmount -e 10.0.6.10 2mount -t nfs 192.168.0.1:/data /mnt/nfs 取消挂载 1umount /app/file 2fuser /app/file 3umount -d -l /app/file 4fuser -m -v -i -k /app/file ","date":"2023-12-28","img":"","permalink":"https://bajie.dev/posts/20231228-nfs_detail/","series":null,"tags":null,"title":"NFS的详细解释"},{"categories":null,"content":"Kvm虚拟机挂载临时急救iso启动的方法： 首先去到宿主机\n编辑虚机文件virsh edit vis-18-32-6\n1 \u0026lt;disk type=\u0026#39;file\u0026#39; device=\u0026#39;cdrom\u0026#39;\u0026gt; 2 \u0026lt;driver name=\u0026#39;qemu\u0026#39; type=\u0026#39;raw\u0026#39;/\u0026gt; 3 \u0026lt;target dev=\u0026#39;hda\u0026#39; bus=\u0026#39;ide\u0026#39;/\u0026gt; 4 \u0026lt;readonly/\u0026gt; 5 \u0026lt;address type=\u0026#39;drive\u0026#39; controller=\u0026#39;0\u0026#39; bus=\u0026#39;0\u0026#39; target=\u0026#39;0\u0026#39; unit=\u0026#39;0\u0026#39;/\u0026gt; 6 \u0026lt;/disk\u0026gt; 在target下面增加一行source：\n1 \u0026lt;disk type=\u0026#39;file\u0026#39; device=\u0026#39;cdrom\u0026#39;\u0026gt; 2 \u0026lt;driver name=\u0026#39;qemu\u0026#39; type=\u0026#39;raw\u0026#39;/\u0026gt; 3 \u0026lt;target dev=\u0026#39;hda\u0026#39; bus=\u0026#39;ide\u0026#39;/\u0026gt; 4\t\u0026lt;source file=\u0026#34;/export/kvm/systemrescuecd-x86-5.2.2.iso\u0026#34;/\u0026gt; 5 \u0026lt;readonly/\u0026gt; 6 \u0026lt;address type=\u0026#39;drive\u0026#39; controller=\u0026#39;0\u0026#39; bus=\u0026#39;0\u0026#39; target=\u0026#39;0\u0026#39; unit=\u0026#39;0\u0026#39;/\u0026gt; 7 \u0026lt;/disk\u0026gt; 再修改boot为cdrom，改回去则是hd\n1 \u0026lt;os\u0026gt; 2 \u0026lt;type arch=\u0026#39;x86_64\u0026#39; machine=\u0026#39;pc-i440fx-rhel7.0.0\u0026#39;\u0026gt;hvm\u0026lt;/type\u0026gt; 3 \u0026lt;boot dev=\u0026#39;cdrom\u0026#39;/\u0026gt; 4 \u0026lt;/os\u0026gt; 然后启动虚机就可以了。\n","date":"2023-12-28","img":"","permalink":"https://bajie.dev/posts/20231228-kvm_isoboot/","series":null,"tags":null,"title":"Kvm虚拟机挂载临时急救iso启动的方法"},{"categories":null,"content":"Shell脚本好的帮助信息的例子 一个好的脚本，必须有好的帮助信息\n给一个比较简洁的例子\n#!/bin/bash\r###\r### my-script — does one thing well\r###\r### Usage:\r### my-script \u0026lt;input\u0026gt; \u0026lt;output\u0026gt;\r###\r### Options:\r### \u0026lt;input\u0026gt; Input file to read.\r### \u0026lt;output\u0026gt; Output file to write. Use '-' for stdout.\r### -h Show this message.\rhelp() {\rawk -F'### ' '/^###/ { print $2 }' \u0026quot;$0\u0026quot;\r}\rif [[ $# == 0 ]] || [[ \u0026quot;$1\u0026quot; == \u0026quot;-h\u0026quot; ]]; then\rhelp\rexit 1\rfi\recho Hello World\r以前丑陋的用法真是没法看！！！\n","date":"2023-12-28","img":"","permalink":"https://bajie.dev/posts/20231228-shell_prompt/","series":null,"tags":null,"title":"Shell脚本的一个好的帮助信息"},{"categories":null,"content":"Nodejs禁止后台偷偷升级 去到172.18.31.2上，会看到很多node进程\n有病噻，系统都没有运行node程序。唯一的程序是nci-ansible-ui-quick-setup，在/export/server下，还没有运行，升级个茄子噻。\n全杀掉，然后全局禁止升级即可\n1npm config set update-notifier false --global 第一次运行会有个提示，再运行就没有了。 ","date":"2023-12-28","img":"","permalink":"https://bajie.dev/posts/20231228-nodejs_noupgrade/","series":null,"tags":null,"title":"Nodejs禁止后台偷偷升级"},{"categories":null,"content":"Tomcat配置不当导致文件泄露 说明：Tomcat由于配置不当会导致tomcat/conf log webapps work temp bin lib等信息暴露在游览器中 例如：\n1http://192.168.89.38:8080/conf/catalina.policy 2http://192.168.89.38:8080/conf/catalina.properties 3http://192.168.89.38:8080/conf/context.xml 4http://192.168.89.38:8080/conf/logging.properties 5http://192.168.89.38:8080/conf/server.xml 6http://192.168.89.38:8080/conf/tomcat-users.xml 7http://192.168.89.38:8080/conf/web.xml 修复方法：\n1将 /export/servers/tomcat 下的 server.xml 2 3\u0026lt;Host name=\u0026#34;localhost\u0026#34; appBase=\u0026#34;\u0026#34; 改成 4\u0026lt;Host name=\u0026#34;localhost\u0026#34; appBase=\u0026#34;webapps\u0026#34; 5 6appBase千万不能为空 修改完后重启生效\n","date":"2023-12-28","img":"","permalink":"https://bajie.dev/posts/20231228-tomcat_config/","series":null,"tags":null,"title":"Tomcat配置不当导致文件泄露"},{"categories":null,"content":"用fail2ban简简单单封掉ssh端口的试探 有人不停试探登录22端口的openssh服务，ip量很大的话，比如1000多ip论番来，会导致服务的Loadavg升高到7左右，系统进程内ssh达到1000多，很困扰\n如果改掉sshd的缺省端口22，那么scp以及sftp的时候会带来很大麻烦\n这种情况就装个fail2ban就好，注意，Centos 7现在用的是fiewalld，所以fail2ban可以用ufw或者iptables，建议用iptables，比较容易看\n1#安装 2yum install -y epel-release 3yum install -y fail2ban 最主要的就是修改/etc/fail2ban/jail.conf\n1#vi /etc/fail2ban/jail.conf 2...... 3[sshd-iptalbes] 4enabled = true 5filter = sshd 6port = 22 7action = iptables[name=SSH, port=ssh, protocol=tcp] 8logpath = /var/log/secure 9maxretry = 3 10bantime = 86400 11...... 注意上面用的是iptables，三次尝试失败就ban一天。\n","date":"2023-12-28","img":"","permalink":"https://bajie.dev/posts/20231228-fail2ban/","series":null,"tags":null,"title":"用fail2ban简简单单封掉ssh端口的试探"},{"categories":null,"content":"用logrotate解决catalinna.out过大的问题 我们的tomcat的catalina.out基本运行一段时间后会出现文件过大的问题，处理一下。\n前提是/export/servers/tomcat/下需要建立个软链接tomcat，指向现在正在用的tomcat版本\n然后就可以用logrotate来解决catalina.out的日志轮转问题。这种方式比较简单。在/etc/logrotate.d/目录下新建一个名为tomcat的文件，\n1cat \u0026gt;/etc/logrotate.d/tomcat \u0026lt;\u0026lt;EOF 2/export/servers/tomcat/tomcat/logs/catalina.out{ 3 copytruncate 4 daily 5 rotate 7 6 missingok 7 compress 8} 9EOF 以上的配置说明：\n/export/servers/tomcat/tomcat/logs/catalina.out{ # 要轮转的文件 copytruncate # 创建新的catalina.out副本后，truncate源catalina.out文件，会丢数据!!! daily # 每天进行catalina.out文件的轮转 rotate 7 # 至多保留7个副本 missingok # 如果要轮转的文件丢失了，继续轮转而不报错 compress # 使用压缩的方式（非常有用，节省硬盘空间；一个2~3GB的日志文件可以压缩成60MB左右） }\n可以手工强制执行logrotate程序。在命令行运行：\n","date":"2023-12-28","img":"","permalink":"https://bajie.dev/posts/20231228-logrotate/","series":null,"tags":null,"title":"用logrotate解决catalinna.out过大的问题"},{"categories":null,"content":"Rsync的基本参数 1-a : 归档模式，递归拷贝，带属性。 2-u : 同步时目标目录中如果某文件比源中文件时间要新，则保留不动。 3-v : 详细显示信息 4-z : 压缩 5-h : 显示友好信息 6-P : 显示传输进度百分比 7-R : 使用相对路径，会保留源目录的目录结构。 8-e : 使用ssh来传输 1 源如果没有/，代表连同目录以及目录下的文件，统统拷贝到目的去 2 源如果最后带/，意思是/*，代表只拷贝目录下的文件，不包括目录本身 3 4 如果参数中带-R，那无论源后面有无/，都按原有目录状态拷过去 -R 使用相对路径，会保留源目录的目录结构。 如rsync -av /foo/bar/baz.c remote::/tmp/，会将baz.c传到/tmp下， 而使用了-R，则在/tmp下的文件结构将会是foo/bar/baz.c。\n下面两种用法一样：\n1rsync -auvPR --exclude \u0026#39;upload\u0026#39; new 172.18.34.38::new/ 2rsync -auvPR --exclude \u0026#39;upload\u0026#39; ./new 172.18.34.38::new/ 大规模传输细小文件的方法： 1find ./new -mindepth 6 -maxdepth 6 -type d -print0 | xargs -P 30 -n 60 -I % -0 rsync -auvPR % 172.18.34.38::new/ -print0 -0 find命令有一个特别的参数-print0，指定输出的文件列表以null分隔。然后，xargs命令的-0参数表示用null当作分隔符。 -I -I指定每一项命令行参数的替代字符串。\n注意上面，从最底层开始，逐步向上同步。同步完第6个，然后第5个，4 3 2 1 \u0026hellip;\u0026hellip;\n最低的是第6层，每个文件价是一个日期，每个文件夹包含3000个文件左右。\n限速 1rsync --bwlimit=500 bwlimit的单位是kb，所以500×8=4000kb=4000000，限速是4M（网络单位）\n借用ssh 1rsync -avzhe ssh backup.tar root@192.168.0.100:/backups/ ","date":"2023-12-28","img":"","permalink":"https://bajie.dev/posts/20231228-rsync/","series":null,"tags":null,"title":"Rsync的日常使用方法"},{"categories":null,"content":"192.168.85.7上面有两个Raid\n一个Raid1, 500G，应该是两块盘，现在丢了一块，正确的槽位不知道是哪一个 另一个是Raid0，4TB，运行正常\n现在在一个新槽位上插了一块硬盘，但是槽位不对，没有自动rebuild，状态是unconfiged good.\n首先查一下丢失的盘\n/opt/MegaRAID/MegaCli/MegaCli64 -Pdgetmissing -a0\r得到参数Array 0，Row 1\n首先进行替换，新盘的id是32:2，替换掉Array0 Row1的盘\n1/opt/MegaRAID/MegaCli/MegaCli64 -PdReplaceMissing -PhysDrv [32:2] -Array0 -row1 -a0 然后强制这块新盘开始重建\n1/opt/MegaRAID/MegaCli/MegaCli64 -PDRbld -Start -PhysDrv [32:2] -a0 随时查看一下进度\n1/opt/MegaRAID/MegaCli/MegaCli64 -PDRbld -ShowProg -PhysDrv [32:2] -a0 上面已经走了5%了，走完就ok了。\n","date":"2023-12-27","img":"","permalink":"https://bajie.dev/posts/20231227-megacli_rebuild/","series":null,"tags":null,"title":"Megacli用来重建不知道正确槽位的Raid"},{"categories":null,"content":"适用于所有MegaCli的机器。无论dell、浪潮还是H3C。\n必须每天上午、下午各检查一下所有物理机的磁盘啊，有问题及时处理。\n172.18.31.2 /usr/local/bin/check_disk.sh 脚本：\nL1到L5，是5个机柜，从上到下是172.18.x.x的ip分布\n1#!/bin/bash 2 3L1=(30.1 30.2 30.3 30.4 30.5 30.6 30.7 30.8 30.9 30.10 30.11 30.12 30.13 30.14 30.15 30.16 20.25 30.18) 4L2=(30.19 20.23 20.24 20.30 20.31 20.9 20.10 20.37 20.38 20.39 30.29 30.30 20.16 20.17 30.33 30.34 30.35 30.36) 5L3=(30.37 20.44 20.45 22.200 30.41 30.42 30.43 30.44 30.45 30.46 30.47 22.201 30.49 30.50 20.51 20.52 20.58 20.59) 6L4=(20.65 20.66 30.57 30.58 30.59 30.60 20.1 20.2 30.63 30.64 30.65 30.66 30.67 30.68 30.69 30.70 30.71 30.72) 7L5=(30.73 30.74 30.75 30.76) 8 9\u0026gt; /tmp/idrac.txt 10\u0026gt; /tmp/idracout.txt 11 12for i in L1 L2 L3 L4 L5 13do 14 var=$i[@] 15 for j in ${!var} 16 do 17 echo \u0026#34;Scan $i 172.18.$j......\u0026#34; \u0026gt;\u0026gt; /tmp/idrac.txt 18 sshpass -p \u0026#34;xxxxxxxx\u0026#34; ssh -oStrictHostKeyChecking=no root@172.18.$j /opt/MegaRAID/MegaCli/MegaCli64 -PDList -a0|grep \u0026#34;S.M.A.R.T\u0026#34; \u0026gt;\u0026gt; /tmp/idrac.txt 19 done 20done 21 22lines=($(grep -n \u0026#34;Yes\u0026#34; /tmp/idrac.txt | cut -f1 -d:)) 23if [ ${#lines[@]} -eq 0 ]; then 24 exit 1 25else 26 for findline in ${lines[@]} 27 do 28 array=($(grep -n \u0026#34;\\.\\.\\.\\.\\.\\.\u0026#34; /tmp/idrac.txt |cut -f1 -d:)) 29 newarray=(${array[*]} ${findline}) 30 sortarray=($(echo ${newarray[@]} | tr \u0026#39; \u0026#39; \u0026#39;\\n\u0026#39;|sort -n)) 31 for((i=0;i\u0026lt;${#sortarray[*]};i++)) 32 do 33 if [ ${sortarray[$i]} -eq $findline ];then 34 beginline=${sortarray[(($i-1))]} 35 endline=${sortarray[(($i+1))]} 36 if [ \u0026#34;$endline\u0026#34; == \u0026#34;\u0026#34; ];then 37 endline=\u0026#34;$\u0026#34; 38 else 39 endline=$(($endline-1)) 40 fi 41 break 42 fi 43 done 44 sed -n \u0026#34;$beginline,$endline p\u0026#34; /tmp/idrac.txt \u0026gt;\u0026gt; /tmp/idracout.txt 45 done 46 /usr/local/bin/mailsend -to \u0026#34;zhangranrui@ddky.com\u0026#34; -from monit@ddky.com -ssl -port 465 -auth -auth-plan -smtp smtp.exmail.qq.com -sub \u0026#34;IDC机器硬件故障\u0026#34; -v -user \u0026#34;monit@ddky.com\u0026#34; -pass \u0026#34;xxxxxxxx\u0026#34; -cs \u0026#34;gb2312\u0026#34; -enc-type \u0026#34;base64\u0026#34; -M \u0026#34;$(cat /tmp/idracout.txt)\u0026#34; 47fi ","date":"2023-12-27","img":"","permalink":"https://bajie.dev/posts/20231227-megacli_disk/","series":null,"tags":null,"title":"Linux下定期检查物理机磁盘的脚本"},{"categories":null,"content":"我们的宿主机上做了4网卡bonding，然后需要上升到br，两个网卡都需要做上vlan。\n比如172.18.30.1，想在上面增加一个VLAN段202，然后在上面生产一台虚机172.18.19.2\n就需要增加一个bond0.202以及br0.202\n做法如下:\n1cd /etc/sysconfig/network-scripts 2 3cp ifcfg-bond0.199 ifcfg-bond0.202 4 5vi ifcfg-bond0.202 6DEVICE=bond0.202 7ONBOOT=yes 8USERCTL=no 9BRIDGE=br0.202 10VLAN=yes 11 12cp ifcfg-br0.199 ifcfg-br0.199 13 14vi ifcfg-br0.202 15DEVICE=br0.202 16BOOTPROTO=static 17ONBOOT=yes 18TYPE=Bridge 然后启动这两个网卡，启动顺序很重要！！！\n1ifup bond0.202 2ifup br0.202 注意，执行完以后务必检查一下两个网卡是否UP\n然后产虚机就好\n1./vm.sh create -b br0.202 -c 4 -m 4096 -d 40 -i 172.18.19.2 -k 255.255.255.0 -g 172.18.19.254 -q 172.18.30.1 -r 172.18.30.2 ucarp-18-19-2 ","date":"2023-12-27","img":"","permalink":"https://bajie.dev/posts/20231227-vlan_bond_br/","series":null,"tags":null,"title":"Linux下bond和br建立vlan的方法"},{"categories":null,"content":"我们有两个出口，一个从无锡出，一个从世纪互联出\n这台机器的em1接世纪互联，em2接无锡尚航，两个网关，有ipv6和ipv4\nISP 1:\rGateways:\r172.16.9.254\r2001:db8:a::1\rInterface: em1\rISP 2:\rGateways:\r172.18.9.254\r2001:db8:b::1\rInterface: em2\r那么这台机器的ECMP等价路由这么做：\nip route replace default proto static scope global \\\rnexthop dev em1 via 172.16.9.254 weight 1 \\\rnexthop dev em2 via 172.18.9.254 weight 1\rip -6 route replace default proto static scope global \\\rnexthop dev em1 via 2001:db8:a::1 weight 1 \\\rnexthop dev em2 via 2001:db8:b::1 weight 1\r这两条命令需要放到 /etc/rc.d/rc.locl 或者ifup里面去\n之后发包的时候就会一左一右轮流发。\n","date":"2023-12-27","img":"","permalink":"https://bajie.dev/posts/20231227-linux_ecmp/","series":null,"tags":null,"title":"Linux下ECMP等价路由的建立"},{"categories":null,"content":"审计的要求：\n线上环境研发只能有只读权限\n那只能曲线救国了，两个用户，supdev是运维用来管理的，logview是研发用来只读日志的\n一、首先确定系统有supdev用户存在 id supdev 通常supdev的id是511\n二、增加新用户logview 我们确定logview的id是512，group是和supdev同组 useradd -u 512 -g supdev logview\n三、程序的app以及数据目录不是在supdev的home目录下，而是在/data/servers下 所以我们把/data/servers下的目录都改成750，文件都改成640\n1#!/bin/sh 2find /export/servers/ -type d ! -perm 0750 -exec chmod 0750 {} 3find /export/servers/ -type f ! -perm 0640 -exec chmod 0640 {} 1# ansible 模块 2- name: make dirs 0755 3 command: find {{ your_path }} -type d ! -perm 0750 -exec chmod 0750 {} \\; 4 5- name: make files 0644 6 command: find {{ your_path }} -type f ! -perm 0640 -exec chmod 0640 {} \\; 这样就可以了。logview可以去到/data/servers目录，可以看文件，但无法执行。\n","date":"2023-12-27","img":"","permalink":"https://bajie.dev/posts/20231227-supdev_logview/","series":null,"tags":null,"title":"审计需求只读用户的建立"},{"categories":null,"content":"H3C服务器主板时间不准确也很讨厌，看主板日志时间都是错的，没办法，修改一下。\nH3C服务器BIOS的NTP设置方法：\n修改主服务器：\n1ipmitool -I lanplus -H 10.18.$1 -U admin -P Password@_ raw 0x32 0xA8 0x01 服务器名的HEX字符串 修改辅服务器：\n1ipmitool -I lanplus -H 10.18.$1 -U admin -P Password@_ raw 0x32 0xA8 0x02 服务器名的HEX字符串 修改第三个服务器：\n1ipmitool -I lanplus -H 10.18.$1 -U admin -P Password@_ raw 0x32 0xA8 0x05 服务器名的HEX字符串 方法很简单，但是服务器名的HEX字符串如何得到？\n1echo -n \u0026#34;172.18.30.1\u0026#34; | od -A n -t x1 | sed \u0026#34;s/ / 0x/g\u0026#34; 20x31 0x37 0x32 0x2e 0x31 0x38 0x2e 0x33 0x30 0x2e 0x31 修改NTP服务器1为172.18.30.1\n1ipmitool -I lanplus -H 10.18.$1 -U admin -P Password@_ raw 0x32 0xA8 0x01 0x31 0x37 0x32 0x2e 0x31 0x38 0x2e 0x33 0x30 0x2e 0x31 ","date":"2023-12-26","img":"","permalink":"https://bajie.dev/posts/20231226-h3c_ntp/","series":null,"tags":null,"title":"H3C服务的BIOS主板设置中如何正确设置NTP服务器"},{"categories":null,"content":"traefik使用digicert付费的证书和使用letencrypt免费证书的方法不一样，下面说一下怎么配置：\ntraefik.yml里面就没有任何配置\n1log: 2 level: DEBUG 3 4api: 5 insecure: false 6 dashboard: true 7 8entryPoints: 9 http: 10 address: \u0026#34;:80\u0026#34; 11 #http: 12 # redirections: 13 # entryPoint: 14 # to: https 15 # scheme: https 16 17 https: 18 address: \u0026#34;:443\u0026#34; 19 20providers: 21 file: 22 directory: /export/servers/traefik/dynamic 23 watch: true 所有的配置都放到到/export/servers/traefik/dynamic目录下了，动态更新：\ncerts.yml来定义证书选项\n1tls: 2 certificates: 3 - certFile: \u0026#34;/export/servers/traefik/ddky.crt\u0026#34; 4 keyFile: \u0026#34;/export/servers/traefik/ddky.key\u0026#34; 5 options: 6 default: 7 sniStrict: true 8 minVersion: VersionTLS12 9 cipherSuites: 10 - TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 11 - TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256 12 - TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256 13 - TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256 14 - TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 test7-01.yml单独test7.ddky.com的配置\n1http: 2 routers: 3 https_01: 4 rule: \u0026#34;Host(`test7.ddky.com`)\u0026#34; 5 service: svc_01 6 tls: 7 domains: 8 - main: \u0026#34;test7.ddky.com\u0026#34; 9 sans: 10 - \u0026#34;*.ddky.com\u0026#34; 11 12 http_01: 13 rule: \u0026#34;Host(`test7.ddky.com`)\u0026#34; 14 service: svc_01 15 entryPoints: 16 - http 17 18 services: 19 svc_01: 20 loadBalancer: 21 servers: test8-02.yml单独test8.ddky.com的配置\n1http: 2 routers: 3 https_02: 4 rule: \u0026#34;Host(`test8.ddky.com`)\u0026#34; 5 service: svc_02 6 tls: 7 domains: 8 - main: \u0026#34;test8.ddky.com\u0026#34; 9 10 http_02: 11 rule: \u0026#34;Host(`test8.ddky.com`)\u0026#34; 12 service: svc_02 13 entryPoints: 14 - http 15 16 services: 17 svc_02: 18 loadBalancer: 19 servers: 20 - url: \u0026#34;http://172.18.31.33:80\u0026#34; 注意上面tls的选项，sans备用域名可加可不加。\n","date":"2023-12-21","img":"","permalink":"https://bajie.dev/posts/20231221-traefik_digicert/","series":null,"tags":null,"title":"Traefik配置digicert家得泛域名证书"},{"categories":null,"content":"在172.18.30.1和172.16.60.10上\n使用vm.sh创建虚机时，会报错\n1- Resizing the disk to 80G ... ERR: Could not resize disk. 去 /export/kvm/虚机目录下查看log\n1Could not open \u0026#39;/var/tmp/.guestfs-101448/appliance.d/root\u0026#39;: No such file or directory 解决方法很简单，清除掉Cache即可：\n1rm -rf /var/tmp/.guestfs-0/ 附上CentOS7安装 kvm 的命令：\n1yum install qemu-kvm libvirt libvirt-python libguestfs-tools virt-install ","date":"2023-12-21","img":"","permalink":"https://bajie.dev/posts/20231221-vm_error/","series":null,"tags":null,"title":"Vm.sh创建虚机失败"},{"categories":null,"content":"终于遇到了inode不够了，细碎文件爆棚了，导致inode不够了。\nXFS 文件系统本质上是没有 inode 的限制的\n只有一个缺省的限制，使用现有文件系统的 25% 来存储 inode 信息。\n这个缺省的配置在大多数情况下都是够用的。\n某些情况下不够用，可以删除一些文件来缓解。\n终极办法是增大这个 25% 的阈值来解决：\n1root@zombie:~# xfs_info /srv/backup/ 2metadane=/dev/mapper/slow-backup isize=256 agcount=17, agsize=2621440 blks 3 = sectsz=512 attr=2 4data = bsize=4096 blocks=44564480, imaxpct=25 5 = sunit=0 swidth=0 blks 6naming =version 2 bsize=4096 7 ascii-ci=0 8log =internal bsize=4096 blocks=20480, version=2 9 = sectsz=512 sunit=0 blks, lazy-count=1 10realtime=brak extsz=4096 blocks=0, rtextents=0 上面，我们看到 imaxpct=25 ，就是这个配置了。\n增大的方法：\n1xfs_growfs -m 30 这样就增大到 30% 了\n","date":"2023-12-21","img":"","permalink":"https://bajie.dev/posts/20231221-inode_xfs/","series":null,"tags":null,"title":"Xfs文件系统下增加inode"},{"categories":null,"content":"在机器上执行命令free -g 和 free -k\n看看有多少空闲\n8G的Swap交换空间都被用光了！！！\n再用kb的单位看看，一共8388604，基本用光掉了\n查看是哪些进程使用了交换空间：\n1find /proc -maxdepth 2 -path \u0026#34;/proc/[0-9]*/status\u0026#34; -readable -exec awk -v FS=\u0026#34;:\u0026#34; \u0026#39;{process[$1]=$2;sub(/^[ \\t]+/,\u0026#34;\u0026#34;,process[$1]);} END {if(process[\u0026#34;VmSwap\u0026#34;] \u0026amp;\u0026amp; process[\u0026#34;VmSwap\u0026#34;] != \u0026#34;0 kB\u0026#34;) printf \u0026#34;%10s %-30s %20s\\n\u0026#34;,process[\u0026#34;Pid\u0026#34;],process[\u0026#34;Name\u0026#34;],process[\u0026#34;VmSwap\u0026#34;]}\u0026#39; \u0026#39;{}\u0026#39; \\; | awk \u0026#39;{print $(NF-1),$0}\u0026#39; | sort -h | cut -d \u0026#34; \u0026#34; -f2- 6504的qemu-kvm用掉了4.8G，看起来不太直观。\n1find /proc -maxdepth 2 -path \u0026#34;/proc/[0-9]*/status\u0026#34; -readable -exec awk -v FS=\u0026#34;:\u0026#34; -v TOTSWP=\u0026#34;$(cat /proc/swaps | sed 1d | awk \u0026#39;BEGIN{sum=0} {sum=sum+$(NF-2)} END{print sum}\u0026#39;)\u0026#34; \u0026#39;{process[$1]=$2;sub(/^[ \\t]+/,\u0026#34;\u0026#34;,process[$1]);} END {if(process[\u0026#34;VmSwap\u0026#34;] \u0026amp;\u0026amp; process[\u0026#34;VmSwap\u0026#34;] != \u0026#34;0 kB\u0026#34;) {used_swap=process[\u0026#34;VmSwap\u0026#34;];sub(/[ a-zA-Z]+/,\u0026#34;\u0026#34;,used_swap);percent=(used_swap/TOTSWP*100); printf \u0026#34;%10s %-30s %20s %6.2f%\\n\u0026#34;,process[\u0026#34;Pid\u0026#34;],process[\u0026#34;Name\u0026#34;],process[\u0026#34;VmSwap\u0026#34;],percent} }\u0026#39; \u0026#39;{}\u0026#39; \\; | awk \u0026#39;{print $(NF-2),$0}\u0026#39; | sort -hr | head | cut -d \u0026#34; \u0026#34; -f2- 一目了然，6504是台Win7的虚机，祸害啊。\n","date":"2023-12-15","img":"","permalink":"https://bajie.dev/posts/20231215-swap_usage/","series":null,"tags":null,"title":"查看SWAP交换空间被哪个进程给用掉了"},{"categories":null,"content":"Linux下多线程下载工具，且支持断点续传，机房传文件必备：\n1axel -s 9000000 -a -n 5 -o ord_his.dmp http://172.16.24.2:8080/ord_his.dmp -s 限速，9000000是70M带宽，不加s就是100M榨干 -n 线程，缺省是4 -o 目的文件 -a 进度条压缩展示，必须加，否则会进度满天飞\n注意：axel 是可以自动断点续传的\n","date":"2023-12-15","img":"","permalink":"https://bajie.dev/posts/20231215-axel_download/","series":null,"tags":null,"title":"Linux支持断点续传、多线程下载的软件"},{"categories":null,"content":"suricata 是跟snort差不多的一个入侵检测工具，加上elk的图形界面，非常的好看。\n原理是suricata的log发到elk里，这样就能通过kibana进行分析了\n环境：\n1、物理机需要开16G内存，16CPU，都不太够 2、物理机172.18.30.2的br3是交换机的Mirror口，进入的全部流量都被镜像了一份 3、suricata-18-31-31是虚机，需要将30.2的br3挂进来\n1virsh attach-interface --domain suricata-18-31-31 --type bridge --source br3 --model e1000 --config --live 2同时在31.31里，ifconfig up eth1把网卡起起来 3tcpdump -i eth1有数据即可 4、首先安装java\n1rpm -ivh jdk-8u201-linux-x64.rpm 安装： 一、编译安装suricata\n1yum -y install epel-release 2 3yum -y install jq cargo openssl-devel PyYAML lz4-devel gcc libpcap-devel pcre-devel libyaml-devel file-devel zlib-devel jansson-devel nss-devel libcap-ng-devel libnet-devel tar make libnetfilter_queue-devel lua-devel GeoIP-devel 4 5wget https://www.openinfosecfoundation.org/download/suricata-4.1.8.tar.gz 6tar zxvf suricata-4.1.8.tar.gz 7cd suricata 8./configure --libdir=/usr/lib64 --prefix=/usr --sysconfdir=/etc --localstatedir=/var --enable-nfqueue --enable-lua --enable-geoip --enable-profiling 9make 10make install-full 11 12验证一下 13suricata -V 14This is Suricata version 4.1.8 RELEASE 15 16查看build参数 17suricata --build-info suricata就装好了，还需要配一下suricata-update，规则才是最主要的，装好后最好每天更新一下规则\n1suricata-update update-sources 2suricata-update list-sources 3 4suricata-update enable-source ptresearch/attackdetection 5suricata-update enable-source oisf/trafficid 6suricata-update enable-source sslbl/ssl-fp-blacklist 7 8suricata-update suricata-update的用法\n1suricata-update list-enabled-sources 2suricata-update disable-source et/pro 3suricata-update remove-source et/pro 测试规则：\n1suricata -T /etc/suricata/suricata.yaml的修改部分\n1HOME_NET: \u0026#34;[43.231.149.0/25]\u0026#34; 2outputs被改过 3outputs: 4app-layer被改过 5app-layer: 具体看附件中的suricata.yaml\n启动：\n1/usr/bin/suricata -c /etc/suricata/suricata.yaml -i eth1 -D 二、编译安装ELK\n1rpm --import https://artifacts.elastic.co/GPG-KEY-elasticsearch 2 3cat \u0026lt;\u0026lt;EOF | sudo tee /etc/yum.repos.d/elasticsearch.repo 4[elasticsearch-7.x] 5name=Elasticsearch repository for 7.x packages 6baseurl=https://artifacts.elastic.co/packages/7.x/yum 7gpgcheck=1 8gpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch 9enabled=1 10autorefresh=1 11type=rpm-md 12EOF 13 14yum clean all 15yum makecache 16 17yum install -y elasticsearch logstash kibana filebeat 安装的时候最好设一下翻墙，否则下这几个包非常慢！！！\n然后一个一个来设置\n11、设置elasticsearch 2vi /etc/elasticsearch/jvm.options 3-Xms4g 4-Xmx4g 5vi /etc/elasticsearch/elasticsearch.yml 6indices.query.bool.max_clause_count: 8192 7search.max_buckets: 250000 8 9systemctl enable --now elasticsearch 10 112、设置logstash 12将附件中logstash目录下的synlite_suricata/目录完整挪到/etc/logstash下 13/etc/logstash/synlite_suricata/ 14 15将附件中logstash目录下的pipelines.yml拷贝到/etc/logstash下 16/etc/logstash/pipelines.yml 17 18将附件中logstash目录下的logstash.service拷贝覆盖掉/etc/systemd/system/logstast.service 19/etc/systemd/system/logstash.service 20 21vi /etc/logstash/jvm.options 22-Xms4g 23-Xmx4g 24 25systemctl daemon-reload 26systemctl enable --now logstash 27 283、设置filebeat 29vi /etc/filebeat/filebeat.yml 30filebeat.inputs: 31- type: log 32 enabled: true 33 paths: 34 - /var/log/suricata/eve.json 35 fields: 36 event.type: suricata 37 38output.logstash: 39 hosts: [\u0026#34;127.0.0.1:5044\u0026#34;] 40 #ssl.certificate_authorities: [\u0026#34;/etc/pki/root/ca.pem\u0026#34;] 41 #ssl.certificate: \u0026#34;/etc/pki/client/cert.pem\u0026#34; 42 #ssl.key: \u0026#34;/etc/pki/client/cert.key\u0026#34; 43 44systemctl enable --now logstash 45 464、设置kibana 47vi /etc/kibana/kibana.yml 48server.host: \u0026#34;172.18.31.31\u0026#34; 49 50systemctl enable --new kibana 跑起来以后打开 http://172.18.31.31:5601 就可以看到了\n如果不出意外，应该有数据了。需要建立一个suricata*的索引。\n去kibana的home \u0026ndash;\u0026gt; Stack Management\n导入的synlite_suricata.kibana.7.1.x.json\nhttps://github.com/robcowart/synesis_lite_suricata/blob/master/kibana/synlite_suricata.kibana.7.1.x.json 然后在dashboard里就可以看到了\n可以清楚的看到各种ipflow，流量分布。\n","date":"2023-12-15","img":"","permalink":"https://bajie.dev/posts/20231215-suricata_elk/","series":null,"tags":null,"title":"Suricata加上elk分析机房入口全流量"},{"categories":null,"content":"如果一台宿主机上面cpu和memory都有富裕，但是磁盘不富裕，那就会面临资源浪费\n所以干脆用ipxe的sanboot来启动远程的iscsi磁盘来启动虚机好了\n首先用《iscsi卷的远程挂载 》这篇文章中所说的方法，在172.18.30.18的20T的loopback vg group上，划出一块80G的硬盘\n1#划个lvc，用的是vg-targetd的20T中的80G 2lvcreate -L 80G -n pvc-vis-18-31-48 vg-targetd 3 4#建立block块设备 5targetcli /backstores/block create vg-targetd:pvc-vis-18-31-48 /dev/vg-targetd/pvc-vis-18-31-48 6 7#建立30.18上的iscsi服务端，似乎用renhe-18-30-18比较好，但是不好区分多个卷，还是用下面的精准 8targetcli /iscsi create iqn.2020-10.com.ddky:vis-18-31-48 9 10#建立luns，会自动建立portal 11targetcli /iscsi/iqn.2020-10.com.ddky:vis-18-31-48/tpg1/luns create /backstores/block/vg-targetd:pvc-vis-18-31-48 12 13#建立客户端的iscsi，不加任何认证 14targetcli /iscsi/iqn.2020-10.com.ddky:vis-18-31-48/tpg1/acls create iqn.2020-10.com.ddky:vis-18-31-48 记下来这个: iqn.2020-10.com.ddky:vis-18-31-48\n然后修改pxe，位于172.18.31.2 /export/html/pxeboot/boot2.php，本质是发送ipxe的命令\nsanhook是加载远程硬盘，并设置为/dev/sda\n1function sansetup() { 2 global $hostname; 3 echo \u0026#34;:sansetup\\n\u0026#34;; 4 echo \u0026#34;set initiator-iqn iqn.2020-10.com.ddky:vis-18-31-48\\n\u0026#34;; 5 echo \u0026#34;set keep-san 1\\n\u0026#34;; 6 echo \u0026#34;sanhook iscsi:172.18.30.18::::iqn.2020-10.com.ddky:vis-18-31-48\\n\u0026#34;; 7 echo \u0026#34;kernel $hostname/repos/centos/7/os/x86_64/images/pxeboot/vmlinuz\\n\u0026#34;; 8 echo \u0026#34;initrd $hostname/repos/centos/7/os/x86_64/images/pxeboot/initrd.img\\n\u0026#34;; 9 echo \u0026#34;imgargs vmlinuz load_ramdisk=1 ks=$hostname/pxeboot/install/centos/centos7_last.php ksdevice=eth0 net.ifnames=0 biosdevname=0\\n\u0026#34;; 10 echo \u0026#34;boot\\n\u0026#34;; 11} 发动一个无盘的虚机从pxe启动（172.18.30.3 /export/kvm/48.sh）：\n1#!/bin/bash 2 virt-install \\ 3 --name=pxe-18-31-48 \\ 4 --vcpu=4 \\ 5 --ram=4096 \\ 6 --pxe \\ 7 --disk none \\ 8 --os-type=linux \\ 9 --network bridge=br0.199 \\ 10 --vnc --vnclisten=0.0.0.0 --vncport=5910 注意上面：指定了pxe 和disk none，这样就会启动起来并安装到远程的iscsi盘上去\n安装完成后，修改ipxe指定该虚机机从sanboot启动，这时候iscsi卷里已经有引导信息了，就需要去掉指定。\n1function sanboot() { 2 global $hostname; 3 echo \u0026#34;:sanboot\\n\u0026#34;; 4 echo \u0026#34;set initiator-iqn iqn.2020-10.com.ddky:vis-18-31-48\\n\u0026#34;; 5 echo \u0026#34;set keep-san 1\\n\u0026#34;; 6 echo \u0026#34;sanboot iscsi:172.18.30.18::::iqn.2020-10.com.ddky:vis-18-31-48\\n\u0026#34;; 7 echo \u0026#34;boot\\n\u0026#34;; 8} 重启后该机器就会从iscsi启动了。\n","date":"2023-12-15","img":"","permalink":"https://bajie.dev/posts/20231215-pxe_sanboot/","series":null,"tags":null,"title":"Ipxe使用sanboot远程启动iscsi硬盘"},{"categories":null,"content":"mysql_exporter 其实是一个mysql客户端，定时收集本机mysql的数据，并暴露出一个web端口展现数据。\n例如：http://172.18.20.9:50001/metrics ，这里我们暴露的端口是50001。\nprometheus 每隔5分钟会访问这个web页面，把上面的数据抓下来入库。（时间间隔可以调整）\n我们点进Prometheus的页面：\n注意右下角的时间戳是不对的，差8小时，我们点右上角的React UI 这个UI时间是对的，跟当地时间一致： 想看 mysql 的指标，就需要编辑公式，输入mysql会显示所有mysql的公式： 注意：我们 mysql_exporter 的端口是50001 ， 写 promsql 选择instance的时候是这样的instance=\u0026ldquo;172.18.20.9:50001\u0026rdquo;\n给一些报警例子：\n1rate(mysql_global_status_threads_connected[5m]) \u0026gt; 200 2 3mysql_global_variables_max_connections - mysql_global_status_threads_connected \u0026lt; 500 4 5mysql_global_status_innodb_num_open_files \u0026gt; (mysql_global_variables_open_files_limit) * 0.75 ","date":"2023-12-15","img":"","permalink":"https://bajie.dev/posts/20231215-prometheus_mysql/","series":null,"tags":null,"title":"Prometheus集成进mysql_exporter"},{"categories":null,"content":"zfs用来顶替Raid控制卡，有相当强悍的性能，TrueNAS用的就是这玩意。\n官方安装文档： https://openzfs.github.io/openzfs-docs/Getting%20Started/RHEL%20and%20CentOS.html CentOS7安装\n1yum install -y epel-release 2yum install -y https://zfsonlinux.org/epel/zfs-release.el7_9.noarch.rpm 3yum install -y kernel-devel zfs 加载模块\n1[root@localhost ~]# lsmod|grep zfs 2 3[root@localhost ~]# modprobe zfs 4 5[root@localhost ~]# lsmod|grep zfs 6zfs 3986850 0 7zunicode 331170 1 zfs 8zlua 151525 1 zfs 9zcommon 89551 1 zfs 10znvpair 94388 2 zfs,zcommon 11zavl 15167 1 zfs 12icp 301854 1 zfs 13spl 104299 5 icp,zfs,zavl,zcommon,znvpair 看看文件系统\n1[root@localhost ~]# zfs list 2no datasets available 192.168.85.100的本地磁盘从sdb 一直到 sdg，首先zpool建立池子，类似raid卡的功能 然后再zfs建立文件系统\n1[root@localhost ~]# zpool create -f zfspool sdb sdc sdd sde sdf sdg 2 3[root@localhost ~]# zpool status 4 pool: zfspool 5 state: ONLINE 6 scan: none requested 7config: 8 9 NAME STATE READ WRITE CKSUM 10 zfspool ONLINE 0 0 0 11 sdb ONLINE 0 0 0 12 sdc ONLINE 0 0 0 13 sdd ONLINE 0 0 0 14 sde ONLINE 0 0 0 15 sdf ONLINE 0 0 0 16 sdg ONLINE 0 0 0 17 18errors: No known data errors 19 20[root@localhost ~]# df -h 21文件系统 容量 已用 可用 已用% 挂载点 22devtmpfs 63G 0 63G 0% /dev 23tmpfs 63G 0 63G 0% /dev/shm 24tmpfs 63G 9.9M 63G 1% /run 25tmpfs 63G 0 63G 0% /sys/fs/cgroup 26/dev/mapper/centos-root 50G 1.7G 49G 4% / 27/dev/sda1 1014M 189M 826M 19% /boot 28/dev/mapper/centos-home 392G 33M 392G 1% /home 29tmpfs 13G 0 13G 0% /run/user/0 30zfspool 53T 128K 53T 1% /zfspool 上面对生产无意义，没有任何冗余的配置在生产是行不通的\n破坏掉先\n1zpool destroy zfspool 条带对机房基本无意义，做mirror 1即Raid1\n1[root@localhost ~]# zpool create -f zfspool mirror sdb sdc 2 3[root@localhost ~]# zpool status 4 pool: zfspool 5 state: ONLINE 6 scan: none requested 7config: 8 9 NAME STATE READ WRITE CKSUM 10 zfspool ONLINE 0 0 0 11 mirror-0 ONLINE 0 0 0 12 sdb ONLINE 0 0 0 13 sdc ONLINE 0 0 0 14 15errors: No known data errors mirror的话往里面增加盘必须成对，单盘禁止往里面加\n1[root@localhost ~]# zpool add -f zfspool mirror sde 2invalid vdev specification: mirror requires at least 2 devices 增加一对盘进去，这样就做成Raid10了\n1[root@localhost ~]# zpool add -f zfspool mirror sde sdf 2 3[root@localhost ~]# zpool status 4 pool: zfspool 5 state: ONLINE 6 scan: none requested 7config: 8 9 NAME STATE READ WRITE CKSUM 10 zfspool ONLINE 0 0 0 11 mirror-0 ONLINE 0 0 0 12 sdb ONLINE 0 0 0 13 sdc ONLINE 0 0 0 14 mirror-1 ONLINE 0 0 0 15 sde ONLINE 0 0 0 16 sdf ONLINE 0 0 0 17 18errors: No known data errors 另外zfs有个特殊的raidz1 raidz2 raidz3 分别代表允许1块盘坏，2块盘坏，3块盘坏 需要的盘最小数量分别是2块，3块，4块\n在生产，比较适合的是允许2块盘坏，并加1块host spare\n1[root@localhost /]# zpool create -f zfspool raidz2 sdb sdc sde sdf 2 3[root@localhost /]# zpool status 4 pool: zfspool 5 state: ONLINE 6 scan: none requested 7config: 8 9 NAME STATE READ WRITE CKSUM 10 zfspool ONLINE 0 0 0 11 raidz2-0 ONLINE 0 0 0 12 sdb ONLINE 0 0 0 13 sdc ONLINE 0 0 0 14 sde ONLINE 0 0 0 15 sdf ONLINE 0 0 0 16 17errors: No known data errors 增加热备盘spare\n1[root@localhost /]# zpool add zfspool spare sdg 2 3[root@localhost /]# zpool status 4 pool: zfspool 5 state: ONLINE 6 scan: none requested 7config: 8 9 NAME STATE READ WRITE CKSUM 10 zfspool ONLINE 0 0 0 11 raidz2-0 ONLINE 0 0 0 12 sdb ONLINE 0 0 0 13 sdc ONLINE 0 0 0 14 sde ONLINE 0 0 0 15 sdf ONLINE 0 0 0 16 spares 17 sdg AVAIL 18 19errors: No known data errors 如果出现坏盘，用好盘sdf换掉坏盘sde\n1[root@localhost /]# zpool replace zfspool sde sdf 2 3[root@localhost /]# zpool status 4 pool: zfspool 5 state: ONLINE 6 scan: resilvered 1.17M in 0 days 00:00:00 with 0 errors on Thu Mar 11 20:02:19 2021 7config: 8 9 NAME STATE READ WRITE CKSUM 10 zfspool ONLINE 0 0 0 11 raidz2-0 ONLINE 0 0 0 12 sdb ONLINE 0 0 0 13 sdc ONLINE 0 0 0 14 sdf ONLINE 0 0 0 15 spares 16 sdg AVAIL 17 18errors: No known data errors 检查zpool磁盘组的完整性\n1zpool scrub testpool 增加cache会增加读速度，类似盘阵热点SSD自动落盘技术\n1$ zpool create mirror /dev/sda /dev/sdb cache /dev/sdk /dev/sdl 增加log会提高写速度，类似盘阵热点SSD自动落盘技术\n1$ zpool create mirror /dev/sda /dev/sdb log /dev/sdk /dev/sdl zpool 建立好zfspool后，可以建文件系统\n1zfs create zfspool/dba-vol 2 3[root@localhost /]# zfs list 4NAME USED AVAIL REFER MOUNTPOINT 5zfspool 1.47M 35.2T 153K /zfspool 6zfspool/dba-vol 262K 35.2T 160K /zfspool/dba-vol 修改挂载点\n1zfs set mountpoint=/path/to/mount zpool-name/dataset-name 快照\n1zfs snapshot [pool]/[dataset name]@[snapshot name] 2zfs snapshot zfspool/dba-vol@dba-vol-20210315 3 4zfs list -t snapshot 5 6注意，快照恢复后，该时间点之后的快照会全部丢失 7zfs rollback -r [pool]/[dataset]@[snapshot name] 8zfs rollback -r zfspool/dba-vol@dba-vol-20210315 ","date":"2023-12-15","img":"","permalink":"https://bajie.dev/posts/20231215-zfs_centos/","series":null,"tags":null,"title":"CentOS7安装ZFS"},{"categories":null,"content":"ucarp我们来实战一下完成ucarp+nginx做内网vip，模拟F5的vip的方法\nucarp的安装参考之前的文章，环境如下：\nucarp1：192.168.19.1 ucarp2：192.168.19.2 vip：172.18.19.10 在172.18.19.1和172.18.19.2上编译Nginx 1.16.1\n1 ./configure --prefix=/export/servers/nginx1161 --with-stream --with-stream_ssl_module 2\tmake 3\tmake install 重点是/export/servers/nginx1161/conf/nginx.conf\n1cat /export/servers/nginx1161/conf/nginx.conf 2 3 user nobody; 4 worker_processes auto; 5 6 events { 7 use epoll; 8 worker_connections 65535; 9 } 10 11 stream { 12 log_format proxy \u0026#39;$remote_addr [$time_local] \u0026#39; 13 \u0026#39;$protocol $status $bytes_sent $bytes_received \u0026#39; 14 \u0026#39;$session_time \u0026#34;$upstream_addr\u0026#34; \u0026#39; 15 \u0026#39;\u0026#34;$upstream_bytes_sent\u0026#34; \u0026#34;$upstream_bytes_received\u0026#34; \u0026#34;$upstream_connect_time\u0026#34;\u0026#39;; 16 } 17 18 access_log logs/tcp-access.log proxy ; 19 open_log_file_cache off; 20 21 upstream stream_backend01 { 22 hash $remote_addr consistent; 23 #server 172.18.31.2:80 weight=5; 24 server 172.18.31.2:80 max_fails=2 fail_timeout=30s; 25 #server 172.18.31.2:80 max_conns=3; 26 } 27 28 server { 29 listen 172.18.19.10:80; 30 proxy_timeout 20s; 31 proxy_pass stream_backend01; 32 } 33 34} 注意： 1、打出了tcp-access.log 2、根据源IP做hash，强制分配到后面的同一台服务器上，保证一致性 3、后端的server可以有权重，最大连接，以及失效检测（30s内无法连通2次，就摘掉这个服务器）\n同时调整vip-up.sh\n1cat /usr/local/bin/vip-up.sh 2 3#!/bin/sh 4/sbin/ip addr add ${2}/24 dev ${1} 5/sbin/ip neigh flush dev ${1} 6/export/servers/nginx/sbin/nginx 7/export/servers/nginx/sbin/nginx -s reload 谁获得了主ip 172.18.19.10，谁就会启动nginx，并且强制刷一下配置\n注意，一开始的时候，由于172.18.19.2没有获得主ip 172.18.19.10，所以上面是不会自动起nginx进程的！！！\n测试一下：\n1在172.18.19.1上面 2kill -usr2 ucarp的进程pid 3 4看172.18.19.2上面，nginx已经自动启动了 5然后访问 6curl http://172.18.19.10/ ok\n","date":"2023-12-14","img":"","permalink":"https://bajie.dev/posts/20231214-ucarp_nginx/","series":null,"tags":null,"title":"Ucarp和nginx提供内网vip"},{"categories":null,"content":"在k8s没有出来之前，我们用的就是LXC，古早版本了。\n跑的LXC，基于docker进行管理，网络部分独立。\n进化到新版本的Docker后，网络部分需要修改，我们决定采用macvlan方式扁平直接接入本地网络：\nDocker装好以后，系统中会出现一个Docker0的网络，用来NAT，我们不用这个\n12: eth0: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 2 link/ether 52:54:00:14:ee:63 brd ff:ff:ff:ff:ff:ff 3 inet 172.18.31.33/24 brd 172.18.31.255 scope global eth0 4 valid_lft forever preferred_lft forever 53: docker0: \u0026lt;NO-CARRIER,BROADCAST,MULTICAST,UP\u0026gt; mtu 1500 qdisc noqueue state DOWN group default 6 link/ether 02:42:ef:af:de:98 brd ff:ff:ff:ff:ff:ff 7 inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0 8 valid_lft forever preferred_lft forever 我们要做的是对eth0做macvlan，不做任何iptable，直接接入本地网络\n宿主机ip: 172.18.31.33 Docker1: 172.18.31.35\n一、建macvlan\n1docker network create -d macvlan \\ 2 --subnet=172.18.31.0/24 --gateway=172.18.31.254 \\ 3 -o parent=eth0 \\ 4 macvlan0 5 6docker network ls 7 8docker inspect macvlan0 会看到\n1... 2\u0026#34;Config\u0026#34;: [ 3 { 4 \u0026#34;Subnet\u0026#34;: \u0026#34;172.18.31.0/24\u0026#34;, 5 \u0026#34;Gateway\u0026#34;: \u0026#34;172.18.31.254\u0026#34; 6 } 7 ] 二、按指定ip建立容器\n1docker run --rm -dit --name=t35 --net macvlan0 --ip=172.18.31.35 alpine:latest ash 随后我们可以用docker exec取得shell\n1docker exec -it t35 ash 2ip a 3ping 172.18.31.35 4ping 172.18.31.2 注意：这个时候从宿主机ping容器 ping 172.18.31.35，或者是从容器ping宿主机 ping 172.18.31.33，都是不通的，这是macvlan的特性，我们必须做个操作来解决这个问题\n三、解决宿主机和容器之间的网络问题\n给eth0做个子接口，解决宿主机和容器之间的通讯问题\n1ip link add macvlan0-shim link eth0 type macvlan mode bridge 2ip addr add 172.18.31.34/32 dev macvlan0-shim 3ip link set macvlan0-shim up 4ip route add 172.18.31.0/24 dev macvlan0-shim 这样就可以通了\n注意，这样宿主机占了两个IP，eth0子接口的ip不能胡乱，必须跟独立ip同段。 浪费了一个IP，但是解决了宿主机和容器之间的网络问题。\n","date":"2023-12-14","img":"","permalink":"https://bajie.dev/posts/20231214-lxc_ip/","series":null,"tags":null,"title":"LXC更新到Docker之后的IP部分修改"},{"categories":null,"content":"在k8s没有出来之前，我们用的就是LXC，古早版本了。\n那时候是1：60的虚拟，一台物理机上跑60个LXC，居然这样运行了8年无异常，现在要升级一下了。\n那时候的LXC，存储空间无法单独设定（缺省10G），cpu和mem的limit也有这样那样的问题。\n进化到新版本的Docker后，存储部分也需要修改，我们采用overlay2：\n1docker info 2 3... 4 Storage Driver: overlay2 5 Backing Filesystem: xfs 6 Supports d_type: true 7 Native Overlay Diff: true 8... 缺省的就是overlay2和xfs\n一、修改boot内核启动参数\n1vi /etc/default/grub 2 3#加上rootflags=uquota,pquota 4GRUB_CMDLINE_LINUX=\u0026#34;console=tty0 crashkernel=auto net.ifnames=0 console=ttyS0 rootflags=uquota,pquota\u0026#34; 5 6#更新 7grub2-mkconfig -o /boot/grub2/grub.cfg 8 9#重启 10reboot 二、检验并启动容器\n1cat /proc/mounts |grep vda 2/dev/vda1 / xfs rw,relatime,attr2,inode64,usrquota,prjquota 0 0 有prjquota即可\n按需启动容器，指定空间大小，size=2G\n1docker run --rm -dit --name=t36 --storage-opt size=2G --net macvlan0 --ip=172.18.31.36 alpine:latest ash 2 3docker exec -it t36 ash 4df -h 这样就可以单独限制容器的大小了。\n","date":"2023-12-14","img":"","permalink":"https://bajie.dev/posts/20231214-lxc_storage/","series":null,"tags":null,"title":"LXC更新到Docker之后的存储部分修改"},{"categories":null,"content":"kvm生产的虚机内仍然可以再产虚拟机，这就是嵌套虚拟化\n我们的目的是要再kvm虚机中安装一套proxmox的系统\n首先在实体机上检查当前的Linux是否支持嵌套\n1Intel的CPU 2cat /sys/module/kvm_intel/parameters/nested 3 4AMD的CPU 5cat /sys/module/kvm_amd/parameters/nested ​\t修改支持，以intel为例：\n1vi /etc/modprobe.d/kvm.conf 2options kvm_intel nested=1 reboot就好\n不重启的话，可以先卸载模块，然后重新加载\n1modprobe -r kvm_intel 2modprobe kvm_intel 然后检查一下\n1cat /sys/module/kvm_intel/parameters/nested proxmox安装的时候，指定cpu=host：\n1#!/bin/sh 2qemu-img create -f qcow2 /export/kvm/proxmox-168-86-103/proxmox-168-86-103.qcow2 200G 3virt-install \\ 4 --name=proxmox-168-86-103 \\ 5 --cpu=host \\ 6 --ram=8192 \\ 7 --disk path=/export/kvm/proxmox-168-86-103/proxmox-168-86-103.qcow2,format=qcow2,size=200 \\ 8 --cdrom=/export/kvm/iso/proxmox-ve_7.0-2.iso \\ 9 --os-type=Linux \\ 10 --network bridge=br0 \\ 11 --vnc --vnclisten=0.0.0.0 --vncport=5903 ","date":"2023-12-14","img":"","permalink":"https://bajie.dev/posts/20231214-kvm_nested/","series":null,"tags":null,"title":"KVM的嵌套虚拟化"},{"categories":null,"content":"缺省情况下kvm会保留一个nat的网络，ip a命令查看，会看到virbr0和virbr0-nic\n18: virbr0: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UNKNOWN 2 link/ether 52:54:00:6c:22:2c brd ff:ff:ff:ff:ff:ff 3 inet 192.168.122.1/24 brd 192.168.122.255 scope global virbr0 49: virbr0-nic: \u0026lt;BROADCAST,MULTICAST\u0026gt; mtu 1500 qdisc noop state DOWN qlen 500 5 link/ether 52:54:00:6c:22:2c brd ff:ff:ff:ff:ff:ff 这个如果虚机都是静态IP，且不做NAT的话，则无必要保留，可以彻底删掉。\nkvm删除掉缺省网络\n1virsh net-destroy default 2virsh net-undefine default 那么如果非用不可，还需要给特定虚机用dhcp指定固定IP 方法如下：\n查看缺省网络\n1$ virsh net-list 2 Name State Autostart Persistent 3---------------------------------------------------------- 4 default active yes yes 找出虚机的mac地址\n1$ virsh dumpxml vis-16-10-33 | grep -i \u0026#39;\u0026lt;mac\u0026#39; 2 \u0026lt;mac address=\u0026#39;f0:00:ac:10:0a:21\u0026#39;/\u0026gt; 编辑网络\n1$ virsh net-edit default 2 3\u0026lt;network\u0026gt; 4 \u0026lt;name\u0026gt;default\u0026lt;/name\u0026gt; 5 \u0026lt;uuid\u0026gt;58e86841-ef4b-4d63-bf4f-7888515b8474\u0026lt;/uuid\u0026gt; 6 \u0026lt;forward mode=\u0026#39;nat\u0026#39;/\u0026gt; 7 \u0026lt;bridge name=\u0026#39;virbr0\u0026#39; stp=\u0026#39;on\u0026#39; delay=\u0026#39;0\u0026#39; /\u0026gt; 8 \u0026lt;mac address=\u0026#39;52:54:00:6C:22:2C\u0026#39;/\u0026gt; 9 \u0026lt;ip address=\u0026#39;192.168.122.1\u0026#39; netmask=\u0026#39;255.255.255.0\u0026#39;\u0026gt; 10 \u0026lt;dhcp\u0026gt; 11 \u0026lt;range start=\u0026#39;192.168.122.2\u0026#39; end=\u0026#39;192.168.122.254\u0026#39; /\u0026gt; 12 \u0026lt;/dhcp\u0026gt; 13 \u0026lt;/ip\u0026gt; 14\u0026lt;/network\u0026gt; 在range下面来一列\n1 \u0026lt;dhcp\u0026gt; 2 \u0026lt;range start=\u0026#39;192.168.122.2\u0026#39; end=\u0026#39;192.168.122.254\u0026#39; /\u0026gt; 3\t\u0026lt;host mac=\u0026#39;f0:00:ac:10:0a:21\u0026#39; name=\u0026#39;vis-16-10-33\u0026#39; ip=\u0026#39;192.168.122.10\u0026#39;/\u0026gt; 4 \u0026lt;/dhcp\u0026gt; 保存退出，重启网络\n1$ virsh net-destroy default 2$ virsh net-start default 然后就可以了。\n","date":"2023-12-14","img":"","permalink":"https://bajie.dev/posts/20231214-kvm_dhcp/","series":null,"tags":null,"title":"KVM网络如何设置DHCP"},{"categories":null,"content":"变量子串\n${var} 返回变量var的内容，单独使用时有没有{}一样，混合多个变量和常量时，用{}界定变量名\n${#var} 返回变量var内容的长度\n${var:offset} 从变量var中的偏移量offset开始截取到字符串结尾的子字符串，offset从0开始\n${var:offset:length} 从变量var中的偏移量offset开始截取长度为length的子字符串\n${var#*.} 从变量var中删除第一个匹配的点（.）及其左边的所有字符\n${var##*.} 从变量var中删除最后一个匹配的点（.）及其左边的所有字符\n${var%.*} 从变量var中删除最后一个匹配的点（.）及其右边的所有字符\n${var%%.*} 从变量var中删除第一个匹配的点（.）及其右边的所有字符\n示例：\n1var=file.txt.tar.gz 2${var#*.} #内容为\u0026#34;txt.tar.gz\u0026#34; 3${var##*.} #内容为\u0026#34;gz\u0026#34; 4${var%.*} #内容为\u0026#34;file.txt.tar\u0026#34; 5${var%%.*} #内容为\u0026#34;file\u0026#34; 也可以使用其它Pattern和表达式，示例：\n1var=/home/xxx/aaa/file.txt #假设xxx为当前用户 2 3# 从路径中获取文件名 4${var##*/} #内容为\u0026#34;file.txt\u0026#34; 5 6# 将绝对路径转为相对路径 7# whoami是获取当前用户名，使用$()执行子shell，$(whoami)将得到xxx 8~${var#*$(whoami)} #内容为\u0026#34;~/aaa/file.txt\u0026#34; ${var/pattern/string} 使用string代替第一个匹配的pattern\n${var//pattern/string} 使用string代替所有匹配的pattern\n${var,} 首字母转小写\n${var,,} 全部转小写\n${var^} 首字母转大写\n${var^^} 全部转大写\n特殊扩展变量\n${var-word} 如果变量var未赋值，则返回字符串word\n${var:-word} 如果变量var未赋值或者值为空，则返回字符串word\n${var+word} 如果变量var有值（包括空串\u0026quot;\u0026quot;），则返回字符串word\n1var1=foo 2var2= 3${var1-word} # 内容为\u0026#34;foo\u0026#34; 4echo ${var2-word} # 内容为\u0026#34;\u0026#34; 5echo ${var2:-word} # 内容为\u0026#34;word\u0026#34; ${var:+word} 如果变量var有值且不为空，则返回字符串word\n${var=word} 如果变量var未赋值，则返回字符串word，并为var赋值为字符串word\n${var:=word} 如果变量var未赋值或者值为空串，则返回字符串word，并为var赋值为字符串word\n${var?word} 如果变量var未赋值，将字符串word作为标准错误输出，否则返回变量var的值\n${var:?word} 如果变量var未赋值或者值为空串，将字符串word作为标准错误输出，否则返回变量var的值\n数组\narray=(1 2 3 a b c) 定义一个名为array的数组，包含了6个元素，元素字段类型不需要统一\n${array[index]} 访问数组中的元素，index从0开始，如果为负表示从数组的末尾开始的偏移量\n${array[*]} 获取数组中所有元素\n${array[@]} 获取数组中所有元素\n${#array[*]} 获取数组的长度\n${#array[@]} 获取数组的长度\n${!array[@]} 获取数组索引列表，返回 0 1 2 3 4 5\narray+=(4 d) 向数组中添加元素，数组内容为 1 2 3 a b c 4 d\nunset array[6] 删除第 7 个元素，数组内容为 1 2 3 a b c d\nunset array[-1] 删除倒数第 1 个元素，数组内容为 1 2 3 a b c\n多行字符串变量\n1var=$(cat \u0026lt;\u0026lt;- \u0026#39;EOF\u0026#39; 2line1 3line2 4... 5EOF) 或者使用单引号或者双引号（单引号中${}和$()等都不会取表达式的值，双引号中才会）：\n1var=\u0026#39;line1 2line2 3...\u0026#39; 注意上述两种写法的差别，避免为变量内容带来不必要的空行。通过循环可依次得到变量中的每一行：\n1while read -r line; do 2echo $line 3done \u0026lt;\u0026lt;\u0026lt; $var Shell系统变量\n$1 表示第一个参数，$2 表示第二个参数 \u0026hellip;\n$# 命令行参数的个数\n$0 当前Shell脚本程序的名称\n$? 前一个命令或函数的返回码\n$* 以 \u0026ldquo;参数1 参数2 \u0026hellip; \u0026quot; 形式获取所有参数\n$@ 以 \u0026ldquo;参数1\u0026rdquo; \u0026ldquo;参数2\u0026rdquo; \u0026hellip; 形式获取所有参数\n$$ 本程序的进程ID，即PID\n$! 上一个命令的PID\n$PPID 父进程的PID\n$UID 执行这个脚本的当前用户ID\n变量输出\n变量内容的输出使用 echo 命令。\n如果未使用 echo，则会将变量内容当成 Shell 命令来执行，常用于调用某个程序并传递参数，如：应用程序启动脚本。\n","date":"2023-12-13","img":"","permalink":"https://bajie.dev/posts/20231212-shell/","series":null,"tags":null,"title":"Shell中变量、字符串、数组、参数的技巧"},{"categories":null,"content":"我们openldap中用户和组的设置\n1用户： 2ou=People,dc=ddky,dc=com 3#uid;#givenName;#sn;#uidNumber;#gidNumber 4 5组： 6ou=group,dc=ddky,dc=com 7#cn;#gidNumber;#memberUID;#description 到librenms中， 172.18.31.10\n1cd /opt/librenms 2vi config.php 3$config[\u0026#39;auth_mechanism\u0026#39;] = \u0026#39;ldap\u0026#39;; 4$config[\u0026#39;auth_ldap_server\u0026#39;] = \u0026#39;172.18.31.27\u0026#39;; 5$config[\u0026#39;auth_ldap_port\u0026#39;] = 389; 6$config[\u0026#39;auth_ldap_starttls\u0026#39;] = False; // Disable TLS on port 389 7$config[\u0026#39;auth_ldap_binddn\u0026#39;] = \u0026#39;cn=admin,dc=ddky,dc=com\u0026#39;; // overrides binduser 8$config[\u0026#39;auth_ldap_bindpassword\u0026#39;] = \u0026#39;nishiwode\u0026#39;; 9$config[\u0026#39;auth_ldap_prefix\u0026#39;] = \u0026#39;cn=\u0026#39;; 10$config[\u0026#39;auth_ldap_suffix\u0026#39;] = \u0026#39;,ou=People,dc=ddky,dc=com\u0026#39;; // appended to usernames 11$config[\u0026#39;auth_ldap_groupbase\u0026#39;] = \u0026#39;ou=group,dc=ddky,dc=com\u0026#39;; // all groups must be inside this 12$config[\u0026#39;auth_ldap_groups\u0026#39;][\u0026#39;admins\u0026#39;][\u0026#39;level\u0026#39;] = 10; // set admins group to admin level 13$config[\u0026#39;auth_ldap_groups\u0026#39;][\u0026#39;pfy\u0026#39;][\u0026#39;level\u0026#39;] = 5; // set pfy group to global read only level 14$config[\u0026#39;auth_ldap_groups\u0026#39;][\u0026#39;support\u0026#39;][\u0026#39;level\u0026#39;] = 1; // set support group as a normal user 15$config[\u0026#39;auth_ldap_debug\u0026#39;] = false; // enable for verbose debug messages 说明： 我们的openldap因为是内部使用，所以无法设置证书，TLS是被禁止的。 openldap是禁止anonymous用户查询的，所以需要设置binddn和bindpassword 实际用户是cn=zhangranrui,ou=People,dc=ddky,dc=com，所以要设prefix librenms缺省用户有三个级别，10 5 1，对应的用户组是admins pfy support\n如果要对接其他系统，也许都需要如法炮制\n","date":"2023-12-13","img":"","permalink":"https://bajie.dev/posts/20231213-librenms_ldap/","series":null,"tags":null,"title":"Librenms使用ldap认证用户"},{"categories":null,"content":"librenms是个非常强悍的工具，对网络不清楚的可以透过这个工具，对网络环境有清晰的了解。\n如何通过prometheus对librenms进行集成呢？\n一、装pushgateway\n1wget https://github.com/prometheus/pushgateway/releases/download/v1.2.0/pushgateway-1.2.0.linux-amd64.tar.gz 2 3把pushgateway放到/usr/local/bin 4 5cat \u0026lt;\u0026lt; EOF \u0026gt;\u0026gt;/etc/systemd/system/pushgateway.service 6[Unit] 7Description=Codis Exporter 8Wants=network-online.target 9After=network-online.target 10 11[Service] 12Type=simple 13ExecStart=/usr/local/bin/pushgateway --web.listen-address=:50004 14 15[Install] 16WantedBy=multi-user.target 17EOF 18 19systemctl daemon-relaod 20systemctl enable --now pushgateway.service 二、配置librenms\n1cd /opt/librenms 2vi config.php 3$config[\u0026#39;prometheus\u0026#39;][\u0026#39;enable\u0026#39;] = true; 4$config[\u0026#39;prometheus\u0026#39;][\u0026#39;url\u0026#39;] = \u0026#39;http://127.0.0.1:50004\u0026#39;; 5$config[\u0026#39;prometheus\u0026#39;][\u0026#39;job\u0026#39;] = \u0026#39;librenms\u0026#39;; # Optional 三、配置prometheus\n1- job_name: \u0026#39;librenms\u0026#39; 2 scrape_interval: 300s 3 honor_labels: true 4 static_configs: 5 - targets: [\u0026#39;172.18.31.10:50004\u0026#39;] 6 labels: 7 sms_to: \u0026#39;18618197196\u0026#39; 首先访问prometheus，http://172.18.31.8:9090/targets\n这样弄好后，再访问 http://172.18.31.10:50004/metrics\n里面的东西是十分癫狂的，从交换机端口到F5，应有尽有\n东西收进prometheus后，可以通过grafana对各种指标进行画图和报警\n","date":"2023-12-13","img":"","permalink":"https://bajie.dev/posts/20231213-librenms_prometheus/","series":null,"tags":null,"title":"Librenms集成进prometheus"},{"categories":null,"content":"librenms对密码的强度有要求，必须使用非常复杂的密码策略才能满足要求. 这就鬼畜了，想改成至少能记得住的密码。\n如何强制修改呢？\n首先弄一段php程序来生成加密串：\n\u0026lt;?php\recho password_hash(\u0026quot;xxxxxxxx\u0026quot;, PASSWORD_DEFAULT);\r?\u0026gt;\r运行后得到加密的字符串：\n$2y$10$EUvwg5kVGXQUPBw4obdPJ.AKCSVgu8ximyyoFKW7hXed0s.sh/ud6\r或者直接到下面的网站来生成：\nhttps://phppasswordhash.com/ 然后登录librenms的机器, 172.18.31.10\nmysql -uroot -p\ruse librenms;\rselect * from users;\rupdate users set password='$2y$10$EUvwg5kVGXQUPBw4obdPJ.AKCSVgu8ximyyoFKW7hXed0s.sh/ud6' where user_id=1;\r这样就强制把密码修改成自己习惯的了。\n","date":"2023-12-13","img":"","permalink":"https://bajie.dev/posts/20231213-librenms_password/","series":null,"tags":null,"title":"Librenms强制修改密码"},{"categories":null,"content":"数据库管理员有个特殊的需求：\n需要一个msyql客户端，可以定时去连接mysql服务器执行SQL语句。且可以自己动态编辑SQL语句。\n找来找去，数据库管理员对spring java这类东西吧一无所知，但是通晓SQL，这种情形XXL-JOB的EXECUTOR代位执行就比较适合。\n主控地址：http://172.18.31.13/xxl-job-admin/\n使用很简单，我们在172.18.31.14装一个被控端的executor，运行模式选择GLUE(Java)模式，GLUE模式就可以随便编辑 JAVA+SQL源代码了。设置执行频率是每30分钟执行一次。\n选择GLUE模式后，我们就可以在GLUE IDE里编辑代码：\n代码如下： 注：在172.18.31.14上面已经装了一个MySQL服务端，有一个user库。\n1package com.xxl.job.service.handler; 2 3import com.xxl.job.core.context.XxlJobHelper; 4import com.xxl.job.core.handler.IJobHandler; 5import org.springframework.jdbc.core.JdbcTemplate; 6import org.springframework.jdbc.datasource.DriverManagerDataSource; 7 8public class DemoGlueJobHandler extends IJobHandler { 9 10\t@Override 11\tpublic void execute() throws Exception { 12 13 DriverManagerDataSource ds = new DriverManagerDataSource(); 14 ds.setDriverClassName(\u0026#34;com.mysql.jdbc.Driver\u0026#34;); 15 ds.setUrl(\u0026#34;jdbc:mysql://localhost:3306/users?characterEncoding=UTF-8\u0026amp;useSSL=false\u0026#34;); 16 ds.setUsername(\u0026#34;root\u0026#34;); 17 ds.setPassword(\u0026#34;xxxxxxxx\u0026#34;); 18 19 JdbcTemplate jdbcTemplate = new JdbcTemplate(); 20 jdbcTemplate.setDataSource(ds); 21 22 23 List rows = jdbcTemplate.queryForList(\u0026#34;select * from user\u0026#34;); 24 Iterator it = rows.iterator(); 25 while(it.hasNext()) { 26 Map userMap = (Map) it.next(); 27 XxlJobHelper.log(userMap.get(\u0026#34;id\u0026#34;) + \u0026#34;\\t\u0026#34;); 28 XxlJobHelper.log(userMap.get(\u0026#34;name\u0026#34;) + \u0026#34;\\t\u0026#34;); 29 } 30 31 XxlJobHelper.log(\u0026#34;XXL-JOB guning, Hello World.\u0026#34;); 32\t} 33 34 35} 为了实现mysql客户端可以对MySQL进行读写，我们需要在xxl-job的xxl-job-executor-sample-springboot示例项目中，pom.xml中增加spring-jdbc和mysql-connector-java的jar包部分。打出一个xxl-job的executor的执行端程序。\n代码在 /root/xxl-job-2.3.0/xxl-job-executor-samples/xxl-job-executor-sample-springboot目录下，用mvn package就可以打出xxl-job-executor-sample-springboot-2.3.0.jar包。\n1 \u0026lt;dependency\u0026gt; 2 \u0026lt;groupId\u0026gt;org.springframework\u0026lt;/groupId\u0026gt; 3 \u0026lt;artifactId\u0026gt;spring-jdbc\u0026lt;/artifactId\u0026gt; 4 \u0026lt;version\u0026gt;4.3.2.RELEASE\u0026lt;/version\u0026gt; 5 \u0026lt;/dependency\u0026gt; 6 \u0026lt;dependency\u0026gt; 7 \u0026lt;groupId\u0026gt;mysql\u0026lt;/groupId\u0026gt; 8 \u0026lt;artifactId\u0026gt;mysql-connector-java\u0026lt;/artifactId\u0026gt; 9 \u0026lt;version\u0026gt;${mysql-connector-java.version}\u0026lt;/version\u0026gt; 10 \u0026lt;/dependency\u0026gt; 启动agent的systemd服务脚本，/etc/systemd/system/xxljob-agent.service\n1[Unit] 2After=network.target 3Wants=network.target 4 5[Service] 6WorkingDirectory=/usr/local/bin 7Type=simple 8ExecStart=/usr/bin/java -jar /usr/local/bin/xxl-job-executor-sample-springboot-2.3.0.jar 9Restart=on-failure 10 11[Install] 12WantedBy=multi-user.target 这样就OK了。\n","date":"2023-12-12","img":"","permalink":"https://bajie.dev/posts/20231212-xxl-job/","series":null,"tags":null,"title":"Xxl-Job的动态编辑并执行java脚本"},{"categories":null,"content":"证书会经常面临过期而没有及时续费的情况，写个脚本提醒一下自己吧：\ncrontab -l\n0 8 * * * /usr/local/bin/check_ssl.sh www.ddky.com\rcheck_ssl.sh的内容：\n1#!/bin/bash 2# Print the number of days till certificate expiration 3# 4# Example: 5# $ check_cert.sh sleeplessbeastie.eu 6# 81 7# $ check_cert.sh lwn.net 8# 630 9# 10# Exit codes: 11# 0 - certificate is not expired 12# 1 - certificate is expired 13# 254 - certificate is empty 14# 255 - DNS resolution failed 15# 16 17# temporary file to store certificate 18certificate_file=$(mktemp) 19 20# delete temporary file on exit 21trap \u0026#34;unlink $certificate_file\u0026#34; EXIT 22 23if [ \u0026#34;$#\u0026#34; -eq \u0026#34;1\u0026#34; ]; then 24 website=\u0026#34;$1\u0026#34; 25 host \u0026#34;$website\u0026#34; \u0026gt;\u0026amp;- 26 if [ \u0026#34;$?\u0026#34; -eq \u0026#34;0\u0026#34; ]; then 27 echo -n | openssl s_client -servername \u0026#34;$website\u0026#34; -connect \u0026#34;$website\u0026#34;:443 2\u0026gt;/dev/null | sed -ne \u0026#39;/-BEGIN CERTIFICATE-/,/-END CERTIFICATE-/p\u0026#39; \u0026gt; $certificate_file 28 certificate_size=$(stat -c \u0026#34;%s\u0026#34; $certificate_file) 29 if [ \u0026#34;$certificate_size\u0026#34; -gt \u0026#34;1\u0026#34; ]; then 30 date=$(openssl x509 -in $certificate_file -enddate -noout | sed \u0026#34;s/.*=\\(.*\\)/\\1/\u0026#34;) 31 date_s=$(date -d \u0026#34;${date}\u0026#34; +%s) 32 now_s=$(date -d now +%s) 33 date_diff=$(( (date_s - now_s) / 86400 )) 34 echo \u0026#34;$date_diff\u0026#34; 35 if [ \u0026#34;$date_diff\u0026#34; -le 37 ]; then 36 /usr/local/bin/mailsend -q -to \u0026#34;zhangranrui@ddky.com\u0026#34; -from monit@ddky.com -ssl -port 465 -auth -auth-plan -smtp smtp.exmail.qq.com -sub \u0026#34;证书就要到期了\u0026#34; -v -user \u0026#34;monit@ddky.com\u0026#34; -pass \u0026#34;xxxxxxxx\u0026#34; -cs \u0026#34;utf-8\u0026#34; -enc-type \u0026#34;base64\u0026#34; -M \u0026#34;$website 还有 $date_diff 天就要到期了！！！\u0026#34; 37 fi 38 if [ \u0026#34;$date_s\u0026#34; -gt \u0026#34;$now_s\u0026#34; ]; then 39 exit 0 # ok 40 else 41 exit 1 # not ok 42 fi 43 else 44 exit 254 45 fi 46 else 47 exit 255 48 fi 49fi ","date":"2023-12-12","img":"","permalink":"https://bajie.dev/posts/20231212-check_ssl/","series":null,"tags":null,"title":"检查证书是否过期的脚本"},{"categories":null,"content":"数据库管理员的 172.18.20.10 和 172.18.20.25 数据库备份脚本是以 root 身份运行的，在 crontab 里跑：\n126 11 * * * /root/scripts/mysql_backup_full_3306.sh \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 但是由于 root 密码会每三个月变更一次，如果没有及时变更，会导致 root 密码失效，从而 crontab 无法正常运行。\n解决方法很简单： 找到 /etc/pam.d/password-auth , 其中 account 的有四行\n1account required pam_unix.so 2account sufficient pam_localuser.so 3account sufficient pam_succeed_if.so uid \u0026lt; 1000 quiet 4account required pam_permit.so 在前面增加两行：\n1account required pam_access.so 2account [success=1 default=ignore] pam_succeed_if.so service in crond quiet use_uid 3 4account required pam_unix.so 5account sufficient pam_localuser.so 6account sufficient pam_succeed_if.so uid \u0026lt; 1000 quiet 7account required pam_permit.so 这样就可以了，不用重启任何服务。\npam 验证的时候，即使密码过期，crond 依然正常跑，就 ok 了。\n","date":"2023-12-11","img":"","permalink":"https://bajie.dev/posts/20231211-crontab_root/","series":null,"tags":null,"title":"Root的crontab由于root密码失效导致不能正常工作"},{"categories":null,"content":"Rsyslog的模板文件按日期存放：\n1$template 10.161.54.11,\u0026#34;/var/log/rsyslog/%fromhost-ip%/netflow_%$YEAR%-%$MONTH%-%$DAY%.log\u0026#34; 2$template 10.161.50.5,\u0026#34;/var/log/rsyslog/%fromhost-ip%/xdns_webeng_%$YEAR%-%$MONTH%-%$DAY%.log\u0026#34; 3$template 10.161.50.7,\u0026#34;/var/log/rsyslog/%fromhost-ip%/xdns_webeng_%$YEAR%-%$MONTH%-%$DAY%.log\u0026#34; 4 5#从特定ip来的日志发到特定rsyslog服务器上去 6#:fromhost-ip, !isequal, \u0026#34;127.0.0.1\u0026#34; ?Remote 7:fromhost-ip, isequal, \u0026#34;10.161.54.11\u0026#34; ?10.161.54.11 8:fromhost-ip, isequal, \u0026#34;10.161.50.5\u0026#34; ?10.161.50.5 9:fromhost-ip, isequal, \u0026#34;10.161.50.7\u0026#34; ?10.161.50.7 Rsyslog打出所有调试信息：\n*.* /var/log/debugfmt;RSYSLOG_DebugFormat\n调试信息:\n1FROMHOST: \u0026#39;172.18.18.9\u0026#39;, fromhost-ip: \u0026#39;172.18.18.9\u0026#39;, HOSTNAME: \u0026#39;172.18.18.9\u0026#39;, PRI: 5, 2syslogtag \u0026#39;time:\u0026#39;, programname: \u0026#39;time\u0026#39;, APP-NAME: \u0026#39;time\u0026#39;, PROCID: \u0026#39;-\u0026#39;, MSGID: \u0026#39;-\u0026#39;, 3TIMESTAMP: \u0026#39;Mar 4 09:04:45\u0026#39;, STRUCTURED-DATA: \u0026#39;-\u0026#39;, 4msg: \u0026#39;2021-03-04 09:04:45;danger_degree:1;breaking_sighn:0;event:[50556]MySQL登录认证成功;src_addr:172.18.5.65;src_port:57953;dst_addr:172.18.20.52;dst_port:3306;user:;smt_user:;proto:MYSQL\u0026#39; 5escaped msg: \u0026#39;2021-03-04 09:04:45;danger_degree:1;breaking_sighn:0;event:[50556]MySQL登录认证成功;src_addr:172.18.5.65;src_port:57953;dst_addr:172.18.20.52;dst_port:3306;user:;smt_user:;proto:MYSQL\u0026#39; 6inputname: imudp rawmsg: \u0026#39;\u0026lt;5\u0026gt;time:2021-03-04 09:04:45;danger_degree:1;breaking_sighn:0;event:[50556]MySQL登录认证成功;src_addr:172.18.5.65;src_port:57953;dst_addr:172.18.20.52;dst_port:3306;user:;smt_user:;proto:MYSQL\u0026#39; 7$!: 8$.: 9$/: Rsyslog的isequal，不建议，建议用==\nif $fromhost isequal 172.18.18.9 then /var/log/nips.log\n1if $fromhost-ip == \u0026#39;172.18.18.9\u0026#39; then { 2 action(type=\u0026#34;ommysql\u0026#34; server=\u0026#34;localhost\u0026#34; db=\u0026#34;Syslog\u0026#34; uid=\u0026#34;nips\u0026#34; pwd=\u0026#34;xxxxxxxx\u0026#34;) 3} Rsyslog的ommysql用法\n1$ModLoad ommysql 2 3*.info;mail.none;authpriv.none;cron.none :ommysql:localhost,Syslog,nips,xxxxxxxx 4 5*.info;mail.none;authpriv.none;cron.none action(type=\u0026#34;ommysql\u0026#34; server=\u0026#34;localhost\u0026#34; db=\u0026#34;Syslog\u0026#34; uid=\u0026#34;nips\u0026#34; pwd=\u0026#34;xxxxxxxx\u0026#34;) 6 7$template dbFormat,\u0026#34;insert into SystemEvents (Message, Facility,FromHost, Priority, DeviceReportedTime, ReceivedAt, InfoUnitID, SysLogTag) values (\u0026#39;%msg%\u0026#39;, %syslogfacility%, \u0026#39;%fromhost-ip%\u0026#39;,%syslogpriority%, \u0026#39;%timereported:::date-mysql%\u0026#39;, \u0026#39;%timegenerated:::date-mysql%\u0026#39;, %iut%, \u0026#39;%syslogtag%\u0026#39;)\u0026#34;,sql 8action(type=\u0026#34;ommysql\u0026#34; server=\u0026#34;localhost\u0026#34; serverport=\u0026#34;3306\u0026#34; db=\u0026#34;Syslog\u0026#34; uid=\u0026#34;nips\u0026#34; pwd=\u0026#34;xxxxxxxx\u0026#34; template=\u0026#34;dbFormat\u0026#34;) 9 10#172.18.31.34上的实际用法： 11if $fromhost-ip == \u0026#39;172.18.18.9\u0026#39; then { 12 if $syslogpriority == 7 then { 13 $template dbFormat1,\u0026#34;insert into SystemEvents (Message, Facility,FromHost, Priority, DeviceReportedTime, ReceivedAt, InfoUnitID, SysLogTag) values (\u0026#39;%msg%\u0026#39;, 10, \u0026#39;%fromhost-ip%\u0026#39;,%syslogpriority%, \u0026#39;%timereported:::date-mysql%\u0026#39;, \u0026#39;%timegenerated:::date-mysql%\u0026#39;, %iut%, \u0026#39;nips\u0026#39;)\u0026#34;,sql 14 action(type=\u0026#34;ommysql\u0026#34; server=\u0026#34;localhost\u0026#34; serverport=\u0026#34;3306\u0026#34; db=\u0026#34;Syslog\u0026#34; uid=\u0026#34;nips\u0026#34; pwd=\u0026#34;xxxxxxxx\u0026#34; template=\u0026#34;dbFormat1\u0026#34;) 15 } 16 else { 17 $template dbFormat2,\u0026#34;insert into SystemEvents (Message, Facility,FromHost, Priority, DeviceReportedTime, ReceivedAt, InfoUnitID, SysLogTag) values (\u0026#39;time:%msg%\u0026#39;, 10, \u0026#39;%fromhost-ip%\u0026#39;,%syslogpriority%, \u0026#39;%timereported:::date-mysql%\u0026#39;, \u0026#39;%timegenerated:::date-mysql%\u0026#39;, %iut%, \u0026#39;nips\u0026#39;)\u0026#34;,sql 18 action(type=\u0026#34;ommysql\u0026#34; server=\u0026#34;localhost\u0026#34; serverport=\u0026#34;3306\u0026#34; db=\u0026#34;Syslog\u0026#34; uid=\u0026#34;nips\u0026#34; pwd=\u0026#34;xxxxxxxx\u0026#34; template=\u0026#34;dbFormat2\u0026#34;) 19 } 20} 21\u0026amp; ~ Rsyslog中\u0026amp; ~的用法\n1:fromhost-ip,startswith,’192.168.1.’ /var/log/remote-devs.log 2\u0026amp; ~ (The next line (“\u0026amp; ~”) is important: it tells rsyslog to stop processing the message after it was written to the log. As such, these messages will not reach the local part. Without that “\u0026amp; ~”, messages would also be written to the local files.\nFacility的级别： Serverity的级别Serverity的级别 ","date":"2023-12-11","img":"","permalink":"https://bajie.dev/posts/20231211-rsyslog_usage/","series":null,"tags":null,"title":"Rsyslog的一些特殊用法"},{"categories":null,"content":"这个其实是网络工程师的工作，有以下两种方法：\n一、用ssh备份Cisco设备的脚本 需要事先在/root/.ssh/config配置好直接登录，并且在Cisco设备里设置好权限级别，可以执行show run\n1#!/bin/sh 2sshcmd=\u0026#34;ssh -o LogLevel=quiet\u0026#34; 3$sshcmd $1 \u0026#34;show run\u0026#34; \u0026gt; /root/backup/$1-$(date \u0026#39;+%Y%m%d\u0026#39;).txt 二、备份Cisco设备的Python脚本，这个可控性更高： 首选需要在/export/servers/python363装好python, pip install netmiko 其次，在路由器上可以配置en的密码\n1#!/export/servers/python363/bin/python3.6 2 3from netmiko import Netmiko 4import time 5 6tw_bgp = { 7 \u0026#34;device_type\u0026#34;: \u0026#34;cisco_ios\u0026#34;, 8 \u0026#34;host\u0026#34;: \u0026#34;tw-bgp\u0026#34;, 9 \u0026#34;ip\u0026#34;: \u0026#34;192.168.1.10\u0026#34;, 10 \u0026#34;username\u0026#34;: \u0026#34;noc\u0026#34;, 11 \u0026#34;use_keys\u0026#34;: True, 12 \u0026#34;secret\u0026#34; : \u0026#34;xxxxxxxx\u0026#34;, 13 \u0026#34;key_file\u0026#34;: \u0026#34;/root/.ssh/id_jump_rsa_new\u0026#34;, 14} 15 16tw_r1_e1 = { 17 \u0026#34;device_type\u0026#34;: \u0026#34;cisco_ios\u0026#34;, 18 \u0026#34;host\u0026#34;: \u0026#34;tw-r1-e1\u0026#34;, 19 \u0026#34;ip\u0026#34;: \u0026#34;192.168.1.11\u0026#34;, 20 \u0026#34;username\u0026#34;: \u0026#34;noc\u0026#34;, 21 \u0026#34;use_keys\u0026#34;: True, 22 \u0026#34;key_file\u0026#34;: \u0026#34;/root/.ssh/id_jump_rsa_new\u0026#34;, 23} 24 25tw_r1_e2 = { 26 \u0026#34;device_type\u0026#34;: \u0026#34;cisco_ios\u0026#34;, 27 \u0026#34;host\u0026#34;: \u0026#34;tw-r1-e2\u0026#34;, 28 \u0026#34;ip\u0026#34;: \u0026#34;192.168.1.12\u0026#34;, 29 \u0026#34;username\u0026#34;: \u0026#34;noc\u0026#34;, 30 \u0026#34;use_keys\u0026#34;: True, 31 \u0026#34;key_file\u0026#34;: \u0026#34;/root/.ssh/id_jump_rsa_new\u0026#34;, 32} 33 34tw_r2_e1 = { 35 \u0026#34;device_type\u0026#34;: \u0026#34;cisco_ios\u0026#34;, 36 \u0026#34;host\u0026#34;: \u0026#34;tw-r2-e1\u0026#34;, 37 \u0026#34;ip\u0026#34;: \u0026#34;192.168.1.13\u0026#34;, 38 \u0026#34;username\u0026#34;: \u0026#34;noc\u0026#34;, 39 \u0026#34;use_keys\u0026#34;: True, 40 \u0026#34;key_file\u0026#34;: \u0026#34;/root/.ssh/id_jump_rsa_new\u0026#34;, 41} 42 43tw_r2_e2 = { 44 \u0026#34;device_type\u0026#34;: \u0026#34;cisco_ios\u0026#34;, 45 \u0026#34;host\u0026#34;: \u0026#34;tw-r2-e2\u0026#34;, 46 \u0026#34;ip\u0026#34;: \u0026#34;192.168.1.14\u0026#34;, 47 \u0026#34;username\u0026#34;: \u0026#34;noc\u0026#34;, 48 \u0026#34;use_keys\u0026#34;: True, 49 \u0026#34;key_file\u0026#34;: \u0026#34;/root/.ssh/id_jump_rsa_new\u0026#34;, 50} 51 52devices=[tw_bgp, tw_r1_e1, tw_r1_e2, tw_r2_e1, tw_r2_e2] 53 54for dev in devices: 55 name = dev[\u0026#34;ip\u0026#34;] 56 connection = Netmiko(**dev) 57 connection.enable() 58 out = connection.send_command(\u0026#34;show running-config\u0026#34;) 59 calender = time.strftime(\u0026#34;%Y%m%d\u0026#34;) 60 file_name = \u0026#39;{}-{}.txt\u0026#39;.format(dev[\u0026#34;host\u0026#34;],calender) 61 file = open(file_name ,\u0026#34;w\u0026#34;) 62 file.write(out) 63 file.close() 64 connection.disconnect() 65 print(\u0026#34;BACKUP for %s done\u0026#34; %dev[\u0026#34;host\u0026#34;]) ","date":"2023-12-11","img":"","permalink":"https://bajie.dev/posts/20231211-netdev_backup/","series":null,"tags":null,"title":"网络设备配置的备份"},{"categories":null,"content":"用pxe远程启动一个iscsi卷的方法已经会了的话。\n如果我们要批量产新虚机，最快的方法应该是把远程的iscsi卷clone一下，供新的虚机用，方法如下：\n在172.18.30.18上操作\n查看一下，原来有两个LV（逻辑卷）\n# lvdisplay\r那就把pvc-vis-18-31-48的lvm虚机卷，先copy做个mirror，-b参数表示后台运行。\n# lvconvert --type mirror --alloc anywhere -m1 /dev/vg-targetd/pvc-vis-18-31-48 -b\rLogical volume vg-targetd/pvc-vis-18-31-48 converted.\r提示一下就运行完毕了，虚假啊，后台正在运行同步信息：\nlvs -a -o +devices | egrep \u0026quot;LV|48\u0026quot;\r看那个Cpy\u0026amp;Sync，那个是进度条，才6.91，务必等走到100，再进行后续操作\n然后破开这个mirror，把副本命名为vis-18-31-49\n#lvconvert --splitmirrors 1 --name vis-18-31-49 /dev/vg-targetd/pvc-vis-18-31-48\rLogical volume vg-targetd/pvc-vis-18-31-48 converted.\r再运行 lvdisplay\n多出一个逻辑卷pvc-vis-18-31-49，之后我们就可以用这个新卷建立iscsi对象了。\n这个卷装的Linux是Centos 7 last，网卡是dhcp，可以当模板复用。\n","date":"2023-12-08","img":"","permalink":"https://bajie.dev/posts/20231208-iscsi_lv_clone/","series":null,"tags":null,"title":"Lvm卷的clone方法"},{"categories":null,"content":"问题说明： 1、研发同事反馈应用程序上传一张图片耗时6分钟，让我们排查一下是什么问题？\n1[INFO] 2022-03-01 17:16:38.295 [vu73wwv64ba4ncfzaiiou2qsem] [DubboServerHandler-172.18.32.254:20881-thread-2400] [FtpFile.java:854:uploadFileToPath()] uploadFileToPath path://c/product/590389/display/,fileName:display.jpg, cost:383667 ms 显示上传一张图片花了383667ms，没道理啊，这事自建的vsftp服务器，也没往oss桶里放东西的。\n问题排查： 1、登录到30.18上面查看vsftpd日志信息，过滤display.jpg的信息，发现上传信息是正常的。\n2、查看prometheus监控各项指标都很正常。唯有那个点的io比平时高一点，什么原因导致未知.\n3、查看vsftpd的全部日志信息，发现ftp每到一个目录就执行一个list操作，由于product目录下面文件很多，所以过了6分钟才相应结果，这就是导致ftp慢的原因。 [图片] 4、找研发部门验证，终于发现ftp公共组件，代码里确实有这个list操作。 总结：开发病得不清，判断目录是否存在居然每次都刷一下所有文件，太弱智了。\n","date":"2023-12-08","img":"","permalink":"https://bajie.dev/posts/20231208-ftp_slowly/","series":null,"tags":null,"title":"开发投诉FTP慢问题的解决"},{"categories":null,"content":"公司使用智齿的机器崩了，要做数据迁移。\n由三台老机器迁移到三台新kvm机器\n我们需要把其中一台34.38的的东西实时同步两份，一份到30.18的glusterfs，一份到34.41的/data\n同时也要注意30.18的GFS中已经有两个虚机文件，32.6和34.38的qcow2\n所以lsync务必要小心，不能删除已有文件\n172.18.34.38上面的lsyncd.conf如下，同步到两个目的地： 注意下面的参数：\nmaxProcesses = 2 # 本机用于rsync的进程数\ndelete = \u0026lsquo;running\u0026rsquo; # 只删除lsync启动之后删除的文件，目的文件夹中原有的文件保存\nexclude = \u0026ldquo;upload\u0026rdquo; # 第二个同步中排除的目录，注意这里是匹配全路径中的部分字串，upload 可以匹配到 /data/new/chatmsg/upload/allajl.jpg，就upload目录下文件大，所以把它排除。**这里的规则是和rsync中exclude的写法不同的！！！**只取路径中的upload字串就可以排除。\n1---- 2-- User configuration file for lsyncd. 3-- 4-- Simple example for default rsync, but executing moves through on the target. 5-- 6-- For more examples, see /usr/share/doc/lsyncd*/examples/ 7-- 8-- sync{default.rsyncssh, source=\u0026#34;/var/www/html\u0026#34;, host=\u0026#34;localhost\u0026#34;, targetdir=\u0026#34;/tmp/htmlcopy/\u0026#34;} 9 10settings { 11 logfile = \u0026#34;/var/log/lsyncd/lsyncd.log\u0026#34;, 12 statusFile = \u0026#34;/var/log/lsyncd/lsyncd-status.log\u0026#34;, 13 statusInterval = 5, 14 maxProcesses = 2 15} 16 17sync { 18 default.rsync, 19 source = \u0026#34;/data/new\u0026#34;, 20 target = \u0026#34;172.18.30.18::new\u0026#34;, 21 delete = \u0026#39;running\u0026#39;, 22 delay = 5, 23 rsync = { 24 binary = \u0026#34;/usr/bin/rsync\u0026#34;, 25 archive = true, 26 compress = false, 27 verbose = true 28 } 29} 30 31sync { 32 default.rsync, 33 source = \u0026#34;/data\u0026#34;, 34 target = \u0026#34;172.18.34.41::new\u0026#34;, 35 delete = \u0026#39;running\u0026#39;, 36 exclude = \u0026#34;upload\u0026#34;, 37 delay = 5, 38 rsync = { 39 binary = \u0026#34;/usr/bin/rsync\u0026#34;, 40 archive = true, 41 compress = false, 42 verbose = true 43 } 44} 对端rsyncd的配置如下：\n1# cat /etc/rsyncd.conf 2# /etc/rsyncd: configuration file for rsync daemon mode 3 4# See rsyncd.conf man page for more options. 5 6# configuration example: 7 8# uid = nobody 9# gid = nobody 10# use chroot = yes 11# max connections = 4 12# pid file = /var/run/rsyncd.pid 13# exclude = lost+found/ 14# transfer logging = yes 15# timeout = 900 16# ignore nonreadable = yes 17# dont compress = *.gz *.tgz *.zip *.z *.Z *.rpm *.deb *.bz2 18 19# [ftp] 20# path = /home/ftp 21# comment = ftp export area 22uid = root 23gid = root 24use chroot = no 25max connections = 200 26timeout = 300 27pid file = /var/run/rsyncd.pid 28lock file = /var/run/rsync.lock 29log file = /var/log/rsyncd.log 30 31[new] 32path = /root 33ignore errors 34read only = false 35list = false 36hosts allow = 172.18.34.38 37hosts deny = 0.0.0.0/32 这是个极端强力的工具，墙裂推荐！。\n","date":"2023-12-08","img":"","permalink":"https://bajie.dev/posts/20231208-lsync/","series":null,"tags":null,"title":"Lsync 同步软件的运用"},{"categories":null,"content":"审计的需求，需要定期备份文件，为了安全起见，这些压缩包需要加密保存，恢复的时候需要密码才能恢复：\n把目录 20231224压缩进加密包：\n1tar -zcvf - 20231224|openssl des3 -salt -k \u0026lt;password\u0026gt; | dd of=/backup/cdrom/20231224.tar.gz.des3 解压：\n1dd if=/backup/cdrom/20231224.tar.gz.des3 |openssl des3 -d -k \u0026lt;password\u0026gt;|tar zxf - ","date":"2023-12-08","img":"","permalink":"https://bajie.dev/posts/20231208-encrypt_tar/","series":null,"tags":null,"title":"Linux加密压缩tar包"},{"categories":null,"content":"在今年微软的挑战中拿了两张免费考卷，考了AZ-104和AZ-305，捞了一张专家证书，今年的考试到头了。\n明年用免费azure考试抵扣税，真是一件好事。\n","date":"2023-09-21","img":"","permalink":"https://bajie.dev/posts/20230921-azure_certificate/","series":null,"tags":null,"title":"今年下半年的两张Azure认证"},{"categories":null,"content":"在今年微软的挑战中拿了两张免费考卷，一张是115$，怕浪费啊，就分别考了AZ-104和AZ-305，基本上你104能过，305就没有问题，az-104挺多知识点的，最讨厌的是步骤题，不知道死记那些步骤有何意思，把104的知识要点分列如下：\nResourceGroup的 Tag不会被resource继承，新建的policy只针对新添加和更新的resource生效，对没有修改的resource不生效，另外需要关注policy的defination是只针对resource还是包括resource groups Adds the specified tag and value when any resource missing this tag is created or updated. Existing resources can be remediated by triggering a remediation task. If the tag exists with a different value it will not be changed. Does not modify tags on resource groups.\nResize Availability Set下的VM， 需要停止Availability Set下所有的VM If the VM you wish to resize is part of an availability set, then you must stop all VMs in the availability set before changing the size of any VM in the availability set.\nThe number of fault domains for managed availability sets varies by region - either two or three per region. Each virtual machine in your availability set is assigned an update domain and a fault domain by the underlying Azure platform. For a given availability set, five non-user-configurable update domains are assigned by default (Resource Manager deployments can then be increased to provide up to 20 update domains) to indicate groups of virtual machines and underlying physical hardware that can be rebooted at the same time.\nYou need to make sure that the password cannot be stored in plain text.You are preparing to create the necessary components to achieve your goal. Key vault + access policy\nThe Add-AzVhd cmdlet uploads on-premises virtual hard disks, in .vhd file format, to a blob storage account as fixed virtual hard disks.\nrecovery For physical servers - Storage Account - Azure Recovery Services Vault - Replication policy https://docs.microsoft.com/en-us/azure/site-recovery/physical-azure-disaster-recovery For Hyper-v server - Hyper-V site - Azure Recovery Services Vault - Replication policy https://docs.microsoft.com/en-nz/azure/site-recovery/hyper-v-prepare-on-premises-tutorial If you make a change to the topology of your network and have Windows VPN clients, the VPN client package for Windows clients must be downloaded and installed again in order for the changes to be applied to the client\nTo configure an Azure internal load balancer as a listener for the availability group, you need to create a TCP health probe on port 1433, which is the default port for SQL Server.\nEach VM will have a minimum of 1 NIC, which can have one or more IPs associated to it(include public and privite ip)\nYou can only restore a VM to the original VM or a new Azure VM. Azure Backup is a cloud-based backup solution, and it doesn\u0026rsquo;t support restoring VMs to on-premise Windows devices.\nYou can set expiration policy only for Office 365 groups in Azure Active Directory (Azure AD).\nEach management group and subscription can only support one parent\nGroups can contain both registered and joined devices as members. Cloud device admin cannot add/join devices,user admin can add device/user/groups, Dynamic groups dont require manual intervention, it uses criteria to add or remove devices/users/groups only assigned groups you can add\nYou can\u0026rsquo;t delete a Recovery Services vault with any of the following dependencies: You can\u0026rsquo;t delete a vault that contains backup data. Once backup data is deleted, it will go into the soft deleted state. You can\u0026rsquo;t delete a vault that contains backup data in the soft deleted state.\nBefore you can delete a Recovery Service vault that contains protected virtual machines, you need to stop the backup of each backup item.\nIf you want to change the recovery service vault you need to disassociate the previous RSV and delete the backup data. To delete backup data, you need to stop the backup first.\nRecover Servies vault backup need resource in the same location with vault. to create a Vault to protect VMs, the Vault must be in the same Region as the VMs. VM,SQL,file share support be backuped by Recover Servies vault Blobs cannot be backup up to service vaults.\nAzure RBAC is the authorization system you use to manage access to Azure resources.\nAzure (RBAC) and Azure AD roles are independent. AD roles do not grant access to resources and Azure roles do not grant access to Azure AD.\nAzure AD Roles like Global Administrator dont provided access to resources. For that RBAC Roles need to be aplied to the users.\nAdvisor helps you optimize and reduce your overall Azure spend by identifying idle and underutilized resources. You can get cost recommendations from the Cost tab on the Advisor dashboard.\nYou can adjust the guest user settings, their access, who can invite them from \u0026ldquo;External collaboration settings\u0026rdquo;\nYou must use Windows Server Active Directory to update the identity, contact info, or job info for users whose source of authority is Windows Server Active. Usage location is an Azure property that can only be modified from Azure AD (for all users including Windows Server AD users synced via Azure AD Connect).\nYour account must have any one of the following Azure roles at the subscription scope to enable traffic analytics: owner, contributor, reader, or network contributor. https://docs.microsoft.com/en-us/azure/network-watcher/traffic-analytics#user-access-requirements both Tags and Locks are available to Subscriptions, Resource Groups, and Resources.\nAdministrative units restrict permissions in a role to any portion of your organization that you define.\nOwner = Grants full access to manage all resources, including the ability to assign roles in Azure RBAC. Contributor = Grants full access to manage all resources, but does NOT allow you to assign roles in Azure RBAC. (you cannot add users or changes their rights) User Access Administrator = Lets you manage user access to Azure resources. Reader = View all resources, but does not allow you to make any changes. Security Admin = View and update permissions for Security Center. Same permissions as the Security Reader role and can also update the security policy and dismiss alerts and recommendations. Network Contributor = Lets you manage networks, but not access to them. (so you can add VNET, subnet, etc)\nBefore you enable Azure AD over SMB for Azure file shares, make sure you have completed the following prerequisites: \\1. Select or create an Azure AD tenant. \\2. To support authentication with Azure AD credentials, you must enable Azure AD Domain Services for your Azure AD tenant.\nyou can assign policy to Tenant Root Group,ManagementGroup1,Subscription1 and RG1 you can exclude policy from ManagementGroup1,Subscription1,RG11 and VM1\nBuilt-in AD roles can\u0026rsquo;t be cloned, but built-in subscription roles can be. Custom roles of either type can be cloned.\nGroup-based licensing currently does not support groups that contain other groups (nested groups). If you apply a license to a nested group, only the immediate first-level user members of the group have the licenses applied（license不支持嵌套group）\nNesting is currently not supported for groups that can be assigned to a role. （role不支持group嵌套）\nGeneral-purpose v2 (GPv2) accounts are storage accounts that support all of the latest features for blobs, files, queues, and tables. Blob storage accounts support all the same block blob features as GPv2, but are limited to supporting only block blobs. General-purpose v1 (GPv1) accounts provide access to all Azure Storage services, but may not have the latest features or the lowest per gigabyte pricing.\nYou may only tier your object storage data to hot, cool, or archive in Blob storage and General Purpose v2 (GPv2) accounts. General Purpose v1 (GPv1) accounts do not support tiering.\nGeo-redundant storage (GRS): Cross-regional replication to protect against region-wide unavailability. Locally-redundant storage (LRS): A simple, low-cost replication strategy. Data is replicated within a single storage scale unit. Read-access geo-redundant storage (RA-GRS): Cross-regional replication with read access to the replica. RA-GRS provides read-only access to the data in the secondary location, in addition to geo-replication across two regions, but is more expensive compared to GRS.\nA sync group contains one cloud endpoint, or Azure file share, and at least one server endpoint. Azure File Sync does not support more than one server endpoint from the same register server in the same Sync Group. Multiple server endpoints can exist on the same volume if their namespaces are not overlapping (for example, F:\\sync1 and F:\\sync2) and each endpoint is syncing to a unique sync group.\nServer Point synced quick, cloud point sync in 24 hours. cloud endpoint, and it is scanned by the detection job every 24 hours the on-premises servers the file is scanned and synced automatically after it\u0026rsquo;s being added.\nAzCopy is a command-line utility that you can use to copy blobs or files to or from a storage account.\nBoth Azure Active Directory (AD) and Shared Access Signature (SAS) token are supported for Blob storage. Only Shared Access Signature (SAS) token is supported for File storage.\nThe location and subscription where this Log Analytics workspace can be created is independent of the location and subscription where your vaults exist. vault和log analytics workspace不按照location和subscription区分 Storage Account must be in the same Region as the Recovery Services Vault.\nPremium file shares are hosted in a special purpose storage account kind, called a FileStorage account. The archive tier supports only LRS, GRS, and RA-GRS.\nServer Message Block (SMB) is used to connect to an Azure file share over the internet. The SMB protocol requires TCP port 445 to be open. Incorrect Answers: Port 80 is required for HTTP to a web server Port 443 is required for HTTPS to a web server Port 3389 is required for Remote desktop protocol (RDP) connections\nWhile a blob is in archive storage, the blob data is offline and can\u0026rsquo;t be read or modified. To read or download a blob in archive, you must first rehydrate it to an online tier.\nbackup VM1 and make sure backup data are stored across three availability zones in the primary region. Create Recovery Services Vault, Set Replication Policy to ZRS (because of the requirement for having in three separate zones) For VM1, create a backup policy\nAzure storage encryption supports RSA and RSA-HSM keys of sizes 2048, 3072 and 4096\nStorage account policy: Maximum number of Stored access policies is 5 Maximum number of Immutable blob storage is 2\nLifecycle management policies are supported for block blobs and append blobs in general-purpose v2, premium block blob, and Blob Storage accounts. Only storage accounts that are configured for LRS, GRS, or RA-GRS support moving blobs to the archive tier. The archive tier isn\u0026rsquo;t supported for ZRS, GZRS, or RA-GZRS accounts.(No Z)\nAccording to documentation only Premium file shares (FileStorage), LRS/ZRS are supported for SMB.\nAzure Blob Storage provides containers for storing blobs and queues for storing messages. Both containers and queues support conditions when assigning RBAC roles to specific resources within the storage account, allowing for more granular access control based on certain conditions.\nIf you want to select an existing virtual network, make sure it\u0026rsquo;s in the same location and Azure subscription as your Kubernetes cluster. VNet should be same location with AKS.\nWe cannot just move a virtual machine between networks. What we need to do is identify the disk used by the VM, delete the VM itself while retaining the disk, and recreate the VM in the target virtual network and then attach the original disk to it.\nWeb App can only created and identified in App Service plan in same region and resource group.\nTo install kubectl locally, use the az aks install-cli command.\nUse Azure Automation State Configuration to manage the ongoing consistency of the virtual machine configurations steps Step 1: Create and upload a configuration to Azure Automation Step 2: Compile a configuration into a node configuration Step 3: Register a VM to be managed by State Configuration Step 4: Specify configuration mode settings Step 5: Assign a node configuration to a managed node Step 6: Check the compliance status of a managed node\nAnswer in exam 1: Upload a configuration to Azure Automation State Configuration 2: Compile a configuration into a node configuration 3: Check the compliance status of the node.\nWhen you redeploy a VM, it moves the VM to a new node within the Azure infrastructure and then powers it back on, retaining all your configuration options and associated resources. 当Azure 维护导致VM需要shutdown时，使用该方式切换\nTo migrate a VM from a VNET to another VNET. The only option is to delete the VM and redeploy it using a new NIC and NIC connected to VNET2.\nWhile resizing the VM it must be in a stopped state.\nrecord all the successful and failed connection attempts to VM1. 1.Create a VM with a network security group 2.Enable Network Watcher and register the Microsoft.Insights provider 3.Enable a traffic flow log for an NSG, using Network Watcher\u0026rsquo;s NSG flow log capability 4.Download logged data 5.View logged data\nScaleSetVM orchestration mode: Virtual machine instances added to the scale set are based on the scale set configuration model. The virtual machine instance lifecycle - creation, update, deletion - is managed by the scale set. It the current default VMSS behavior. (Scale set VMs are created in a single shot). VM (virtual machines) orchestration mode: Virtual machines created outside of the scale set can be explicitly added to the scale set. The orchestration mode VM will only create an empty VMSS without any instances, and you will have to manually add new VMs into it by specifying the VMSS ID during the creation of the VM. (Separately VMs are created and added to scale set later)\nRecover Servies vault backup need resource in the same location with vault. to create a Vault to protect VMs, the Vault must be in the same Region as the VMs.\nYou can assign an NSG to the subnet of the virtual network in the same region\nBy default, Azure virtual machines can communicate only with other virtual machines that are connected to the same virtual network. If you want a virtual machine to communicate with other virtual machines that are connected to other virtual networks, you must configure network peering.\nYou can use a network security group (NSG) to be assigned to a network interface. NSGs can be associated with subnets or individual virtual machine instances within that subnet. When an NSG is associated with a subnet, the access control list (ACL) rules apply to all virtual machine instances of that subnet.\nApplication gateway allows you to configure load-balanced virtual machines on a private IP address and provide a web app firewall to block any SQL injection, header, and cross-site scripting XSS attacks. An internal load balancer cannot provide load balancing on a public front. A network security group (NSG) is only used to open ports on virtual machines. A public load balancer does not provide web app firewall capabilities to block attacks.\nAzure Application Gateway is a web traffic load balancer that operates at Layer 7 of the OSI model. Application Gateway can make routing decisions based on additional attributes of an HTTP request, such as the URI path or host headers.\nYou can detach a disk from a running virtual machine (hot removal).\nAzure Spot instances allow you to provision virtual machines at a reduced cost, but these virtual machines can be stopped by Azure when Azure needs the capacity for other pay-as-you-go workloads, or when the price of the spot instance exceeds the maximum price that you have set. These virtual machines are good for dev, testing, or for workloads that do not require any specific SLA.\nOnly zone-redundant replication (ZRS) supports StorageV2, FileStorage, and BlockBlobStorage accounts. Live migration is not supported for read-access geo-redundant storage (RA-GRS) and only standard storage accounts can be used.\nFor accessing the file share, port 445 must be open. Port 5671 is used to send health information to Azure AD.\nBy default, backups of virtual machines are kept for 30 days.\nA maximum of one SMS message can be sent every five minutes. Therefore, a maximum of 12 messages will be sent per hour.\nYour VMs should use managed disks if you want to move them to an Availability Zone by using Site Recovery\nBasic Azure Load Balancer supports deployment in a single availability zone. Basic Azure Load Balancer supports only Basic SKU public IP. Azure Standard Load Balancer is zone-redundant, but has a higher cost.\nYou can use delete locks to block the deletion of virtual machines, subscriptions, and resource groups. You cannot use delete locks on management groups or storage account data.\nCommand in SecurityEvent table in Azure Monitor. Summarize is used to group records from one or more columns of data. Where is used to filter the rows. Project is used to rename and select columns. Extend is used to add columns.\nYou can use the Log Analytics agent for Linux as part of a solution to collect JSON output from the Linux virtual machines.\nThe Azure Custom Script Extension is used for post-deployment configuration, software installation, or any other configuration or management task.\nDesired State Configuration (DSC) is a management platform that you can use to manage an IT and development infrastructure with configuration as code.\nThe Azure VMAccess extension acts as a KVM switch that allows you to access the console to reset access to Linux or perform disk-level maintenance.\nA lifecycle management rule can be used to move or delete blobs automatically. The rule can be based on the time the blob was last modified or the time the blob was last accessed (read or write). To perform an action based on the access time, access tracking must be enabled. This can incur additional storage costs.\nAdd-AzVhd: Uploads an on-premises VHD to Azure New-AzVM: Used to create a new virtual machine New-AzDisk: Used to create a managed disk New-AzDataShare: Used to create an Azure data share\nVersioning must be enabled for the source and target. An object type container is needed to replicate the images. You must create a StandardV2 storage account. File shares are not needed, and queues are unsupported for replication.\nAzure Network Watcher is a regional service that allows you to monitor and diagnose conditions at a network scenario level in, to, and from Azure. When you create or update a virtual network in a subscription, Network Watcher will be enabled automatically in the virtual network\u0026rsquo;s region. There is no impact on resources or associated charges for automatically enabling Network Watcher.\nYou can use API server authorized IP ranges if you want to maintain a public endpoint for the API server while restricting access to a set of trusted IP ranges. You can use a private cluster if you want to limit the API server to be accessible only from within your virtual network.\nObject replication can be used to replicate blobs between storage accounts. Before configuring object replication, you must enable blob versioning for both storage accounts, and you must enable the change feed for the source account.\nData pinned on a shared dashboard can only be displayed for a maximum of 14 days.\nBy default, backups of virtual machines are kept for 30 days.\nA timed-based retention policy or legal hold policies can be applied to block deletion. Immutability policies can be scoped to a blob version or to a container.\nSMS: No more than 1 SMS every 5 minutes. ✑ Voice: No more than 1 Voice call every 5 minutes. ✑ Email: No more than 100 emails in an hour. ✑ Other actions are not rate limited.\nTo create a vault to protect virtual machines, the vault must be in the same region as the virtual machines\nUse the az aks command and the Azure portal to configure cluster autoscaler for AKS1.\nThe Linux Diagnostic Extension should be used which downloads the Diagnostic Extension (LAD) agent on Linux server.\nthe DNS port 53\no deploy a YAML file, the command is: kubectl apply -f \u0026lt;file_name\u0026gt;.yaml\nThe virtual machine you attach a network interface to and the virtual network you connect it to must exist in the same location,\nYou can target your deployment to a resource group, subscription, management group, or tenant. Depending on the scope of the deployment, you use different commands. To deploy to a resource group, use New-AzResourceGroupDeployment. To deploy to a tenant, use New-AzTenantDeployment. To deploy to a subscription, use New-AzSubscriptionDeployment which is an alias of the New-AzDeployment cmdlet. To deploy to a management group, use New-AzManagementGroupDeployment.\nApp Service can back up the following information to an Azure storage account and container that you have configured your app to use App configuration - File content - Database connected to your app -\nAzure Web Application Firewall (WAF) on Azure Application Gateway provides centralized protection of your web applications from common exploits and vulnerabilities. Web applications are increasingly targeted by malicious attacks that exploit commonly known vulnerabilities.\nA public and a private IP address can be assigned to a single network interface.\nThe virtual machines are registered (added) to the private zone as A records pointing to their private IP addresses.\nBefore creating a network interface, you must have an existing virtual network in the same location and subscription you create a network interface in.\nWith Azure CNI, every pod gets an IP address from the subnet and can be accessed directly. These IP addresses must be unique across your network space.\nThe connection monitor capability monitors communication at a regular interval and informs you of reachability, latency, and network topology changes between the VM and the endpoint\nload balancers with VMs The Basic tier load balancer is quite restrictive. A load balancer is restricted to a single availability set, virtual machine scale set, or a single machine. The Standard tier load balancer can span any virtual machine in a single virtual network, including blends of scale sets, availability sets, and machines.\nAzure DNS provides automatic registration of virtual machines from a single virtual network that\u0026rsquo;s linked to a private zone as a registration virtual network.\nA Site-to-Site (S2S) VPN gateway connection is used to connect your on-premises network to an Azure virtual network over an IPsec/IKE (IKEv1 or IKEv2) VPN tunnel. IKEv2 supports 10 S2S connections, while IKEv1 only supports 1.\nNetwork Watcher variable packet capture allows you to create packet capture sessions to track traffic to and from a virtual machine. Packet capture helps to diagnose network anomalies both reactively and proactively.\nVM apply azure firewall shoule use same location with Azure firewall A Basic Load Balancer supports virtual machines in a single availability set or virtual machine scale set\n","date":"2023-09-13","img":"","permalink":"https://bajie.dev/posts/20230913-az-104/","series":null,"tags":null,"title":"Azure认证az-104的考点"},{"categories":null,"content":"上一篇我们用losetup建了一个iscsi卷，现在空间不够了，需要释放掉之前建立的iscsi-volumes的20T空间。\n首先去isci卷的宿主机查看一下\n1targetcli ls / 开始删除，先删除backstores，然后是iscsi，lv，vg，pv:\n1# targetcli /backstores/block delete vg-targetd:pvc-harbor 2Deleted storage object vg-targetd:pvc-harbor. 3# targetcli /backstores/block delete vg-targetd:pvc-vis-18-31-48 4Deleted storage object vg-targetd:pvc-vis-18-31-48. 5# targetcli /backstores/block delete vg-targetd:pvc-vis-18-31-49 6Deleted storage object vg-targetd:pvc-vis-18-31-49. 7 8# targetcli /iscsi delete iqn.2020-07.com.ddky:renhe-18-30-18 9Deleted Target iqn.2020-07.com.ddky:renhe-18-30-18. 10# targetcli /iscsi delete iqn.2020-10.com.ddky:vis-18-31-48 11Deleted Target iqn.2020-10.com.ddky:vis-18-31-48. 12# targetcli /iscsi delete iqn.2020-10.com.ddky:vis-18-31-49 13Deleted Target iqn.2020-10.com.ddky:vis-18-31-49. 14 15# lvs 16 LV VG Attr LSize Pool Origin Data% Meta% Move Log Cpy%Sync Convert 17 pvc-harbor vg-targetd -wi-a----- 200.00g 18 pvc-vis-18-31-48 vg-targetd -wi-a----- 80.00g 19 pvc-vis-18-31-49 vg-targetd -wi-a----- 80.00g 20# lvremove /dev/vg-targetd/pvc-harbor 21Do you really want to remove active logical volume vg-targetd/pvc-harbor? [y/n]: y 22 Logical volume \u0026#34;pvc-harbor\u0026#34; successfully removed 23# lvremove /dev/vg-targetd/pvc-vis-18-31-48 24Do you really want to remove active logical volume vg-targetd/pvc-vis-18-31-48? [y/n]: y 25 Logical volume \u0026#34;pvc-vis-18-31-48\u0026#34; successfully removed 26# lvremove /dev/vg-targetd/pvc-vis-18-31-49 27Do you really want to remove active logical volume vg-targetd/pvc-vis-18-31-49? [y/n]: y 28 Logical volume \u0026#34;pvc-vis-18-31-49\u0026#34; successfully removed 29 30# vgremove vg-targetd 31 Volume group \u0026#34;vg-targetd\u0026#34; successfully removed 32# lvs 33# vgs 34# pvs 35 PV VG Fmt Attr PSize PFree 36 /dev/loop0 lvm2 --- 19.53t 19.53t 37 38# pvremove /dev/loop0 39 Labels on physical volume \u0026#34;/dev/loop0\u0026#34; successfully wiped. 一整套下来，基本都干净了。\n最后清理文件，注意如果不停掉targetd服务，是无法remove loop设备的\n1systemctl stop targetd 2losetup -a 3losetup -d /dev/loop0 4losetup -a 5cd /glusterfs/iscsi-volumes 6rm -f k8s-iscsi-volumes.img 这样就多了20T磁盘空间。\n","date":"2023-09-05","img":"","permalink":"https://bajie.dev/posts/20230905-iscsi_volume_release/","series":null,"tags":null,"title":"Iscsi卷的释放"},{"categories":null,"content":"如果机器的磁盘空间不够，可以用iscsi把服务器172.18.30.18上面划出一片空间，远程挂上来用。\n注意，服务器用losetup的这种做法是为了将来k8s也可以这样用动态iscsi卷\n服务器端安装 登录172.18.30.18\n安装：\n1yum install -y targetcli targetd` 用文件来虚拟LVM卷：\n1cd /glusterfs/iscsi-volumes/ 2生成20TB文件 3dd if=/dev/zero of=k8s-iscsi-volumes.img bs=1G count=20000 4export LOOP=`losetup -f` 5losetup $LOOP k8s-iscsi-volumes.img 6vgcreate vg-targetd $LOOP 修改targetd.yaml:\n1vi /etc/target/targetd.yaml 2password: xxxxxxxx 3# defaults below; uncomment and edit 4# if using a thin pool, use \u0026lt;volume group name\u0026gt;/\u0026lt;thin pool name\u0026gt; 5# e.g vg-targetd/pool 6pool_name: vg-targetd 7user: admin 8ssl: false 9target_name: iqn.2020-04.com.ddky:renhe-18-30-18 注意，这个文件生成后，就不需要改动了，如果以后target_name变了，也不用管，也不需要重启targetd\n启动服务：\n1systemctl enable --now target 2systemctl enable --now targetd 运行一下命令，看看显示结果 pvdisplay vgdisplay lvdisplay targetcli ls /\n注意：lvdisplay结果和targetcli ls /结果都是空\n如果有定义好的iscsi，3260端口才会开放。所以现在只开启了18700的targetd，3260未开放中。\n如果以后定义好了，那么显示结果如下图：\n那如果想要彻底删除某个已经存在的iscsi卷，三步曲：\n1首先删掉block设备 2targetcli /backstores/block delete vg-targetd:pvc-harbor 3 4然后删除iscsi 5targetcli /iscsi delete iqn.2020-07.com.ddky:renhe-18-30-18 6 7最后删除lv 8lvremove /dev/vg-targetd/pvc-harbor 建立新的卷的方法：\n1建立新的lv pvc-harbor, 200G 2lvcreate -L 200G -n pvc-harbor vg-targetd 3 4建立新的block设备 5targetcli /backstores/block create vg-targetd:pvc-harbor /dev/vg-targetd/pvc-harbor 6 7建立新的iqn 8targetcli /iscsi create iqn.2020-07.com.ddky:renhe-18-30-18 9 10建立新的lun，portal会自动建立，3260端口会开放 11targetcli /iscsi/iqn.2020-07.com.ddky:renhe-18-30-18/tpg1/luns create /backstores/block/vg-targetd:pvc-harbor 12 13建立新的acls 14targetcli /iscsi/iqn.2020-07.com.ddky:renhe-18-30-18/tpg1/acls create iqn.2020-07.com.ddky:harbor-18-31-28 15 16设置acls的鉴权 17targetcli /iscsi/iqn.2020-07.com.ddky:renhe-18-30-18/tpg1/acls/iqn.2020-07.com.ddky:harbor-18-31-28 set attribute authentication=0 18targetcli /iscsi/iqn.2020-07.com.ddky:renhe-18-30-18/tpg1/acls/iqn.2020-07.com.ddky:harbor-18-31-28 set auth userid=admin password=nishiwode 19 20如果lun只对srv2开放，不对srv1开放，方法如下： 21/\u0026gt; iscsi/iqn.2003-01.local.rhce.ipa:target/tpg1/acls create iqn.1994-05.com.redhat:srv1 add_mapped_luns=false 22/\u0026gt; iscsi/iqn.2003-01.local.rhce.ipa:target/tpg1/acls create iqn.1994-05.com.redhat:srv2 ok，如上，服务器端就设置好了\n这里没有重启，只是运行了targetcli saveconfig 实测，真的不需要重启target，systemctl restart target\n客户端安装 安装iscsi客户端\n1yum install iscsi-initiator-utils -y 修改initiatorname.iscsi，也就是自己的iqn号\n1vi /etc/iscsi/initiatorname.iscsi 2InitiatorName=iqn.2020-07.com.ddky:harbor-18-31-28 修改iscsid.conf\n1vi /etc/iscsi/iscsid.conf，增加3行 2node.session.auth.authmethod = CHAP 3node.session.auth.username = admin 4node.session.auth.password = xxxxxxxx 重启服务：\n1systemctl restart iscsid 发现一下对端：\n1# iscsiadm -m discovery -t sendtargets -p 172.18.30.18:3260 2172.18.30.18:3260,1 iqn.2020-07.com.ddky:renhe-18-30-18 登录ISCSI：\n1# iscsiadm -m node -T iqn.2020-07.com.ddky:renhe-18-30-18 -p 172.18.30.18:3260 --login 2Logging in to [iface: default, target: iqn.2020-07.com.ddky:renhe-18-30-18, portal: 172.18.30.18,3260] (multiple) 3Login to [iface: default, target: iqn.2020-07.com.ddky:renhe-18-30-18, portal: 172.18.30.18,3260] successful. 查看session:\n1iscsiadm -m session -P3 | less 系统中会多出一块盘，/dev/sda\n直接格式化，不要分区，mkfs.xfs /dev/sda\n查出uuid\n1blkid /dev/sda 2/dev/sda: UUID=\u0026#34;58024012-aa03-4091-a11a-0bb74beeed5a\u0026#34; TYPE=\u0026#34;xfs\u0026#34; 编辑/etc/fstab\n1vi /etc/fstab 2UUID=58024012-aa03-4091-a11a-0bb74beeed5a /mnt xfs _netdev 0 0 Ok, 搞定。\n增加给vis-18-31-48增加pxe的sanboot启动硬盘的方法：\n1#划个lvc，用的是vg-targetd的20T中的80G 2lvcreate -L 80G -n pvc-vis-18-31-48 vg-targetd 3 4#建立block块设备 5targetcli /backstores/block create vg-targetd:pvc-vis-18-31-48 /dev/vg-targetd/pvc-vis-18-31-48 6 7#建立30.18上的iscsi服务端，似乎用renhe-18-30-18比较好，但是不好区分多个卷 8targetcli /iscsi create iqn.2020-10.com.ddky:vis-18-31-48 9 10#建立luns，会自动建立portal 11targetcli /iscsi/iqn.2020-10.com.ddky:vis-18-31-48/tpg1/luns create /backstores/block/vg-targetd:pvc-vis-18-31-48 12 13#建立客户端的iscsi，不加认证 14targetcli /iscsi/iqn.2020-10.com.ddky:vis-18-31-48/tpg1/acls create iqn.2020-10.com.ddky:vis-18-31-48 这样客户端用 iqn.2020-10.com.ddky:vis-13-31-48 就可以mount出来\n","date":"2023-08-01","img":"","permalink":"https://bajie.dev/posts/20230801-iscsi_volume/","series":null,"tags":null,"title":"Iscsi卷的远程挂载使用"},{"categories":null,"content":"F5-Bigip利用irule强行给请求植入Cookie的方法。\nirule有两种做法可以让链接重定向\nHTTP::redirect \u0026quot;http://redirect.domain.com[HTTP::uri]\u0026quot;\r或者：\nHTTP::respond 302 Location \u0026quot;http://redirect.domain.com[HTTP::uri]\u0026quot; \u0026quot;locale\u0026quot; $cookie\r我们可以利用第二种方法来强行塞进cookie\nwhen HTTP_REQUEST {\rif {[HTTP::host] equals “find.domain.com” and [HTTP::path] equals “/” } {\rset local_cookie [HTTP::cookie value lg_locale]\rset cookie [format \u0026quot;locale=%s; path=/; domain=%s\u0026quot; $local_cookie \u0026quot;\u0026lt;cookiedomain\u0026gt;\u0026quot;]\rHTTP::respond 302 Location “http://redirect.domain.com[HTTP::uri]” “Set-Cookie” $cookie\r}\r}\r结果： ","date":"2023-07-27","img":"","permalink":"https://bajie.dev/posts/20230727-f5_irule_cookie/","series":null,"tags":null,"title":"F5利用irule强行植入cookie"},{"categories":null,"content":"F5-Bigip利用irule防止爬虫的一法。\n爬虫的请求：\n1GET /cms/rest.htm?method=ddky.cms.search.recommend.h5.o2o\u0026amp;pageNo=1\u0026amp;pageSize=6\u0026amp;shopId=201790\u0026amp;ordertypeId=0\u0026amp;suite=1\u0026amp;searchType=o2o\u0026amp;searchPanel=1\u0026amp;wd=%E6%B4%9B%E4%B8%81%E6%96%B0\u0026amp;lat=22.520712193695\u0026amp;lng=113.9233553732\u0026amp;city=%E6%B7%B1%E5%9C%B3%E5%B8%82\u0026amp;type=90\u0026amp;unique=05685D2A5DAB8ABBD2E5E5B26E0C960F\u0026amp;versionName=5.7.5\u0026amp;plat=H5\u0026amp;platform=H5\u0026amp;t=2020-12-15%2014%3A19%3A11\u0026amp;v=1.0\u0026amp;sign=A6BD136BC1B6F91E5C7DD5A0DA03DD79\u0026amp;callback=jsonp1 里面的t值是时间，t=2020-12-15%2014%3A19%3A11\n但是有个问题，这个值一直不变了，那我们就利用这一点。如果T值跟当前时间对比，是3分钟前的，那就封！\nF5的irule，直接return的是白名单：\n1when HTTP_REQUEST { 2 3 set t [URI::decode [URI::query [HTTP::uri] t]] 4 set before [clock scan \u0026#34;180 seconds ago\u0026#34; ] 5 6 if { [IP::addr [IP::client_addr] equals 124.206.168.0/255.255.255.224]} { 7 return} 8 if { [IP::addr [IP::client_addr] equals 61.135.14.96/255.255.255.240]} { 9 return} 10 if { [IP::addr [IP::client_addr] equals 114.251.7.112/255.255.255.240]} { 11 return} 12 if { [string tolower [HTTP::uri]] contains \u0026#34;/cms/\u0026#34;} { 13 14 if {$before \u0026gt; [clock scan $t]} { 15 drop 16 } 17 } 18} ","date":"2023-07-27","img":"","permalink":"https://bajie.dev/posts/20230727-f5_irule_t/","series":null,"tags":null,"title":"F5利用irule防爬虫"},{"categories":null,"content":"elasticflow 是个流量分析工具，通过对各种flow流量的抓取，分析数据，可以清晰的看到局域网中的流量。\n网管的必备啊。首先要把sflow流量给发过来。(这里172.18.31.23是服务器端)\n1sflow collector 2 ip 172.18.31.23 description flow-server 拉取源代码：\n1git clone https://github.com/robcowart/elastiflow 启动集群\n1docker-compose up -d 这样整个数据会被清空，需要重新生成一遍，先把kibana的数据文件拉回来\n1wget https://raw.githubusercontent.com/robcowart/elastiflow/master/kibana/elastiflow.kibana.7.8.x.ndjson 然后登录http://172.18.31.23:5601 先到配置，导入 导入对象，选择elastiflow.kibana.7.8.x.ndjson文件上传 导入成功，导入了300多个对象 然后配置索引，应该不用配，直接选一个做default 这样就ok了，去dashboard的overview就能看到东西了 然后去修改一下shard策略，省得索引报黄色\n1PUT /_template/elastiflow-3.5.3 2{ 3 \u0026#34;index_patterns\u0026#34;: \u0026#34;*\u0026#34;, 4 \u0026#34;settings\u0026#34;: { 5 \u0026#34;number_of_shards\u0026#34;: 1 6 } 7} 8PUT /_template/index_defaults 9{ 10 \u0026#34;index_patterns\u0026#34;: \u0026#34;*\u0026#34;, 11 \u0026#34;settings\u0026#34;: { 12 \u0026#34;number_of_shards\u0026#34;: 1 13 } 14} 15PUT /_template/elastiflow-3.5.3 16{ 17 \u0026#34;index_patterns\u0026#34;: \u0026#34;elastiflow-3.5.3-*\u0026#34;, 18 \u0026#34;settings\u0026#34;: { 19 \u0026#34;number_of_shards\u0026#34;: 1 20 } 21} 查看一下：\n1curl -s -X GET \u0026#39;http://localhost:9200/_cat/indices?v\u0026#39; 2curl -s -X GET \u0026#39;http://localhost:9200/_template\u0026#39;| jq ","date":"2023-07-24","img":"","permalink":"https://bajie.dev/posts/20230724-elasticflow/","series":null,"tags":null,"title":"绝版的elasticflow的安装"},{"categories":null,"content":"pod 容器内要用中文雅黑字体生成 jpg 图片，没办法，只能把字体给装进去\n首先进入容器，确定容器的基底是什么，是yum、apt或者apk\n通常都是用apk最小化安装的，这样做法如下：\n1apk update 2apk add --update ttf-dejavu fontconfig 3rm -rf /var/cache/apk/* 4 5mkdir /usr/share/fonts/chinese 6 7cp /usr/local/jre1.8.0_201/lib/fonts/simsun.ttc /usr/share/fonts/chinese 8 9mkfontscale \u0026amp;\u0026amp; mkfontdir \u0026amp;\u0026amp; fc-cache 这样就搞定了，当然这只是临时的。\n要想长久就得修改Dockerfile，把文件拷进容器，然后同样得执行命令即可。\n","date":"2023-07-06","img":"","permalink":"https://bajie.dev/posts/20230706-fonts_in_pod/","series":null,"tags":null,"title":"如何在容器内安装字体文件"},{"categories":null,"content":"2023年上半年经过努力，又考了3张证书，下半年继续努力奋斗\u0026hellip;\u0026hellip;\n","date":"2023-06-30","img":"","permalink":"https://bajie.dev/posts/20230630-certificates/","series":null,"tags":null,"title":"2023年获得的证书"},{"categories":null,"content":"Freeipa接入Yapi.\nvi my-api/config.json\n1... 2 \u0026#34;ldapLogin\u0026#34;: { 3 \u0026#34;enable\u0026#34;: true, 4 \u0026#34;server\u0026#34;: \u0026#34;ldap://ldap.bybon.cn\u0026#34;, 5 \u0026#34;baseDn\u0026#34;: \u0026#34;uid=manager,cn=users,cn=accounts,dc=bybon,dc=cn\u0026#34;, 6 \u0026#34;bindPassword\u0026#34;: \u0026#34;xxxxxxxx\u0026#34;, 7 \u0026#34;searchDn\u0026#34;: \u0026#34;cn=users,cn=accounts,dc=bybon,dc=cn\u0026#34;, 8 \u0026#34;searchStandard\u0026#34;: \u0026#34;mail\u0026#34;, 9 \u0026#34;emailPostfix\u0026#34;: \u0026#34;@bybon.cn\u0026#34;, 10 \u0026#34;emailKey\u0026#34;: \u0026#34;mail\u0026#34;, 11 \u0026#34;usernameKey\u0026#34;: \u0026#34;displayName\u0026#34; 12 } 这里需要修改一下，vi my-yapi/vendors/server/controllers/user.js\n理由如下，登录的时候，yapi的逻辑是先判断用户邮件，把邮件中的用户名摘出来，然后加上配置中的邮件域。\n这个逻辑在ldap中就不对了，改成如下格式，这样直接输入ldap用户名就可以登录了\n1 /** 2 * ldap登录 3 * @interface /user/login_by_ldap 4 * @method 5 * @category user 6 * @foldnumber 10 7 * @param {String} email email名称，不能为空 8 * @param {String} password 密码，不能为空 9 * @returns {Object} 10 * 11 */ 12 async getLdapAuth(ctx) { 13 try { 14 const { email, password } = ctx.request.body; 15 //no const username = email.split(/\\@/g)[0]; 16 //1 const { info: ldapInfo } = await ldap.ldapQuery(email, password); 17 //2 const emailPrefix = email.split(/\\@/g)[0]; 18 //3 const emailPostfix = yapi.WEBCONFIG.ldapLogin.emailPostfix; 19 20 //zrr 21 const emailPrefix = email.split(/\\@/g)[0]; 22 const emailPostfix = yapi.WEBCONFIG.ldapLogin.emailPostfix; 23 const { info: ldapInfo } = await ldap.ldapQuery( 24 (emailPostfix ? emailPrefix + emailPostfix : email),password); 25 //zrr 26 27 28 const emailParams = 29 ldapInfo[yapi.WEBCONFIG.ldapLogin.emailKey || \u0026#39;mail\u0026#39;] || 30 (emailPostfix ? emailPrefix + emailPostfix : email); 31 const username = ldapInfo[yapi.WEBCONFIG.ldapLogin.usernameKey] || emailPrefix; ","date":"2023-04-12","img":"","permalink":"https://bajie.dev/posts/20230412-freeipa_yapi/","series":null,"tags":null,"title":"Yapi集成进freeIPA进行统一认证"},{"categories":null,"content":"新公司的dell服务器idrac居然没有license，无法远程，找了dell要了一个临时license给装上，其实装好系统就不会太用到了，记录一下，以后备用。\n1\u0026lt;?xml version=\u0026#34;1.0\u0026#34;?\u0026gt; 2\u0026lt;!--Copyright (c) 2010-2011 Dell Inc. All Rights Reserved.--\u0026gt; 3\u0026lt;lns:LicenseClass xmlns:ds=\u0026#34;http://www.w3.org/2000/09/xmldsig#\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xmlns:lns=\u0026#34;http://www.dell.com/2011/12G/licensing\u0026#34;\u0026gt; 4 \u0026lt;lns:LicenseData\u0026gt; 5 \u0026lt;lns:Schema lns:Vendor=\u0026#34;Dell\u0026#34; lns:ID=\u0026#34;iDRAC\u0026#34; lns:maxDepth=\u0026#34;255\u0026#34; lns:SchemaVersion=\u0026#34;2.0\u0026#34;/\u0026gt; 6 \u0026lt;lns:TransferableLicense\u0026gt;false\u0026lt;/lns:TransferableLicense\u0026gt; 7 \u0026lt;lns:UTCdateSold\u0026gt;2011-09-20T16:10:37Z\u0026lt;/lns:UTCdateSold\u0026gt; 8 \u0026lt;lns:EntitlementID\u0026gt;56r8irR7fV5w3MIxlJUFL9Ph_Lori_Matthews\u0026lt;/lns:EntitlementID\u0026gt; 9 \u0026lt;lns:DeviceClass lns:ID=\u0026#34;iDRAC\u0026#34;/\u0026gt; 10 \u0026lt;lns:ProductDescription\u0026gt; 11 \u0026lt;lns:lang_en\u0026gt;iDRAC7 Enterprise Evaluation License\u0026lt;/lns:lang_en\u0026gt; 12 \u0026lt;lns:lang_es\u0026gt;iDRAC7 Enterprise Evaluation License\u0026lt;/lns:lang_es\u0026gt; 13 \u0026lt;lns:lang_fr\u0026gt;iDRAC7 Enterprise Evaluation License\u0026lt;/lns:lang_fr\u0026gt; 14 \u0026lt;lns:lang_de\u0026gt;iDRAC7 Enterprise Evaluation License\u0026lt;/lns:lang_de\u0026gt; 15 \u0026lt;lns:lang_it\u0026gt;iDRAC7 Enterprise Evaluation License\u0026lt;/lns:lang_it\u0026gt; 16 \u0026lt;lns:lang_ja\u0026gt;iDRAC7 Enterprise Evaluation License\u0026lt;/lns:lang_ja\u0026gt; 17 \u0026lt;lns:lang_zh\u0026gt;iDRAC7 Enterprise Evaluation License\u0026lt;/lns:lang_zh\u0026gt; 18 \u0026lt;/lns:ProductDescription\u0026gt; 19 \u0026lt;lns:LicenseTerm\u0026gt; 20 \u0026lt;lns:Evaluation lns:Duration=\u0026#34;P30D\u0026#34;/\u0026gt; 21 \u0026lt;/lns:LicenseTerm\u0026gt; 22 \u0026lt;lns:DeviceInfo lns:ID=\u0026#34;1\u0026#34; lns:VendorID=\u0026#34;0x1912\u0026#34; lns:DeviceID=\u0026#34;0x0011\u0026#34;/\u0026gt; 23 \u0026lt;lns:Feature lns:ID=\u0026#34;1\u0026#34; lns:Description=\u0026#34;License Management\u0026#34; lns:Enabled=\u0026#34;true\u0026#34;/\u0026gt; 24 \u0026lt;lns:Feature lns:ID=\u0026#34;2\u0026#34; lns:Description=\u0026#34;RACADM\u0026#34; lns:Enabled=\u0026#34;true\u0026#34;/\u0026gt; 25 \u0026lt;lns:Feature lns:ID=\u0026#34;3\u0026#34; lns:Description=\u0026#34;WSMAN\u0026#34; lns:Enabled=\u0026#34;true\u0026#34;/\u0026gt; 26 \u0026lt;lns:Feature lns:ID=\u0026#34;4\u0026#34; lns:Description=\u0026#34;SNMP\u0026#34; lns:Enabled=\u0026#34;true\u0026#34;/\u0026gt; 27 \u0026lt;lns:Feature lns:ID=\u0026#34;5\u0026#34; lns:Description=\u0026#34;Auto Discovery\u0026#34; lns:Enabled=\u0026#34;true\u0026#34;/\u0026gt; 28 \u0026lt;lns:Feature lns:ID=\u0026#34;6\u0026#34; lns:Description=\u0026#34;USC Firmware Update\u0026#34; lns:Enabled=\u0026#34;true\u0026#34;/\u0026gt; 29 \u0026lt;lns:Feature lns:ID=\u0026#34;7\u0026#34; lns:Description=\u0026#34;Update Package\u0026#34; lns:Enabled=\u0026#34;true\u0026#34;/\u0026gt; 30 \u0026lt;lns:Feature lns:ID=\u0026#34;8\u0026#34; lns:Description=\u0026#34;USC Operating System Deployment\u0026#34; lns:Enabled=\u0026#34;true\u0026#34;/\u0026gt; 31 \u0026lt;lns:Feature lns:ID=\u0026#34;9\u0026#34; lns:Description=\u0026#34;USC Device Configuration\u0026#34; lns:Enabled=\u0026#34;true\u0026#34;/\u0026gt; 32 \u0026lt;lns:Feature lns:ID=\u0026#34;10\u0026#34; lns:Description=\u0026#34;USC Diagnostics\u0026#34; lns:Enabled=\u0026#34;true\u0026#34;/\u0026gt; 33 \u0026lt;lns:Feature lns:ID=\u0026#34;11\u0026#34; lns:Description=\u0026#34;Power Budget\u0026#34; lns:Enabled=\u0026#34;true\u0026#34;/\u0026gt; 34 \u0026lt;lns:Feature lns:ID=\u0026#34;12\u0026#34; lns:Description=\u0026#34;Power Monitoring\u0026#34; lns:Enabled=\u0026#34;true\u0026#34;/\u0026gt; 35 \u0026lt;lns:Feature lns:ID=\u0026#34;13\u0026#34; lns:Description=\u0026#34;Virtual Media\u0026#34; lns:Enabled=\u0026#34;true\u0026#34;/\u0026gt; 36 \u0026lt;lns:Feature lns:ID=\u0026#34;14\u0026#34; lns:Description=\u0026#34;Telnet\u0026#34; lns:Enabled=\u0026#34;true\u0026#34;/\u0026gt; 37 \u0026lt;lns:Feature lns:ID=\u0026#34;15\u0026#34; lns:Description=\u0026#34;SMASH CLP\u0026#34; lns:Enabled=\u0026#34;true\u0026#34;/\u0026gt; 38 \u0026lt;lns:Feature lns:ID=\u0026#34;16\u0026#34; lns:Description=\u0026#34;IPv6\u0026#34; lns:Enabled=\u0026#34;true\u0026#34;/\u0026gt; 39 \u0026lt;lns:Feature lns:ID=\u0026#34;17\u0026#34; lns:Description=\u0026#34;Dynamic DNS\u0026#34; lns:Enabled=\u0026#34;true\u0026#34;/\u0026gt; 40 \u0026lt;lns:Feature lns:ID=\u0026#34;18\u0026#34; lns:Description=\u0026#34;Dedicated NIC\u0026#34; lns:Enabled=\u0026#34;true\u0026#34;/\u0026gt; 41 \u0026lt;lns:Feature lns:ID=\u0026#34;19\u0026#34; lns:Description=\u0026#34;Directory Services\u0026#34; lns:Enabled=\u0026#34;true\u0026#34;/\u0026gt; 42 \u0026lt;lns:Feature lns:ID=\u0026#34;20\u0026#34; lns:Description=\u0026#34;Two-Factor Authentication\u0026#34; lns:Enabled=\u0026#34;true\u0026#34;/\u0026gt; 43 \u0026lt;lns:Feature lns:ID=\u0026#34;21\u0026#34; lns:Description=\u0026#34;Single Sign-On\u0026#34; lns:Enabled=\u0026#34;true\u0026#34;/\u0026gt; 44 \u0026lt;lns:Feature lns:ID=\u0026#34;22\u0026#34; lns:Description=\u0026#34;PK Authentication\u0026#34; lns:Enabled=\u0026#34;true\u0026#34;/\u0026gt; 45 \u0026lt;lns:Feature lns:ID=\u0026#34;23\u0026#34; lns:Description=\u0026#34;Crash Screen Capture\u0026#34; lns:Enabled=\u0026#34;true\u0026#34;/\u0026gt; 46 \u0026lt;lns:Feature lns:ID=\u0026#34;24\u0026#34; lns:Description=\u0026#34;Crash Video Capture\u0026#34; lns:Enabled=\u0026#34;true\u0026#34;/\u0026gt; 47 \u0026lt;lns:Feature lns:ID=\u0026#34;25\u0026#34; lns:Description=\u0026#34;Boot Capture\u0026#34; lns:Enabled=\u0026#34;true\u0026#34;/\u0026gt; 48 \u0026lt;lns:Feature lns:ID=\u0026#34;26\u0026#34; lns:Description=\u0026#34;Virtual Console\u0026#34; lns:Enabled=\u0026#34;true\u0026#34;/\u0026gt; 49 \u0026lt;lns:Feature lns:ID=\u0026#34;27\u0026#34; lns:Description=\u0026#34;Virtual Flash Partitions\u0026#34; lns:Enabled=\u0026#34;true\u0026#34;/\u0026gt; 50 \u0026lt;lns:Feature lns:ID=\u0026#34;28\u0026#34; lns:Description=\u0026#34;Console Collaboration\u0026#34; lns:Enabled=\u0026#34;true\u0026#34;/\u0026gt; 51 \u0026lt;lns:Feature lns:ID=\u0026#34;29\u0026#34; lns:Description=\u0026#34;Device Monitoring\u0026#34; lns:Enabled=\u0026#34;true\u0026#34;/\u0026gt; 52 \u0026lt;lns:Feature lns:ID=\u0026#34;30\u0026#34; lns:Description=\u0026#34;Remote Inventory\u0026#34; lns:Enabled=\u0026#34;true\u0026#34;/\u0026gt; 53 \u0026lt;lns:Feature lns:ID=\u0026#34;31\u0026#34; lns:Description=\u0026#34;Storage Monitoring\u0026#34; lns:Enabled=\u0026#34;true\u0026#34;/\u0026gt; 54 \u0026lt;lns:Feature lns:ID=\u0026#34;32\u0026#34; lns:Description=\u0026#34;Remote Firmware Update\u0026#34; lns:Enabled=\u0026#34;true\u0026#34;/\u0026gt; 55 \u0026lt;lns:Feature lns:ID=\u0026#34;33\u0026#34; lns:Description=\u0026#34;Remote Firmware Configuration\u0026#34; lns:Enabled=\u0026#34;true\u0026#34;/\u0026gt; 56 \u0026lt;lns:Feature lns:ID=\u0026#34;34\u0026#34; lns:Description=\u0026#34;Remote Inventory Export\u0026#34; lns:Enabled=\u0026#34;true\u0026#34;/\u0026gt; 57 \u0026lt;lns:Feature lns:ID=\u0026#34;35\u0026#34; lns:Description=\u0026#34;Remote Operating System Deployment\u0026#34; lns:Enabled=\u0026#34;true\u0026#34;/\u0026gt; 58 \u0026lt;lns:Feature lns:ID=\u0026#34;36\u0026#34; lns:Description=\u0026#34;Backup and Restore\u0026#34; lns:Enabled=\u0026#34;true\u0026#34;/\u0026gt; 59 \u0026lt;lns:Feature lns:ID=\u0026#34;37\u0026#34; lns:Description=\u0026#34;Part Replacement\u0026#34; lns:Enabled=\u0026#34;true\u0026#34;/\u0026gt; 60 \u0026lt;lns:Feature lns:ID=\u0026#34;38\u0026#34; lns:Description=\u0026#34;SSH\u0026#34; lns:Enabled=\u0026#34;true\u0026#34;/\u0026gt; 61 \u0026lt;lns:Feature lns:ID=\u0026#34;39\u0026#34; lns:Description=\u0026#34;Remote File Share\u0026#34; lns:Enabled=\u0026#34;true\u0026#34;/\u0026gt; 62 \u0026lt;lns:Feature lns:ID=\u0026#34;40\u0026#34; lns:Description=\u0026#34;Virtual Folders\u0026#34; lns:Enabled=\u0026#34;true\u0026#34;/\u0026gt; 63 \u0026lt;lns:Feature lns:ID=\u0026#34;41\u0026#34; lns:Description=\u0026#34;Web GUI\u0026#34; lns:Enabled=\u0026#34;true\u0026#34;/\u0026gt; 64 \u0026lt;lns:Feature lns:ID=\u0026#34;42\u0026#34; lns:Description=\u0026#34;Network Time Protocol\u0026#34; lns:Enabled=\u0026#34;true\u0026#34;/\u0026gt; 65 \u0026lt;lns:Feature lns:ID=\u0026#34;43\u0026#34; lns:Description=\u0026#34;Email Alerts\u0026#34; lns:Enabled=\u0026#34;true\u0026#34;/\u0026gt; 66 \u0026lt;lns:Feature lns:ID=\u0026#34;44\u0026#34; lns:Description=\u0026#34;Security Lockout\u0026#34; lns:Enabled=\u0026#34;true\u0026#34;/\u0026gt; 67 \u0026lt;lns:Feature lns:ID=\u0026#34;45\u0026#34; lns:Description=\u0026#34;Remote Syslog\u0026#34; lns:Enabled=\u0026#34;true\u0026#34;/\u0026gt; 68 \u0026lt;lns:Feature lns:ID=\u0026#34;253\u0026#34; lns:Description=\u0026#34;Integrated Dell Remote Access Controller 7 Enterprise\u0026#34; lns:Enabled=\u0026#34;true\u0026#34;/\u0026gt; 69 \u0026lt;/lns:LicenseData\u0026gt; 70\u0026lt;dsig:Signature xmlns:dsig=\u0026#34;http://www.w3.org/2000/09/xmldsig#\u0026#34;\u0026gt; 71\u0026lt;dsig:SignedInfo\u0026gt; 72\u0026lt;dsig:CanonicalizationMethod Algorithm=\u0026#34;http://www.w3.org/2001/10/xml-exc-c14n#\u0026#34;/\u0026gt; 73\u0026lt;dsig:SignatureMethod Algorithm=\u0026#34;http://www.w3.org/2000/09/xmldsig#rsa-sha1\u0026#34;/\u0026gt; 74\u0026lt;dsig:Reference URI=\u0026#34;\u0026#34;\u0026gt; 75\u0026lt;dsig:Transforms\u0026gt; 76\u0026lt;dsig:Transform Algorithm=\u0026#34;http://www.w3.org/2000/09/xmldsig#enveloped-signature\u0026#34;/\u0026gt; 77\u0026lt;/dsig:Transforms\u0026gt; 78\u0026lt;dsig:DigestMethod Algorithm=\u0026#34;http://www.w3.org/2000/09/xmldsig#sha1\u0026#34;/\u0026gt; 79\u0026lt;dsig:DigestValue\u0026gt;DrtnjP0vUsyT+18jazjmiaGrvc0=\u0026lt;/dsig:DigestValue\u0026gt; 80\u0026lt;/dsig:Reference\u0026gt; 81\u0026lt;/dsig:SignedInfo\u0026gt; 82\u0026lt;dsig:SignatureValue\u0026gt;Qg4Omx1ZGrVllUPbg/X25aJxK5qlNCF/G04NLwXhbmpqoplSRkCCUgb+6TvVz9b3 83Ut7sSa/WjA0mv+mbcqIENTAnpveIkIOQPR3mdjCBwX2cLYieV9nOIGobxqHU7o97 84QjbSAkmTHcRo0PI6mP8tc7Od4WNWMZ48rrUBeOrVOr1EZeptPUbeaSofy4nvlzbC 85pcpzZLbjAITT157r9KiFe9joG2hCEClrQPO0ScXHgKXrAWrQE9wX7e2De4uCvJwI 86hGWpJzDQNJJZbsWhDoZJn/59G/KRjzxIHIzIpUt1XPPIGHl5yMXDaRFcIMES0RuJ 87SWZS8tt9E001Fr/8/jQNgA==\u0026lt;/dsig:SignatureValue\u0026gt; 88\u0026lt;dsig:KeyInfo\u0026gt; 89\u0026lt;dsig:X509Data\u0026gt; 90\u0026lt;dsig:X509Certificate\u0026gt;MIIDTjCCAjagAwIBAgIBATANBgkqhkiG9w0BAQUFADBRMRMwEQYDVQQKEwpEZWxs 91LCBJbmMuMSEwHwYDVQQLExhFbWJlZGRlZCBMaWNlbnNlIE1hbmFnZXIxFzAVBgNV 92BAMTDkNBIENlcnRpZmljYXRlMB4XDTEwMDEwMTAwMDAwMFoXDTM1MTIzMTIzNTk1 93OVowVjETMBEGA1UEChMKRGVsbCwgSW5jLjEhMB8GA1UECxMYRW1iZWRkZWQgTGlj 94ZW5zZSBNYW5hZ2VyMRwwGgYDVQQDExNTaWduaW5nIENlcnRpZmljYXRlMIIBIjAN 95BgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAqbRo2DZtkjxl5YtqD5ePYdzrWbkU 96YQJwVaWYe1tE7ZAdou5TLTsjPnaa1cLcPTexn+cq8YjukIVwkwJP7yJ5GkrYGUnf 970Q6unWWgwcgTStlpflz31e8AbxXqNYZEFvEktojYS0kAfiYES+H02GUU5PtV7B9Y 98BbtZEowU2DPuqRGG1FF8mAsp1vojcbQGx+nS2Of47oQJRrJlh28COXyf2w/+IRAz 99RmeYin+9pisfrT9fmlUtxa7sAAV/KZFRx8ED31YiktXgI/u/PNnHlchiCMaL6pzA 100HMBf115O7A2y6IZ9sXUHvH8V9QnDkWT1XHMn8GCW8HXOA5zA232OxiaRmQIDAQAB 101oywwKjAJBgNVHRMEAjAAMB0GA1UdDgQWBBQAoZ7yMjDHMAFtmmmO/zyz3BJ6hjAN 102BgkqhkiG9w0BAQUFAAOCAQEAHHgoOg57S+lAEejahdBE1HMwe6BF3b9bzUMCynn9 1037buXa3cnRFO3H3674WKU6nBjv4nkT3qMyXwgi7MvXcu69msK4eM6QA8XeC7G1rD+ 1042bb/ENR9R9Zo0BWLym/ij8uUA/BzX8hnbzWxN82+FMdY9WD4fJAJwJ5ZPEbU1Vfy 1057wOWosHgDPXjeAhlhkxDQi6vlRTJdfED6tBY7iGD4AQXfzrHzAZpZlIvKbM2c54B 10665wMSlqfEWMBDhT5qcwGCq82hmi7/sCtu9Z20g2s9F0fp4XlGX8L7l0hCa46zjay 10737GffYsScEDFg/DmkIpcXnGzyx8l1msLzpj8Gt4zHhPlgA==\u0026lt;/dsig:X509Certificate\u0026gt; 108\u0026lt;/dsig:X509Data\u0026gt; 109\u0026lt;/dsig:KeyInfo\u0026gt; 110\u0026lt;/dsig:Signature\u0026gt;\u0026lt;/lns:LicenseClass\u0026gt; 文件下载：iDRAC7_Ent_Trial.xml ","date":"2023-04-06","img":"","permalink":"https://bajie.dev/posts/20230406-dell_idrac/","series":null,"tags":null,"title":"Dell得Idrac临时license"},{"categories":null,"content":"graphviz是很强烈的描述语言绘图工具\n安装：\n1apt install graphviz 或 yum install graphviz 生成png\n1dot -Tpng hn.gv -o hn.png vi hg.gv\rdigraph MyGraph {\rcompound = true\rmargin=\u0026quot;0,0\u0026quot;\rranksep = 1\rnodesep = 1\rrankdir=LR\r{rank=same;防火墙;日志审计}\rsubgraph cluster_app {\rlabel=\u0026quot;海南应用\u0026quot;\rrankdir=LR\rmargin = 10\r{rank=same;app01;app02}\rapp01 [label=\u0026quot;app01\\n内网：192.168.0.10\u0026quot;]\rapp02 [label=\u0026quot;app02\\n内网：192.168.0.11\u0026quot;]\rstorage01 [label=\u0026quot;storage01\\n内网：192.168.0.30\u0026quot;]\rsubgraph cluster_db {\rstyle = dotted\rlabel=\u0026quot;数据库主/备\u0026quot;\r{rank=same;db02;db01}\rdb01 [label=\u0026quot;db01\\n内网：192.168.0.20\u0026quot;]\rdb02 [label=\u0026quot;db02\\n内网：192.168.0.21\u0026quot;]\rdb01 -\u0026gt; db02 [dir=both]\r}\rapp01 -\u0026gt; db01 [splines=true,lhead=cluster_db]\rapp02 -\u0026gt; db01 [splines=true,lhead=cluster_db]\rapp01 -\u0026gt; storage01\rapp02 -\u0026gt; storage01\r}\r​\nsubgraph cluster_wjw {\rlabel=\u0026quot;卫健委\u0026quot;\rstyle = dotted\rmargin = 10\r{rank=same;front02;front01}\rfront01 [label=\u0026quot;front01\\n内网：192.168.0.5\u0026quot;]\rfront02 [label=\u0026quot;front02\\n内网：192.168.0.6\u0026quot;]\rfront01 -\u0026gt; front02 [dir=both]\r}\r​\n堡垒机 [shape=box,style=rounded,color=red,fontname=\u0026quot;wqy-microhei\u0026quot;,fontcolor=black,fontsize=16,label=\u0026quot;堡垒机\\n外网：124.225.67.34\\n内网：192.168.0.40\u0026quot;]\r防火墙 [shape=box,style=filled,color=green,fontname=\u0026quot;wqy-microhei\u0026quot;,fontsize=16,label=\u0026quot;防火墙\\n外网：124.225.200.190\\n内网：192.168.0.181\u0026quot;]\r日志审计 [shape=box,style=filled,color=green,fontname=\u0026quot;wqy-microhei\u0026quot;, fontsize=16,label=\u0026quot;日志审计\\n外网：124.225.71.204\\n内网：192.168.0.41\u0026quot;]\r堡垒机 -\u0026gt; 防火墙\r防火墙 -\u0026gt; 日志审计\r防火墙 -\u0026gt; app01 [lhead = cluster_app]\rstorage01 -\u0026gt; front01 [ltail = cluster_app, lhead= cluster_wjw]\r}\r生成的图: Ubuntu下命令行打开这图 xdg-open hn.png\n厉害的工具，生成的手绘图： https://sketchviz.com/new ","date":"2023-04-03","img":"","permalink":"https://bajie.dev/posts/20230403-graphviz/","series":null,"tags":null,"title":"利用graphviz描述语言绘图"},{"categories":null,"content":"之前公司世纪互联和无锡的TureNAS现有硬盘都是38块，满配是60块，所以都需要扩容，扩满再增加22块。\nTrueNAS的Raid是使用的RaidZ3，基于ZFS的，最多允许3块盘坏\n首先会建立zpool，然后在zpool里面增加vdev，注意，vdev一旦增加，不可更改。\n我们这里就犯了第一个错误，所有vdev的磁盘数量最好相等，所以第一次应该先增加30块盘，然后第二次再增加30块，这样两个vdev就是均衡的\n现在我们这种状况，第一个vdev是38块，第二个vdev是22块，不对等了，会警告\n具体添加步骤如下：\n首先浪潮工程师到现场加盘，盘必须做好清除信息，用以下命令通过\n1dd if=/dev/zero of=/dev/da59 bs=1M count=32 然后插好盘后，必须重启Trunas，才能正常认出盘来，不能热插拔（很奇怪） Storage \u0026ndash;\u0026gt; Pools \u0026ndash;\u0026gt; 点击齿轮 \u0026ndash;\u0026gt; Add Vdevs\n然后选中所有左边的Available Disks，移到右边的Data VDevs 然后看最下面会立刻出红色警告，提示两个vdev的disks不对等\n点击ADD VDEVS，会弹窗警告，选中Confirm，然后点Continue\n会继续弹出一个警告窗，这回就明晰了，旧的数据不会破坏，然后继续选中Confirm，然后点ADD VDEVS\n然后就会开始初始化硬盘\n最后查看zpool，看到有两个RAIDZ3，就加好了\n","date":"2023-03-31","img":"","permalink":"https://bajie.dev/posts/20230331-truenas_adddisk/","series":null,"tags":null,"title":"TrueNAS系统如何增加新硬盘"},{"categories":null,"content":"收录一下 dell 服务器 idrac 操作常用脚本\n显示Raid卡硬盘\rsshpass -p \u0026quot;calvin\u0026quot; ssh -oLogLevel=ERROR -oStrictHostKeyChecking=no root@10.224.$1 racadm raid get controllers\rsshpass -p \u0026quot;calvin\u0026quot; ssh -oLogLevel=ERROR -oStrictHostKeyChecking=no root@10.224.$1 racadm raid get vdisks\rsshpass -p \u0026quot;calvin\u0026quot; ssh -oLogLevel=ERROR -oStrictHostKeyChecking=no root@10.224.$1 racadm raid get pdisks 清理Foreign磁盘状态\rsshpass -p \u0026quot;calvin\u0026quot; ssh -oLogLevel=ERROR -oStrictHostKeyChecking=no root@10.224.$1 racadm storage clearconfig:RAID.Integrated.1-1\rsshpass -p \u0026quot;calvin\u0026quot; ssh -oLogLevel=ERROR -oStrictHostKeyChecking=no root@10.224.$1 racadm jobqueue create RAID.Integrated.1-1 -s TIME_NOW --realtime\r删除vdisk\rsshpass -p \u0026quot;calvin\u0026quot; ssh -oLogLevel=ERROR -oStrictHostKeyChecking=no root@10.224.5.14 racadm raid deletevd:Disk.Virtual.0:RAID.Integrated.1-1\rsshpass -p \u0026quot;calvin\u0026quot; ssh -oLogLevel=ERROR -oStrictHostKeyChecking=no root@10.224.$1 racadm jobqueue create RAID.Integrated.1-1 -s TIME_NOW --realtime\r建立Raid0\rsshpass -p \u0026quot;calvin\u0026quot; ssh -oLogLevel=ERROR -oStrictHostKeyChecking=no root@10.224.$1 racadm raid createvd:RAID.Integrated.1-1 -rl r0 -wp wb -rp ra -name raid_0 -pdkey:Disk.Bay.0:Enclosure.Internal.0-1:RAID.Integrated.1-1\rsshpass -p \u0026quot;calvin\u0026quot; ssh -oLogLevel=ERROR -oStrictHostKeyChecking=no root@10.224.$1 racadm jobqueue create RAID.Integrated.1-1 -s TIME_NOW --realtime\r建立Raid5\rsshpass -p \u0026quot;calvin\u0026quot; ssh -oLogLevel=ERROR -oStrictHostKeyChecking=no root@10.224.$1 racadm raid createvd:RAID.Integrated.1-1 -rl r5 -wp wb -rp ra -name raid_5 -pdkey:Disk.Bay.1:Enclosure.Internal.0-1:RAID.Integrated.1-1,Disk.Bay.2:Enclosure.Internal.0-1:RAID.Integrated.1-1,Disk.Bay.3:Enclosure.Internal.0-1:RAID.Integrated.1-1\rsshpass -p \u0026quot;calvin\u0026quot; ssh -oLogLevel=ERROR -oStrictHostKeyChecking=no root@10.224.$1 racadm jobqueue create RAID.Integrated.1-1 -s TIME_NOW --realtime\r重启服务器\rsshpass -p \u0026quot;calvin\u0026quot; ssh -oLogLevel=ERROR -oStrictHostKeyChecking=no root@10.224.$1 racadm serveraction hardreset\r设置硬盘第一启动，禁止F1/F2等待\rsshpass -p \u0026quot;calvin\u0026quot; ssh -oLogLevel=ERROR -oStrictHostKeyChecking=no root@10.224.$1 racadm set BIOS.biosbootsettings.BootSeq HardDisk.List.1-1,NIC.Integrated.1-1-1\rsshpass -p \u0026quot;calvin\u0026quot; ssh -oLogLevel=ERROR -oStrictHostKeyChecking=no root@10.224.$1 racadm set BIOS.MiscSettings.ErrPrompt Disabled\rsshpass -p \u0026quot;calvin\u0026quot; ssh -oLogLevel=ERROR -oStrictHostKeyChecking=no root@10.224.5.16 racadm jobqueue create BIOS.Setup.1-1 sshpass -p \u0026quot;calvin\u0026quot; ssh -oLogLevel=ERROR -oStrictHostKeyChecking=no root@10.224.$1 racadm serveraction hardreset\r设置vnc\rsshpass -p \u0026quot;calvin\u0026quot; ssh -oLogLevel=ERROR -oStrictHostKeyChecking=no root@10.224.$1 racadm set idrac.vncserver.enable Enabled\rsshpass -p \u0026quot;calvin\u0026quot; ssh -oLogLevel=ERROR -oStrictHostKeyChecking=no root@10.224.$1 racadm set idrac.vncserver.Password calvin\r设置idrac其他用户\rsshpass -p \u0026quot;calvin\u0026quot; ssh -oLogLevel=ERROR -oStrictHostKeyChecking=no root@10.224.$1 racadm config -g cfgUserAdmin -o cfgUserAdminUserName -i 4 newuser\rsshpass -p \u0026quot;calvin\u0026quot; ssh -oLogLevel=ERROR -oStrictHostKeyChecking=no root@10.224.$1 racadm config -g cfgUserAdmin -o cfgUserAdminPassword -i 4 123456\rsshpass -p \u0026quot;calvin\u0026quot; ssh -oLogLevel=ERROR -oStrictHostKeyChecking=no root@10.224.$1 racadm config -g cfgUserAdmin -o cfgUserAdminPrivilege -i 4 0x000001ff\rsshpass -p \u0026quot;calvin\u0026quot; ssh -oLogLevel=ERROR -oStrictHostKeyChecking=no root@10.224.$1 racadm config -g cfgUserAdmin -o cfgUserAdminEnable -i 4 1\r改掉密码\rsshpass -p \u0026quot;calvin\u0026quot; ssh -oLogLevel=ERROR -oStrictHostKeyChecking=no root@10.224.$1 racadm config -g cfgUserAdmin -o cfgUserAdminPassword -i 4 987654\rNTP的设置\rsshpass -p \u0026quot;calvin\u0026quot; ssh -oLogLevel=ERROR -oStrictHostKeyChecking=no root@10.224.$1 racadm set idrac.ipv4static.dns1 8.8.8.8\rsshpass -p \u0026quot;calvin\u0026quot; ssh -oLogLevel=ERROR -oStrictHostKeyChecking=no root@10.224.$1 racadm set idrac.NTPConfigGroup.ntp1 0.asia.pool.ntp.org\rsshpass -p \u0026quot;calvin\u0026quot; ssh -oLogLevel=ERROR -oStrictHostKeyChecking=no root@10.224.$1 racadm set idrac.NTPConfigGroup.ntp2 1.asia.pool.ntp.org\rsshpass -p \u0026quot;calvin\u0026quot; ssh -oLogLevel=ERROR -oStrictHostKeyChecking=no root@10.224.$1 racadm set idrac.NTPConfigGroup.ntp3 2.asia.pool.ntp.org\rsshpass -p \u0026quot;calvin\u0026quot; ssh -oLogLevel=ERROR -oStrictHostKeyChecking=no root@10.224.$1 racadm set idrac.NTPConfigGroup.ntp1 129.250.35.250\rsshpass -p \u0026quot;calvin\u0026quot; ssh -oLogLevel=ERROR -oStrictHostKeyChecking=no root@10.224.$1 racadm set idrac.NTPConfigGroup.ntp2 180.211.88.50\rsshpass -p \u0026quot;calvin\u0026quot; ssh -oLogLevel=ERROR -oStrictHostKeyChecking=no root@10.224.$1 racadm set idrac.NTPConfigGroup.ntp3 202.112.29.82\rsshpass -p \u0026quot;calvin\u0026quot; ssh -oLogLevel=ERROR -oStrictHostKeyChecking=no root@10.224.$1 racadm set idrac.NTPConfigGroup.NTPEnable Enabled\rsshpass -p \u0026quot;calvin\u0026quot; ssh -oLogLevel=ERROR -oStrictHostKeyChecking=no root@10.224.$1 racadm set idrac.NTPConfigGroup.NTPMaxDist 16\rsshpass -p \u0026quot;calvin\u0026quot; ssh -oLogLevel=ERROR -oStrictHostKeyChecking=no root@10.224.$1 racadm set idrac.time.timezone Japan\r修改网卡启动为Legacy PXE\rsshpass -p \u0026quot;calvin\u0026quot; ssh -oLogLevel=ERROR -oStrictHostKeyChecking=no root@10.224.$1 racadm get nic.nicconfig.1 | grep Legacy\rsshpass -p \u0026quot;calvin\u0026quot; ssh -oLogLevel=ERROR -oStrictHostKeyChecking=no root@10.224.$1 racadm set nic.nicconfig.1.legacybootproto PXE\rsshpass -p \u0026quot;calvin\u0026quot; ssh -oLogLevel=ERROR -oStrictHostKeyChecking=no root@10.224.$1 racadm get nic.nicconfig.1 | grep Legacy\rsshpass -p \u0026quot;calvin\u0026quot; ssh -oLogLevel=ERROR -oStrictHostKeyChecking=no root@10.224.$1 racadm jobqueue create NIC.Integrated.1-1-1\ripmitool -I lanplus -H $ip -U root -P calvin chassis power reset ","date":"2023-03-31","img":"","permalink":"https://bajie.dev/posts/20230331-idrac_dell/","series":null,"tags":null,"title":"Dell服务器idrac常用操作脚本"},{"categories":null,"content":"无锡浪潮TrueNAS系统的安装\n这一版的浪潮定制NAS机器比较特别，没有任何Raid卡，配了Avago的一块pcie sas直通卡：搏通9400卡\n然后是定制机，整体4U的高度，满配是60块16T的大盘，主板在机箱侧面立着，2块小盘做系统。\n由于机房限制，无法直接从本地访问10.18.30.97，导致无法将本地文件挂入idrac的cdrom，只能曲线救国，在172.18.31.2建立一个nfs share，把光盘文件放进去\n到 idrac 的 Remote control ， 再到Virtual media，配好nfs共享 ip: 172.18.31.2 path: /export/nfsshare\n在remote image redirction中可以看到光盘文件，点击Start\n然后重启，CDROM是第一启动媒介\n开始安装，缺省会进入第一项进行安装\n然后会报这个错，panic: AP #1 (PHY# 2) failed!\n见了鬼了，唯一的地方可能是主板部分和博通卡冲突，我们重启进入BIOS，到SATA和sSATA的地方\n把主板的SATA给关掉，禁止AHCI\n同样关掉sSATA，也禁止AHCI\n然后重启安装TrueNAS，这次就可以通过了 安装盘选择前两块SSD小盘，230G openbsd会自动将这两款小盘做成软Raid\n提示会抹掉两块盘的所有数据\n输入root的密码\n选择Boot via BIOS\n建立16G的swap空间\n然后就开始安装，这里没什么动静，也没有进度条，会等很久\n安装完成后会让你重启，然后出来配置界面\n我们首先要配置第二项，Bonding端口 然后再配第一项的静态IP 再配第四项缺省路由 配完就http和https显示正确地址\n输入地址：http://172.18.30.97 登录就完成安装了\n","date":"2023-03-30","img":"","permalink":"https://bajie.dev/posts/20230330-truenas_install/","series":null,"tags":null,"title":"TureNAS系统在浪潮定制系统上的安装"},{"categories":null,"content":"特别注意 两台GFS主机172.18.30.18和172.18.30.36上面务必配置/etc/hosts，否则peer的时候会有问题\n172.18.30.18 renhe-18-30-18\r172.18.30.18 renhe-18-30-36\r客户端安装 1yum -y install epel-release 2 3然后 yum install glusterfs-fuse 就可以了 挂载： mount.glusterfs 172.18.30.18:/borui-vol /data/br/nfs\nfstab 自动挂载 172.18.30.18:/borui-vol /data/br/nfs glusterfs defaults,_netdev,backupvolfile-server=172.18.30.36 0 0\n在172.18.30.18上建立新卷，因为只有2个节点，就必须force了 1gluster volume create test-zhichi-vol replica 2 transport tcp 172.18.30.18:/glusterfs/test-zhichi-vol 172.18.30.36:/glusterfs/test-zhichi-vol force 启动 gluster volume start test-zhichi-vol\n查看一下 gluster volume info test-zhichi-vol\n查看卷信息(禁止查看inode，太多了) 1gluster volume status test-zhichi-vol detail 2gluster volume status test-zhichi-vol clients 3gluster volume status test-zhichi-vol mem 4gluster volume status test-zhichi-vol fd 5gluster volume status test-zhichi-vol inode 开启限额 1gluster volume quota test-zhichi-vol enable 2gluster volume quota test-zhichi-vol limit-usage / 50GB 3gluster volume quota test-zhichi-vol list 限制IP访问 例如允许172.18.31.*网段的主机访问rep-volume：\ngluster volume set test-zhichi-vol auth.allow 172.18.31.*`\n如果客户端想用NFS挂载 gluster volume set test-zhichi-vol nfs.disable off\n开启性能分析（慎用） gluster volume profile test-zhichi-vol start\n开启后性能分析后，显示I/O信息: gluster volume profile gv0 info\nGlusterd 启动失败，原因未知处理办法（慎用，拷出数据后再用）： 1rm -rf /var/lib/glusterd/* 2systemctl start glusterd 脑裂解决方法： 脑裂： 使用replica模式的时候，如果发生网络故障（比如交换机坏了、网线被碰掉了），而两台机器都还活着的时候，它们各自的数据读写还会继续。\n当网络恢复时，它们都会认为自己的数据才是正确的，对方的是错误的，这就是俗称的脑裂。\n双方谁都不肯妥协，结果就是文件数据读取错误，系统无法正常运行。 在正常的机器上执行以下操作：\n1gluster volume status nfsp （看看这个节点有没有在线） 2gluster volume heal nfsp full （启动完全修复） 3gluster volume heal nfsp info （查看需要修复的文件） 4gluster volume heal nfsp info healed （查看修复成功的文件） 5gluster volume heal nfsp info heal-failed （查看修复失败的文件） 6gluster volume heal nfsp info split-brain （查看脑裂的文件） 1Gathering Heal info on volume nfsp has been successful 2 3Brick gls03.mycloud.com.cn:/glusterfs/nfsp 4Number of entries: 24 5 6at path on brick 7----------------------------------- 8 92014-05-30 10:22:20 /36c741b8-2de2-46e9-9e3c-8c7475e4dd10 10。。。。。。。。。。。。。。。。。。。。。。。。。。。。。 11。。。。。。。。。。。。。。。。。。。。。。。。。。。。。 12。。。。。。。。。。。。。。。。。。。。。。。。。。。。。 在有病的那台机器上，删除脑裂的文件： （注意！要删除的文件是在gluster volume info nfsp看到的目录中，不要去删除挂载的目录中的文件，不然就等着哭吧） 把硬链接文件找出来，也要删除：\n1find /glusterfs/nfsp/ -samefile /glusterfs/nfsp/36c741b8-2de2-46e9-9e3c-8c7475e4dd10 -print /glusterfs/nfsp/36c741b8-2de2-46e9-9e3c-8c7475e4dd10 /glusterfs/nfsp/.glusterfs/65/26/6526e766-cb26-434c-9605-eacb21316447 （这里看得出硬链接文件的目录名和日志中的gfid的对应关系）\n删除掉：\n1rm /glusterfs/nfsp/36c741b8-2de2-46e9-9e3c-8c7475e4dd10 2rm /glusterfs/nfsp/.glusterfs/65/26/6526e766-cb26-434c-9605-eacb21316447 在正常的机器上执行\n1tail -1 /nfsprimary/36c741b8-2de2-46e9-9e3c-8c7475e4dd10 （读一下这个文件，触发修复） 2ls -l /glusterfs/nfsp/36c741b8-2de2-46e9-9e3c-8c7475e4dd10 人工查看一下两台机器的数据是否一致\n其它的脑裂文件也是一样处理。 没问题的话，重新挂载目录:\n1umount /nfsprimary 2/bin/mount -t glusterfs 10.10.10.21:/nfsp /nfsprimary/ ","date":"2023-03-30","img":"","permalink":"https://bajie.dev/posts/20230330-glusterfs/","series":null,"tags":null,"title":"GlusterFS的实际应用"},{"categories":null,"content":"突然来的个需求，要求屏蔽国外的用户登录公司的openvpn，防止滥用，搜了一下教程倒是真不少，问题不少都是要去下载那个无比大的ip地址库，好不容易找了一个可用的。\n记录下来，备用，原理很简单，ipset建立一个china的hashset，不停添加条目，最后用iptalbes阻挡一下：\n1#!/bin/sh 2ipset create china hash:net hashsize 10000 maxelem 1000000 3ipset add china 1.0.1.0/24 4...... 5ipset add china 91.234.36.0/24 6iptables -I INPUT -m set --match-set china src -p udp -m udp --dport 1194 -j ACCEPT 7iptables -A INPUT -p udp --dport 1194 -j DROP 相应脚本的下载：\nipset-rules.txt ","date":"2023-03-29","img":"","permalink":"https://bajie.dev/posts/20230329-ipset_block/","series":null,"tags":null,"title":"使用ipset来禁止国外的用户登录openvpn"},{"categories":null,"content":"换了公司，也搞起了桌面运维，配置交换机的过程记录一下，其实很简单，就是命令记不住。\n拿到一台华为交换机，重新初始化后，配置如下：\n1\u0026lt;HUAWEI\u0026gt;dis cur 2!Software Version V200R008C00SPC500 3# 4sysname HUAWEI 5# 6aaa 7 authentication-scheme default 8 authorization-scheme default 9 accounting-scheme default 10 domain default 11 domain default_admin 12 local-user admin password irreversible-cipher %^%#7:.iL]+u\u0026#34;4\\j8ZFhGeg/-m.\u0026amp;\u0026#34;^0}kMznjk%\u0026gt;;BaUDO/\u0026#39;6m\\X\\=V8JGY:W;i,%^%# 13 local-user admin service-type http 14# 15interface Vlanif1 16# 17interface MEth0/0/1 18# 19interface GigabitEthernet0/0/1 20# 21interface NULL0 22# 23user-interface con 0 24 authentication-mode password 25 set authentication password cipher $1a$~!;$-JF0-W$Z\u0026gt;\u0026lt;1.F]\u0026lt;sF.R_NBj34CJ/JPe=/tZDMM(Ws3\u0026#39;9u%+$ 26user-interface vty 0 4 27user-interface vty 16 20 28# 29return 一、配名称 1sysname BJ_FANGHENG_JIERU 二、配置登录用户 11、aaa 2local-user admin password irreversible-cipher abcdefg 3local-user admin privilege level 3 4local-user admin service-type telnet terminal ssh 5 62、user-interface con 0 7authentication-mode aaa 8 93、user-interface vty 0 4 10authentication-mode aaa 11protocol inbound all 三、配置下联交换机端口 1interface GigabitEthernet0/0/23 2 description == H3cSW --\u0026gt; 24 3 port link-type trunk 4 port trunk allow-pass vlan 2 to 4094 四、配置上联交换机端口 1interface GigabitEthernet0/0/24 2 description == RuijieRoute --\u0026gt; lan0 3 port link-type trunk 4 port trunk allow-pass vlan 2 to 4094 五、配置vlan地址 1vlan 11 2vlan 100 3 4vlan 11 5 interface Vlanif11 6 ip address 10.8.0.7 255.255.254.0 7 8vlan 100 9 interface Vlanif100 10 ip address 192.168.10.7 255.255.255.0 六、配置缺省路由 1ip route-static 0.0.0.0 0.0.0.0 10.8.0.1 七、批量配置端口 1port-group group-member GigabitEthernet0/0/1 to GigabitEthernet0/0/23 2port link-type access 3port default vlan 11 4stp edged-port enable 八、lldp 1lldp enable 九、dhcp 1dhcp enable 2ip pool 11 3 gateway-list 10.8.0.1 4 network 10.8.0.0 mask 255.255.254.0 5 static-bind ip-address 10.8.0.4 mac-address 8005-88f1-fa62 6 dns-list 114.114.114.114 7 8interface Vlanif11 9 description === Ke hu duan 10 ip address 10.8.0.1 255.255.254.0 11 dhcp select global 十、telnet和ssh 1dsa local-key-pair create 2 3telnet server enable 4 5stelnet server enable 6ssh authentication-type default password 这样就完成了一台核心设备的简单配置\n","date":"2023-03-24","img":"","permalink":"https://bajie.dev/posts/20230324-huawei_switch/","series":null,"tags":null,"title":"核心交换机的配置过程"},{"categories":null,"content":"换了公司，现在的公司用的是钉钉，不是企业微信，那么 hubot 就得改接入钉钉了\n前文回顾：Hubot集成企业微信+jenkins+ansible 不明白的可以先看那一篇，那么首先的步骤是一样的，同样要去钉钉开放平台，用管理员登录：\nhttps://open.dingtalk.com/ 登陆后，点击应用开发\u0026ndash;\u0026gt;企业应用开发：\n’\n然后应用开发，机器人，点击创建应用：\n建好后，点击应用信息，可以看到应用凭证\n我们记录下来 AppSecret，之后要用到\n然后再点击开发管理，这里需要你把 hubot 的服务器地址给公布出去，需要有个公网地址\n因为hubot是监听的8080端口，所以映射是 xxxx.ip:80 \u0026ndash;\u0026gt; hubotip:8080\n服务器出口 IP 的地方 , 需要在 hubot 的服务器上，curl http://ipinfo.io ，得到地址，然后填上（我们的 ip 非常特殊，每一次访问都有可能会换个ip，所以只好把整段填写进去，而且把公网映射ip也填进去）\n消息接收地址填上映射后的地址：https://bot.rendoumi.com/hubot/dingtalk/message/（用不用nginx加证书变https随具体情况定）\n然后去hubot安装dingtalk插件，在hubot安装根目录运行\n1npm install hubot-dingtalk 2 3export HUBOT_DINGTALK_AUTH_TYPE=sign 4export HUBOT_DINGTALK_SECRET=xxxxxxxxxx 5export HUBOT_DINGTALK_MODE=1 6 7./bin/hubot -a dingtalk 然后我们去用浏览器访问 https://bot.rendoumi.com/hubot/dingtalk/message/，会返回这个\n然后就可以了。至于把 hubot 做成服务，就参考上一篇文章，把jenkins和ansible都加上，做一个好用的机器人。\n","date":"2023-03-01","img":"","permalink":"https://bajie.dev/posts/20230301-hubot_dingding/","series":null,"tags":null,"title":"Hubot集成企业钉钉"},{"categories":null,"content":"接上上一篇，Freeipa接完confluence，接下来是接入数据库上线软件Yearning.\n先下结论：同样存在先有 Yearning 账户、然后才有的 FreeIPA LDAP 目录，这次要命了，因为 Yearning 不支持 LDAP 认证方式用户与已有用户进行合并！所以，等于ldap新建了一个用户，跟老用户的邮箱一模一样，但是是个 Newbee，没有权限，需要重新赋权。比较麻烦！希望之后能pr修改一下！！！！\n连接属性如下：\n1ldap地址：freeipa.bybon.cn:389 2 3LDAP管理员DN：uid=manager,cn=users,cn=accounts,dc=bybon,dc=cn 4LDAP管理员密码：xxxxxx 5 6LDAP_Search Filter：(\u0026amp;(objectclass=inetorgperson)(memberOf=cn=confluence-users,cn=groups,cn=accounts,dc=bybon,dc=cn)(uid=%s)) 7 8LDAP_SCBASE：cn=users,cn=accounts,dc=bybon,dc=cn 9 10LDAP用户属性：{ \u0026#34;real_name\u0026#34;: \u0026#34;displayName\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;mail\u0026#34;, \u0026#34;department\u0026#34;: \u0026#34;技术中心\u0026#34; } 解释一下：\nLDAP_Search Filter 指搜索用户时所用的filter，这里偷了个懒，按道理应该是独立建一个组yearning-users，懒得干了，复用confluence-users好了。admin组更懒得建，必须用freeipa的admins组\nLDAP_SCBASE 指搜索用户的BASE DN\nLDAP用户属性指yearning中的属性与ldap属性之间的映射关系，department是没有对应的，也没有人会没事干建一个去吧。\n截图如下：\n","date":"2023-02-28","img":"","permalink":"https://bajie.dev/posts/20230228-freeipa_yearning/","series":null,"tags":null,"title":"Yearning集成进freeIPA进行统一认证"},{"categories":null,"content":"k8s里面建立一个用户，然后给特定权限，再做rolebinding的过程，给个标准的建立jenkins-admin的用户的过程：\n简单来说，三步，ServiceAccout \u0026ndash;\u0026gt; Role \u0026ndash;\u0026gt; Rolebinding\n1apiVersion: v1 2kind: ServiceAccount 3metadata: 4 name: jenkins-admin 5 namespace: devops-tools 6--- 7apiVersion: rbac.authorization.k8s.io/v1 8kind: Role 9metadata: 10 name: jenkins 11 namespace: default 12 labels: 13 \u0026#34;app.kubernetes.io/name\u0026#34;: \u0026#39;jenkins\u0026#39; 14rules: 15- apiGroups: [\u0026#34;\u0026#34;] 16 resources: [\u0026#34;pods\u0026#34;] 17 verbs: [\u0026#34;create\u0026#34;,\u0026#34;delete\u0026#34;,\u0026#34;get\u0026#34;,\u0026#34;list\u0026#34;,\u0026#34;patch\u0026#34;,\u0026#34;update\u0026#34;,\u0026#34;watch\u0026#34;] 18- apiGroups: [\u0026#34;\u0026#34;] 19 resources: [\u0026#34;pods/exec\u0026#34;] 20 verbs: [\u0026#34;create\u0026#34;,\u0026#34;delete\u0026#34;,\u0026#34;get\u0026#34;,\u0026#34;list\u0026#34;,\u0026#34;patch\u0026#34;,\u0026#34;update\u0026#34;,\u0026#34;watch\u0026#34;] 21- apiGroups: [\u0026#34;\u0026#34;] 22 resources: [\u0026#34;pods/log\u0026#34;] 23 verbs: [\u0026#34;get\u0026#34;,\u0026#34;list\u0026#34;,\u0026#34;watch\u0026#34;] 24- apiGroups: [\u0026#34;\u0026#34;] 25 resources: [\u0026#34;secrets\u0026#34;] 26 verbs: [\u0026#34;get\u0026#34;] 27--- 28apiVersion: rbac.authorization.k8s.io/v1 29kind: RoleBinding 30metadata: 31 name: jenkins-role-binding 32 namespace: default 33roleRef: 34 apiGroup: rbac.authorization.k8s.io 35 kind: Role 36 name: jenkins 37subjects: 38- kind: ServiceAccount 39 name: jenkins-admin 40 namespace: default 资源的链接：https://kubernetes.io/zh-cn/docs/reference/access-authn-authz/rbac/\n","date":"2023-02-22","img":"","permalink":"https://bajie.dev/posts/20230222-kubernetes_rbac/","series":null,"tags":null,"title":"Kubernetes中用户rbac的建立"},{"categories":null,"content":"接上一篇，Freeipa接完confluence，接下来是接入Gitlab.\n先下结论：同样道理，也是存在先有 Gitlab 账户、然后才有的 FreeIPA LDAP 目录，这也不要紧，因为 Gitlab 同样支持 LDAP 认证方式的用户与现有用户进行合并。\n举个例子，如果我已经在 Gitlab 中创建了用户名为 zhangranrui、邮箱为 zhangranrui@rendoumi.com 的用户，那么我在 LDAP 目录中只需要把 mail 字段也写成 zhangranrui@rendoumi.com 即可被 Gitlab 识别成同一用户。或者说，我们可以在 Gitlab 中增加 LDAP 目录中的 mail 字段的邮箱（Gitlab 支持同一用户绑定多个邮箱），这样在 Gitlab 中使用 LDAP 认证的时候也会被视为同一用户。跟 Confluence 是一模一样的。\n注意：这样改了之后，无论登录或是拉取代码，就只能使用LDAP的账号密码，不能使用旧的 GitLab Standard 的账号。\n我们的 Gitlab 版本比价老11.1.0，所以改的是 gitlab.yml\n1 ldap: 2 enabled: true 3 servers: 4 main: # \u0026#39;main\u0026#39; is the GitLab \u0026#39;provider ID\u0026#39; of this LDAP server 5 label: \u0026#39;LDAP\u0026#39; 6 7 host: \u0026#39;freeipa.bybon.cn\u0026#39; 8 port: 389 9 uid: \u0026#39;uid\u0026#39; 10 11 encryption: \u0026#39;plain\u0026#39; # \u0026#34;start_tls\u0026#34; or \u0026#34;simple_tls\u0026#34; or \u0026#34;plain\u0026#34; 12 verify_certificates: false 13 ca_file: \u0026#39;\u0026#39; 14 ssl_version: \u0026#39;\u0026#39; 15 16 bind_dn: \u0026#39;uid=admin,cn=users,cn=accounts,dc=bybon,dc=cn\u0026#39; 17 password: \u0026#39;xxxxxx\u0026#39; 18 19 timeout: 10 20 21 active_directory: false 22 allow_username_or_email_login: false 23 24 block_auto_created_users: false 25 26 base: \u0026#39;dc=bybon,dc=cn\u0026#39; 27 28 user_filter: \u0026#39;(\u0026amp;(objectclass=inetorgperson)(memberOf=cn=gitlab-users,cn=groups,cn=accounts,dc=bybon,dc=cn))\u0026#39; 29 30 attributes: 31 username: [\u0026#39;uid\u0026#39;] 32 email: [\u0026#39;mail\u0026#39;] 33 34 name: \u0026#39;displayName\u0026#39; 35 first_name: \u0026#39;givenName\u0026#39; 36 last_name: \u0026#39;sn\u0026#39; 37 38 lowercase_usernames: false 39 新版本的话，按字段比较即可。注意上面，我是提前建立了一个 gitlab-users 的组，把用户先都放进组里，这样方便管理。\n","date":"2023-02-17","img":"","permalink":"https://bajie.dev/posts/20230217-freeipa_gitlab/","series":null,"tags":null,"title":"Gitlab集成进freeIPA进行统一认证"},{"categories":null,"content":"公司决定用统一用户管理系统，那必然是微软的 AD 和开源的 LDAP 二选一了\n自然用 Freeipa 作为首选。\n花时间仔细调研了一下，先说结论：\n如果 confluence 已经有很多用户，那么对不起，这些用户的密码都必须通知到个人，强制进行修改，旧有的密码完全无法导出（除非一个一个人问出来）。\nconfluence 实际上是用了一个内置的目录软件来管理用户的，用户唯一存在的凭据就是邮箱。\nconfluence 支持同时从多个目录树中按照顺序来查询用户，查询到的结果会合并。举例来说，排第一的目录树是freeipa，排第二位的目录树是内置目录，两个目录树都有一个用户，两条记录的邮件是一致的，那么会在排第一的 freeipa 树中认证用户，但会把两个目录树中用户的信息拼接合并起来总体返回。\n这样就明白了把，我们只用第一个 freeipa 目录树来认证用户，原有的组权限还是用第二个内置目录树中的信息，保持两棵树中用户的 mail 保持一致即可，步骤如下。\n我们首先要提前在freeIPA里面建立两个组：\nconfluence-administrators\nconfluence-users\n然后跑到 Confluence 里，用户目录，添加一个 freeipa 的目录服务放在前面\n详细配置的参数如下：\n1配置如下： 2 3Server Settings: 4- Namel: freeipa 5- Directory Type: OpenLDAP 6- Server: example.com 7- Port: 389 8- Use SLL: false 9- Username: uid=admin,cn=users,cn=accounts,dc=bybon,dc=cn 10- Password: \u0026lt;insert password here\u0026gt; 11 12 13LDAP Schema: 14- Base DN: dc=bybon,dc=cn 15- Additional User DN: cn=users,cn=accounts 16- Additional Group DN:cn=groups,cn=accounts 17 18 19LDAP Permissions: 20Select Read/Write 21 22 23Advanced Settings: Default 24 25 26User Schema Settings 27- User Object Class: inetorgperson 28- User Object Filter: (\u0026amp;(objectclass=inetorgperson)(memberOf=cn=confluence-users,cn=groups,cn=accounts,dc=bybon,dc=cn)) 29- User Name Attribute: uid 30- User Name RDN Attribute: uid 31- User First Name Attribute: giveName 32- User Last Name Attribute: sn 33- User Display Name Attribute: displayName 34- User Email Attribute: mail 35- User Password Attribute: userPassword 36- User Password Encryption: SHA 37- User Unique ID Attribute: uid 38 39 40Group Schema Settings 41- Group Object Class: groupofnames # all lowercase 42- Group Object Filter: (objectclass=groupofnames) # all lowercase 43- Group Name Attribute: cn 44- Group Description Attribute: description 45 46 47Membership Schema Settings 48- Group Members Attribute: member #lowercase 49- User Membership Attribute:memberOf 50 51 52 放全中文的图如下：\n折磨了很久才弄出来，验证一下用户：\n跑到 confluence 里面查看，明显是两个目录树合并的结果：\n关于 admin 的问题，注意一下，我们上面找用户的 filter 是找不到 admin 用户的。但是第一步认证是 ldap 做的，认证过了以后，freeipa 的 ldap filter 过滤后找不到 admin 这个用户，所以只会在内置目录有 admin 的信息，下面验证一下，admin 只存在于 Confluence Internal Directory 中。\n再补充一点，freeipa 是可以匿名访问的，但是匿名状态下，一些属性字段是看不见的，比如 mail 。\n所以想要查到全部字段，必须登录，放一个查询的脚本：\n1ldapsearch -W -D uid=admin,cn=users,cn=accounts,dc=bybon,dc=cn -h localhost -b cn=users,cn=accounts,dc=bybon,dc=cn \u0026#39;(\u0026amp;(objectclass=inetorgperson)(memberOf=cn=confluence-users,cn=groups,cn=accounts,dc=bybon,dc=cn))\u0026#39; ","date":"2023-02-16","img":"","permalink":"https://bajie.dev/posts/20230216-freeipa_confluence/","series":null,"tags":null,"title":"Confluence集成进freeIPA进行统一认证"},{"categories":null,"content":"nginx和traefik都可以做ingress，在入口处做证书的卸载，并转发tcp、udp、https、http流量\nnginx是比较通常的做法，traefik配置比较简单，尤其是配置自动续签的证书\n1wget https://github.com/traefik/traefik/releases/download/v2.4.8/traefik_v2.4.8_linux_amd64.tar.gz 解压释放出来traefik文件，建立目录/export/servers/traefik\n结构如下：\ntraefik.yml\n1log: 2 level: DEBUG 3 4api: 5 insecure: false 6 dashboard: true 7 8entryPoints: 9 http: 10 address: \u0026#34;:80\u0026#34; 11 #http: 12 # redirections: 13 # entryPoint: 14 # to: https 15 # scheme: https 16 17 https: 18 address: \u0026#34;:443\u0026#34; 19 20 21 22certificatesResolvers: 23 letsEncrypt: 24 acme: 25 storage: /export/servers/traefik/acme.json 26 email: zhangranrui@rendoumi.com 27 tlsChallenge: {} 28 httpChallenge: 29 entryPoint: http 30 31providers: 32 file: 33 directory: /export/servers/traefik/dynamic 34 watch: true 上面我们定义了log的level为DEBUG，并且开放了dashboard\n定义了2个入口，http和https，可以直接用中间件强制http跳转https\n然后定义了letsEncrypt的证书机构\n最后定义了动态监控 /export/servers/traefik/dynamic 目录，如果下面有增加文件会自动更新配置。\n然后再dynamic目录下定义转发routes\n注意命名文件，test7是域名，01是序列号，文件内容中svc的序列号最好跟文件名一致，如果多文件重复会导致配置不可用！！！\ntest7-01.yml\n1http: 2 routers: 3 https_01: 4 rule: \u0026#34;Host(`test7.ddky.com`)\u0026#34; 5 service: svc_01 6 tls: 7 certresolver: letsEncrypt 8 9 http: 10 rule: \u0026#34;Host(`test7.ddky.com`)\u0026#34; 11 service: svc_01 12 entryPoints: 13 - http 14 15 services: 16 svc_01: 17 loadBalancer: 18 servers: 19 - url: \u0026#34;http://172.16.8.1:80\u0026#34; test8-02.yml\n1http: 2 routers: 3 https_02: 4 rule: \u0026#34;Host(`test8.ddky.com`)\u0026#34; 5 service: svc_02 6 tls: 7 certresolver: letsEncrypt 8 9 http_02: 10 rule: \u0026#34;Host(`test8.ddky.com`)\u0026#34; 11 service: svc_02 12 entryPoints: 13 - http 14 15 services: 16 svc_02: 17 loadBalancer: 18 servers: 19 - url: \u0026#34;http://172.18.31.33:80\u0026#34; dashboard.yml\n1http: 2 routers: 3 api-router: 4 rule: \u0026#34;PathPrefix(`/api`) || PathPrefix(`/dashboard`)\u0026#34; 5 service: api@internal 6 entryPoints: 7 - http 8 middlewares: 9 - dashboard-login 10 11 middlewares: 12 dashboard-login: 13 basicAuth: 14 users: 15 - \u0026#34;admin:$apr1$u1xEoYqW$V5O5t4rmdly58WqS4nTVq1\u0026#34; 打开http://192.168.85.202/dashboard/#/\nuser: admin pass: xxxxxxxx\n这样就可以了\n","date":"2023-01-31","img":"","permalink":"https://bajie.dev/posts/20230131-traefik_certificate/","series":null,"tags":null,"title":"Traefik自动签发并续费证书+端口转发"},{"categories":null,"content":"普遍的大数据抽数的方式都是从散落在各个地方的 MySQL 数据库中开账号，然后把数据同步过来。\n这不，大数据的同事提出了一个需求，想把某个库中的某个表的数据单独同步到大数据的统一库中的某个表中。\n研究了一个星期，canal 的配置对运维来说非常的不友好，除非用它那个 admin 的 dashboard，但是运维通常是 cli 界面，谁没事装个 dashboard 来配置呢，太 low 了。\n用的是 canal 1.1.5 的版本，不能升级，升级后 java 不对了。还没有到用 1.1.6 的时候！\n背景：\n1源数据库：10.8.2.12 2库：xxl_job 3表：demotable666 4 5目的数据库：10.8.2.17 6库：xxl_job_bak 7表：demotable666 而且不用任何的Zookeeper、Kafka、RabbitMQ的中间件，越简洁越好。\n一、安装canal 1wget https://github.com/alibaba/canal/releases/download/canal-1.1.5/canal.deployer-1.1.5.tar.gz 2wget https://github.com/alibaba/canal/releases/download/canal-1.1.5/canal.adapter-1.1.5.tar.gz 3 4#都装到 /app 目录下 5mkdir /app 6mkdir /app/deploy 7mkdir /app/adapter 8 9cd /app/deploy 10tar zxvf canal.deployer-1.1.5.tar.gz 11 12cd /app/adapter 13tar zxvf canal.adapter-1.1.5.tar.gz 二、MySQL源准备 1# /etc/my.cnf 加入以下几项并重启 2[mysqld] 3log-bin=mysql-bin # Start log bin. 4binlog_format=ROW # Log format. 5server-id=1 # Server id,different from slave. 授权binlog同步：\n1# 创建用户授权 2mysql\u0026gt; CREATE USER canal IDENTIFIED BY \u0026#39;canal\u0026#39;; 3mysql\u0026gt; GRANT SELECT, REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO \u0026#39;canal\u0026#39;@\u0026#39;10.8.2.17\u0026#39; identified by \u0026#34;xxxxxx\u0026#34;; 4mysql\u0026gt; FLUSH PRIVILEGES; 5# 查看授权 6mysql\u0026gt; show grants for \u0026#39;canal\u0026#39;; 三、配置deploy 其实关键就是两个配置\n1、canal.properties\n1mkdir /app/deploy/conf/10.8.2.12 2cp /app/deploy/conf/example/instance.properties /app/deploy/conf/10.8.2.12 3 4vi /app/deploy/conf/canal.properties 5 6#就改两个地方 7canal.destinations = 10.8.2.12 8canal.auto.scan = false 注意，上面一定要把 canal.auto.scan 修改成 false，否则它会自动扫描 conf 目录下的所有配置，这样 example 子目录无论是否有用到，配置都会被扫进去。这不是神经病么，典型的 java 程序猿自以为是的配置。\n2、instance.properties 的配置\n1vi /app/deploy/conf/10.8.2.12/instance.properties 2 3# instance.properties 是拷贝example目录中的 4# 改动 5 6canal.instance.master.address=10.8.2.12:3306 7canal.instance.tsdb.enable=false 8 9# username/password 10canal.instance.dbUsername=canal 11canal.instance.dbPassword=xxxxxx 12 13# table regex 14#1. 所有表：.* or .*\\\\..* 15#2. canal schema下所有表： canal\\\\..* 16#3. canal下的以canal打头的表：canal\\\\.canal.* 17#4. canal schema下的一张表：canal\\\\.test1 18#5. 多个规则组合使用：canal\\\\..*,mysql.test1,mysql.test2 (逗号分隔) 19#canal.instance.filter.regex=.*\\\\..* 20canal.instance.filter.regex=xxl_job\\\\..* 上面其实就是设置了 canal 去伪装成一个 slave，读取 binlog 日志，并且过滤只读取 xxl_job 的日志。\n启动 deploy\n1cd /app/deploy/bin 2./startup.sh 看看日志无异常即可：\n四、准备好目的数据库的库表结构 源数据库已经有 demotable666 的表了，我们需要在目的数据库事先准备好 demotable666\n1# 在 10.8.2.17 的 xxl_job_bak 库上执行 2mysql\u0026gt; use xxl_job_bak; 3mysql\u0026gt; create table demotable666( 4 UserId int NOT NULL AUTO_INCREMENT PRIMARY KEY, 5 UserName varchar(100), 6 UserLoginDate date NOT NULL 7); 8Query OK, 0 rows affected (0.04 sec) 五、设置adapter 1、application.yml\n首先给 canal 的机器对目的库赋 mysql 权\n1mysql\u0026gt; grant all privileges on xxl_job_bak.* to sync_17@\u0026#39;%\u0026#39; identified by \u0026#34;xxxxxx\u0026#34;; 2mysql\u0026gt; flush privileges; 然后编辑application.yml\n1vi /app/adapter/conf/application.yml 2 3#注释掉srcDataSources，mysql 同步到 mysql的场景中完全用不到这个，es要用到 4# srcDataSources: 5# defaultDS: 6# url: jdbc:mysql://127.0.0.1:3306/mytest?useUnicode=true 7# username: root 8# password: 121212 9 10 canalAdapters: 11 - instance: 10.8.2.12 # canal instance Name or mq topic name 12 groups: 13 - groupId: g1 14 outerAdapters: 15 - name: logger 16 - name: rdb 17 key: mysqlbak1 18 properties: 19 jdbc.driverClassName: com.mysql.jdbc.Driver 20 jdbc.url: jdbc:mysql://10.8.2.17:3306/xxl_job_bak?useUnicode=true 21 jdbc.username: sync_17 22 jdbc.password: xxxxxx 注意上面配的instance，这个跟deploy里面的instance没有半毛钱关系，这个是为了rdb里面的配置参考用的。\n2、编辑表同步配置文件mysqlbak1.yml\n注意上面的jdbc.url指定了库是xxl_job_bak，所以下面配置的 targetTable 就是单独一个 demotable666，很多文章里写库名.表名， xxl_job_bak.demotable666，八戒是没这样成功。\n1#用不到的挪成备份 2mv /app/adapter/conf/rdb/mytest_user.yml /app/adapter/conf/rdb/mytest_user.bak 3 4vi /app/adapter/conf/rdb/mysqlbak1.yml 5#dataSourceKey: mysqlbakDS1 6# cannal的instance或者MQ的topic 7destination: 10.8.2.12 8groupId: g1 9outerAdapterKey: mysqlbak1 10concurrent: true 11 12dbMapping: 13 database: xxl_job 14 table: demotable666 15 targetTable: demotable666 16 targetPk: 17 UserId: UserId 18# mapAll: true 19 targetColumns: 20 UserId: 21 UserName: 22 UserLoginDate: 23# etlCondition: \u0026#34;where c_time\u0026gt;={}\u0026#34; 24 commitBatch: 1 # 批量提交的大小 启动 adapter：\n1cd /app/adapter/bin 2./startup.sh 这样就好了。\n在10.8.2.12的源数据库上插入数据：\n1insert into demotable666(UserName,UserLoginDate) values(\u0026#39;john\u0026#39;,DATE(NOW())); 在 10.8.2.17 可以看到新数据自动过来了，这样就搞定了。\n六、拓展整库同步 上面我们实现了表同步，下面拓展说一下如何实现整库的同步：\n1、设置application.yml，rdb增加一个key: mysqlbak2，jdbc.url 连接串不设置库名\n1 canalAdapters: 2 - instance: 10.8.2.12 # canal instance Name or mq topic name 3 groups: 4 - groupId: g1 5 outerAdapters: 6 - name: logger 7 - name: rdb 8 key: mysqlbak1 9 properties: 10 jdbc.driverClassName: com.mysql.jdbc.Driver 11 jdbc.url: jdbc:mysql://10.8.2.17:3306/xxl_job_bak?useUnicode=true 12 jdbc.username: sync_17 13 jdbc.password: xxxxxx 14 - name: rdb 15 key: mysqlbak2 16 properties: 17 jdbc.driverClassName: com.mysql.jdbc.Driver 18 jdbc.url: jdbc:mysql://10.8.2.17:3306/?useUnicode=true 19 jdbc.username: sync_17 20 jdbc.password: xxxxxx 2、编辑表同步配置文件mysqlbak2.yml\n注意，里面其实只有一个配置起作用 database，而且目的库中必须事先建立好xxl_job的库表结构，库名也必须和源中库名完全一致，举例来说，从 xxl_job 只能同步到 xxl_job，不能从xxl_job同步到xxl_jobbak中。\n1#dataSourceKey: defaultDS 2# cannal的instance或者MQ的topic 3destination: 10.8.2.12 4groupId: g1 5outerAdapterKey: mysqlbak2 6concurrent: true 7 8dbMapping: 9 mirrorDb: true 10 database: xxl_job 然后我们看 10.8.2.17 上面 xxl_job 库里的数据就会增量开始跟源库同步了。\n","date":"2023-01-15","img":"","permalink":"https://bajie.dev/posts/20230115-canal_mysql/","series":null,"tags":null,"title":"运维方案之canal数据库同步"},{"categories":null,"content":"之前公司一直用的是 seafile 来保存文档，非常好用，也出过一次大事，一个离职的员工清空电脑，然后直接把sefaile文件夹也同步清空了，好在有版本，最后找了回来。\n换到新公司，财务也提了共享云盘的要求，还要求能多人同时在线编辑。\n那就试着搭建 seafile + onlyoffice 了，同时要求提高安全性，在网上搜索了一圈，没几个对的，尤其是对https这一块，花了2天时间搭建，记录一下整个过程：\n一、下载seafile： 没有选定高版本的，最新版本的变化太多，缺省全部都安装到 /app 目录下\n1wget https://download.seadrive.org/seafile-server_7.0.0_x86-64.tar.gz 2tar zxvf seafile-server_7.0.0_x86-64.tar.gz 3mkdir /app 4mv seafile-server-7.0.0 /app 二、设定CentOS7 依然活在 CentOS 7.10 的时代，再往上升级，要升到 rokey linux 了\n提醒：seafile 的安装根据版本不同，yum 装的东西也不尽然相同的，要去官方文档看\n1yum install python python-setuptools MySQL-python python-urllib3 python-ldap -y 三、安装MySQL数据库 这个就仁者见仁、智者见智了，我现在的方式都是二进制装，选用的是 mysql-5.6.51-linux-glibc2.12-x86_64.tar.gz 安装的，\n1tar zxvf mysql-5.6.51-linux-glibc2.12-x86_64.tar.gz 2mv mysql-5.6.51-linux-glibc2.12-x86_64 /app 3 4yum -y install autoconf libaio* 5 6cat\u0026lt;\u0026lt;EOF\u0026gt;/etc/my.cnf 7[mysql] 8# 设置mysql客户端默认字符集 9default-character-set=utf8 10socket=/tmp/mysql.sock 11 12[mysqld] 13skip-name-resolve 14#设置3306端口 15port = 3306 16socket=/tmp/mysql.sock 17# 设置mysql的安装目录 18basedir=/app/mysql-5.6.51-linux-glibc2.12-x86_64/ 19# 设置mysql数据库的数据的存放目录 20datadir=/app/mysql-5.6.51-linux-glibc2.12-x86_64/data 21# 允许最大连接数 22max_connections=200 23# 服务端使用的字符集默认为8比特编码的latin1字符集 24character-set-server=utf8 25# 创建新表时将使用的默认存储引擎 26default-storage-engine=INNODB 27#lower_case_table_name=1 28max_allowed_packet=16M 29sql_mode=NO_ENGINE_SUBSTITUTION,STRICT_TRANS_TABLES 30EOF 31 32groupadd -g mysql 33useradd -g mysql mysql 34 35# 初始化DB 36cd /app/mysql-5.6.51-linux-glibc2.12-x86_64/ 37/app/mysql-5.6.51-linux-glibc2.12-x86_64/scripts/mysql_install_db --user=mysql 38 39#建立 /etc/init.d/mysql 的软链接 40mysql -\u0026gt; /app/mysql-5.6.51/support-files/mysql.server 41 42#启动 43/etc/init.d/mysql start 44 45# 安全设置DB 46cd /app/mysql-5.6.51-linux-glibc2.12-x86_64/ 47/app/mysql-5.6.51-linux-glibc2.12-x86_64/bin/mysql_secure_installation 四、准备onlyoffice 这里有大坑啊，其实onlyoffice的版本万万不能用最新的，因为JWT的Token认证\n所以必须用低版本的，然后呢由于它其实是一组程序，调用的时候又是只用web，所以封在docker里最好\n1cd /etc/yum.repos.d/ 2wget https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo 3 4yum -y install docker-ce 5systemctl enable --now docker.service 6 7#准备好证书目录 8mkdir /app/onlyoffice/certs 9#用lego申请好的let\u0026#39;s encrypt免费证书或者正式证书 10#只能叫这两个名字，其他不行！！！！ 11onlyoffice.crt 12onlyoffice.key 13#生成dh 14cd /app/onlyoffice/certs 15openssl dhparam -out dhparam.pem 2048 16 17#跑容器 18docker run -i -t -d -p 8443:443 --restart=always -v /app/onlyoffice:/var/www/onlyoffice/Data onlyoffice/documentserver:6.4.2.6 我们这里选用 onlyoffice 6.4.2.6 版本的，同时打开8443:443的端口，意味着我们准备要跑https，然后onlyoffice的证书是如上的设定方法，网上一堆胡说八道直接放开443但是无证书的，有的还要进去docker改，都不对。\n五、安装seafile 这个很简单，但也很坑\n1cd /app/seafile-server-7.0.0/ 2./setup-seafile-mysql.sh 3 4#回答问题，第一个答： 5seafile 6#第二个答域名，因为我们是真的用域名 7seafile.rendoumi.com 8#剩下选回车 这里遇到个大坑，由于我们mysql安装选择了secure的模式，所以这里会安不过去\n报seafile用户无权限，没办法，用navicat，进去看用户，安装脚本给我们生成了一个用户，但是是localhost权限的，我们改成127.0.0.1，然后保存\n重新安装，就可以安装成功了。\n然后就是先初始化一下：\n1cd /app/seafile-server-latest/ 2 3./seafile.sh start 4 5#设置管理员和密码 6./seahub.sh start 7 8然后都停掉 9./seahub.sh stop 10./seafile.sh stop 准备安全配置\n1cd /app/conf/ 2 3#vi ccnet.conf 4# NAME和SERVIcE_URL就是上面安装时我们回答的2个问题 5NAME = seafile 6SERVICE_URL = https://seafile.rendoumi.com 7 8#vi seafile.conf 9# 增加host监听地址为127.0.0.1，稍后我们设置nginx转发，更加安全 10[fileserver] 11host = 127.0.0.1 12port = 8082 13 14#vi seahub_settings.py 15# 增加以下各项，有onlyoffice的，有8082的 16FILE_SERVER_ROOT = \u0026#39;https://seafile.rendoumi.com/seafhttp\u0026#39; 17 18MAX_NUMBER_OF_FILES_FOR_FILEUPLOAD = 5000 19 20ENABLE_ONLYOFFICE = True 21VERIFY_ONLYOFFICE_CERTIFICATE = False 22ONLYOFFICE_APIJS_URL = \u0026#39;https://seafile.rendoumi.com:8443/web-apps/apps/api/documents/api.js\u0026#39; 23ONLYOFFICE_FILE_EXTENSION = (\u0026#39;doc\u0026#39;, \u0026#39;docx\u0026#39;, \u0026#39;ppt\u0026#39;, \u0026#39;pptx\u0026#39;, \u0026#39;xls\u0026#39;, \u0026#39;xlsx\u0026#39;, \u0026#39;odt\u0026#39;, \u0026#39;fodt\u0026#39;, \u0026#39;odp\u0026#39;, \u0026#39;fodp\u0026#39;, \u0026#39;ods\u0026#39;, \u0026#39;fods\u0026#39;) 24ONLYOFFICE_EDIT_FILE_EXTENSION = (\u0026#39;docx\u0026#39;, \u0026#39;pptx\u0026#39;, \u0026#39;xlsx\u0026#39;,\u0026#39;doc\u0026#39;,\u0026#39;xls\u0026#39;,\u0026#39;ppt\u0026#39;) 准备systemctl的启动文件\n1cat \u0026lt;\u0026lt;EOF\u0026gt;/etc/systemd/system/seafile.service 2[Unit] 3Description=Seafile Server 4After=network.target mariadb.service 5 6[Service] 7Type=oneshot 8ExecStart=/app/seafile-server-latest/seafile.sh start 9ExecStart=/app/seafile-server-latest/seahub.sh start 10ExecStop=/app/seafile-server-latest/seafile.sh stop 11ExecStop=/app/seafile-server-latest/seahub.sh stop 12RemainAfterExit=yes 13User=root 14Group=root 15 16[Install] 17WantedBy=multi-user.target 18EOF 然后我们现在可以正式启动seafile了\n1systemctl daemon-reload 2systemctl start seafile 六、安装nginx 1yum -y install epel-release 2yum -y install nginx 3 4# 生成dh 5cd /etc/nginx/ 6openssl dhparam -out dhparam.pem 2048 7 8vi /etc/nginx/nginx.conf 9 10#增加以下这个大段 11 server { 12 listen 80; 13 server_name seafile.rendoumi.com; 14 rewrite ^ https://$http_host$request_uri? permanent; 15 } 16 17 server { 18 listen 443 ssl; 19 server_name seafile.rendoumi.com; 20 ssl_certificate /etc/nginx/seafile.crt; #cacert.pem 文件路径 21 ssl_certificate_key /etc/nginx/seafile.key; #privkey.pem 文件路径 22 ssl_dhparam /etc/nginx/dhparam.pem; 23 ssl_protocols TLSv1 TLSv1.1 TLSv1.2; 24 ssl_ciphers \u0026#39;ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-SHA384:ECDHE-ECDSA-AES128-SHA256:ECDHE-RSA-AES256-SHA384:ECDHE-RSA-AES128-SHA256:ECDHE-RSA-AES256-SHA:ECDHE-ECDSA-AES256-SHA:ECDHE-RSA-AES128-SHA:ECDHE-ECDSA-AES128-SHA:DHE-RSA-AES256-GCM-SHA384:DHE-RSA-AES256-SHA256:DHE-RSA-AES256-SHA:DHE-RSA-CAMELLIA256-SHA:DHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES128-SHA256:DHE-RSA-AES128-SHA:DHE-RSA-SEED-SHA:DHE-RSA-CAMELLIA128-SHA:HIGH:!aNULL:!eNULL:!LOW:!3DES:!MD5:!EXP:!PSK:!SRP:!DSS\u0026#39;; 25 ssl_prefer_server_ciphers on; 26 27 proxy_set_header X-Forwarded-For $remote_addr; 28 29 add_header Strict-Transport-Security \u0026#34;max-age=31536000; includeSubDomains\u0026#34;; 30 server_tokens off; 31 32 location / { 33 proxy_pass http://127.0.0.1:8000; 34 proxy_set_header Host $host; 35 proxy_set_header X-Real-IP $remote_addr; 36 proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; 37 proxy_set_header X-Forwarded-Host $server_name; 38 proxy_set_header X-Forwarded-Proto https; 39 40 access_log /var/log/nginx/seahub.access.log; 41 error_log /var/log/nginx/seahub.error.log; 42 proxy_read_timeout 1200s; 43 client_max_body_size 0; 44 } 45 46 location /seafhttp { 47 rewrite ^/seafhttp(.*)$ $1 break; 48 proxy_pass http://127.0.0.1:8082; 49 client_max_body_size 0; 50 proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; 51 proxy_connect_timeout 36000s; 52 proxy_read_timeout 36000s; 53 proxy_send_timeout 36000s; 54 send_timeout 36000s; 55 } 56 57 location /media { 58 root /app/seafile-server-latest/seahub; #seahub的路径 59 } 60 61 } 以上我们可以看到，seafile实际监听了127.0.0.1:8000和127.0.0.1:8082端口，被代理到了nginx，这样监听在本地，nginx再套上https证书，就很安全了，启动\n1systemctl enbale --now nginx 七、设置seafile 然后我们登录 https://seafile.rendoumi.com, 修改一下配置\n确保设置正确：\n1SERVICE_URL设置的是 `https://seafile.rendoumi.com` 2FILE_SERVER_ROOT 设置的是 `https://seafile.rendoumi.com/seafhttp` 然后就可以了，seafile就可以在线编辑了。\n八、附加onlyoffice修改 我们还需要增加一点特色，给onlyoffice做一些修改：\n1、自动保存\n进入onlyoffice容器\n1docker exec -it 容器 /bin/bash 2 3echo \u0026#34;Asia/Shanghai\u0026#34; \u0026gt; /etc/timezone 4ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime 5date -R 6 7nano /etc/onlyoffice/documentserver/local.json 8{ 9 \u0026#34;services\u0026#34;: { 10 \u0026#34;CoAuthoring\u0026#34;: { 11 // 在 CoAuthoring 中 增加 autoAssembly 属性配置 12 \u0026#34;autoAssembly\u0026#34;: { 13 \u0026#34;enable\u0026#34;: true, 14 \u0026#34;interval\u0026#34;: \u0026#34;5m\u0026#34; 15 }, //注意这个，逗号 16 } 17 } 18} 2、安装字体\n字体文件放进一个zip的压缩包，其中一个目录名为office，里面是文件\n然后把文件传进Linux机器中，解压\n1export LANG=zh_CN.UTF-8 2unzip -x office.zip 3cd office 4ls 查看一下，务必像下面一样，是中文的\n然后在office目录那一级，把字体文件拷贝进容器\n1docker cp office 472bf8a36f14:/usr/share/fonts/truetype/custom 进入容器\n1docker exec -it 472bf8a36f14 /bin/bash 2 3# 更新字体 4mkfontscale 5mkfontdir 6fc-cache -fv 7documentserver-generate-allfonts.sh 8 9#退出容器，重启 10docker restart 472bf8a36f14 然后onlyoffice就会5分钟自动保存一下，然后多了一堆好看的中文字体：\n","date":"2023-01-15","img":"","permalink":"https://bajie.dev/posts/20230115-seafile_onlyoffice/","series":null,"tags":null,"title":"Seafile配搭onlyoffice的安装"},{"categories":null,"content":"刚通过了CKA的考试，拿到了证书。\n说老实话，想考这个必须翻墙啊，在考试的时候正好是新冠的第三天，烧得正厉害。\n然后考试的过程真是一波三折，PSI 足足重新认证了三次，浪费40分钟。\n最后都感觉要完蛋了，只剩下不到10分钟检查，结果还不错，考了90分。\n有第一次考的可以联系我，我给你说一些必须注意的事项。\n然后 CKA 有一个模拟考试，36小时时间，实际考试的环境跟模拟的完全不一样啊。\n但是模拟如果你能过，那考试你一定能过，模拟的难度不低，相当高。\n这就把模拟的题给放出来，供大家参考，非常有参考价值。\n大家可以学到很多东西。\n文件下载：cka.html 单html文件\n","date":"2023-01-12","img":"","permalink":"https://bajie.dev/posts/20230112-cka_exam/","series":null,"tags":null,"title":"Cka的模拟试题"},{"categories":null,"content":"其实一直用的终端软件是terminus，全终端可用，以前用 ubuntu，非常好用。\n现在换到新公司，用 Win10，也装了一个。\n一直到机房改造前，还都觉得行，但是机房改造需要收回核心交换机、路由器的权限，开终端 console 居然要收费会员，没办法，找老刘换回了免费的xshell 7。\nxshell的配色实在是受不了，还是喜欢绿色的字符界面。\n备份一个绿色的方案：\n1[mycolor] 2text(bold)=e9e9e9 3magenta(bold)=ff00ff 4text=00ff80 5white(bold)=fdf6e3 6green=80ff00 7red(bold)=ff0000 8green(bold)=3c5a38 9black(bold)=808080 10red=ff4500 11blue=00bfff 12black=000000 13blue(bold)=1e90ff 14yellow(bold)=ffff00 15cyan(bold)=00ffff 16yellow=c0c000 17magenta=c000c0 18background=042028 19white=c0c0c0 20cyan=00c0c0 21[Names] 22count=1 23name0=mycolor 保存成 mycolor.xcs 文件，然后工具\u0026ndash;\u0026gt; 配色方案，导入即可：\n整体效果还是比较好的。\n是自己喜欢的样子，当然，字体从9也增加到了12。\n","date":"2023-01-12","img":"","permalink":"https://bajie.dev/posts/20230112-xshell_colors/","series":null,"tags":null,"title":"Xshell的绿色配色方案"},{"categories":null,"content":"2022年经过努力，考了4张证书，2023年继续努力奋斗\u0026hellip;\u0026hellip;\n阿里云的ACP证书：\nAWS的SAA-C03证书：\nK8S的CKA证书：\nK8S的CKS证书：\n","date":"2023-01-03","img":"","permalink":"https://bajie.dev/posts/20230103-certificates/","series":null,"tags":null,"title":"2022年获得的证书"},{"categories":null,"content":"DNSMASQ做局域网内的pxe installl是最合适不过的软件了。不用单独搭建 tftpserver，用自建的即可。\n而且支持给各种子网打标签，还可以按标签发送各种dhcp包的特定信息。\n1interface=eth0 2bind-dynamic 3 4pxe-prompt=\u0026#34;Rendoumi PXE System\u0026#34;, 5 5 6domain-needed 7bogus-priv 8no-resolv 9no-poll 10 11#address=/jump.dedi.jp/install/103.108.236.5 12 13#UP Stream DNS Server 14server=114.114.114.114 15server=202.106.196.115 16server=202.106.0.20 17strict-order 18 19dhcp-authoritative 20dhcp-sequential-ip 21dhcp-no-override 22 23log-facility=/var/log/dnsmasq.log 24log-dhcp 25log-queries 26 27 28enable-tftp 29tftp-root = /export/servers/tftpboot 30 31#only new add host is dynamic, delete or modify not. 32#dhcp-hostsfile=/etc/dhcp-host/hosts.conf 33dhcp-hostsdir=/etc/dhcp-host 34 35# 36#Setup different options for each of the unique subnets, since default gateways will be different 37#The format for this is: dhcp-options=\u0026lt;your_tags_here\u0026gt;,\u0026lt;option\u0026gt;,\u0026lt;option_value\u0026gt; - 38#3 is router 39#1:netmask, 15:domain-name, 3:router, 6:dns-server, 40#44:netbios-ns, 46:netbios-nodetype, 47:netbios-scope, 41#31:router-discovery, 33:static-route, 121:classless-static-route, 42#43:vendor-encap 43# 44dhcp-option=option:dns-server,172.18.30.1,172.18.30.2 45dhcp-option=option:all-subnets-local,1 46dhcp-option=option:T1,2m 47dhcp-option=option:T2,4m 48 49#dhcp-range=[tag:\u0026lt;tag\u0026gt;[,tag:\u0026lt;tag\u0026gt;],][set:\u0026lt;tag\u0026gt;,]\u0026lt;start-addr\u0026gt;[,\u0026lt;end-addr\u0026gt;|\u0026lt;mode\u0026gt;][,\u0026lt;netmask\u0026gt;[,\u0026lt;broadcast\u0026gt;]][,\u0026lt;lease time\u0026gt;] 50#vlan 1 native vlan 51#dhcp-range=set:net1,172.18.29.100,172.18.29.250,static,255.255.254.0,2m 52dhcp-range=set:net1,172.18.29.100,static,2m 53dhcp-option-force=tag:net1,option:router,172.18.29.254 54 55#vlan 199 wuli 56#dhcp-range=set:net2,172.18.31.100,static,255.255.254.0,2m 57#dhcp-range=set:net2,172.18.31.100,255.255.254.0,2m 58dhcp-range=set:net2,172.18.31.100,static,2m 59dhcp-option-force=tag:net2,option:router,172.18.31.254 60 61 62# set tag \u0026#34;ipxe\u0026#34; if request comes from iPXE (\u0026#34;iPXE\u0026#34; user class) 63dhcp-userclass=set:ipxe,iPXE 64 65# alternative way, look for option 175 66#dhcp-match=set:ipxe,175 # gPXE/iPXE sends a 175 option. 67 68# if request comes from dumb firmware, send them iPXE (via TFTP) 69#dhcp-boot=tag:!ipxe,undionly.kpxe,boothost,172.18.29.2 70dhcp-boot=tag:!ipxe,undionly.kpxe 71 72 73# if request comes from iPXE, direct it to boot from boot1.php 74#dhcp-boot=tag:ipxe,http://172.18.29.2/pxeboot/boot1.php 75#--dhcp-boot=[tag:\u0026lt;tag\u0026gt;,]\u0026lt;filename\u0026gt;,[\u0026lt;servername\u0026gt;[,\u0026lt;server address\u0026gt;|\u0026lt;tftp_servername\u0026gt;]] 76dhcp-boot=tag:ipxe,tag:net1,http://172.18.29.2/pxeboot/boot1.php,172.18.29.2 77dhcp-boot=tag:ipxe,tag:net2,http://172.18.31.2/pxeboot/boot1.php,172.18.31.2 78 79# Args add(old del) mac ip hostname 80# add ac:1f:6b:21:3e:d8 103.108.236.22 1403-s27 81#dhcp-script=/bin/echo 82#dhcp-leasefile=/var/log/dnsmasq.leases 83# !!!! tag is alphanumeric label, fuck !!!!! 附上undionly.kpxe：undionly.kpxe ","date":"2022-12-22","img":"","permalink":"https://bajie.dev/posts/20221222-dnsmasq_pxe/","series":null,"tags":null,"title":"DNSMASQ配置PXE的方式"},{"categories":null,"content":"合规审计经常需要给各种软件打补丁，比如openssh，如果是高版本还好说。如果让你补 centos 6的 openssh，\n那恐怕就要喝一壶了。替代方案如下：\ngit：https://github.com/mkj/dropbear 下载地址：https://matt.ucc.asn.au/dropbear/dropbear.html\n一、介绍 dropbear作为一款基于ssh协议的轻量级sshd服务器，相比OpenSSH，其更简洁，更小巧，运行起来内存占用比也更小。在应用进程上，OpenSSH会开启两个sshd进程服务，而dropbear只开启一个进程，相较于OpenSSH，其对于硬件要求也更低，也更节约系统资源。\ndropbear实现完整的SSH客户端和服务器版本2协议，不支持SSH版本1协议的向后兼容性，以节省空间和资源，并避免在SSH版本1的固有的安全漏洞。\ndropbear主要有以下程序：\n服务程序：dropbear（类似于Openssh的sshd） 客户程序：dbclinet（累世于Openssh的ssh） 密钥生成程序：dropbearkey\n二、下载安装\n1wget https://matt.ucc.asn.au/dropbear/dropbear-2020.81.tar.bz2 2tar jxvf dropbear-2020.81.tar.bz2 3cd dropbear-2020.81 4 5./configure 6make PROGRAMS=\u0026#34;dropbear dbclient dropbearkey dropbearconvert scp\u0026#34; 7make PROGRAMS=\u0026#34;dropbear dbclient dropbearkey dropbearconvert scp\u0026#34; install 8 9mkdir /etc/dropbear/ 10dropbearkey -t rsa -f /etc/dropbear/dropbear_rsa_host_key 11dropbear -E -p 2222 #启动ssh服务 12dropbear -FE -p 2222 #-F指定前台运行 13 14客户端连接： 15ssh 192.168.89.37 -p 2222 ++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n1报错：zlib error 或者make PROGRAMS=\u0026#34;dropbear dbclient dropbearkey dropbearconvert scp\u0026#34;出错 2 3解决方法： 4wget https://www.zlib.net/fossils/zlib-1.2.10.tar.gz 5tar zxvf zlib-1.2.10.tar.gz 6cd zlib-1.2.10 7./configure --prefix=/usr/local/zlib ++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n1#centos6环境安装方法，找到三个包 2rpm -vih libtomcrypt-1.17-25.el6.x86_64.rpm 3rpm -vih libtommath-0.42.0-5.el6.x86_64.rpm 4rpm -vih dropbear-2017.75-1.el6.x86_64.rpm 5 6chkconfig sshd off 7chkconfig dropbear on 8 9/etc/init.d/dropbear status 10 11dropbearkey -t rsa -f /etc/dropbear/dropbear_rsa_host_key 12 13/etc/init.d/sshd stop \u0026amp;\u0026amp; /etc/init.d/dropbear start ","date":"2022-12-19","img":"","permalink":"https://bajie.dev/posts/20221219-dropbear/","series":null,"tags":null,"title":"Dropbear配置SSH服务"},{"categories":null,"content":"一：我们在公司内部建立了Docker内部镜像仓库：\nharbor是vmware出的一个docker镜像仓库，本质是一组容器的集合体，算是一个多容器的pod.\n数据卷缺省是宿主机的/data，所以我们把iscsiu挂在/data\n主机：172.18.31.28\n首先安装docker-ce\n添加docker-ce源：\nyum install epel-release\ryum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo\ryum install docker-ce -y\ryum install ntp -y\r启动自动同步时间:\ntimedatectl set-ntp yes #此处可用yes,no,1或0\r配置时区:\ntimedatectl set-timezone Asia/Shanghai\r配置Docker启动参数：\nmkdir -p /etc/docker\rcat \u0026lt;\u0026lt; EOF \u0026gt;\u0026gt; /etc/docker/daemon.json\r{\r\u0026quot;registry-mirrors\u0026quot;: [\u0026quot;https://docker.mirrors.ustc.edu.cn/\u0026quot;]\r}\rEOF\r之后装的所有Docker的宿主机，如果要用到这个私有仓库的话：\ncat \u0026lt;\u0026lt; EOF \u0026gt;\u0026gt; /etc/docker/daemon.json\r{\r\u0026quot;insecure-registries\u0026quot;:[\u0026quot;172.18.31.28\u0026quot;],\r\u0026quot;registry-mirrors\u0026quot;: [\u0026quot;https://docker.mirrors.ustc.edu.cn/\u0026quot;]\r}\rEOF\r启动docker\nsystemctl enable docker \u0026amp;\u0026amp; systemctl start docker\r​\t安装Docker-compose\nyum install docker-compose\rpip3 install -I requests==2.9 来强制修正\r下载harbor的离线安装包\ncd /root\rhttps://github.com/goharbor/harbor/releases/download/v2.0.1/harbor-offline-installer-v2.0.1.tgz\rtar zxvf harbor-offline-installer-v2.0.1-rc1.tgz\rcd harbor\r编辑配置文件：\ncp harbor.yml.tmpl harbor.yml\rvi harbor.yml\rhostname: 172.18.31.28\rharbor_admin_password: Fuckxxxbbaasskk\r隐掉443，因为我们是内网用，配个证书也是假的，所以关了443\r#https:\r# https port for harbor, default is 443\r# port: 443\r# The path of cert and key files for nginx\r# certificate: /your/certificate/path\r换掉DB的pass\rdatabase:\r# The password for the root user of Harbor DB. Change this before any production use.\rpassword: xxxxxxx\r# private_key: /your/private/key/path\r然后直接安装：\n./install.sh\r安装完成了就，看一眼：\ndocker-compose ps\r然后直接登录 http://172.18.31.28 就好\n缺省有一个library的开放项目，我们推一个busybox过去测试一下：\n首先拉一个busybox到本地\rdocker pull busybox\r打个tag\rdocker tag busybox:latest 172.18.31.28/library/busybox\r推上去\rdocker push 172.18.31.28/library/busybox\r这时候再去31.28的library项目里看，就能看到新推上去的busybox镜像了\n在其他的机器上，首先登录，然后就可以拉镜像了。 再举个实际例子，我们把metallb给推上去备用：\n打tag\rdocker tag docker.io/metallb/controller:v0.9.3 172.18.31.28/library/docker.io/metallb/controller:v0.9.3\r推上去\rdocker push 172.18.31.28/library/docker.io/metallb/controller:v0.9.3\r","date":"2022-12-19","img":"","permalink":"https://bajie.dev/posts/20221219-docker_harbor/","series":null,"tags":null,"title":"Docker镜像仓库harbor的搭建与使用"},{"categories":null,"content":"docker默认网段是172.17，和公司的网段172.16和172.18有时候会冲突，解决方法就是换docker网段。\n方案：不改docker网段，创建不和公司网段冲突的docker子网段\n1docker network create --driver=bridge --subnet=172.19.0.0/24 monitor_net 运行容器时指定\n1docker run -it --name \u0026lt;容器名\u0026gt; ---network monitor_net \u0026lt;镜像名\u0026gt; 在docker-compose同样通过networks指定，形式如下：\n1version: \u0026#39;3\u0026#39; 2networks: 3 monitor: 4 #使用已经存在的网络 5 external: 6 name: monitor_net 7 8services: 9 prometheus: 10 image: prom/prometheus 11 container_name: prometheus 12 hostname: prometheus 13 privileged: true 14 restart: always 15 volumes: 16 - /usr/local/src/config/prometheus.yml:/etc/prometheus/prometheus.yml 17 - /usr/local/src/config/node_down.yml:/etc/prometheus/node_down.yml 18 ports: 19 - \u0026#34;9091:9090\u0026#34; 20 networks: 21 - monitor 22 links: 23 - alertmanager 24 - node-exporter ","date":"2022-12-19","img":"","permalink":"https://bajie.dev/posts/20221219-docker_network/","series":null,"tags":null,"title":"Docker的缺省网段冲突问题"},{"categories":null,"content":"Dell服务器设置远程文件共享\n用来加载iso文件，用于安装系统或者修复\n输入172.18.31.2:/export/nfsshare/centos7-live-docker.iso\n缺省是支持4种格式的共享：\n1 CIFS — //\u0026lt;IP to connect for CIFS file system\u0026gt;/\u0026lt;file path\u0026gt;/\u0026lt;image name\u0026gt; 2 NFS — \u0026lt; IP to connect for NFS file system\u0026gt;:/\u0026lt;file path\u0026gt;/\u0026lt;image name\u0026gt; 3 HTTP — http://\u0026lt;URL\u0026gt;/\u0026lt;file path\u0026gt;/\u0026lt;image name\u0026gt; 4 HTTPs — https://\u0026lt;URL\u0026gt;/\u0026lt;file path\u0026gt;/\u0026lt;image name\u0026gt; 点击Connect 好的话，就会蹦出Success 然后启动虚拟控制台，Boot菜单选中启动：Virtual CD/DVD/ISO 重启，就可以进入centos7-live-docker.iso的系统了\n","date":"2022-12-18","img":"","permalink":"https://bajie.dev/posts/20221218-dell_idrac_remote_share/","series":null,"tags":null,"title":"Dell服务器设置远程文件共享"},{"categories":null,"content":"vlt0204, NDC瓦特超了，十有八九是网卡烧了，然后导致主板电压过载。\n前面板日志：VLT0204，登录idrac看到日志：\nThe system board NDC PG voltage is outside of range.\r悲剧了，可能是主板烧了\n处理方法如下：\n一、主板放电\n1、关机 2、拔电源线 3、长按电源开关半分钟不放手 4、半分钟后放手，接回电源线开机看看\n如果以上方法不行，就必须采用第二种方法：\n二、最小化负载排错\n1、拔掉电源2 2、拔掉CPU2 3、拔掉所有内存（只留1条或2条） 4、拔掉网卡 5、启动看看\nR740的内存插法：\nR730的内存插法：\n","date":"2022-12-18","img":"","permalink":"https://bajie.dev/posts/20221218-dell_memory/","series":null,"tags":null,"title":"Dell机器主板VLT0204故障处理以及内存插法"},{"categories":null,"content":"CentOS7更新内核 内核下载地址：https://elrepo.org/linux/kernel/el7/x86_64/RPMS/\n内核包有两种，ml主流，lt长期支持。\n我们升级核心肯定是为了主流的功能，lt还升它做甚。\n#升级：\n1rpm -ivh https://elrepo.org/linux/kernel/el7/x86_64/RPMS/kernel-ml-6.1.0-1.el7.elrepo.x86_64.rpm #查看可用内核\n1awk -F\\\u0026#39; \u0026#39;$1==\u0026#34;menuentry \u0026#34; {print i++ \u0026#34; : \u0026#34; $2}\u0026#39; /etc/grub2.cfg #初始化第一个为默认内核\n1grub2-set-default 0 #重新创建内核配置\n1grub2-mkconfig -o /boot/grub2/grub.cfg 2reboot #后续有需要再装，不需要搞开发就别装！\n1#更新kernel-ml-headers 2rpm -ivh https://elrepo.org/linux/kernel/el7/x86_64/RPMS/kernel-ml-headers-6.1.0-1.el7.elrepo.x86_64.rpm 3 4#更新kernel-ml-devel 5rpm -ivh https://elrepo.org/linux/kernel/el7/x86_64/RPMS/kernel-ml-devel-6.1.0-1.el7.elrepo.x86_64.rpm ","date":"2022-12-18","img":"","permalink":"https://bajie.dev/posts/20221218-centos7_upgrade_kernel/","series":null,"tags":null,"title":"升级CentOS7的kernel核心"},{"categories":null,"content":"各种合规要求中都对用户策略有着要求，什么PCI啊，上市审计啊，都有着密码复杂程度的要求：\nCentOS 7 的用户密码策略：\n修改vi /etc/pam.d/system-auth\n其中有一行：\n1password requisite pam_pwquality.so try_first_pass local_users_only retry=3 authtok_type= 后面加上：\n1minlen=12 lcredit=-1 ucredit=-1 dcredit=-1 ocredit=-1 enforce_for_root 修改成为：\n1password requisite pam_pwquality.so try_first_pass local_users_only retry=3 authtok_type= minlen=12 lcredit=-1 ucredit=-1 dcredit=-1 ocredit=-1 enforce_for_root 意思是：\n1密码长度12，须包含：一个小写字符，一个大写字符，一个数字，一个特殊字符，强制root也遵守此规则 参数全部解释如下：\n1retry=3: This option will prompt the user 3 times before exiting and returning an error. 2minlen=12: This specifies that the password cannot be less than 12 characters. 3maxrepeat=3: This allows implies that only a maximum of 3 repeated characters can be included in the password. 4ucredit=-1: The option requires at least one uppercase character in the password. 5lcredit=-1: The option requires at least one lowercase character in the password. 6dcredit=-1: This implies that the password should have at last a numeric character. 7ocredit=-1: The option requires at least one special character included in the password. 8difok=3: This implies that only a maximum of 3 character changes in the new password should be present in the old password. 9reject_username: The option rejects a password if it consists of the username either in its normal way or in reverse. 10enforce_for_root: This ensures that the password policies are adhered to even if it’s the root user configuring the passwords. ","date":"2022-12-18","img":"","permalink":"https://bajie.dev/posts/20221218-centos7_passwd/","series":null,"tags":null,"title":"合规要求之CentOS7的用户密码策略"},{"categories":null,"content":"一：介绍： DNSMasq主要用来解决内网DNS域名缓存、DHCP、网络启动和路由通告功能，我们主要是将DNSMasq作为内网DNS域名劫持使用\n系统环境：centos7.6 需求：一个域名对应多个ip 主机ip：（内网主机172.18.30.1和172.18.30.2）\n二：配置方法 1，添加如下配置到/etc/dnsmasq.conf addn-hosts=/etc/ty.ddky.local\naddn-hosts：如果要支持一个域名对应多个 IP，就必须用 addn-hosts 选项\n2，/etc/ty.ddky.local 文件内容如下： 172.18.33.100 ty.ddky.local 172.18.33.104 ty.ddky.local\n3，修改完成后重启 DNSMasq systemctl restart dnsmasq.service\n","date":"2022-12-18","img":"","permalink":"https://bajie.dev/posts/20221219-dnsmqsq_multi_ip/","series":null,"tags":null,"title":"Dnsmasq配置一个域名对应多个ip"},{"categories":null,"content":"网络工程师选择网络设备的监控工具是老牌的cacti，当然作为系统工程师的我，还是觉得librenms是个好选择，但是，librenms会对网络设备cpu造成比较大的压力。那还是遵从他的选择，最近网工报告说：\n选择图形的时候时间Filter失效了。选择了以后图形无变化，崩溃，查了一下，都上古的玩意了。\n修改方法：\n修改graph_image.php\nunixtime = 1600000000，也就是到 2020-09-13 20:26:40。放大到 2600000000，也就是到2052-05-22 22:13:20，写这个php程序的人防越界，却没想到时间真的来到了这个节点。\n注：Unix 时间戳是从1970年1月1日（UTC/GMT的午夜）开始所经过的秒数，不考虑闰秒。\n","date":"2022-12-18","img":"","permalink":"https://bajie.dev/posts/20221218-cacti_time_filter/","series":null,"tags":null,"title":"老版本cacti的图形filter突然失效的解决办法"},{"categories":null,"content":"线上服务器172.18.30.67 有4个网口\n其中eno1和eno2是两个光纤万兆口，enp26s0f0和enp26s0f1是两个以太千兆网口。\neno2网卡在没做bonding的时候，通过NetworkManager的dhcp获得了地址。然后eno1和eno2做了bonding，但是eno2实际还在单独起作用，bonding后地址没去掉，导致有两个网关。路由表如下：\n1[root@renhe-18-30-67 ~]# ip r 2default via 172.18.29.254 dev eno2 3default via 172.18.31.254 dev br0.199 proto static metric 426 4172.18.28.0/23 dev eno2 proto kernel scope link src 172.18.28.67 5172.18.30.0/23 dev br0.199 proto kernel scope link src 172.18.30.67 metric 426 6192.168.122.0/24 dev virbr0 proto kernel scope link src 192.168.122.1 eno2是从dhcp获得了172.18.28.67的地址\n1ip a 2eno2: flags=6211\u0026lt;UP,BROADCAST,RUNNING,SLAVE,MULTICAST\u0026gt; mtu 1500 3 inet 172.18.28.67 netmask 255.255.254.0 broadcast 172.18.29.255 4 ether b4:05:5d:08:e0:d8 txqueuelen 1000 (Ethernet) 5 RX packets 81768780 bytes 5398659503 (5.0 GiB) 6 RX errors 0 dropped 7 overruns 0 frame 0 7 TX packets 10044620 bytes 467566304 (445.9 MiB) 8 TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 这种情况下，处理方法如下：\nbonding的模式是 物理网卡 \u0026ndash;\u0026gt; bond \u0026ndash;\u0026gt; bond.xxx \u0026ndash;\u0026gt;br.xxx\n先查bonding模式:\n1cat /sys/class/net/bond0/bonding/mode 2active-backup 1 再看bonding网卡状态：\n1cat /proc/net/bonding/bond0 2Ethernet Channel Bonding Driver: v3.7.1 (April 27, 2011) 3 4Bonding Mode: fault-tolerance (active-backup) 5Primary Slave: eno1 (primary_reselect always) 6Currently Active Slave: eno1 7MII Status: up 8MII Polling Interval (ms): 100 9Up Delay (ms): 0 10Down Delay (ms): 0 11 12Slave Interface: eno1 13MII Status: up 14Speed: 10000 Mbps 15Duplex: full 16Link Failure Count: 1 17Permanent HW addr: b4:05:5d:08:e0:d8 18Slave queue ID: 0 19 20Slave Interface: eno2 21MII Status: up 22Speed: 10000 Mbps 23Duplex: full 24Link Failure Count: 1 25Permanent HW addr: b4:05:5d:08:e0:d9 26Slave queue ID: 0 再次确认active\n1cat /sys/class/net/bond0/bonding/active_slave 2eno1 再确认第二个default网关有效\n1ping -I br0.199 172.18.31.254 从以上可以确定主网卡是eno1，shutdown了eno2不会影响任何东西。\n接下来的步骤：\n先查出来dhclient的进程号，是2252\n1 1 2252 2252 2252 ? -1 Ss 0 8:50 /sbin/dhclient -1 -q -lf /var/lib/dhclient/dhclient--eno2.lease -pf /var/run/dhclient-eno2.pid -H renhe-18-30-67 eno2 然后处理：\n1systemctl stop NetworkManager 2systemctl disable NetworkManager 3# 杀掉dhclient 4kill -9 2252 这样eno2的ip地址在过一段时间后会消失掉。\n如果不消失：\n1ip link set eno2 down 然后等等\n1ip link set eno2 up 就可以了。\n由于是线上服务器，无法停机，所以操作才搞得这么小心谨慎。\n","date":"2022-12-18","img":"","permalink":"https://bajie.dev/posts/20221218-multi_gateway/","series":null,"tags":null,"title":"线上服务器出现多网关问题的处理"},{"categories":null,"content":"114.114.114.114 这个DNS修改记录等待的生效时间非常的长，远远超出了TTL的时长，基本是你设置TTL时间的6倍。\n这是上次我们割接得到的教训啊。我们要割接m.ddky.com的域名。\n注：在修改m.ddky.com的记录前，已经把ddky.com的m.ddky.com的TTL改成了1分钟，正常的来说，第一轮1分钟内不更新，第二轮2分钟内也应该更新了。\n看下图，在带外机上做的查询，查询的时候8.8.8.8和223.5.5.5都已经生效了，变成了帝联的地址。\n06:06:57 做了查询，去掉小时，06：57定义为起始时间。 一直到 12:22 分还没有变过来，大概 13:00 变过来\n起始 06:57 \u0026ndash; 13:00 ，一共用了6分钟。而TTL是1分钟，所以基本是6倍时间。\n往前推测5月25日的DNS割接，TTL是10分钟，6倍，就是60分钟，基本吻合。\n如果想让DNS尽快生效，TTL改成10秒是极限了。 ","date":"2022-12-18","img":"","permalink":"https://bajie.dev/posts/20221218-dns_114/","series":null,"tags":null,"title":"114dns的ttl超时的教训"},{"categories":null,"content":"首先ssh到idrac去，直接会进入racadm的界面\n我们以2022年最新的dell机器为例，老的机器显示的东西不一致\n1ssh 10.18.30.104 2Password: 3racadm\u0026gt;\u0026gt; 注意，shell命令有自动补全功能，按两下tab键会自动补\n做raid的全部命令都在storage的子命令下\n一、首先拿到raid卡的名称：\n1racadm\u0026gt;\u0026gt;storage get controllers 2RAID.Slot.1-1 3AHCI.Embedded.2-1 4AHCI.Embedded.1-1 RAID.Slot.1-1 是Raid卡控制器名称，下面两个是主板内置的，忽略。\n二、然后拿到所有物理盘的名称：\n1racadm\u0026gt;\u0026gt;storage get pdisks 2Disk.Bay.0:Enclosure.Internal.0-1:RAID.Slot.1-1 3Disk.Bay.1:Enclosure.Internal.0-1:RAID.Slot.1-1 4Disk.Bay.2:Enclosure.Internal.0-1:RAID.Slot.1-1 5Disk.Bay.3:Enclosure.Internal.0-1:RAID.Slot.1-1 三、看看有没有已经做好的虚拟磁盘\n1racadm\u0026gt;\u0026gt;storage get vdisks 显示为空，那我们就可以放心大胆的去做了\n四、划分RAID 根据级别不同，精简过的名令如下：\n1racadm storage createvd:\u0026lt;控制器\u0026gt; -rl {r0|r1|r5|r6|r10|r50|r60} -pdkey:\u0026lt;磁盘组，用逗号分割\u0026gt; 2 3-rl — Sets the storage level. 4 r0 — storage 0-Striping 5 r1 — storage 1-Mirroring 6 r5 — storage 5-Striping with Parity 7 r6 — storage 6-Striping with Extra Parity 8 r10 — storage 10-Spanned Striping with Mirroring 9 r50 — storage 50-Spanned Striping with Parity 10 r60 — storage 60-Spanned Striping with Extra Parity 给出例子：\n1#raid0 2racadm storage createvd:RAID.Slot.1-1 -rl r0 -pdkey:Disk.Bay.0:Enclosure.Internal.0-1:RAID.Slot.1-1 3 4#raid1 5racadm storage createvd:RAID.Slot.1-1 -rl r0 -pdkey:Disk.Bay.0:Enclosure.Internal.0-1:RAID.Slot.1-1,Disk.Bay.1:Enclosure.Internal.0-1:RAID.Slot.1-1 6 7#raid10 8racadm storage createvd:RAID.Slot.1-1 -rl r0 -pdkey:Disk.Bay.0:Enclosure.Internal.0-1:RAID.Slot.1-1,Disk.Bay.1:Enclosure.Internal.0-1:RAID.Slot.1-1,Disk.Bay.2:Enclosure.Internal.0-1:RAID.Slot.1-1,Disk.Bay.3:Enclosure.Internal.0-1:RAID.Slot.1-1 五、建立job 首先尝试不重启在线建Raid：\n1racadm\u0026gt;\u0026gt;jobqueue create RAID.Slot.1-1 --realtime 如果不行，就建立重启的\n1racadm\u0026gt;\u0026gt;jobqueue create RAID.Slot.1-1 2racadm jobqueue create RAID.Slot.1-1 3RAC1024: Successfully scheduled a job. 4Verify the job status using \u0026#34;racadm jobqueue view -i JID_xxxxx\u0026#34; command. 5Commit JID = JID_222873363294 六、如果要重启，就重启。能在线建的就不需要重启\n1racadm\u0026gt;\u0026gt;serveraction powercycle 2racadm serveraction powercycle 3Server power operation successful 补充：\n如果要清理掉Raid卡之前的配置完全重建，命令如下：\n1storage resetconfig:RAID.Slot.1-1 2jobqueue create RAID.Slot.1-1 -r pwrcycle -s TIME_NOW -e TIME_NA ","date":"2022-12-18","img":"","permalink":"https://bajie.dev/posts/20221219-dell_ssh_raid/","series":null,"tags":null,"title":"Dell服务器用ssh方式划分磁盘Raid的方法"},{"categories":null,"content":"这Megacli是操作磁盘Raid的常用软件，日常用的大多是dell家的机器，都是这软件，记录一下。\n1#!/bin/sh 2#/opt/MegaRAID/MegaCli/MegaCli64 -PDList -a0 3/opt/MegaRAID/MegaCli/MegaCli64 -PDList -a0 | grep -A1 \u0026#34;Enclosure Device\u0026#34; 4 5# clear foreign disk 6#/opt/MegaRAID/MegaCli/MegaCli64 -cfgforeign -scan -a0 7#/opt/MegaRAID/MegaCli/MegaCli64 -cfgforeign -clear -a0 8 9# clear Firmware state 10#/opt/MegaRAID/MegaCli/MegaCli64 -PDList -a0 |grep \u0026#39;Firmware state\u0026#39; 11#/opt/MegaRAID/MegaCli/MegaCli64 -PDMakeGood -Physdrv \u0026#34;[32:3]\u0026#34; -a0 12 13# clear Firmware JBOD mode, first turn into unconfig mode, then MakeGood 14#/opt/MegaRAID/MegaCli/MegaCli64 -PDList -a0 |grep \u0026#39;Firmware state\u0026#39; 15#/opt/MegaRAID/MegaCli/MegaCli64 -PDMakeGood -Physdrv [32:3] -force -a0 16 17#/opt/MegaRAID/MegaCli/MegaCli64 AdpGetProp EnableJBOD -aALL 18#/opt/MegaRAID/MegaCli/MegaCli64 -AdpSetProp -EnableJBOD -1 -a0 19#/opt/MegaRAID/MegaCli/MegaCli64 -PDMakeJBOD -PhysDrv[252:0] -a0 20 21# create Raid0 22#/opt/MegaRAID/MegaCli/MegaCli64 -CfgLdAdd -r0[252:3] -a0 23# Get old Cache, ex: Virtual Drive(Target ID 02) 24#/opt/MegaRAID/MegaCli/MegaCli64 -GetPreservedCacheList -a0 25# Clear cache, ex: Virtual Drive(Target ID 02) 26#/opt/MegaRAID/MegaCli/MegaCli64 -DiscardPreservedCache -L2 -a0 27# create Raid1 28#/opt/MegaRAID/MegaCli/MegaCli64 -CfgLdAdd -r1[252:4,252:5] -a0 29# create Raid5 30#/opt/MegaRAID/MegaCli/MegaCli64 -CfgLdAdd -r5[252:2,252:3,252:4,252:5] -a0 31# create Raid10 32#/opt/MegaRAID/MegaCli/MegaCli64 -CfgSpanAdd -r10 -Array0[32:4,32:5] -Array1[32:6,32:7] -a0 33 34#delete raid 35#/opt/MegaRAID/MegaCli/MegaCli64 -CfgLdDel -L0 -a0 36#Virtual Drive: 0 (Target Id: 0) 1# check Raid 2#/opt/MegaRAID/MegaCli/MegaCli64 -LDInfo -Lall -aALL 3#RAID 1 4#RAID Level : Primary-1, Secondary-0, RAID Level Qualifier-0 5#RAID 5 6#RAID Level : Primary-5, Secondary-0, RAID Level Qualifier-3 7 8# check Raid disks 9#/opt/MegaRAID/MegaCli/MegaCli64 -LdPdInfo -aAll | egrep \u0026#34;^Adapter|^Number of Virtual|^Virtual Drive:|^Name|^Enclosure Device ID:|^Slot Number:\u0026#34; 10 11# GPT part 12#parted -s /dev/sde mklabel gpt mkpart primary 0% 100% 13 14# check disk rebuild progress 15#/opt/MegaRAID/MegaCli/MegaCli64 -PDRbld -ShowProg -physdrv[32:1] -aALL 16#Rebuild Progress on Device at Enclosure 32, Slot 1 Completed 7% in 3 Minutes. 17 18# Force rebuild 19#/opt/MegaRAID/MegaCli/MegaCli64 -PDRbld -Start -physdrv[32:1] -a0 20# OR 21#/opt/MegaRAID/MegaCli/MegaCli64 -pdlocate -start -physdrv[32:1] -a0 22 23#Start rebuild, first clean the foreign configuration and then make the device hot spare (only if the above command failed) 24#/opt/MegaRAID/MegaCli/MegaCli64 -CfgForeign -Clear -aALL 25#set global hostspare 26#/opt/MegaRAID/MegaCli/MegaCli64 -PDHSP -Set -PhysDrv [32:1] -a0 27 28#If you need to unset/remove a global hotspare: 29#/opt/MegaRAID/MegaCli/MegaCli64 -PDHSP -Rmv -PhysDrv [32:1] -aN 30 31#downgrade raid6 --\u0026gt; raid5 ,and more space now can be used 32# -L0 virtual disk 0 33# /opt/MegaRAID/MegaCli/MegaCli64 -LDRecon -Start -r5 -L0 -a0 34# echo 1 \u0026gt; /sys/block/sda/device/rescan 35 36#Add disk to a raid5 37#/opt/MegaRAID/MegaCli/MegaCli64 -LDRecon -Start -r5 -Add -PhysDrv[32:3] -L0 -a 38 39#Configure WriteThrough or WriteBack 40#/opt/MegaRAID/MegaCli/MegaCli64 -LDSetProp -WT -Immediate -Lall -aAll 41#/opt/MegaRAID/MegaCli/MegaCli64 -LDSetProp -WB -Immediate -Lall -aAll 42 43#/opt/MegaRAID/MegaCli/MegaCli64 -PDList -aALL |grep \u0026#34;Inquiry Data:\u0026#34; 44#Inquiry Data: SEAGATE ST600MP0005 VS08S7M04B0C 45#Inquiry Data: SEAGATE ST600MP0005 VS08S7M04AFD 46#Inquiry Data: SEAGATE ST600MP0005 VS08S7M04JAY 47#Inquiry Data: SEAGATE ST600MP0005 VS08S7M04AAG 48 49#/opt/MegaRAID/MegaCli/MegaCli64 -PDList -aALL | grep \u0026#34;Drive Temperature\u0026#34; 50#Drive Temperature :36C (96.80 F) 51#Drive Temperature :35C (95.00 F) ","date":"2022-12-17","img":"","permalink":"https://bajie.dev/posts/20221217-megacli/","series":null,"tags":null,"title":"Megacli操作磁盘的常用命令"},{"categories":null,"content":"这一篇正规的DBA看到肯定会呲之以鼻，但对于没有用过oracle的系统运维来说，就是必须知道的事情了。\n公司的 Oracle 实例有两台数据库，定时用rman备份，但是没有自动清理机制 ，过一阵子磁盘就会超过 80% 告警，需要手动清理，烦不胜烦。\n凡是要手动做三次的事情必须自动化处理，启荣大师如是说，照办就是。\n1cat del_log.sh 2 3#!/bin/bash 4 5source /home/oracle/.bash_profile 6 7rman target / \u0026lt;\u0026lt; EOF 8delete noprompt archivelog until time \u0026#39;sysdate-7\u0026#39;; 9exit; 10EOF 很简单吧。上面引用的 .bash_profile 内容如下：\n1# .bash_profile 2 3# Get the aliases and functions 4if [ -f ~/.bashrc ]; then 5 . ~/.bashrc 6fi 7 8# User specific environment and startup programs 9 10PATH=$PATH:$HOME/.local/bin:$HOME/bin 11export PATH 12export ORACLE_BASE=/data/u01/app/oracle 13export ORACLE_HOME=/data/u01/app/oracle/product/18.3.0/dbhome_1 14export ORACLE_SID=oradb 15export PATH=/usr/sbin:$PATH 16export PATH=$ORACLE_HOME/bin:$PATH:/usr/local/bin 17export LD_LIBRARY_PATH=$ORACLE_HOME/lib:/lib:/usr/lib:$ORACLE_HOME/rdbms/lib 18export CLASSPATH=$ORACLE_HOME/JRE:$ORACLE_HOME/jlib:$ORACLE_HOME/rdbms/jlib 19export NLS_DATE_FORMAT=\u0026#39;yyyy/mm/dd hh24:mi:ss\u0026#39; 20export EDITOR=vi 21umask 022 这个 del_log.sh 的脚本放进 oracle 用户的 crontab 里执行就好\n10 3 * * * /usr/local/bin/del_log.sh ","date":"2022-12-08","img":"","permalink":"https://bajie.dev/posts/20221208-oracle_rman/","series":null,"tags":null,"title":"Oracle使用rman定时清除7天前的日志"},{"categories":null,"content":"CoreDNS是k8s的御用DNS解析软件\n支持很多插件，各种DoH、DoT、gRPC ，各种特性，安装配置也十分简单\n替代dnsmasq后，基本dnsmasq的功能都支持，修改劫持域名后也不用重启，会自动刷文件更新\n下面说一下装法：\n安装：\nwget https://github.com/coredns/coredns/releases/download/v1.7.0/coredns_1.7.0_linux_amd64.tgz\r解压后就一个文件，直接放到/usr/local/bin\n做个systemd启动文件\n1cat /etc/systemd/system/coredns.service 2[Unit] 3Description=CoreDNS DNS server 4After=network.target 5 6[Service] 7PermissionsStartOnly=true 8LimitNOFILE=1048576 9LimitNPROC=512 10CapabilityBoundingSet=CAP_NET_BIND_SERVICE 11AmbientCapabilities=CAP_NET_BIND_SERVICE 12NoNewPrivileges=true 13ExecStart=/usr/local/bin/coredns -conf=/usr/local/bin/coredns.conf 14ExecReload=/bin/kill -SIGUSR1 $MAINPID 15Restart=on-failure 16 17[Install] 18WantedBy=multi-user.target 东西全都在/usr/local/bin/coredns.conf配置文件里：\n1.:5353 https://.:443 { 2 any 3 log 4 debug 5 bind 172.18.30.1 6 tls /usr/local/bin/full.pem /usr/local/bin/key.pem 7 hosts /etc/hosts { 8 ttl 60 9 reload 1m 10 no_reverse 11 fallthrough 12 } 13 cache 120 14 reload 6s 15 loadbalance 16 forward . 114.114.114.114:53 223.5.5.5:53 223.6.6.6:53 { 17 policy sequential 18 } 19 prometheus 172.18.30.1:9253 20 21} 解释一下：\n1. 表示所有域名，.:5353 和 https://.:443 表示开了5353和443端口来服务所有域名的查询 2bind 172.18.30.1 绑定指定ip 3tls full.pem和key.pem是lego生成的*.rendoumi.com的证书链和私钥 4hosts 表示从文件解析，reload 1m会自动扫描文件变动并加载 5 最有用的是fallthrough ，如果在/etc/hosts 找不到记录，那就继续，去下面的配置里找 6loadbalance 表示如果查到的记录有多条，那就按round-robin轮询返回结果 7forward 这里指定上游服务器，最多可以有15个上游 8 policy是严格按顺序来，首先114，114坏了5.5.5，5.5.5坏了6.6.6，如果不指定会从upstream里随机 9\t挑一个 10prometheus 露出端口，供prometheus刮数据用 然后启动，就可以了，这里为了避免跟dnsmasq冲突，用了5353，实际应该是53端口\nsystemctl start coredns\r测一下标准的dns查询\ndig -t a m.ddky.com @172.18.30.1 -p5353\r下面来解释DoH，DNS-Over-HTTPS，这个特性非常鬼畜 注意，准确的说https提交的是dns-message\n先测一下CF\ncurl -H 'accept: application/dns-json' 'https://cloudflare-dns.com/dns-query?name=m.ddky.com\u0026amp;type=A' | jq .\rjson的结果很华丽，但是请注意，上面实际发的是dns-json，coredns不支持，鬼畜吧！\nDoH实际提交的是dns-message：\ncurl -H 'accept: application/dns-message' -v 'https://dns.ddky.com/dns-query?dns=q80BAAABAAAAAAAAA3d3dwdleGFtcGxlA2NvbQAAAQAB' | hexdump\rcurl -H 'accept: application/dns-message' -v 'https://cloudflare-dns.com/dns-query?dns=q80BAAABAAAAAAAAA3d3dwdleGFtcGxlA2NvbQAAAQAB' | hexdump\r注意那串dns=的字符串，那是个实际的查询，被Base64加码了，但绝不是对 name=www.example.com\u0026amp;type=A 这样简单的base64，这个东西完全无法造出来，有二进制的东西在里面。\n所以我们必须用程序来了，必须下载一个土造的DoH的客户端\nwget https://github.com/ameshkov/dnslookup/releases/download/v1.3.0/dnslookup-linux-amd64-v1.3.0.tar.gz\r解压出来一个dnslookup，用这个来查才行\n./dnslookup www.ddky.com https://dns.ddky.com/dns-query\r./dnslookup www.ddky.com https://dns.alidns.com/dns-query\r这样就ok了，最后补充一下，如果你非要用curl做DoH，也行，得下载最新版本的curl\ncurl -v --doh-url https://cloudflare-dns.com/dns-query https://m.ddky.com\r看过程，是先去CF查了DNS，然后又发了个请求到m.ddky.com\n","date":"2022-12-07","img":"","permalink":"https://bajie.dev/posts/20221207-cordns_dnsmasq/","series":null,"tags":null,"title":"CoreDNS替代dnsmasq"},{"categories":null,"content":"之前说过sed的高阶用法，其实普通情况下，正则是最常用的，下面就来说一下，用不到的就暂时不说\n1^ # 匹配行开始，如：/^sed/匹配所有以sed开头的行。 2$ # 匹配行结束，如：/sed$/匹配所有以sed结尾的行。 3. # 匹配一个非换行符的任意字符，如：/s.d/匹配s后接一个任意字符，最后是d。 4* # 匹配0个或多次字符，如：/a*sed/匹配所有模板是0个或多个a后紧跟sed的行。后紧跟sed的行。 5\\? # 匹配0次或1次他前面的字符 6\\+ # 匹配1次或多次他前面的字符，如：空格\\+ 或 “\\+“匹配至少一个或多个空格 7| #管道符号用来匹配两边任意一个子表达式，如：\u0026#39;/101\\|102/p\u0026#39; 匹配包含101或者102的行打印 8[] # 匹配一个指定范围内的字符，如/[sS]ed/匹配sed和Sed。 9[^] # 匹配一个不在指定范围内的字符，如：/[^A-RT-Z]ed/匹配不包含A-R和T-Z的一个字母开头，紧跟ed的行。 10\\(..\\) # 匹配子串，保存匹配的字符，如s/\\(love\\)able/\\1rs，loveable被替换成lovers。 11\u0026amp; # 保存搜索字符用来替换其他字符，如s/love/ **\u0026amp;** /，love这成 **love** 。 12\\\u0026lt; # 匹配单词的开始，如:/\\\u0026lt;love/匹配包含以love开头的单词的行。 13\\\u0026gt; # 匹配单词的结束，如/love\\\u0026gt;/匹配包含以love结尾的单词的行。 14\\{m\\} # 重复字符x，m次，如：/0\\{5\\}/匹配包含5个0的行。 15\\{m,\\} # 重复字符x，至少m次，如：/0\\{5,\\}/匹配至少有5个0的行。 16\\{m,n\\} # 重复字符x，至少m次，不多于n次，如：/0\\{5,10\\}/匹配5~10个0的行。 例子很多:\n用户名，后面跟1个或者多个空格，再跟密码；更换成用户名 密码 MAC地址\n1sed -i \u0026#34;s/${username} \\+${password}$/${username} ${password} ${IV_HWADDR}/\u0026#34; ${PASSFILE} 跟 shell 脚本的参数一起记比较好：\n1$0 脚本本身的名字 2$1 传递给该shell脚本的第1个参数 3$2 传递给该shell脚本的第2个参数 4$@ 传给脚本的所有参数的列表 5$# 传给脚本的参数个数 6$* 以一个单字符串显示所有向脚本传递的参数，与位置变量不同，\u0026gt;参数可超过9个 7$$ 脚本运行的当前进程ID号 8$? 命令执行结果反馈，0表示执行成功，其余数字表示执行不成功。 还有一个重要的地方，sed 使用 -i 对文件进行修改时，执行者需要有对文件目录的写权限，因为sed实际是产生了一个临时文件，然后再挪回去得！！！！\n","date":"2022-12-07","img":"","permalink":"https://bajie.dev/posts/20221207-sed_regex/","series":null,"tags":null,"title":"Sed中的正则表达式用法"},{"categories":null,"content":"zabbix 是很熟悉的东西，但是实际上博主已经跳过了这个东西，直接蹦到 Prometheus 去了\n但是，存在即合理，当下公司用的是这个，那么用就用吧，zabbix发到钉钉告警。\n那么我们也研究一下如何发到钉钉告警，而且好看一些\n原理：\n原理就是用 post 向钉钉机器人的 webhook 地址提交 Markdown 的 json 信息\n首先我们要建立个钉钉群，然后在群中添加一个群机器人，这里就会有两个选择，一个是这个机器人只接受特定的词语，二是向机器人发送消息的机器的ip是固定的。\n阿里云建议的是关键词：云监控、云服务、监控、Monitor、ECS、报警\n当然，这里更加建议IP，IP是死的，报警里带关键词意味着发送内容被部分固定了。\n如上，我们会得到一个钉钉机器人的Webhook地址：\n1https://oapi.dingtalk.com/robot/send?access_token=xxxxxxxxxx 然后我们要准备一个发送的脚本，python很合适\n1#!/usr/bin/env python 2#coding:utf-8 3 4#zabbix钉钉报警 5import requests,json,sys,os,datetime 6#说明：这里改为自己创建的机器人的webhook 7webhook=\u0026#34;https://oapi.dingtalk.com/robot/send?access_token=xxxxxx\u0026#34; 8 9def log(info): 10 if os.path.isfile(\u0026#34;/tmp/dingding.log\u0026#34;) == False: 11 f = open(log_file, \u0026#39;a+\u0026#39;) 12 else: 13 f = open(log_file,\u0026#39;w+\u0026#39;) 14 f.write(info) 15 f.close() 16 17def msg(text,user): 18 json_text= { 19 \u0026#34;msgtype\u0026#34;: \u0026#34;markdown\u0026#34;, 20 \u0026#34;markdown\u0026#34;: { 21 \u0026#34;title\u0026#34;: \u0026#34;zabbix monitor\u0026#34;, 22 \u0026#34;text\u0026#34;: text 23 }, 24 \u0026#34;at\u0026#34;: { 25 \u0026#34;atMobiles\u0026#34;: [ 26 user 27 ], 28 \u0026#34;isAtAll\u0026#34;: False 29 } 30 } 31 32 headers = {\u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;} 33 r=requests.post(url=webhook,data=json.dumps(json_text),headers=headers).json() 34 code = r[\u0026#34;errcode\u0026#34;] 35 time=time.strftime(\u0026#34;%Y-%m-%d %H:%M:%S\u0026#34;, time.localtime()) 36 if code == 0: 37 log(time + \u0026#34;:消息发送成功 返回码:\u0026#34; + str(code) + \u0026#34;\\n\u0026#34;) 38 else: 39 log(time + \u0026#34;:消息发送失败 返回码:\u0026#34; + str(code) + \u0026#34;\\n\u0026#34;) 40 exit(3) 41 42if __name__ == \u0026#39;__main__\u0026#39;: 43 user = sys.argv[1] #用户 44 subject = sys.argv[2] #标题 45 text = sys.argv[3] #消息 46 msg(text,user) 我们把上面的文件内容放到 /usr/lib/zabbix/alertscripts 目录下，名字就叫做 dingding.py ，只能是这个目录，因为这是 zabbix 缺省外挂脚本文件的目录。\n从上面代码里我们可以看到，实际是发送了一个 markdown 的文本，那么自然，各种markdown的语法就可以用起来了，仔细看一下钉钉markdown参数的详解：\n可以根据自己需求来修改。\n接下来我们配置Zabbix的告警配置\n创建新的告警媒介：媒介类型选择脚本，提前将上方的脚本放置于/usr/lib/zabbix/alertscripts中并在脚本名称处填写你命名的文件名。 添加告警参数：这里用到了3个参数，第一个是{ALERT.SENDTO}告警对象，第二个是{ALERT.SUBJECT}告警对象，第三个是{ALERT.MESSAGE}告警正文，按照填写即可。\nALERT.SENDTO}\n#对应Python脚本中的，user=sys.argv1 。\n{ALERT.SUBJECT}\n#发送的信息的标题\n{ALERT.MESSAGE}\n#对应Python脚本中的，text=sys.argv3 。\n增加Message type：一般增加3个就足够了，发现问题、问题恢复、问题更新，详细的设置在下方说明。\n这里就是关键了：\n发现问题的模板：\n1![告警平台信息](http://www.rendoumi.com/fire.png) 2\u0026gt; * ##### 告警主机: {HOSTNAME1} 3\u0026gt; * ##### 告警时间: {EVENT.DATE} {EVENT.TIME} 4\u0026gt; * ##### 告警等级: {TRIGGER.SEVERITY} 5\u0026gt; * ##### 告警信息: {TRIGGER.NAME} 6\u0026gt; * ##### 告警项目: {TRIGGER.KEY1} 7\u0026gt; * ##### 问题详情: {ITEM.NAME}:{ITEM.VALUE} 8\u0026gt; * ##### 当前状态: {TRIGGER.STATUS}:{ITEM.VALUE1} 9\u0026gt; * ##### 事件ID: {EVENT.ID} Problem recovery(问题恢复)的模板：\n1![告警平台信息](http://www.rendoumi.com/recover.png) 2\u0026gt; * #### 警告解除：{EVENT.RECOVERY.DATE} {EVENT.RECOVERY.TIME} 3\u0026gt; * ##### 告警持续时长: {EVENT.DURATION} 4\u0026gt; * ##### 告警主机: {HOST.NAME} 5\u0026gt; * ##### 告警信息: {EVENT.NAME} 6\u0026gt; * ##### 告警等级: {EVENT.SEVERITY} 7\u0026gt; * ##### 事件ID: {EVENT.ID} 8\u0026gt; * ##### {TRIGGER.URL} 有这两个就够了\n创建动作（点击左边菜单的配置\u0026mdash;\u0026gt;动作）\n添加告警条件\n添加 触发器示警度 大于等于 警告\n操作，选择发送给Admin组，或者其他组。\n在操作细节里，我们发送告警到Admin组，然后方式选dingding，这样就跟报警媒介联系起来了。\n这样一轮轮的更新以后，就可以使用了。注意要研究Markdown的语法。另外提前准备好图片。\n我们就得到一个跟阿里云告警一摸一样的东西了，说实话，好看，没有鸟用。\n","date":"2022-11-30","img":"","permalink":"https://bajie.dev/posts/20221130-zabbix_dingding/","series":null,"tags":null,"title":"Zabbix下发往钉钉告警"},{"categories":null,"content":"用NFS来做kubernetes的持久化卷虽然不是最佳实践，但存在即合理。在现在的公司就是这么用的，记录一下，可用但不是最佳。\n一、搭建NFS服务端和所有工作节点安装客户端 1#NFS服务端 (centos7) 2yum install nfs-utils rpcbind -y 3 4mkdir -p /data/nfs/{download,bakup,www} 5echo \u0026#34;/data/nfs 10.10.0.0/16(rw,all_squash,insecure,sync)\u0026#34; \u0026gt;\u0026gt; /etc/exports 6exportfs -r 7 8systemctl enable rpcbind nfs-server 9systemctl restart rpcbind nfs-server 10 11showmount -e localhost 12 13#客户端根本不装也可以，客户端不需要启动任何nfs服务 14#只是为了使用showmount工具来验证 15 16#NFS客户端（CentOS，所有WorkNode节点都需要执行） 17yum install -y nfs-utils 18 19#NFS客户端（Ubuntu，所有WorkNode节点都需要执行） 20apt install -y nfs-common NFS 服务端的参数如下：\nrw 能读能写 insecure NFS通过1024以上的端口发送 root_squash root会被变成noboby，其他人不变 no_root_squash root身份保持不变，其他人不变。（不能用这个，容易被黑） all_squash 不论登入NFS的使用者身份为何，他的身份都会被压缩成为匿名使用者，通常也就是nobody sync 数据直接落盘，性能略损。 async 数据先落内存，然后落盘，性能略升。 综上，保险的参数就是\n1/data/nfs 10.10.0.0/16(rw,insecure,all_squash,sync) 注意：修改了/etc/exports后，并不需要重启nfs服务，只要用exportfs重新扫描一次/etc/exports，并且重新加载即可\n1exports -rv 二、POD内直接使用 由于nfs是内核自带的东西，所以最简单的使用方法就是直接在pod内使用\n1apiVersion: v1 2kind: Pod 3metadata: 4 name: test 5 labels: 6 app.kubernetes.io/name: alpine 7spec: 8 containers: 9 - name: alpine 10 image: alpine:latest 11 command: 12 - touch 13 - /data/test 14 volumeMounts: 15 - name: nfs-volume 16 mountPath: /data 17 volumes: 18 - name: nfs-volume 19 nfs: 20 server: 10.10.247.38 21 path: /data 22 readOnly: no 这个pod是一次性运行的，运行完后就可以看到nfs中多了个test文件\n三、用PV和PVC使用 先建立PV，注意策略是Retain，不会丢数据。如果是Recycle会清洗数据\n1apiVersion: v1 2kind: PersistentVolume 3metadata: 4 name: nfs-pv-pvname 5spec: 6 accessModes: 7 - ReadWriteOnce 8 - ReadOnlyMany 9 - ReadWriteMany 10 capacity: 11 storage: 10Gi 12 storageClassName: \u0026#34;\u0026#34; 13 persistentVolumeReclaimPolicy: Retain 14 volumeMode: Filesystem 15 nfs: 16 server: 10.10.247.38 17 path: /data 18 readOnly: no 然后建立 PVC\n1apiVersion: v1 2kind: PersistentVolumeClaim 3metadata: 4 name: nfs-pvc-pvcname 5spec: 6 accessModes: 7 - ReadWriteMany 8 resources: 9 requests: 10 storage: 1Gi 建一个Deployment，用一下\n1apiVersion: extensions/v1beta1 2kind: Deployment 3metadata: 4 name: nfs-pvc 5spec: 6 replicas: 1 7 template: 8 metadata: 9 labels: 10 app: nfs-pvc 11 spec: 12 containers: 13 - name: nginx 14 image: alphine/nginx:latest 15 imagePullPolicy: IfNotPresent 16 ports: 17 - containerPort: 80 18 name: web 19 #使用volume 20 volumeMounts: 21 - name: www 22 subPath: nginx-pvc #远程子路径 23 mountPath: /usr/share/nginx/html 24 volumes: 25 - name: www 26 persistentVolumeClaim: 27 claimName: nfs-pvc-pvcname 注意：\nStorage 10Gi 1Gi 这都是骗鬼玩儿呢，根本不起任何作用！！！ ","date":"2022-11-24","img":"","permalink":"https://bajie.dev/posts/20221124-k8s_nfs/","series":null,"tags":null,"title":"Kubernetes使用nfs做持久化卷存储"},{"categories":null,"content":" 1#!/usr/bin/env python 2#coding:utf-8 3 4#zabbix钉钉报警 5import requests 6import json 7import sys 8import os 9import datetime 10 11#这里是自己创建的机器人的webhook 12webhook=\u0026#34;https://oapi.dingtalk.com/robot/send?access_token=xxxxxx\u0026#34; 13 14user=sys.argv[1] 15text=sys.argv[3] 16data={ 17 \u0026#34;msgtype\u0026#34;: \u0026#34;text\u0026#34;, 18 \u0026#34;text\u0026#34;: { 19 \u0026#34;content\u0026#34;: text 20 }, 21 \u0026#34;at\u0026#34;: { 22 \u0026#34;atMobiles\u0026#34;: [ 23 user 24 ], 25 \u0026#34;isAtAll\u0026#34;: False 26 } 27} 28headers = {\u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;} 29x=requests.post(url=webhook,data=json.dumps(data),headers=headers) 下面放shell脚本的\n1#!/bin/bash 2 3function ddmsg() { 4 Token=\u0026#34;xxxxxxxxxxxx\u0026#34; 5 Weburl=\u0026#34;https://oapi.dingtalk.com/robot/send?access_token=\u0026#34; 6 curl -ks -m 2 \u0026#34;${Weburl}${Token}\u0026#34; \\ 7 -H \u0026#39;Content-Type: application/json;charset=utf-8\u0026#39; \\ 8 -d \u0026#34;{\u0026#39;msgtype\u0026#39;: \u0026#39;text\u0026#39;, 9 \u0026#39;text\u0026#39;: { \u0026#39;content\u0026#39;: \u0026#39;$*\u0026#39;} 10 }\u0026#34; \u0026amp;\u0026gt;/tmp/ddmsg.log 11 if [ `grep \u0026#34;errmsg.*ok\u0026#34; /tmp/ddmsg.log |wc -l` -ne 1 ]; then 12 echo \u0026#39;send error !\u0026#39;;cat /tmp/ddmsg.log;exit 1; 13 fi 14} 15 16#测试内容 17echo \u0026#34;@警报 18主机:$(hostname) 19信息:Node test 20时间:$(date +\u0026#34;%F %T\u0026#34;) 21\u0026#34;\u0026gt;.msg 22 23cat .msg 24ddmsg \u0026#34;`cat .msg`\u0026#34; ","date":"2022-11-23","img":"","permalink":"https://bajie.dev/posts/20221123-dingding/","series":null,"tags":null,"title":"钉钉告警发送"},{"categories":null,"content":"客户发了一个需求如下：\n1用shell 或者python3 控制aws去获取新IP、绑定IP到实例 这个就非常简单了，因为本身aws就提供python的工具，一切都可以api化的\n做法如下：\n1#安装aws的工具 2cd /tmp 3curl -kL \u0026#34;https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\u0026#34; -o \u0026#34;awscliv2.zip\u0026#34; 4unzip awscliv2.zip 5sudo ./aws/install 6aws --version 7 8#配置aws key 9aws configure 10 11#查看配置 12aws configure list 13 14---------------------------- 15 16#申请一个新IP并且打tag 17aws ec2 allocate-address --region $Region |tee /tmp/eip.log 18eip_id=$(jq -r \u0026#34;.AllocationId\u0026#34; /tmp/eip.log) 19aws ec2 create-tags --resources ${eip_id} --tags Key=Name,Value=eip-01 20 21eip=$(jq -r \u0026#34;.PublicIp\u0026#34; /tmp/eip.log) 22 23#给ec2赋ip，前提是知道ec2_id 24aws ec2 associate-address --instance-id ${ec2_id} --allocation-id ${eip_id} 25 26 27---------------------------- 28 29#启动新ec2的方法： 30#启动ec2 31aws ec2 run-instances \\ 32--region $Region --count 1 \\ 33--instance-type t2.micro \\ 34--subnet-id ${lan_a_public_id} \\ 35--security-group-ids $gid \\ 36--key-name MySshKey \\ 37--image-id ami-04ff9e9b51c1f62ca \\ 38 |tee /tmp/ec2.log 39 40ec2_id=$(jq -r \u0026#34;.Instances[0].InstanceId\u0026#34; /tmp/ec2.log) 参考： https://www.cnblogs.com/elvi/p/16542406.html ","date":"2022-11-23","img":"","permalink":"https://bajie.dev/posts/20221123-freelancer_aws/","series":null,"tags":null,"title":"Freelancer之aws"},{"categories":null,"content":"客户发了一个需求如下：\n1TODO: 在sortFiles中，获取最新的域名，生成二维码，覆盖QRCodePath图片 2TODO:将QRCodePath图片上传到阿里云0S根目录，重复则覆盖 又具体询问了一下细节，是一个 csharp 的程序\n1var sortFiles = Directory.GetFiles(zipPath, \u0026#34;*.zip\u0026#34;).Select(fn =\u0026gt; new FileInfo(fn)).OrderBy(f =\u0026gt; f.LastAccessTime); 2 3// TODO: 在sortFiles中，获取最新的域名（文件名去掉.zip就是域名），结合logoUrl生成二维码，覆盖QRCodePath图片 4 5 6// TODO: 将QRCodePath图片上传到阿里云OOS根目录，重复则覆盖， 看了一下，觉得思路如下，这是个管道流，无论徒手写代码生成QRCODE(还有个logo图需要放到二维码中央)，还是集成OSS的SDK都麻烦\n最快速方法是直接调用外部成程序做这两步，第一步用Go写个程序生成二维码，第二步用阿里的ossutil64工具上传\n一、Go程序Gen出QRCode go 有很多库，我们直接拿来用好了，但是基本上都是老代码，所以需要下载 go1.13.15 来用\n1# 下载go 1.13.15，解压到E:\\go，并建立一个E:\\go\\go-work目录，下载的模块会放到这里 2https://dl.google.com/go/go1.13.15.windows-amd64.zip 3 4# 设置三个变量 5GOROOT=E:\\go 6GOPATH=E:\\go\\go-work 7PATH中增加好E:\\go\\bin的目录 8 9# 设置一下代理，有时候模块拉不下来 10go env -w GOPROXY=https://goproxy.cn,direct 然后就准备gen qrcode的程序，main.go\n1package main 2 3import ( 4\t\u0026#34;flag\u0026#34; 5\t\u0026#34;image\u0026#34; 6\t\u0026#34;image/draw\u0026#34; 7\t\u0026#34;image/jpeg\u0026#34; 8\t_ \u0026#34;image/png\u0026#34; 9\t\u0026#34;os\u0026#34; 10\t\u0026#34;path/filepath\u0026#34; 11 12\t\u0026#34;github.com/LyricTian/logger\u0026#34; 13\t\u0026#34;github.com/nfnt/resize\u0026#34; 14\t\u0026#34;github.com/skip2/go-qrcode\u0026#34; 15) 16 17var ( 18\ttext string 19\tlogo string 20\tpercent int 21\tsize int 22\tout string 23) 24 25func init() { 26\tflag.StringVar(\u0026amp;text, \u0026#34;t\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;二维码内容\u0026#34;) 27\tflag.StringVar(\u0026amp;logo, \u0026#34;l\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;二维码Logo(png)\u0026#34;) 28\tflag.IntVar(\u0026amp;percent, \u0026#34;p\u0026#34;, 15, \u0026#34;二维码Logo的显示比例(默认15%)\u0026#34;) 29\tflag.IntVar(\u0026amp;size, \u0026#34;s\u0026#34;, 256, \u0026#34;二维码的大小(默认256)\u0026#34;) 30\tflag.StringVar(\u0026amp;out, \u0026#34;o\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;输出文件\u0026#34;) 31} 32 33func main() { 34\tflag.Parse() 35 36\tif text == \u0026#34;\u0026#34; { 37\tlogger.Fatalf(\u0026#34;请指定二维码的生成内容\u0026#34;) 38\t} 39 40\tif out == \u0026#34;\u0026#34; { 41\tlogger.Fatalf(\u0026#34;请指定输出文件\u0026#34;) 42\t} 43 44\tif exists, err := checkFile(out); err != nil { 45\tlogger.Fatalf(\u0026#34;检查输出文件发生错误：%s\u0026#34;, err.Error()) 46\t} else if exists { 47\tlogger.Fatalf(\u0026#34;输出文件已经存在，请重新指定\u0026#34;) 48\t} 49 50\tcode, err := qrcode.New(text, qrcode.Highest) 51\tif err != nil { 52\tlogger.Fatalf(\u0026#34;创建二维码发生错误：%s\u0026#34;, err.Error()) 53\t} 54 55\tsrcImage := code.Image(size) 56\tif logo != \u0026#34;\u0026#34; { 57\tlogoSize := float64(size) * float64(percent) / 100 58 59\tsrcImage, err = addLogo(srcImage, logo, int(logoSize)) 60\tif err != nil { 61\tlogger.Fatalf(\u0026#34;增加Logo发生错误：%s\u0026#34;, err.Error()) 62\t} 63\t} 64 65\toutAbs, err := filepath.Abs(out) 66\tif err != nil { 67\tlogger.Fatalf(\u0026#34;获取输出文件绝对路径发生错误：%s\u0026#34;, err.Error()) 68\t} 69 70\tos.MkdirAll(filepath.Dir(outAbs), 0777) 71\toutFile, err := os.Create(outAbs) 72\tif err != nil { 73\tlogger.Fatalf(\u0026#34;创建输出文件发生错误：%s\u0026#34;, err.Error()) 74\t} 75\tdefer outFile.Close() 76 77\tjpeg.Encode(outFile, srcImage, \u0026amp;jpeg.Options{Quality: 100}) 78 79\tlogger.Infof(\u0026#34;二维码生成成功，文件路径：%s\u0026#34;, outAbs) 80} 81 82func checkFile(name string) (bool, error) { 83\t_, err := os.Stat(name) 84\tif err != nil { 85\tif os.IsNotExist(err) { 86\treturn false, nil 87\t} 88\treturn false, err 89\t} 90\treturn true, nil 91} 92 93func resizeLogo(logo string, size uint) (image.Image, error) { 94\tfile, err := os.Open(logo) 95\tif err != nil { 96\treturn nil, err 97\t} 98\tdefer file.Close() 99 100\timg, _, err := image.Decode(file) 101\tif err != nil { 102\treturn nil, err 103\t} 104 105\timg = resize.Resize(size, size, img, resize.Lanczos3) 106\treturn img, nil 107} 108 109func addLogo(srcImage image.Image, logo string, size int) (image.Image, error) { 110\tlogoImage, err := resizeLogo(logo, uint(size)) 111\tif err != nil { 112\treturn nil, err 113\t} 114 115\toffset := image.Pt((srcImage.Bounds().Dx()-logoImage.Bounds().Dx())/2, (srcImage.Bounds().Dy()-logoImage.Bounds().Dy())/2) 116\tb := srcImage.Bounds() 117\tm := image.NewNRGBA(b) 118\tdraw.Draw(m, b, srcImage, image.ZP, draw.Src) 119\tdraw.Draw(m, logoImage.Bounds().Add(offset), logoImage, image.ZP, draw.Over) 120 121\treturn m, nil 122} 上面很简单，用到了别人的三个模块，需要安装一下\n1go get -u github.com/LyricTian/logger@latest 2go get -u github.com/nfnt/resize@latest 3go get -u github.com/skip2/go-qrcode@latest 注意，这些命令是针对 go 1.13.15 的，如果用高版本的，命令是不一样的\n然后就编译\n1go build -o qr.exe main.go 找个中心图，然后测试\n1E:\\qr\\qr.exe -h 2Usage of qr: 3 -l string 4 二维码Logo(png) 5 -o string 6 输出文件 7 -p int 8 二维码Logo的显示比例(默认15%) (default 15) 9 -s int 10 二维码的大小(默认256) (default 256) 11 -t string 12 二维码内容 13 14E:\\qr\\qr.exe -t \u0026#34;http://www.zz.ha.cn\u0026#39; -l logo.png -o qr.png 就会生成二维码图像\n二、用ossutils64.exe上传OSS 这个就简单了，下载阿里的工具，大概是这个样子\n1ossutil64.exe --config-file oss.conf cp 上传文件 s3://桶名/\u0026#34;; 三、整合进csharp 这步也比较麻烦，涉及到csharp的单独语法\n1var sortFiles = Directory.GetFiles(zipPath, \u0026#34;*.zip\u0026#34;).Select(fn =\u0026gt; new FileInfo(fn)).OrderBy(f =\u0026gt; f.LastAccessTime); 2 3// TODO: 在sortFiles中，获取最新的域名（文件名去掉.zip就是域名），结合logoUrl生成二维码，覆盖QRCodePath图片 4 for (int i = 0; i \u0026lt; sortFiles.Length; ++i) { 5 sortFiles[i] = Path.GetFileNameWithoutExtension(sortFiles[i]); 6 7 System.Diagnostics.Process process = new System.Diagnostics.Process(); 8 System.Diagnostics.ProcessStartInfo startInfo = new System.Diagnostics.ProcessStartInfo(); 9 startInfo1.WindowStyle = System.Diagnostics.ProcessWindowStyle.Hidden; 10 startInfo1.FileName = \u0026#34;qr.exe\u0026#34;; 11 string parm01 = string.Format(\u0026#34;-t {0} -o {1} -l {2}\u0026#34;, sorFiles[i], sortFiles[i], logoUrl\u0026#34;); 12 startInfo1.Arguments = param01; 13 process.StartInfo = startInfo1; 14 process.Start(); 15 16 // TODO: 将QRCodePath图片上传到阿里云OOS根目录，重复则覆盖，AccessKey ID: LTAI5bbbb AccessKey Secret: NZPwtssss 17 System.Diagnostics.Process process = new System.Diagnostics.Process(); 18 System.Diagnostics.ProcessStartInfo startInfo = new System.Diagnostics.ProcessStartInfo(); 19 startInfo1.WindowStyle = System.Diagnostics.ProcessWindowStyle.Hidden; 20 startInfo1.FileName = \u0026#34;ossutil64.exe\u0026#34;; 21 startInfo1.Arguments = \u0026#34;--config-file oss.conf cp sortFiles[i] s3:///\u0026#34;; 22 string parm01 = string.Format(\u0026#34;-t {0} -o {1} -l {2}\u0026#34;, sorFiles[i], sortFiles[i], logoUrl\u0026#34;); 23 process.StartInfo = startInfo1; 24 process.Start(); 25 26 } 关键点有两个：\n1、获取sortFiles有后缀，需要去掉\n1for (int i = 0; i \u0026lt; sortFiles.Length; ++i) { 2 sortFiles[i] = Path.GetFileNameWithoutExtension(sortFiles[i]); 3} 2、要执行命令，需要后台执行\n1 System.Diagnostics.Process process = new System.Diagnostics.Process(); 2 System.Diagnostics.ProcessStartInfo startInfo = new System.Diagnostics.ProcessStartInfo(); 3 startInfo1.WindowStyle = System.Diagnostics.ProcessWindowStyle.Hidden; 4 startInfo1.FileName = \u0026#34;qr.exe\u0026#34;; 5 string parm01 = string.Format(\u0026#34;-t {0} -o {1} -l {2}\u0026#34;, sorFiles[i], sortFiles[i], logoUrl\u0026#34;); 6 startInfo1.Arguments = param01; 7 process.StartInfo = startInfo1; 8 process.Start(); 这样就搞完了，但是，没真机测试啊，go和oss的部分没问题，csharp的部分没有真机，无法验证。\n","date":"2022-11-23","img":"","permalink":"https://bajie.dev/posts/20221123-freelancer_qrcode/","series":null,"tags":null,"title":"Freelancer之QRCode"},{"categories":null,"content":"我们来说恢复的第二种情况，就是需要从binlog中指定位置恢复\nbinlog如何设置不说了，我们假设上次用mysqldump做过一次全量备份。\n1mysqldump -uroot -p -h 192.168.1.35 -P3306 --opt --triggers -R --hex-blob --single-transaction --flush-logs --master-data=2 -B 库名 \u0026gt; 库名.sql 由于我们在备份中使用了参数\u0026ndash;flush-logs \u0026ndash;master-data=2，所以 库名.sql 中会有binglog的信息供我们使用。\n一、用mysqlbinlog来恢复 我们首先要查到随后的binlog文件是那个，从那时候起又生成多少个binglog文件。\n然后去库里查询：\n1show master logs; 2show master status; 我们要分析一下SQL\n1# show binlog events [IN \u0026#39;log_name\u0026#39;] [FROM pos] [LIMIT [offset,] row_count]; 2 3show binlog events in \u0026#39;javaboy_logbin.000002\u0026#39; limit 5,10; 我们来翻看日志：\n可以看到是从 764\u0026ndash;\u0026gt;865 ，发生了删除，那么回放到这个764这个position前即可，764不会执行\n1mysqlbinlog /var/lib/mysql/mysql-bin.000204 --stop-position=764 --database=bbb | mysql -uroot -p \u0026ndash;start-position指定从哪里开始恢复，如果不指定，就会从binlog文件开头的position开始\n1mysqlbinlog /var/lib/mysql/mysql-bin.000204 --start-position=205 --stop-position=764 --database=bbb | mysql -uroot -p 二、用binglog2sql来恢复 binlog2sql 是大众点评公司的DBA 开发的一款基于通过解析binlog将delete 恢复为insert,update 的值 set 字段和where条件做对调的原理来恢复数据的。 使用限制 MySQL的binlog format 必须是row 安装\n简单说它的道理就是变删为增，生成新的SQL执行。感觉 yearning 的回滚应该就是基于这个东西搞得。\n地址：https://github.com/danfengcao/binlog2sql\n解析出回滚的SQL：\n1python binlog2sql.py --flashback -h127.0.0.1 -P3306 -uadmin -p\u0026#39;admin\u0026#39; -dtest -ttest3 --start-file=\u0026#39;mysql-bin.000002\u0026#39; --start-position=763 --stop-position=1147 \u0026gt; rollback.sql 回滚前务必再备份一下\n1mysql -h127.0.0.1 -P3306 -uadmin -p \u0026lt; rollback.sql ","date":"2022-11-22","img":"","permalink":"https://bajie.dev/posts/20221122-mysqldump_binlog/","series":null,"tags":null,"title":"MySQL数据库的备份和恢复之二"},{"categories":null,"content":"喜大普奔，干了一个月的MySQL DBA的工作，又学到了一部分知识。\n记录一下，以备不时只需\nmysqldump 的备份其实很麻烦，要考虑很多因素\n备份时候不能锁表 恢复的时候要快 有二进制数据的话需要备份二进制数据 有触发器、存储过程的都备份 通常 mysqldump 是做每天的fullbackup，要为之后的 binlog 做好准备，万一要恢复要提前做好准备 我们一点一点来说：\n一、备份时不能锁表\n1--single-transaction 2通过将导出操作封装在一个事务内来使得导出的数据是一个一致性快照 3该选项在导出数据之前提交一个BEGIN SQL语句，BEGIN不会阻塞任何应用程序且能保证导出时数据库的一致性状态。 4它只适用于InnoDB存储引擎。 5本选项和--lock-tables选项是互斥的，因为LOCK TABLES会使任何挂起的事务隐含提交。 二、恢复时要尽量快\n1--opt 2等同于--add-drop-table, --add-locks, --create-options, --quick, --extended-insert, --lock-tables, --set-charset, --disable-keys 该选项默认开启, 可以用--skip-opt禁用. 3 4--extended-insert, -e 5--extended-insert=false 6使用具有多个VALUES列的INSERT语法。这样使导出文件更小，并加速导入时的速度。 7默认为打开状态，使用--skip-extended-insert取消选项。 三、有二进制的按二进制备份\n1--hex-blob 2使用十六进制格式导出二进制字符串字段。如果有二进制数据就必须使用该选项。影响到的字段类型有BINARY、VARBINARY、BLOB。 四、有触发器、存储过程的都备份\n1--triggers 2导出触发器。该选项默认启用，用--skip-triggers禁用它。 3--routines, -R 4导出存储过程以及自定义函数 五、为之后如果要做binlog恢复提前做好准备\n1--flush-logs 2开始导出之前刷新binlog日志。 3请注意：假如一次导出多个数据库(使用选项--databases或者--all-databases)，将会逐个数据库刷新日志。 4除非使用--lock-all-tables或者--master-data外。在这种情况下，日志将会被刷新一次，相应的所以表同时被锁定。 5因此，如果打算同时导出和刷新日志应该使用--lock-all-tables 或者--master-data 和--flush-logs 6 7--master-data 8在导出的时候同时生成binlog文件名和位置在导出的文件开头。 9这个非常重要。binlog 的文件和位置可以从这里拿到。 10例如： 11-- CHANGE MASTER TO MASTER_LOG_FILE=\u0026#39;binlog.000001\u0026#39;, MASTER_LOG_POS=157; 12 13该选项将binlog的位置和文件名追加到输出文件中。 14如果为1，将会输出CHANGE MASTER 命令； 15如果为2，输出的CHANGE MASTER命令前添加注释信息。 16该选项将打开--lock-all-tables 选项，除非--single-transaction也被指定（在这种情况下，全局读锁在开始导出时获得很短的时间；其他内容参考下面的--single-transaction选项）。 17该选项自动关闭--lock-tables选项。 18mysqldump -uroot -p --host=localhost --all-databases --master-data=1; 19mysqldump -uroot -p --host=localhost --all-databases --master-data=2; 那么综上所述，总结一行非常使用的备份语句：\n1mysqldump -uroot -p -h 192.168.1.35 -P3306 --opt --triggers -R --hex-blob --single-transaction --flush-logs --master-data=2 -B 库名 \u0026gt; 库名.sql ","date":"2022-11-21","img":"","permalink":"https://bajie.dev/posts/20221121-mysqldump/","series":null,"tags":null,"title":"MySQL数据库的备份和恢复之一"},{"categories":null,"content":"公司用的 OpenVPN ，研发部门和普通员工的权限不一样，技术部门需要访问服务器，普通员工则不需要\n这样的话就需要根据用户定制路由了，技术部门推送特定的路由，普通员工不推送。\n那么做法如下，在 server.conf 做如下设定：\n1ifconfig-pool-persist /etc/openvpn/conf/ipp.txt 2client-config-dir /etc/openvpn/conf/static 第一行是指定记录 ip 地址的文件，作用是如果服务器进程重启了，重启后会读取这个文件，客户端重新拨号后获得的 IP 就不会变，还是重启前的 IP。\n第二行是指定了客户端的单独配置\n我们假设一个用户是 zhangranrui，那么就去 /etc/openvpn/conf/static 下建立一个 zhangranrui 的文本文件，内容如下：\n1# 前面是客户端固定IP地址，后面是网关地址 2ifconfig-push 172.18.0.10 172.18.0.9 3 4# 推送路由，后面是掩码 5push \u0026#34;route 10.10.0.0 255.255.0.0\u0026#34; 6 7# 重定向客户端的所有流量，否则访问服务端内网要像上面一样推送路由 8# push \u0026#34;redirect-gateway def1\u0026#34; 9 10# 推送给客户端的DNS解析地址 11# push \u0026#34;dhcp-option DNS 223.5.5.5\u0026#34; 12# push \u0026#34;dhcp-option DNS 114.114.114.114\u0026#34; 上面我们固定了IP，也推送了特定路由，那么怎么知道一个客户的 ip 呢，去看 ipp 文件：\n1cat /etc/openvpn/conf/ipp.txt 2zhangranrui,172.18.0.8 注意，上面文件里的 172.18.0.8 不是 ip ，是 网段 地址\n172.18.0.9 是网关地址，172.18.0.10才是真正的 IP 地址，所以不要搞错了\n也可以针对用户推送一些其他选项。\n","date":"2022-11-19","img":"","permalink":"https://bajie.dev/posts/20221119-openvpn_client/","series":null,"tags":null,"title":"OpenVPN客户端定制IP和路由"},{"categories":null,"content":"面试 infosys 的时候是一个印度老哥做考官，他问我的问题我是一句都听不懂的，确实是听不懂。\n然后他把问题在 webex 中打出来我回答的。\n其中有一个问题是：如何用CLI查出系统中内存占用最高的5个进程\n1ps aux --sort=-%mem | head -n 5 上面一句如何用英语表达出来呢，不好说啊\n所以记录一下，以备将来不时之需\n1． period 句号 2， comma 逗号 3： colon 冒号 4； semicolon 分号 5！ exclamation 惊叹号 6？ question 问号 7- hyphen 连字符 8* asterisk 星号 9\u0026#39; apostrophe 所有格符号，单词内部的省略 10— dash 破折号 11_ underscore 12‘ ’ single quotation marks 单引号 13“ ” double quotation marks 双引号 14( ) round brackets 圆括号 15[ ] square brackets 方括号 16\u0026lt;\u0026gt; Angle brackets 尖括号 17{} curly brackets or braces 大括号 18《 》French quotes 法文引号；书名号 19... ellipsis 省略号 20¨ tandem colon 双点号 21\u0026#34; ditto 同上 22‖ parallel 双线号 23／ slash 斜线号 24＆ ampersand = and 25～ tilde 波浪号 26§ section; division 分节号 27→ arrow 箭号；参见号 28| vertical bar 竖线 29backslash 反斜线 30% percent 百分号 ","date":"2022-11-17","img":"","permalink":"https://bajie.dev/posts/20221117-sign/","series":null,"tags":null,"title":"英语的符号表达"},{"categories":null,"content":"现在也开始搞起桌面运维了，其实还感觉蛮有意思的。\n公司的主路由器是锐捷的NBR6205-E，作为IPSec的服务端，而阿里云作为客户端。\n首先先普及一下IPSec的知识，IPSec目前只支持IPv4单播：\n密钥安全有 IKE 负责；数据安全方面，有 IPSec 负责。\n但是IKE也是 IPSec 的 一 部分，不能独立存在。存在两个SA分别为\nISAKMP Security Association（IKE SA）\nIPsec Security Association（IPsec SA）\n在这里注意一点就是IKE SA=ISAKMP SA\nIKE SA的 lifetime 默认为 86400 秒，即一天，默认没有volume limit。\n用户的数据流量真正是在 IPsec SA 上传递的，而不是在IKE SA；\nIPsec SA直接为用户数据流服务，IPsec SA中的所有安全策略都是为了用户数据流的安全。\nIPsec SA的 lifetime 默认为3600 秒，即1小时；默认volume limit为4608000 Kbytes，即4.608 Gbyte。\n因为SA有两个，分为IKE SA和IPsec SA，两个SA分别定义了如何保护密钥以及如何保护数据，其实这两个SA都是由IKE建立起来的，所以将IKE的整个运行过程分成了两个Phase（阶段），即 ：\nIKE Phase Two IKE Phase One 一、IKE Phase One第一阶段 IKE Phase One的主要工作就是建立IKE SA（ISAKMP SA），IKE SA的服务对象并不是用户数据，而是密钥流量，以及为IPsec SA服务的；IKE SA的协商阶段被称为main mode（主模式）还有 aggressive 野蛮模式，IKE也是需要保护自己的流量安全的（这些流量并非用户流量），所以IKE SA之间也需要协商出一整套安全策略，否则后续的密钥和IPsec SA的建立就不能得到安全保证；\nIKE SA之间需要协商的套安全策略包括：\n认证方式（Authentication）\n共总有Pre-Shared Keys (PSK)，Public Key Infrastructure (PKI)，RSA encrypted nonce，默认为PKI。\n加密算法（Encryption）\n总共有DES，3DES，AES 128，AES 192，AES 256，默认为DES。\nHash算法（HMAC）\n总共有SHA-1，MD5，默认为SHA-1。\n密钥算法（Diffie-Hellman）\nGroups 1 （768 bit），Group 2（1024 bit），Group 5（1536 bit），默认为Groups 1 （768 bit）。\nLifetime\n随用户定义，默认为86,400 seconds，但没有volume limit。\n二、IKE Phase Two第二阶段 IKE Phase Two的目的是要建立IPsec SA，由于IKE SA的服务对象并不是用户数据，而是密钥流量，以及为IPsec SA服务的，IKE SA是为IPsec SA做准备的，所以如果没有IKE SA，就不会有IPsec SA；IPsec SA是基于IKE SA来建立的，建立IPsec SA的过程称为 快速模式（quick mode）。IPsec SA才是真正为用户数据服务的，用户的所有流量都是在IPsec SA中传输的，用户流量靠IPsec SA来保护，IPsec SA同样也需要协商出一整套安全策略，其中包括：\n加密算法（Encryption）\n总共有DES，3DES，AES 128，AES 192，AES 256，默认为DES。\nHash算法（HMAC）\n总共有SHA-1，MD5，默认为SHA-1。\nLifetime\n随用户定义，默认为3600 seconds，即1小时；默认volume limit为4,608,000 Kbytes，即4.608 Gbyte。\n注意：IPsec SA中没有协商认证方式（Authentication）和密钥算法（Diffie-Hellman），因为IKE SA时已经认证过了，所以后面已经不需要再认证；并且密钥是在IKE SA完成的，所以在IPsec SA中也就谈不了密钥算法了，但也可以强制再算。\n上面很啰嗦了一把，但是没办法，因为等会设置的时候都要用到的。\n三、设置锐捷路由器 参数统统提取一下：\n对端IP：就是阿里云客户端那边的IP，47.94.106.58\n预共享密钥：两边都共享的一串字符串\nIPSec隧道生命周期：这里应该是第二阶段的时间，28800秒\nIKE 策略：第一阶段的IKE设置\n机密算法：3DES 散列算法：MD5 DH组：Group2 生命周期：28800秒\n转换集1：esp-3des esp-md5-hmac\n转换集2：esp-3des esp-sha-hmac\n完美向前加密：group2(1024-bit)\nDPD探测类型：periodic\nDPD探测周期：30秒\n协商模式：野蛮模式\n应用到接口：Gi0/7\n需经隧道访问的网段\n本地网段：10.8.0.0/255.255.252.0 总部网段(其实就是对端阿里的vpc网段)：10.10.240.0/255.255.240.0 四、配置阿里云的IPSec 1、首先设置VPN网关 2、建立用户网关 3、建立IPsec连接 4、编辑IPsec连接 上面配置要注意，VPN网关和用户网关选之前加好的，本端网段和对端网段和锐捷路由器上是相反的。预共享密钥是一样。\n另外就是感兴趣流模式，选了这个，就需要在路由表里手动发布路由！！\n再来就是高级配置，第一部分IKE配置\n注意参数，协商模式 aggressive 就是野蛮模式。\n第二部分IPsec配置：\n然后需要到VPN网关，因为选了感兴趣路由，所以IPSec建立好了，这里的策略路由表会出现一条路由\n源：10.10.240/21 目标：10.8.0.0/22，状态是未发布，点一下发布。就好了\n这样两端的 IPSec 就建立好了。\n","date":"2022-11-15","img":"","permalink":"https://bajie.dev/posts/20221115-ipsec/","series":null,"tags":null,"title":"阿里云IPSec与锐捷路由器得连接"},{"categories":null,"content":"这是个很奇怪的事情，应聘了一家搞 CDN 的公司，结果上去看了一下根本不对路。就立刻辞了，但是发现它给员工开的 L2TP VPN 确实非常好用。\n于是就自己也搭一个，方便自用。下面记录一下安装过程，环境是CentOS 7\n一、装L2TP 1# yum install epel-release 2# yum install xl2tpd libreswan 二、修改核心参数 1# vi /etc/sysctl.conf 2 3vm.swappiness = 0 4net.ipv4.neigh.default.gc_stale_time=120 5net.ipv4.conf.all.rp_filter=0 6net.ipv4.conf.default.rp_filter=0 7net.ipv4.conf.default.arp_announce = 2 8net.ipv4.conf.all.arp_announce=2 9net.ipv4.tcp_max_tw_buckets = 5000 10net.ipv4.tcp_syncookies = 1 11net.ipv4.tcp_max_syn_backlog = 1024 12net.ipv4.tcp_synack_retries = 2 13net.ipv4.conf.lo.arp_announce=2 14net.ipv4.ip_forward = 1 15net.ipv4.conf.default.accept_redirects = 0 16net.ipv4.conf.default.send_redirects = 0 17net.ipv4.conf.default.accept_source_route = 0 18 19# sysctl -p 三、配置 IPSEC 1# vi /etc/ipsec.d/l2tp_psk.conf 2 3conn L2TP-PSK-NAT 4 rightsubnet=vhost:%priv 5 also=L2TP-PSK-noNAT 6conn L2TP-PSK-noNAT 7 authby=secret 8 pfs=no 9 auto=add 10 keyingtries=3 11 dpddelay=30 12 dpdtimeout=120 13 dpdaction=clear 14 rekey=no 15 ikelifetime=8h 16 keylife=1h 17 type=transport 18 left=192.168.10.232 19 leftprotoport=17/1701 20 right=%any 21 rightprotoport=17/%any 注意上面的 left=192.168.10.232，这是服务器的ip地址，要更换为自己服务器的地址（如果在防火墙后，写内网地址，非映射后的公网IP）\n然后修改密钥，之后建立L2TP的 iphone 连接时会用到：\n1# vim /etc/ipsec.secrets 2 3192.168.10.232 %any: PSK \u0026#34;123456789\u0026#34; 验证一下：\n1# ipsec setup start 2 3# ipsec verify 看到上面两行红字不要慌张，Ignore，忽略掉即可。是内核特性中的reverse path filter特性，没关系。\n然后让 ipsec 自启动\n1# systemctl enable ipsec 四、配置xl2tpd 1# 先备份 2# mv /etc/xl2tpd/xl2tpd.conf /etc/xl2tpd/xl2tpd.conf.old 3 4# vim /etc/xl2tpd/xl2tpd.conf 5[global] 6listen-addr = 192.168.10.232 7ipsec saref = yes 8[lns default] 9ip range = 192.168.100.128-192.168.100.254 10local ip = 192.168.100.99 11require chap = yes 12refuse pap = yes 13require authentication = yes 14name = LinuxVPNserver 15ppp debug = yes 16pppoptfile = /etc/ppp/options.xl2tpd 17length bit = yes 注意更换上面 192.168.10.232 的服务器ip地址，同时记住name=LinuxVPNserver\n1# 先备份 2# mv /etc/ppp/options.xl2tpd /etc/ppp/options.xl2tpd.old 3 4# vim /etc/ppp/options.xl2tpd 5ipcp-accept-local 6ipcp-accept-remote 7ms-dns 8.8.8.8 8ms-dns 4.2.2.4 9noccp 10#noauth 11crtscts 12idle 1800 13mtu 1410 14mru 1410 15nodefaultroute 16debug 17lock 18proxyarp 19connect-delay 5000 最后编辑用户名和密码\n1vim /etc/ppp/chap-secrets 2 3# Secrets for authentication using CHAP 4# client server secret IP addresses 5test LinuxVPNserver test * 注意格式，第一列是用户名，第二列是上面xl2tpd.conf中记住的name名，第三列是密码，第四列是获取到的ip地址\n然后启动\n1# systemctl start xl2tpd 2# systemctl enable xl2tpd 3# systemctl status xl2tpd 五、配置防火墙 1iptables -A INPUT -p gre -j ACCEPT 2iptables -A OUTPUT -p gre -j ACCEPT 3iptables -A FORWARD -m state --state RELATED,ESTABLISHED -j ACCEPT 4iptables -A FORWARD -s 192.168.100.0/24 -j ACCEPT 5iptables -t nat -A POSTROUTING -s 192.168.100.0/24 -o eth0 -j MASQUERADE 注意上面的eth0，根据服务器的具体情况进行修改\n然后这样就完成了。\n六、配置iphone手机 配置好服务器ip，账户，密码以及密钥就可以了。\n","date":"2022-11-13","img":"","permalink":"https://bajie.dev/posts/20221113-l2tp_vpn/","series":null,"tags":null,"title":"L2TP VPN在CentOS7系统下的搭建"},{"categories":null,"content":"生产环境要用到kafka，记录一下安装过程，其实最重要的不是安装，而是使用。\n时间节点是2022年7月10日，zookeeper的版本是3.4.14：\n1wget https://mirrors.cnnic.cn/apache/zookeeper/stable/zookeeper-3.4.14.tar.gz kafka的版本是3.2，注意前面的2.13是scala的版本\n1wget http://archive.apache.org/dist/kafka/3.2.0/kafka_2.13-3.2.0.tgz 软件都下载好以后，找三台服务器，软件都放到 /usr/local 路径下：\n服务器的ip是：\n1172.18.31.50 2172.18.31.51 3172.18.31.52 移动软件：\n1tar zxvf zookeeper-3.4.14.tar.gz 2mv apache-zookeeper-3.6.3-bin /usr/local 3 4tar zxvf kafka_2.13-3.2.0.tgz 5mv kafka_2.13-3.2.0 /usr/local 然后得装 java 了，centos 版本\n1yum -y install epel-release 2yum -y install java-11-openjdk-devel 首先去编辑zookeeper的配置文件，然后启动\n1cp /usr/local/apache-zookeeper-3.6.3-bin/conf/zoo_sample.cfg /usr/local/apache-zookeeper-3.6.3-bin/conf/zoo.cfg 编辑zoo.cfg，增加几行\n1dataDir=/data/zookeeper 2 3metricsProvider.className=org.apache.zookeeper.metrics.prometheus.PrometheusMetricsProvider 4metricsProvider.httpPort=7000 5metricsProvider.exportJvmInfo=true 6 7server.0=172.18.31.50:20881:30881 8server.1=172.18.31.51:20881:30881 9server.2=172.18.31.52:20881:30881 注意上面，我们的数据目录是在/data/zookeeper，所以要先建立好目录结构\n然后有三台服务器么，把各自的id放到文件中，然后20881是三台服务器之间的通讯端口，30881是选举端口\n1mkdir /data/zookeeper -p 2echo 0 \u0026gt;/data/zookeeper/myid 三台服务器分别启动\n1/usr/local/apache-zookeeper-3.6.3-bin/bin/zkServer.sh start 验证一下\n1/usr/local/apache-zookeeper-3.6.3-bin/bin/zkCli.sh 2 3[zk: localhost:2181(CONNECTED) 1] ls / 4[admin, brokers, cluster, config, consumers, controller, controller_epoch, feature, isr_change_notification, latest_producer_id_block, log_dir_event_notification, zookeeper] 看看 / 下有东西就行了，上面是已经装好了 kafka 和 cruisecontrol的情形，东西就比较多了\n然后装 kafka，先修改配置文件\n1vi /usr/local/kafka_2.13-3.2.0/config/server.properties 2 3#需要修改的地方 4 5broker.id=0 6log.dirs=/data/kafka 7zookeeper.connect=172.18.31.50:2181,172.18.31.51:2181,172.18.31.52:2181 然后启动就可以了\n1/usr/local/kafka_2.13-3.2.0/bin/kafka-server-start.sh -daemon /usr/local/kafka_2.13-3.2.0/config/server.properties 验证一下：\n1# 新版的 kafka-topics.sh 命令已经变了 2 3cd /usr/local/kafka_2.13-3.2.0/bin 4 5# bootstrap-server 写一个就好，我们建立了一个 topic ，叫做 swimmingpool 6 7./kafka-topics.sh --create --bootstrap-server 172.18.31.50:9092 --replication-factor 3 --partitions 3 --topic swimmingpool 8 9# 看看细节 10 11./kafka-topics.sh --describe --bootstrap-server 172.18.31.50:9092 --topic swimmingpool 12Topic: swimmingpool TopicId: gOLXSh5NQPKMAwEroi_HcQ PartitionCount: 3 ReplicationFactor: 3 Configs: segment.bytes=1073741824 13 Topic: swimmingpool Partition: 0 Leader: 2 Replicas: 0,2,1 Isr: 2,1,0 14 Topic: swimmingpool Partition: 1 Leader: 2 Replicas: 2,1,0 Isr: 2,0,1 15 Topic: swimmingpool Partition: 2 Leader: 2 Replicas: 1,0,2 Isr: 2,0,1 16 17 18# 生产一些消息，ctrl+c 断出 19 20./kafka-console-producer.sh --bootstrap-server 172.18.31.50:9092 --topic swimmingpool 21 22\u0026gt;i am ok 23\u0026gt;i am fine 24\u0026gt;i am good 25 26# 消费这些消息，同样用 ctrl+c 断出，注意看，消息是倒序的 27 28./kafka-consoconsumer.sh --bootstrap-server 172.18.31.50:9092 --topic swimmingpool --from-beginning 29i am good 30i am fine 31i am ok 这样就装好了，但是安装只是第一步，重要的是 topic 的 reblance 以及 partion 的重新分布\n","date":"2022-11-13","img":"","permalink":"https://bajie.dev/posts/20221113-kafka/","series":null,"tags":null,"title":"Kafka的安装和验证"},{"categories":null,"content":"其实本身自己是比较喜欢 javascripts 的，但是 Python 也是必须掌握的一项技能。\n干 devops ansible 跟 python 也脱不了干系，所以准备用 django 开发一个自动上线的系统。\n先准备一下 Python 以及 Django 的环境好了。\n一、准备 Python 秘籍，不要用什么 venv 之类的东西，污染环境。直接下载源代码编译安装，然后把 py 封到自己的密闭是王道，最后引用一下 $PATH ，想用哪个就用哪个。\n注意以下的步骤，先装 gcc 的编译环境，然后装 openssl 的高版本，并且配置好 ldconfig，否则 py 的 ssl 会报错。\n最后下载 python 3.8.15 编译安装，生产环境，最好采用最新版本往后错两个版本。\n编译安装到 /export/servers/python3目录\n1yum -y install zlib-devel bzip2-devel openssl-devel ncurses-devel sqlite-devel readline-devel tk-devel gcc make libffi-devel 2 3wget https://www.openssl.org/source/openssl-1.1.1q.tar.gz 4./config --prefix=/export/servers/openssl 5 6vi /etc/ld.so.conf 7/export/servers/openssl/lib 8ldconfig -v 9 10wget https://www.python.org/ftp/python/3.8.15/Python-3.8.15.tgz 11./configure --prefix=/export/servers/python3 --with-openssl=/export/servers/openssl --with-ssl-default-suites=openssl 最后一步：\n1export $PATH=/export/servers/python3/bin:$PATH 这样就完成了 python 的安装\n二、配置Django 先升级一下pip到最新版本\n1/export/servers/python3/bin/python3.8 -m pip install --upgrade pip 2 3pip3 install django 4pip3 install pysqlite3 5pip3 install pysqlite3-binary 为什么要这么搞，因为 django 要求 sqlite 的版本比较高，无法升级，干脆换掉它改成 pysqlite3 就好了\n1vi /export/servers/python3/lib/python3.8/site-packages/django/db/backends/sqlite3/base.py 2 3 4#from sqlite3 import dbapi2 as Database 5from pysqlite3 import dbapi2 as Database 这样 django 的环境就搭好了\n","date":"2022-10-27","img":"","permalink":"https://bajie.dev/posts/20221027-python_django/","series":null,"tags":null,"title":"Python下Django环境的准备"},{"categories":null,"content":"Gitlab 和 Jenkins 一样，都是很流行的 CI/CD 工具，当然，本站之前推过国人自产的东西 onedev，那个也相当不错，但是用的人毕竟还是少，这回还是用大家都耳熟能详的东西。\n本篇就是 Giblab 在生产环境打包发布的一个全流程。\n解释一下上图：\n首先有两套Git，一套是程序员的Code仓库，另一套是运维的操作代码，里面是一些yaml啊，一些ansible脚本啊\n然后流程就是先取出程序员的Code，build出来jar，然后打成镜像推到仓库，然后再取出运维的代码，进行合并，生成k8s的yaml文件，最后推到 kubernetes 中去，这样整个 GitOPS 的流程就完备了\n在 gitlab 中非常简单，就是编辑 .gitlab-ci.yaml 文件\n1image: docker:19.03.12 2variables: 3 DOCKER_DRIVER: overlay2 4 DOCKER_TLS_CERTDIR: \u0026#34;\u0026#34; 5 MAVEN_OPTS: \u0026#34;-Dmaven.repo.local=.m2/repository\u0026#34; 6 TIMEZONE: \u0026#34;Asia/Shanghai\u0026#34; 7 http_proxy: \u0026#34;\u0026#34; 8 https_proxy: \u0026#34;\u0026#34; 9 no_proxy: \u0026#34;\u0026#34; 10 11cache: 12 paths: 13 - .m2/repository/ 14 - target/ 15 16stages: 17 - build 18 - push 19 - deploy 20 21 22Build: 23 stage: build 24 image: maven:3.5-jdk-8-alpine 25 before_script: 26 - export COMMIT_TIME=$(TZ=CST-8 date +%F-%H-%M) 27 - echo $COMMIT_TIME 28 - echo \u0026#34;COMMIT_TIME=$COMMIT_TIME\u0026#34; \u0026gt;\u0026gt; build.env 29 script: 30 - ./mvnw package 31 artifacts: 32 reports: 33 dotenv: build.env 34 tags: 35 - yunwei 36 37Push: 38 stage: push 39 before_script: 40 - docker info || true 41 - echo \u0026#34;$HARBOR_REGISTRY $HARBOR_USERNAME $HARBOR_PASSWORD\u0026#34; 42 - echo \u0026#34;echo Dingfangwen | docker login 172.18.31.28 -u dingfangwen --password-stdin\u0026#34; 43 - echo -n $HARBOR_PASSWORD | docker login $HARBOR_REGISTRY -u $HARBOR_USERNAME --password-stdin 44 script: 45 - docker pull $HARBOR_REGISTRY_IMAGE:latest || true 46 - \u0026gt; 47 docker build 48 --cache-from $HARBOR_REGISTRY_IMAGE:latest 49 --build-arg http_proxy=$http_proxy 50 --build-arg https_proxy=$https_proxy 51 --build-arg no_proxy=$no_proxy 52 --cache-from HARBOR_REGISTRY_IMAGE:latest 53 --tag $HARBOR_REGISTRY_IMAGE:$COMMIT_TIME 54 --tag $HARBOR_REGISTRY_IMAGE:latest 55 . 56 - docker push $HARBOR_REGISTRY_IMAGE:$COMMIT_TIME 57 - docker push $HARBOR_REGISTRY_IMAGE:latest 58 - docker logout $HARBOR_REGISTRY 59 dependencies: 60 - Build 61 tags: 62 - yunwei 63 64Deploy: 65 stage: deploy 66 cache: {} 67 image: cnych/kustomize:v1.0 68 script: 69 - echo $COMMIT_TIME 70 - git config --global user.email \u0026#34;gitlab@git.k8s.local\u0026#34; 71 - git config --global user.name \u0026#34;GitLab CI/CD\u0026#34; 72 - git clone git://172.18.31.50:30000/test-k8s.git 73 - cd test-k8s/prod 74 - kustomize edit set image $HARBOR_REGISTRY_IMAGE:$COMMIT_TIME 75 - cat kustomization.yaml 76 - git commit -am \u0026#39;PROD image update\u0026#39; 77 - git push origin master 78 dependencies: 79 - Build 80 tags: 81 - yunwei 82 only: 83 - master 84 when: manual 先不着急看这个文件，再普及一下 gitlab 的运行机制，它是通过 gitlab-runner 来执行cd的过程的。配置gitlab runner其实也是一项非常复杂的工作，里面可以配的东西太多了，不是本篇的范畴。我们这里简单来说，runner就是一个Docker容器。更准确的说，就是Docker in Docker。\n好，接下来一步步分析整个部署文件：\n1image: docker:19.03.12 2variables: 3 DOCKER_DRIVER: overlay2 4 DOCKER_TLS_CERTDIR: \u0026#34;\u0026#34; 5 MAVEN_OPTS: \u0026#34;-Dmaven.repo.local=.m2/repository\u0026#34; 6 TIMEZONE: \u0026#34;Asia/Shanghai\u0026#34; 7 http_proxy: \u0026#34;\u0026#34; 8 https_proxy: \u0026#34;\u0026#34; 9 no_proxy: \u0026#34;\u0026#34; 10 11cache: 12 paths: 13 - .m2/repository/ 14 - target/ 首先指定了 Build 过程使用的 Docker 容器的底版，是 docker:19.03.12，设置了 MAVEN_OPTS 的参数，这一点也非常重要，因为每次启动一个 Docker ，在容器里运行 mvn build 的时候，你不会想每次都重新下载所有的依赖包吧。\n下面的 cache 部分，设置了整个 build 过程中乃至以后，都保留中间产物。target 目录中存放的是 build 出来的 jar 包。\n继续，整个部署过程分三个步骤，1、build 出 jar 包，2、打包成镜像推到仓库去，3、最后发布到k8s去\n1stages: 2 - build 3 - push 4 - deploy 首先第一步，build jar包\n1Build: 2 stage: build 3 image: maven:3.5-jdk-8-alpine 4 before_script: 5 - export COMMIT_TIME=$(TZ=CST-8 date +%F-%H-%M) 6 - echo $COMMIT_TIME 7 - echo \u0026#34;COMMIT_TIME=$COMMIT_TIME\u0026#34; \u0026gt;\u0026gt; build.env 8 script: 9 - ./mvnw package 10 artifacts: 11 reports: 12 dotenv: build.env 13 tags: 14 - yunwei 很简单，其实就是又起了一个容器，底版是maven:3.5-jdk-8-alpine，在容器里执行了一条命令， mvnw package，这条命令如果成功执行，会在当前目录下新建一个 target 目录，生成一个jar包。\n上面比较莫名的地方是这个镜像吧，居然不支持 date +%Y-%m-%d 方式，所以八戒查了半天，用了一个巨别扭的命令生成了时间戳，然后呢把这个时间戳放到一个build.env文件里，然后通过 artifacts 产物，把这个文件以环境变量的方式传到随后的 docker 容器里，这样就可以在随后的容器里用环境变量引用这个一开始就生成的时间戳了。\n第二步，生成镜像并push到自己的harbor镜像仓库中去\n注意，程序员的源代码里是有 Dockerfile 这个文件的\n1Push: 2 stage: push 3 before_script: 4 - docker info || true 5 - echo \u0026#34;$HARBOR_REGISTRY $HARBOR_USERNAME $HARBOR_PASSWORD\u0026#34; 6 - echo \u0026#34;echo Dingfangwen | docker login 172.18.31.28 -u dingfangwen --password-stdin\u0026#34; 7 - echo -n $HARBOR_PASSWORD | docker login $HARBOR_REGISTRY -u $HARBOR_USERNAME --password-stdin 8 script: 9 - docker pull $HARBOR_REGISTRY_IMAGE:latest || true 10 - \u0026gt; 11 docker build 12 --cache-from $HARBOR_REGISTRY_IMAGE:latest 13 --build-arg http_proxy=$http_proxy 14 --build-arg https_proxy=$https_proxy 15 --build-arg no_proxy=$no_proxy 16 --cache-from HARBOR_REGISTRY_IMAGE:latest 17 --tag $HARBOR_REGISTRY_IMAGE:$COMMIT_TIME 18 --tag $HARBOR_REGISTRY_IMAGE:latest 19 . 20 - docker push $HARBOR_REGISTRY_IMAGE:$COMMIT_TIME 21 - docker push $HARBOR_REGISTRY_IMAGE:latest 22 - docker logout $HARBOR_REGISTRY 23 dependencies: 24 - Build 25 tags: 26 - yunwei 上面就是在docker容器里build一个镜像并推到我们的私有harbor仓库172.18.31.28去，这个仓库是需要登录验证的。\n然后 docker build 的时候又有一些技巧，docker 本身文件是分层的，\u0026ndash;cache-from 可以加快 build 的过程\n同样我们看到直接引用 $COMMIT_TIME 就可以引用第一步生成的时间戳，注意我们推了两遍，保留了历史时间戳版本，和最新版本。\n给出 Dockerfile 的内容：\n1FROM openjdk:8-jre-alpine 2 3ENV LANG C.UTF-8 4ENV LANGUAGE C.UTF-8 5ENV LC_ALL C.UTF-8 6 7EXPOSE 8080 8 9WORKDIR /apps 10 11COPY target/spring-petclinic-2.4.5.jar /apps/app.jar 12 13RUN echo \u0026#34;Asia/Shanghai\u0026#34; \u0026gt; /etc/timezone 14 15ENTRYPOINT [\u0026#34;java\u0026#34;,\u0026#34;-jar\u0026#34;,\u0026#34;/apps/app.jar\u0026#34;] 最后一步的发布过程：\n1Deploy: 2 stage: deploy 3 cache: {} 4 image: cnych/kustomize:v1.0 5 script: 6 - echo $COMMIT_TIME 7 - git config --global user.email \u0026#34;gitlab@git.k8s.local\u0026#34; 8 - git config --global user.name \u0026#34;GitLab CI/CD\u0026#34; 9 - git clone git://172.18.31.50:30000/test-k8s.git 10 - cd test-k8s/prod 11 - kustomize edit set image $HARBOR_REGISTRY_IMAGE:$COMMIT_TIME 12 - cat kustomization.yaml 13 - git commit -am \u0026#39;PROD image update\u0026#39; 14 - git push origin master 15 dependencies: 16 - Build 17 tags: 18 - yunwei 19 only: 20 - master 21 when: manual 这个就没什么说的了，用kustomize的底版，git下载yaml的模板文件，然后用 kustomize 定制化，最后push到git里面去，做好版本记录。然后kubectl apply -f *.yaml的步骤没有做，大家可以自己加上就没有问题了。\nover，这就是一个生产的实际例子。\n","date":"2022-10-25","img":"","permalink":"https://bajie.dev/posts/20221025-gitlab_cicd/","series":null,"tags":null,"title":"Gitlab的CICD实际生产环境应用"},{"categories":null,"content":"sed 和 awk 以及 cut 算是常用工具了，sed的高级用法也需要知道一下\nsed 里面有2个空间，一个是pattern space，一个是hold space，默认都是空的\n开始处理的时候，就从文件里一行一行读入 pattern space ，进行处理，hold space 只在你需要用到它的时候才会出现：\n1d ： 清空pattern space中的内容，立即开始下个循环(意思是跳过默认的输出pattern space内容的阶段？？？不知理解的对不对) 2h ： 用pattern space中的内容替换hold pattern中的内容 3H ： 在hold space中的内容后面追加一个换行，把pattern space中的内容追加到hold space中 4g ： 用hold space中的内容替换pattern space中的内容 5G ： 在pattern space中的内容后面追加一个换行，把hold space中的内容追加到pattern space中 6 7h, g会替换（可以理解为先清空，再复制）, H, G是追加。 hH是放过去，gG是拿过来，小写是替换，大写是追加\n分析一下经典的将文件内容反向打印\n1cat 1.txt 21 32 43 5 6cat 1.txt | sed -n \u0026#39;1!G;h;$p\u0026#39; 73 82 91 \u0026lsquo;1!G;h;$p\u0026rsquo; 分析一下，1!G就是说第一行不执行G，从第二行开始执行G；然后h是每行都执行，最后一行的时候执行p\n1input | pattern | hold | command | pattern | hold | command | pattern | hold 21 | 1 | 空行 | | 1 | 空行 | h | 1 | 1 32 | 2 | 1 | G | 21 | 1 | h | 21 | 21 43 | 3 | 21 | G | 321 | 21 | h | 321 | 321 因为之前的参数是 -n 不打印，直到最后一行，$p打印Pattern空间里的内容，这样就直接被反转打印出来了。\n。\n","date":"2022-10-24","img":"","permalink":"https://bajie.dev/posts/20221024-sed_advanced/","series":null,"tags":null,"title":"Sed的进阶用法"},{"categories":null,"content":"面试的时候经常会被问到 iptables 的问题，那么下面就运维角度来总结一下基本的用法。\n看晕了吧，不要紧，我们关注上面的5个红色，5个链条，然后继续看下面的表，详细解释了上面的流程：\n两张图结合起来看，意思就是整个 iptables 可以动手的地方太多了。\n我们完全没必要关注那么多的细节，常用的地方就2个：\n一、最后的出口nat postrouting 1iptables -t nat -A POSTROUTING -s 10.11.0.0/16 -j SNAT --to 172.16.8.1 上面是做 openvpn 时常用的，一定记得不要乱用 MASQUERADE，而要指定 特定网段（10.11.0.0/16） 从 特定IP（172.16.8.1） 出去。\n二、filter表 filter表是 iptables 缺省不带参数查看的表，用于过滤数据包，这是我们操作的最多的地方\n1iptables -I INPUT -s 185.207.178.236/32 -p tcp --dport 12530 -j ACCEPT 2iptables -A INPUT -p tcp --dport 12530 -j DROP 上面就只开放了一个IP 185.207.178.236可以访问本机的12530端口，其余的统统封掉，这是最常用的脚本了。\nOK，以上两点是最常用的。其余的端口转发之类的，最好中间用haproxy和nginx进行代理，否则查看 iptables 系统的架构就变得不清晰了。\n下面贴上常用的参数：\n1-p tcp/udp/icmp/all\t匹配协议，all会匹配所有协议 2-s addr[/mask]\t匹配源地址 3-d addr[/mask] 匹配目标地址\t4--sport\t匹配源端口（可指定连续的端口）如--sport80 5--dport\t匹配目的端口（可指定连续的端口）如--dport80 6-o interface\t匹配出口网卡，只适用于FORWARD、POSTROUTING、OUTPUT（例：iptables -A FORWARD -o eth0） 7-i interface\t匹配入口网卡，只适用于PREROUTING、INPUT、FORWARD 8--icmp-type\t匹配icmp类型（使用iptables -p icmp -h可查看可用的icmp类型） 9--tcp-flags mask comp\t匹配TCP标记，mask表示检查范围，comp表示匹配mask中的哪些标记（例：iptables -A FORWARD -p tcp --tcp-flags ALL SYN,ACK -j ACCEPT 表示匹配SYN和ACK标记的数据包） 10-j DROP/ACCEPT/REJECT/LOG\t拒绝/允许/拒绝并发出消息/在/var/log/messages中登记分组匹配的记录 11-m mac -mac\t绑定MAC地址 12-m limit -limit 1/s 1/m\t设置时间策略 13-s 192.168.1.153或192.168.1.0/24\t指定源地址或地址段 14-d 192.168.1.153或192.168.1.0/24\t指定目标地址或地址段 15-s ! 192.168.1.0\t指定源地址以外的 基本的2个用法足够满足日常运维的需要了。\n","date":"2022-10-24","img":"","permalink":"https://bajie.dev/posts/20221024-iptables/","series":null,"tags":null,"title":"Iptables的基本用法"},{"categories":null,"content":"时代已经进化到 systemd 的年代了，service 应该是彻底没有市场了\nsystemd 的好处是写程序的时候再也不用 fork 甩脱父进程了，日志直接输出终端即可\n对 java 来说也是个好事，所有的日志比如WARN ERROR INFO都可以交给journal来管理，这样要查找日志也非常方便了。\n举个例子，我们要把一个java启动的程序做成 systemd 的：\nvim /etc/systemd/system/circle.service\n1[Unit] 2After=network.target 3Wants=network.target 4 5[Service] 6WorkingDirectory=/export/prod/server 7Type=simple 8ExecStart=/usr/bin/java -jar -Dspring.config.location=application.properties -Dlog4j2.formatMsgNoLookups=true server.jar 9Restart=on-failure 10RestartSec=1s 11 12[Install] 13WantedBy=multi-user.target 然后就可以运行了：\n1systemctl daemon-reload 2systemctl start circle 注意上面的WorkingDirectory，因为下面java启动指定 application.properties 配置文件的时候没有用绝对路径，那么这里就要指定当前工作目录了。\n另外RestartSec=1s也改了，缺省是10ms，太快了\n当然这只是第一步，日志是归journal管了，journal还是需要进一步配置的\n首先必须要持久化存储到磁盘，否则只会在/run/log/journal内存中存放，重启就没了\n1#disk-usage查看的是: 内存+/var/log/journal的加起来的总和大小 2journalctl --disk-usage 然后我们需要修改配置，让它持久化\n1vi /etc/systemd/journald.conf 2Storage=persistent 修改重启\n1mkdir /var/log/journal 2systemd-tmpfiles --create --prefix /var/log/journal 3systemctl restart systemd-journald 最后刷一下，把内存的文件刷到磁盘中\n1journalctl --flush 还可以设置保存天数：\n1journalctl --vacuum-time=31d 查看是从那一天开始保存的，进目录查看时间戳即可\n1cd /var/log/journal 2ls -lha 最后就是一个秘籍了，如果把日志弄出来查看\n1journalctl -u circle --since \u0026#34;2022-10-19 14:30:00\u0026#34; --until \u0026#34;2022-10-19 15:00:00\u0026#34; 开发人员经常问我要日志，这样就特别方便快捷了，比从log4j的日志目录里拉方便的多。\n","date":"2022-10-19","img":"","permalink":"https://bajie.dev/posts/20220925-awk_usage/","series":null,"tags":null,"title":"Systemd与journalctl的双剑合璧"},{"categories":null,"content":"awk 是比较强力的工具，跟cut和sed组合起来，会有意想不到的作用\n举个简单例子，我们nginx的access.log如下：\n1110.244.232.88 - - [12/Apr/2022:16:50:06 +0800] help.jjwxc.net \u0026#34;GET /user/article/248 HTTP/1.1\u0026#34; 200 10616 \u0026#34;http://my.jjwxc.net/\u0026#34; 2220.244.232.88 - - [12/Apr/2022:16:50:06 +0800] help.jjwxc.net \u0026#34;GET /user/article/248 HTTP/1.1\u0026#34; 200 20616 \u0026#34;http://my.jjwxc.net/\u0026#34; 3330.244.232.88 - - [12/Apr/2022:16:50:06 +0800] help.jjwxc.net \u0026#34;GET /user/article/248 HTTP/1.1\u0026#34; 200 30616 \u0026#34;http://my.jjwxc.net/\u0026#34; 4410.244.232.88 - - [12/Apr/2022:16:50:06 +0800] help.jjwxc.net \u0026#34;GET /user/article/248 HTTP/1.1\u0026#34; 200 10616 \u0026#34;http://my.jjwxc.net/\u0026#34; 5530.244.232.88 - - [12/Apr/2022:16:50:06 +0800] help.jjwxc.net \u0026#34;GET /user/article/248 HTTP/1.1\u0026#34; 200 10616 \u0026#34;http://my.jjwxc.net/\u0026#34; 6650.244.232.88 - - [12/Apr/2022:16:50:06 +0800] help.jjwxc.net \u0026#34;GET /user/article/248 HTTP/1.1\u0026#34; 200 50616 \u0026#34;http://my.jjwxc.net/\u0026#34; 7760.244.232.88 - - [12/Apr/2022:16:50:06 +0800] help.jjwxc.net \u0026#34;GET /user/article/248 HTTP/1.1\u0026#34; 200 60616 \u0026#34;http://my.jjwxc.net/\u0026#34; 第一个字段是ip，然后倒数第3个是200，表示200 OK，后面的就是传输了多少数据两给客户端 我们要统计一下流量大的前5名是谁。\n思路如下： 我们需要造一个数组，然后数组的键值就是ip，然后值就是数据量，如果ip重复，那么就累加数据\n1cat access.log | awk -F\\\u0026#34; \u0026#39;{print $1 $3}\u0026#39;|cut -d\u0026#39; \u0026#39; -f 1,9|sort -nrk 2| awk \u0026#39;{ip[$1] +=$2}END{for (i in ip) {print i, ip[i]}}\u0026#39; | sed -n \u0026#39;1,5p\u0026#39; 最后结果：\n1160.244.232.88 60616 2220.244.232.88 20616 3330.244.232.88 41232 4450.244.232.88 50616 5510.244.232.88 21232 注意 10.244.232.88 和 30.244.232.88 后面的数据都是累加过的。\n","date":"2022-09-25","img":"","permalink":"https://bajie.dev/posts/20221019-systemctl/","series":null,"tags":null,"title":"Awk的用法"},{"categories":null,"content":"ansible的脚本中我们可能会要启动某项服务并等待，直到服务启动起来，然后再进行下一步\n这个非常重要，我们举例来实验一下：\n1- hosts: localhost 2 vars: 3 local__service: ssh 4 tasks: 5 - block: 6 - name: \u0026#34;Stop {{ local__service }} service\u0026#34; 7 systemd: 8 service: \u0026#34;{{ local__service }}\u0026#34; 9 state: stopped 10 11 - name: \u0026#34;Populate ansible_facts.services variable\u0026#34; 12 ansible.builtin.service_facts: 13 14 - name: \u0026#34;{{ local__service }} state will be stopped as expected\u0026#34; 15 assert: 16 that: 17 ansible_facts.services[local__service].state == \u0026#39;stopped\u0026#39; 18 19 - name: \u0026#34;Start {{ local__service }} service\u0026#34; 20 systemd: 21 service: \u0026#34;{{ local__service }}\u0026#34; 22 state: started 23 24 - name: \u0026#34;Registered {{ local__service }} state will still be stopped as it was not refreshed\u0026#34; 25 assert: 26 that: 27 ansible_facts.services[local__service].state == \u0026#39;stopped\u0026#39; 28 29 - name: \u0026#34;Refresh ansible_facts.services variable\u0026#34; 30 ansible.builtin.service_facts: 31 32 - name: \u0026#34;{{ local__service }} state will be running as expected\u0026#34; 33 assert: 34 that: 35 ansible_facts.services[local__service].state == \u0026#39;running\u0026#39; 上面的 playbook 一共有7步：\n1、停止 ssh 服务\n2、获取 ansible_fact 变量\n3、断言 ansible_facts 中的 ssh 服务是 stopped 状态\n4、重新启动 ssh 服务\n5、断言 ansible_facts 中的 ssh 服务仍然是 stopped 状态\n6、重新获取 ansible_fact 变量\n7、这时 ansible_facts 中的 ssh 服务状态刷新了，变成了 running\n就是服务启动后，必须去不断刷新 ansible_facts，才能获得刷新后的状态，所以我们利用这一点，脚本如下：\n1- hosts: localhost 2 vars: 3 local__service: ssh 4 tasks: 5 - block: 6 - name: \u0026#34;Stop {{ local__service }} service\u0026#34; 7 systemd: 8 service: \u0026#34;{{ local__service }}\u0026#34; 9 state: stopped 10 11 - name: \u0026#34;Wait until {{ local__service }} service is stopped\u0026#34; 12 ansible.builtin.service_facts: 13 register: temp__service_facts 14 until: temp__service_facts.ansible_facts.services[local__service].state == \u0026#39;stopped\u0026#39; 15 retries: 20 16 delay: 2 17 18 - name: \u0026#34;Start {{ local__service }} service\u0026#34; 19 systemd: 20 service: \u0026#34;{{ local__service }}\u0026#34; 21 state: started 22 23 - name: \u0026#34;Wait until {{ local__service }} service is running\u0026#34; 24 ansible.builtin.service_facts: 25 register: temp__service_facts 26 until: temp__service_facts.ansible_facts.services[local__service].state == \u0026#39;running\u0026#39; 27 retries: 20 28 delay: 2 上面wait until的步骤就是，注册一个临时的变量，然后不断去测试，重试20次，每次的延时是2秒。这样就可以解决了。\n","date":"2022-07-29","img":"","permalink":"https://bajie.dev/posts/20220729-ansible_until/","series":null,"tags":null,"title":"Ansible之等待服务状态变成成功"},{"categories":null,"content":"在生产系统中使用LVM务必要注意lvm metadata的备份，曾经在京东的时候发生过虚机用了lvm的磁盘系统，然后虚机文件qcow2突然坏掉了，想尽了办法也无法恢复，这给我们拉响了警钟啊，使用lvm的时候务必要备份分区信息，否则坏的时候就欲哭无泪了。\n缺省在 /etc/lvm/backup/ 目录下是最新的备份，同样历史版本都在 /etc/lvm/archive/ 目录下。\n一、先备份 在开始恢复之前，一定先做个备份\n1cp -pr /etc/lvm /etc/lvm_bkp 然后去看看archive下的备份信息\n1ls -l /etc/lvm/archive/vg_storage_00* 2-rw-------. 1 root root 13722 Oct 28 23:45 /etc/lvm/archive/vg_storage_00419-1760023262.vg 3-rw-------. 1 root root 14571 Oct 28 23:52 /etc/lvm/archive/vg_storage_00420-94024216.vg 4... 5-rw-------. 1 root root 14749 Nov 23 15:11 /etc/lvm/archive/vg_storage_00676-394223172.vg 6-rw-------. 1 root root 14733 Nov 23 15:29 /etc/lvm/archive/vg_storage_00677-187019982.vg 7# 二、最坏的情形恢复pv 警告：这一步只能在VG无法正常运行的时候再运行！！！\n先说最坏的情形，LVM的建立路径是 pv \u0026mdash;\u0026gt; vg \u0026mdash;\u0026gt; lv，假设连 pv（物理卷都没了）\n我们选择最近的备份，/etc/lvm/archive/vg_storage_00677-187019982.vg\n1less /etc/lvm/archive/vg_storage_00677-187019982.vg 2... 3 physical_volumes { 4 pv0 { 5 id = \u0026#34;BgR0KJ-JClh-T2gS-k6yK-9RGn-B8Ls-LYPQP0\u0026#34; 6... 从里面拿到 pv 的 id ，重建 pv\n1pvcreate --uuid \u0026#34;BgR0KJ-JClh-T2gS-k6yK-9RGn-B8Ls-LYPQP0\u0026#34; \\ 2 --restorefile /etc/lvm/archive/vg_storage_00677-187019982.vg 三、恢复VG 如果 vg 是正常的，那么就不用做第二步了，直接从备份中恢复即可，先查看一下\n1vgcfgrestore --list vg1 2 3File: /etc/lvm/archive/vg1_00000-1238318622.vg 4VG name: vg1 5Description: Created *before* executing \u0026#39;vgcreate vg1 /dev/sda6\u0026#39; 6Backup Time: Mon Feb 29 10:58:51 2016 7 8File: /etc/lvm/archive/vg1_00001-285796155.vg 9VG name: vg1 10Description: Created *before* executing \u0026#39;lvcreate -L 1G -n lv2 vg1\u0026#39; 11Backup Time: Mon Feb 29 10:59:23 2016 12 13File: /etc/lvm/archive/vg1_00002-1661997476.vg ---\u0026gt; just before removal of volume (this is the archive we need) 14VG name: vg1 15Description: Created *before* executing \u0026#39;lvremove /dev/vg1/lv2\u0026#39; 16Backup Time: Mon Feb 29 13:55:08 2016 17 18File: /etc/lvm/backup/vg1 19VG name: vg1 20Description: Created *after* executing \u0026#39;lvremove /dev/vg1/lv2\u0026#39; 21Backup Time: Mon Feb 29 13:55:08 2016 有信息，我们先加 \u0026ndash;test 测试一下\n1vgcfgrestore vg01 --test -f /etc/lvm/archive/vg_data_00003-586203914.vg 2 3 TEST MODE: Metadata will NOT be updated and volumes will not be (de)activated. 4 Volume group vg01 has active volume: lv001. 5 WARNING: Found 1 active volume(s) in volume group \u0026#34;vg01\u0026#34;. 6 Restoring VG with active LVs, may cause mismatch with its metadata. 7 Do you really want to proceed with restore of volume group \u0026#34;vg01\u0026#34;, while 1 volume(s) are active? [y/n]: y 8 Restored volume group vg01. 没问题，那就继续，真的操作了\n1vgcfgrestore vg01 -f /etc/lvm/archive/vg_data_00003-586203914.vg 2 3 Volume group vg01 has active volume: lv001. 4 WARNING: Found 1 active volume(s) in volume group \u0026#34;vg01\u0026#34;. 5 Restoring VG with active LVs, may cause mismatch with its metadata. 6 Do you really want to proceed with restore of volume group \u0026#34;vg01\u0026#34;, while 1 volume(s) are active? [y/n]: y 7 Restored volume group vg01. 四、恢复后进行校验 1# 显示信息 2vgdisplay VG1 3 4# 激活 5vgchange -ay VG1 6 7# 显示逻辑卷 8lvs -a -o +devices 9 10# 重新扫描 11lvscan 12inactive \u0026#39;/dev/vg1/lv2\u0026#39; [1.00 GiB] inherit ### its in inactive state and make it active to use. 13ACTIVE \u0026#39;/dev/vg0/lv1\u0026#39; [1.00 GiB] inherit 14 15# 上面如果有inactive的，重新激活 16lvchange -a y /dev/vg1/lv2 17 18# 再显示一次 19lvs -a -o +devices 20 21# 再扫描 22lvscan 23ACTIVE \u0026#39;/dev/vg1/lv2\u0026#39; [1.00 GiB] inherit 24ACTIVE \u0026#39;/dev/vg0/lv1\u0026#39; [1.00 GiB] inherit 25 26# mount上测试 27mount /dev/vg1/lv2 /lv2 28 29# 看看东西都在不在 30ls -lh /lv2 五、备份和恢复的脚本 备份一个 VG 下所有卷，每个卷一个 snapshot 快照备份文件，backup_snapshot_lvm.pl\n1#!/usr/bin/perl -w 2# 3# Run through a particular LVM volume group and perform a snapshot 4# and compressed file backup of each volume. 5# 6# This script is intended for use with backing up complete system 7# images of VMs, in addition to data level backups. 8# 9 10my $source_lvm_volgroup = \u0026#39;vg_storage\u0026#39;; 11my $source_lvm_snapsize = \u0026#39;5G\u0026#39;; 12my @source_lvm_excludes = (\u0026#39;lv_unwanted\u0026#39;, \u0026#39;lv_tmpfiles\u0026#39;); 13 14my $dest_dir=\u0026#39;/mnt/backup/snapshots\u0026#39;; 15 16foreach $volume (glob(\u0026#34;/dev/$source_lvm_volgroup/*\u0026#34;)) 17{ 18 $volume =~ /\\/dev\\/$source_lvm_volgroup\\/(\\S*)$/; 19 my $volume_short = $1; 20 21 if (\u0026#34;$volume_short\u0026#34; ~~ @source_lvm_excludes) 22 { 23 # Excluded volume, we skip it 24 print \u0026#34;[info] Skipping excluded volume $volume_short ($volume)\\n\u0026#34;; 25 next; 26 } 27 28 print \u0026#34;[info] Processing volume $volume_short ($volume)\\n\u0026#34;; 29 30 # Snapshot volume 31 print \u0026#34;[info] Creating snapshot...\\n\u0026#34;; 32 system(\u0026#34;lvcreate -n ${volume_short}_snapshot --snapshot $volume -L $source_lvm_snapsize\u0026#34;); 33 34 # Write compressed backup file from snapshot, but only replace existing one once finished 35 print \u0026#34;[info] Creating compressed snapshot file...\\n\u0026#34;; 36 system(\u0026#34;dd if=${volume}_snapshot | gzip --fast \u0026gt; $dest_dir/$volume_short.temp.gz\u0026#34;); 37 system(\u0026#34;mv $dest_dir/$volume_short.temp.gz $dest_dir/$volume_short.gz\u0026#34;); 38 39 # Delete snapshot 40 print \u0026#34;[info] Removing snapshot...\\n\u0026#34;; 41 system(\u0026#34;lvremove --force ${volume}_snapshot\u0026#34;); 42 43 print \u0026#34;[info] Volume $volume_short backup completed.\\n\u0026#34;; 44} 45 46print \u0026#34;[info] FINISHED! VolumeGroup backup completed\\n\u0026#34;; 47 48exit 0; 恢复的脚本，restore_snapshot_lvm.pl：\n1#!/usr/bin/perl -w 2# 3# Runs through LVM snapshots taken by backup_snapshot_lvm.pl and 4# restores them to the LVM volume in question. 5# 6 7my $dest_lvm_volgroup = \u0026#39;vg_storage\u0026#39;; 8my $source_dir = \u0026#39;/mnt/backup/snapshots\u0026#39;; 9 10print \u0026#34;[WARNING] Beginning restore process in 5 SECONDS!!\\n\u0026#34;; 11sleep(5); 12 13foreach $volume (glob(\u0026#34;$source_dir/*\u0026#34;)) 14{ 15 $volume =~ /$source_dir\\/(\\S*).gz$/; 16 my $volume_short = $1; 17 18 print \u0026#34;[info] Processing volume $volume_short ($volume)\\n\u0026#34;; 19 20 # Just need to decompress \u0026amp; write into LVM volume 21 system(\u0026#34;zcat $source_dir/$volume_short.gz \u0026gt; /dev/$dest_lvm_volgroup/$volume_short\u0026#34;); 22 23 print \u0026#34;[info] Volume $volume_short restore completed.\\n\u0026#34;; 24} 25 26print \u0026#34;[info] FINISHED! VolumeGroup restore completed\\n\u0026#34;; 27 28exit 0; 当然要谨记，这个是块设备级别的备份，最好还是要有另外的数据的备份。\n","date":"2022-07-28","img":"","permalink":"https://bajie.dev/posts/20220728-lvm_restore/","series":null,"tags":null,"title":"LVM系统的Restore"},{"categories":null,"content":"现在版本的 dmesg -T 都是带时间戳的。\n但是老的机器，很有可能是没有 -T 这个参数的，直接 dmesg 是这样的：\n这个时间戳是天书啊，这还算好的，好歹有。还有更差的，连戳子都没有：\n戳子都没有的，需要做以下步骤来加上，机器重启后必须再执行一遍：\n1echo 1 \u0026gt; /sys/module/printk/parameters/printk_time 同时写个脚本，/usr/local/bin/dmesgt.sh\n1#!/bin/bash 2# Translate dmesg timestamps to human readable format 3 4# desired date format 5date_format=\u0026#34;%a %b %d %T %Y\u0026#34; 6 7# uptime in seconds 8uptime=$(cut -d \u0026#34; \u0026#34; -f 1 /proc/uptime) 9 10# run only if timestamps are enabled 11if [ \u0026#34;Y\u0026#34; = \u0026#34;$(cat /sys/module/printk/parameters/time)\u0026#34; ]; then 12 dmesg | sed \u0026#34;s/^\\[[ ]*\\?\\([0-9.]*\\)\\] \\(.*\\)/\\\\1 \\\\2/\u0026#34; | while read timestamp message; do 13 printf \u0026#34;[%s] %s\\n\u0026#34; \u0026#34;$(date --date \u0026#34;now - $uptime seconds + $timestamp seconds\u0026#34; +\u0026#34;${date_format}\u0026#34;)\u0026#34; \u0026#34;$message\u0026#34; 14 done 15else 16 echo \u0026#34;Timestamps are disabled (/sys/module/printk/parameters/time)\u0026#34; 17fi 这样就可以了，最后我们测试一下：\n1echo \u0026#34;Enabled timestamps\u0026#34; | tee /dev/kmsg 看到带时间戳的信息就可以了：\n1$ dmesg 2[...] 3Bluetooth: RFCOMM TTY layer initialized 4Bluetooth: RFCOMM socket layer initialized 5Bluetooth: RFCOMM ver 1.11 6[...] 7[271309.434405] Enabled timestamps 8[...] ","date":"2022-07-25","img":"","permalink":"https://bajie.dev/posts/20220725-dmesg_time/","series":null,"tags":null,"title":"Dmesg -T 无时间戳的解决方法"},{"categories":null,"content":"接到同事的通知，说mongodb的cpu很高，下不去，看有什么办法。\n看了一下图，cpu都99.4%了，确实很高：\n登上机器，首先去看mongod的log，都试如下的查询，都很慢：\ncopy出一条来，备用\n12022-07-17T21:30:42.556+0800 I COMMAND [conn196670] command beyou.behaviorAnalysis command: find { find: \u0026#34;behaviorAnalysis\u0026#34;, filter: { behavior: { $regex: \u0026#34;.*互动操作.*\u0026#34;, $options: \u0026#34;i\u0026#34; }, log: { $regex: \u0026#34;.*\\Q观看：《罗马的房子》,\\E.*\u0026#34;, $options: \u0026#34;i\u0026#34; }, userId: 7345, _class: { $in: [ \u0026#34;com.linkyee.api.domain.po.BehaviorAnalysis\u0026#34; ] } }, $db: \u0026#34;beyou\u0026#34; } planSummary: IXSCAN { _class: 1, behavior: 1, log: 1, userId: 1 } keysExamined:111564 docsExamined:2 cursorExhausted:1 numYields:890 nreturned:2 reslen:571 locks:{ Global: { acquireCount: { r: 891 } }, Database: { acquireCount: { r: 891 } }, Collection: { acquireCount: { r: 891 } } } storage:{} protocol:op_msg 2268ms 发现确实很慢啊，op_msg 居然要2268ms，但是mongodb的数据目录总共才6G多，不客观啊。\n看看表结构和索引：\n1\u0026gt; show dbs; 2admin 0.000GB 3beyou 0.063GB 4config 0.000GB 5local 0.000GB 6 7\u0026gt; use beyou; 8switched to db beyou 9 10\u0026gt; show collections; 11activityLog 12behaviorAnalysis 13 14\u0026gt; db.behaviorAnalysis.find({},{_id:0}).limit(1); 15{ \u0026#34;name\u0026#34; : \u0026#34;812304163055734784\u0026#34;, \u0026#34;createTime\u0026#34; : ISODate(\u0026#34;2022-02-19T12:46:44.360Z\u0026#34;), \u0026#34;behavior\u0026#34; : \u0026#34;互动操作\u0026#34;, \u0026#34;log\u0026#34; : \u0026#34;加入看单：《爱的迫降》\u0026#34;, \u0026#34;userId\u0026#34; : 2, \u0026#34;_class\u0026#34; : \u0026#34;com.linkyee.api.domain.po.BehaviorAnalysis\u0026#34; } 16 17\u0026gt; db.behaviorAnalysis.getIndexes(); 18[ 19 { 20 \u0026#34;v\u0026#34; : 2, 21 \u0026#34;key\u0026#34; : { 22 \u0026#34;_id\u0026#34; : 1 23 }, 24 \u0026#34;name\u0026#34; : \u0026#34;_id_\u0026#34;, 25 \u0026#34;ns\u0026#34; : \u0026#34;beyou.behaviorAnalysis\u0026#34; 26 }, 27 { 28 \u0026#34;v\u0026#34; : 2, 29 \u0026#34;key\u0026#34; : { 30 \u0026#34;_class\u0026#34; : 1, 31 \u0026#34;behavior\u0026#34; : 1, 32 \u0026#34;log\u0026#34; : 1, 33 \u0026#34;userId\u0026#34; : 1 34 }, 35 \u0026#34;name\u0026#34; : \u0026#34;_class_1_behavior_1_log_1_userId_1\u0026#34;, 36 \u0026#34;ns\u0026#34; : \u0026#34;beyou.behaviorAnalysis\u0026#34; 37 } 38] 发现表结构不复杂，索引呢有两个，一个缺省的单字段索引_id，另一个是多字段索引，4合一的（ _class、behavior、log、userId ）\n继续，来看看查询：\n1find { 2 find: \u0026#34;behaviorAnalysis\u0026#34;, 3 filter: { 4 behavior: { $regex: \u0026#34;.*互动操作.*\u0026#34;, $options: \u0026#34;i\u0026#34; }, 5 log: { $regex: \u0026#34;.*\\Q观看：《罗马的房子》,\\E.*\u0026#34;, $options: \u0026#34;i\u0026#34; }, 6 userId: 7345, 7 _class: { $in: [ \u0026#34;com.linkyee.api.domain.po.BehaviorAnalysis\u0026#34; ] } 8 }, 9 $db: \u0026#34;beyou\u0026#34; } 10 planSummary: IXSCAN { _class: 1, behavior: 1, log: 1, userId: 1 } keysExamined:111564 planSummary里，IXSCAN是查了索引，但是keyExamined居然有111564个，太多了吧。\n再看看索引，明显4合一的效率有问题。\n那就简单了，删掉这个4合一的索引，新建一个userId的索引，既然叫userId，那推测它是唯一的，重建索引需要后台执行，并且在业务不繁忙的时候再跑\n1db.behaviorAnalysis.createIndex({\u0026#34;userId\u0026#34;:1},{\u0026#34;name\u0026#34;:\u0026#39;idx_userId\u0026#39;,background:true,unique:true}); 同事执行的时候报错：\n居然key重复了，那就把 unique:true 去掉了再执行，就成功了。\n然后cpu就立竿见影，从100%降低到了2.8%\nover，如果再遇到瓶颈，估计就只能采用分片大法了。\n","date":"2022-07-21","img":"","permalink":"https://bajie.dev/posts/20220721-mongodb_cpu/","series":null,"tags":null,"title":"一次mongodb Cpu很high的解决方法"},{"categories":null,"content":"先普及一下概念，Infrastructure as Code，也就是从代码开始定义整个网络环境、虚机、各种资源等等。\n简单说就是在云上用代码来管理一切，无论是vpc、subnetwork、lb、snat、sg、ec2\u0026hellip;\u0026hellip;\n非常符合我的胃口，因为就连架构图，都是用 graphviz 来画的。\n那么 Infrastructure as Code （IAC） 可以分为以下五个部分：\nAd hoc scripts Configuration Management tools Orchestration tools Provisioning tools Server Templating tools 一、Ad hoc scripts 就是用软件对目的主机进行 point to point 操作，用shell或者ansible都可以。推荐ansible。\n在 infosys 面试被一个印度老外问到这问题，因为平时根本不用ansible 的 ad hoc 点对点模式，结果被当场问住。现在才知道这玩意是什么。当然，用 ansible 的话不建议用这个，因为 playbook 是可追溯的。\n二、Configuration Management tools 配置管理，这里当然推荐 ansible，每一步的操作都可以有 inventory 和 playbook 可以追溯。\n三、Orchestration tools 协同工具，k8s和kvm\n四、Provisioning tools 生产工具，当然是terraform，另外，阿里云是支持plumi的，华为腾讯不支持。\n五、Server Templating tools 模板工具，这里就是 Packer，其实我们公司现在的模板工具，是八戒从openstack学的，改动Cloud-init的东西。\nPacker更标准一下，是进化版的东西，它既可以打kvm镜像，也可以打Docker镜像。\n下面我们就看看怎么使用吧，这里先说kvm，因为kvm的比较难，docker的八戒现在还是用Dockerfile，有空了再研究packer：\n安装就不多说了，就一个执行文件，下载下来就行，不用装。\nPacker的核心是三个部分\nbuilders provisioners post-processors 我们先建立一个空目录，名字随便，就叫 test-image\n1mkdir test-images 2cd test-image 然后在目录下面，建立三个文件：\npacker.json, variable.json, setup.sh\n首先看variable.json，对应AWS长这样\n1{ 2 \u0026#34;description\u0026#34;: \u0026#34;test image\u0026#34;, 3 \u0026#34;access_key\u0026#34;: \u0026#34;enter-aws-your-key\u0026#34;, 4 \u0026#34;secret_key\u0026#34;: \u0026#34;enter-aws-your-secret\u0026#34; 5 \u0026#34;source_ami\u0026#34;: \u0026#34;enter-yours\u0026#34; 6 } 对应腾讯云就是这样\n1{ 2 \u0026#34;description\u0026#34;: \u0026#34;test image\u0026#34;, 3 \u0026#34;tc_secret_id\u0026#34;: \u0026#34;TENCENTCLOUD_ACCESS_KEY\u0026#34;, 4 \u0026#34;tc_secret_key\u0026#34;: \u0026#34;TENCENTCLOUD_SECRET_KEY\u0026#34;, 5 \u0026#34;source_tc\u0026#34;: \u0026#34;enter-yours\u0026#34; 6} 稍微解释一下，无论哪家云，你都需要去申请secret的key，才可以用，然后就是source_ami和source_tc了，这个指镜像的母版\naws的长这样：\n腾讯的长这样：\nok，变量都定义好了。\n下面是packer.json的正文\naws的这样：\n1{ 2\t\u0026#34;builders\u0026#34;: [ 3 { 4 \u0026#34;type\u0026#34;: \u0026#34;amazon-ebs\u0026#34;, 5 \u0026#34;access_key\u0026#34;: \u0026#34;{{user `access_key` }}\u0026#34;, 6 \u0026#34;secret_key\u0026#34;: \u0026#34;{{user `secret_key` }}\u0026#34;, 7 \u0026#34;region\u0026#34; : \u0026#34;us-east-1\u0026#34;, 8 \u0026#34;ami_name\u0026#34; : \u0026#34;myfirstami\u0026#34;, 9 \u0026#34;source_ami\u0026#34; : \u0026#34;{{user `source_ami` }}\u0026#34;, 10 \u0026#34;instance_type\u0026#34; : \u0026#34;t2.micro\u0026#34;, 11 \u0026#34;ssh_username\u0026#34; : \u0026#34;ec2-user\u0026#34; 12 } 13 ], 14 \u0026#34;provisioners\u0026#34;: [ 15 { 16 \u0026#34;type\u0026#34;: \u0026#34;shell\u0026#34;, 17 \u0026#34;script\u0026#34;: \u0026#34;setup.sh\u0026#34; 18 } 19 ], 20\t\u0026#34;post-processors\u0026#34;: [ 21 { 22 \u0026#34;type\u0026#34;: \u0026#34;manifest\u0026#34;, 23 \u0026#34;output\u0026#34;: \u0026#34;out.json\u0026#34; 24 } 25 ] 26} 看见了吧，核心三部分。那么换成腾讯，就长这样\n1{ 2 \u0026#34;builders\u0026#34;: [ 3 { 4 \u0026#34;type\u0026#34;: \u0026#34;tencentcloud-cvm\u0026#34;, 5 \u0026#34;secret_id\u0026#34;: \u0026#34;{{user `tc_secret_id`}}\u0026#34;, 6 \u0026#34;secret_key\u0026#34;: \u0026#34;{{user `tc_secret_key`}}\u0026#34;, 7 \u0026#34;region\u0026#34;: \u0026#34;ap-guangzhou\u0026#34;, 8 \u0026#34;zone\u0026#34;: \u0026#34;ap-guangzhou-3\u0026#34;, 9 \u0026#34;instance_type\u0026#34;: \u0026#34;S2.SMALL1\u0026#34;, 10 \u0026#34;disk_type\u0026#34;: \u0026#34;CLOUD_PREMIUM\u0026#34;, 11 \u0026#34;associate_public_ip_address\u0026#34;: true, 12 \u0026#34;image_name\u0026#34;: \u0026#34;myfirsttc\u0026#34;, 13 \u0026#34;source_image_id\u0026#34;: \u0026#34;{{user `source_tc` }}\u0026#34;, 14 \u0026#34;ssh_username\u0026#34; : \u0026#34;root\u0026#34; 15 } 16 ], 17 \u0026#34;provisioners\u0026#34;: [ 18 { 19 \u0026#34;type\u0026#34;: \u0026#34;shell\u0026#34;, 20 \u0026#34;script\u0026#34;: \u0026#34;setup.sh\u0026#34; 21 } 22 ], 23\t\u0026#34;post-processors\u0026#34;: [ 24 { 25 \u0026#34;type\u0026#34;: \u0026#34;manifest\u0026#34;, 26 \u0026#34;output\u0026#34;: \u0026#34;out.json\u0026#34; 27 } 28 ] 29} 大差不差吧。\n详细参数可以去看：https://www.packer.io/plugins/builders/tencentcloud\n那最后就是setup.sh了，举例装个jenkins好了，其他的可以根据需要进行注入：\n1sleep 30 2sudo yum update –y 3sudo wget -O /etc/yum.repos.d/jenkins.repo \\ 4 https://pkg.jenkins.io/redhat-stable/jenkins.repo 5sudo rpm --import https://pkg.jenkins.io/redhat-stable/jenkins.io.key 6sudo yum upgrade 7sudo yum instqll -y epel-release 8sudo yum install java-openjdk11 -y 9sudo yum install jenkins -y 10sudo systemctl enable jenkins 11sudo systemctl start jenkins 12sudo systemctl status jenkins 最后run一下：\n1packer build -var-file=\u0026#34;variable.json\u0026#34; packer.json 叽哩咕噜一顿，就build好了\nOver，这个工具在IAS中是不可缺少的一环。\n","date":"2022-07-17","img":"","permalink":"https://bajie.dev/posts/20220717-terraform/","series":null,"tags":null,"title":"Infrastructure as Code中packer的使用"},{"categories":null,"content":"在日常工作中我们经常要对 Cisco 的网络设备的配置进行备份，或者和 suricata 联动的时候要执行操作。\n方法其实很简单，调用 python 的相应模块即可。\n准备工作如下：\n首选需要在/export/servers/python363装好 python 3.6, pip install netmiko\n其次，在路由器上可以配置en的密码\n然后依次执行备份就可以了。\n1#!/export/servers/python363/bin/python3.6 2from netmiko import Netmiko 3import time 4tw_bgp = { 5 \u0026#34;device_type\u0026#34;: \u0026#34;cisco_ios\u0026#34;, 6 \u0026#34;host\u0026#34;: \u0026#34;tw-bgp\u0026#34;, 7 \u0026#34;ip\u0026#34;: \u0026#34;192.168.1.10\u0026#34;, 8 \u0026#34;username\u0026#34;: \u0026#34;noc\u0026#34;, 9 \u0026#34;use_keys\u0026#34;: True, 10 \u0026#34;secret\u0026#34; : \u0026#34;xxxxxxxx\u0026#34;, 11 \u0026#34;key_file\u0026#34;: \u0026#34;/root/.ssh/id_jump_rsa_new\u0026#34;, 12} 13tw_r1_e1 = { 14 \u0026#34;device_type\u0026#34;: \u0026#34;cisco_ios\u0026#34;, 15 \u0026#34;host\u0026#34;: \u0026#34;tw-r1-e1\u0026#34;, 16 \u0026#34;ip\u0026#34;: \u0026#34;192.168.1.11\u0026#34;, 17 \u0026#34;username\u0026#34;: \u0026#34;noc\u0026#34;, 18 \u0026#34;use_keys\u0026#34;: True, 19 \u0026#34;key_file\u0026#34;: \u0026#34;/root/.ssh/id_jump_rsa_new\u0026#34;, 20} 21tw_r1_e2 = { 22 \u0026#34;device_type\u0026#34;: \u0026#34;cisco_ios\u0026#34;, 23 \u0026#34;host\u0026#34;: \u0026#34;tw-r1-e2\u0026#34;, 24 \u0026#34;ip\u0026#34;: \u0026#34;192.168.1.12\u0026#34;, 25 \u0026#34;username\u0026#34;: \u0026#34;noc\u0026#34;, 26 \u0026#34;use_keys\u0026#34;: True, 27 \u0026#34;key_file\u0026#34;: \u0026#34;/root/.ssh/id_jump_rsa_new\u0026#34;, 28} 29tw_r2_e1 = { 30 \u0026#34;device_type\u0026#34;: \u0026#34;cisco_ios\u0026#34;, 31 \u0026#34;host\u0026#34;: \u0026#34;tw-r2-e1\u0026#34;, 32 \u0026#34;ip\u0026#34;: \u0026#34;192.168.1.13\u0026#34;, 33 \u0026#34;username\u0026#34;: \u0026#34;noc\u0026#34;, 34 \u0026#34;use_keys\u0026#34;: True, 35 \u0026#34;key_file\u0026#34;: \u0026#34;/root/.ssh/id_jump_rsa_new\u0026#34;, 36} 37tw_r2_e2 = { 38 \u0026#34;device_type\u0026#34;: \u0026#34;cisco_ios\u0026#34;, 39 \u0026#34;host\u0026#34;: \u0026#34;tw-r2-e2\u0026#34;, 40 \u0026#34;ip\u0026#34;: \u0026#34;192.168.1.14\u0026#34;, 41 \u0026#34;username\u0026#34;: \u0026#34;noc\u0026#34;, 42 \u0026#34;use_keys\u0026#34;: True, 43 \u0026#34;key_file\u0026#34;: \u0026#34;/root/.ssh/id_jump_rsa_new\u0026#34;, 44} 45 46devices=[tw_bgp, tw_r1_e1, tw_r1_e2, tw_r2_e1, tw_r2_e2] 47 48for dev in devices: 49 name = dev[\u0026#34;ip\u0026#34;] 50 connection = Netmiko(**dev) 51 connection.enable() 52 out = connection.send_command(\u0026#34;show running-config\u0026#34;) 53 calender = time.strftime(\u0026#34;%Y%m%d\u0026#34;) 54 file_name = \u0026#39;{}-{}.txt\u0026#39;.format(dev[\u0026#34;host\u0026#34;],calender) 55 file = open(file_name ,\u0026#34;w\u0026#34;) 56 file.write(out) 57 file.close() 58 connection.disconnect() 59 print(\u0026#34;BACKUP for %s done\u0026#34; %dev[\u0026#34;host\u0026#34;]) ","date":"2022-07-17","img":"","permalink":"https://bajie.dev/posts/20220717-backup_cisco/","series":null,"tags":null,"title":"Cisco设备自动执行和备份的脚本"},{"categories":null,"content":"NFS Server 很常用，但是坑也是巨大的。之前在京东运维 Hadoop 集群的时候碰到过脑裂，原因就是 NFS 引起的。\n关键 NFS 是内核态的，一旦崩溃，那么客户端的所有命令，ls、df、du等等，统统无反应，结果是很悲剧的。\n下面要推荐一下 nfs-ganesha ，它是用户态的nfs-server，支持v3和v4，而nfs缺省是内核态的v3。\n强烈推荐大家用这个，而不是用内核态的，并且 nfs-ganesha 还是支持 glusterFS 的。\n1yum install -y centos-release-nfs-ganesha30.noarch 2 3vi /etc/ganesha/ganesha.conf 4%include /etc/ganesha/exports/gv0.conf 5 6vi /etc/ganesha/exports/gv0.conf 7EXPORT{ 8 Export_Id = 1 ; # Export ID unique to each export 9 Path = \u0026#34;/mnt/upload\u0026#34;; 10 Pseudo = /upload; 11 FSAL { 12 name = VFS; 13 } 14 Access_type = RW; # Access permissions 15 Squash = No_root_squash; # To enable/disable root squashing 16 Disable_ACL = TRUE; # To enable/disable ACL 17 Protocols = \u0026#34;4\u0026#34; ; # NFS protocols supported 18 Transports = \u0026#34;UDP\u0026#34;,\u0026#34;TCP\u0026#34; ; # Transport protocols supported 19 SecType = \u0026#34;sys\u0026#34;; # Security flavors supported 20} 21 22#启动 23systemctl restart nfs-ganesha 24 25#手动mount的方法 26mount -t nfs -o soft,intr,rsize=8192,wsize=8192,timeo=900,proto=tcp,vers=4 192.168.31.2:/upload /mnt/nfs-upload 27 28#自动mount的方法 29cat /etc/fstab 30192.168.31.2:/upload /mnt/nfs-upload nfs rw,vers=4,addr=192.168.31.2,clientaddr=192.168.31.8 0 0 注意id对齐问题可能要用到rpcidmapd\n","date":"2022-07-06","img":"","permalink":"https://bajie.dev/posts/20220706-nfs_usermode/","series":null,"tags":null,"title":"用户态的NFS Server"},{"categories":null,"content":"JavaScript 到底是如何工作的？ 一、工作原理 JavaScript到底是：\n同步还是异步？ 单线程还是多线程？ JavaScript 中的一切都发生在\nExecution Context （执行上下文）中\n您可以假设这个执行上下文 是一个大盒子或一个容器，在其中执行整个 JavaScript 代码。 这个大盒子里有两个组件： Memory（内存组件）：这是所有变量和函数存储为键值对的地方。这个**“内存组件”也称为变量环境**。因此，它是一种环境，其中所有这些变量和函数都存储为键值对。 Code（代码组件）：这是代码逐行执行的地方。这个“代码组件”也称为执行线程。所以，这个执行线程是一个单线程，整个代码一次只执行一行。 结论：JavaScript 是一种同步单线程语言。\n单线程 意味着 JavaScript 一次只能执行一个命令。 同步单线程 意味着 JavaScript 一次只能以特定顺序每次执行一个命令。这意味着它只能在当前行完成执行后转到下一行。这就是同步单线程的意思。 很惊诧吧，实际 javascripts 有单线程 event loop 大循环来完成很多不可思议的事情。\n二、实际工作过程分析 JavaScript 代码是如何执行的？ 当你运行 JavaScript 代码时会发生什么？\n会创建一个Execution Context（执行上下文）。\n让我们使用实际的代码来举个例子：\n1var n = 2; 2function square(num) { 3 var ans = num * num; 4 return ans; 5} 6var square2 = square(n); 7var square4 = square(4); 执行上述代码时，会创建 一个执行上下文\n此执行上下文分两个阶段创建：\n一、创建：\n创建阶段也称为内存创建阶段\n。这是一个非常关键的阶段。\n在内存创建的第一阶段，JavaScript 会为所有的变量和函数分配内存。\n首先 JavaScript 遇到var n = 2;，它就会分配内存给n.\n当为n分配内存时，它会先存储一个特殊值undefined。undefined在 JavaScript 中被视为特殊的占位符。\n在遇到 functionsquare(num)时，它也会为这个 function () 分配内存。\n在为 function() 分配内存的情况下，它将该函数的整个代码存储在内存空间中。\n后面为两个变量square2和square4分配了内存，存储的同样是undefined。\n为了完成这个创建阶段，JavaScript 会逐行从上到下遍历扫描代码。\n二、代码执行：\n扫描完毕，现在 JavaScript 再次逐行运行整个程序。 当它遇到时var n = 2;，它实际上将2作为值n放入内存组件中。 当它遇到 的函数定义时square(num)，它没有什么可执行的，所以 JavaScript 简单地跳过了。 当它遇到 时var square2 = square(n);，我们现在正在调用一个函数。 函数是 JavaScript 的核心。它们在 JavaScript 中的行为与在任何其他语言中的行为非常不同。 每当调用一个函数时，都会创建一个全新的Execution Context（执行上下文）。 因此，从技术上讲，在整个**Execution Context 的代码组件中又创建了一个全新的Execution Context ** 。 这个新的内部 Exection Context 执行上下文也有它自己的内存组件和代码组件。 现在在内部发生的事情是： 在这种情况下，有 2 个变量，即num（参数）和ans。 所以内存将分配给num和ans。 在第 1 阶段，与外部执行上下文一样，undefined将分配给num和ans。 现在进入阶段 2（代码执行阶段），参数的值被分配给参数。因此，在我们调用函数的语句var square2 = square(n);时，我们将参数n的值 2 传递给函数square(num)，并且该参数的值替换了内部执行上下文中num的内存组件中的占位符undefined。 计算后num * num，将值存储在 中ans。 在遇到 时return ans;，将存储的值ans返回到调用的位置，并且此内部执行上下文结束。当它结束时，内部执行上下文实际上被删除了。 现在，遇到下一行时遵循相同的过程var square4 = square(4);。 最后一行成功执行后，整个执行上下文也被删除。这种*“整体”*执行上下文也称为全局执行上下文。 那么，JavaScript 是如何管理这种链条式的 ** 执行上下文 ** 的呢？\n它实际上在后台管理一个堆栈。 此堆栈也称为Call Stack（调用堆栈）。 GEC（Global Execution Context 全局执行上下文）始终位于此堆栈的底部。 每当创建一个新的执行上下文时，它就会被压入这个堆栈，并在完成其目的时被弹出。 控件与此调用堆栈的最顶部元素保持一致。 此调用堆栈仅用于管理执行上下文。 在成功执行最后一条语句时，调用堆栈被清空。 调用堆栈维护执行上下文的*执行顺序。*\n给个图更好理解：\n调用堆栈具有以下花哨的名称，也可以通过这些名称来引用它：\nExecution Context Stack（执行上下文堆栈） Program Stack（程序栈） Control Stack（控制堆栈） Runtime Stack（运行时堆栈） Machine Stack（机器堆栈） 这样大家就理解了吧，这样后面的变量提升 Hoisting 就很好理解了。\n","date":"2022-06-21","img":"","permalink":"https://bajie.dev/posts/20220621-javascripts_how_to_run/","series":null,"tags":null,"title":"JavaScript 到底是如何执行的呢 -- JS的作原理"},{"categories":null,"content":"Javascripts 中的 if \u0026hellip; else 用起来倒是很方便，但是看起来就不舒服了\n1if (true) { 2} 3 4if (true) { 5} 6 7if (true) { 8 if (true) { 9 if (true) { 10 } 11 } else { 12 } 13} else { 14} 上面看起来很闹心吧，下面讲讲优化：\n一、三元优化 if (ture) else可以用三元操作符来优化它：\n1// Bad 😥 2if (true) { 3 console.log(\u0026#34;Congratutions!\u0026#34;) 4} else { 5 console.warn(\u0026#34;Oops, something went wrong!\u0026#34;) 6} 7 8// Great 🥰 9true 10 ? console.log(\u0026#34;Congratutions\u0026#34;) 11 : console.warn(\u0026#34;Oops, something went wrong!\u0026#34;) 二、与优化 true的情况下才执行可以用与来进行优化：\n1if (true) { 2 alert(1) 3} 4 5// is equals to: 6 7true \u0026amp;\u0026amp; alert(1) 三、提前返回 如果有多个判断，按照复杂程度，从上到下，提前返回。\n1function handleRequest(req) { 2 if (req.code \u0026gt;= 500) { 3 return \u0026#34;Server error\u0026#34;; 4 } 5 if (req.code \u0026gt;= 404) { 6 return \u0026#34;Cilent error\u0026#34;; 7 } 8 return \u0026#34;Suceess\u0026#34;; 9} 四、类表格优化 下面是类似表格似的选择，通常会用 switch 来进行优化\n1// Bad 😥 2const weekday = (num) =\u0026gt; { 3 if (num === 1) { 4 return \u0026#34;Monday\u0026#34; 5 } else if (num === 2) { 6 return \u0026#34;Tuesday\u0026#34; 7 } else if (num === 3) { 8 return \u0026#34;Wednesday\u0026#34; 9 } else if (num === 4) { 10 return \u0026#34;Thursday\u0026#34; 11 } else if (num === 5) { 12 return \u0026#34;Friday\u0026#34; 13 } else if (num === 6) { 14 return \u0026#34;Saturday\u0026#34; 15 } else if (num === 7) { 16 return \u0026#34;Sunday\u0026#34; 17 } else { 18 return \u0026#34;Unknown\u0026#34; 19 } 20} 21 22console.log(weekday(1)) // Monday switch 也不是最优解，用对象的键值对来解：\n1const weekday = (option) =\u0026gt; { 2 let obj = { 3 1: \u0026#34;Monday\u0026#34;, 4 2: \u0026#34;Tuesday\u0026#34;, 5 3: \u0026#34;Wednesday\u0026#34;, 6 4: \u0026#34;Thursday\u0026#34;, 7 5: \u0026#34;Friday\u0026#34;, 8 6: \u0026#34;Saturday\u0026#34;, 9 0: \u0026#34;Sunday\u0026#34; 10 } 11 return obj[option] ?? \u0026#34;Unknown\u0026#34; 12} 13 14console.log(weekday(1)) // Monday 也可以用 ES6 的 map 来实现：\n1// Great 🥰 2const weekday = (num) =\u0026gt; { 3 const map = new Map() 4 .set(1, \u0026#34;Monday\u0026#34;) 5 .set(2, \u0026#34;Tuesday\u0026#34;) 6 .set(3, \u0026#34;Wednesday\u0026#34;) 7 .set(4, \u0026#34;Thursday\u0026#34;) 8 .set(5, \u0026#34;Friday\u0026#34;) 9 .set(6, \u0026#34;Saturday\u0026#34;) 10 .set(7, \u0026#34;Sunday\u0026#34;); 11 return map.has(num) ? map.get(num) : \u0026#34;Unknown\u0026#34;; 12}; 13 14console.log(weekday(1)); 五、选项多对一的数组优化 看如下代码，多个选项对应一个返回：\n1const getContinent = (option) =\u0026gt; { 2 if (option === \u0026#34;China\u0026#34; || option === \u0026#34;Japan\u0026#34;) { 3 return \u0026#34;Asia\u0026#34;; 4 } 5 if (option === \u0026#34;Germany\u0026#34; || option === \u0026#34;France\u0026#34;) { 6 return \u0026#34;Europe\u0026#34;; 7 } 8}; 9 10console.log(getContinent(\u0026#34;China\u0026#34;)); 使用数组来容纳多个选项，然后用 includes 来判断并返回\n1const getContinent = (option) =\u0026gt; { 2 const Asia = [\u0026#34;China\u0026#34;, \u0026#34;Japan\u0026#34;]; 3 const Europe = [\u0026#34;Germany\u0026#34;, \u0026#34;Franch\u0026#34;]; 4 if (Asia.includes(option)) return \u0026#34;Asia\u0026#34;; 5 if (Europe.includes(option)) return \u0026#34;Europe\u0026#34;; 6 return \u0026#34;Unknown\u0026#34;; 7}; 8 9console.log(getContinent(\u0026#34;China\u0026#34;)); // Asia 进一步优化，用 \u0026amp;\u0026amp; 与优化\n1const getContinent = (option) =\u0026gt; { 2 let [result, setResult] = [\u0026#34;unknown\u0026#34;, (str) =\u0026gt; (result = str)]; 3 const Asia = [\u0026#34;China\u0026#34;, \u0026#34;Japan\u0026#34;]; 4 const Europe = [\u0026#34;Germany\u0026#34;, \u0026#34;Franch\u0026#34;]; 5 Asia.includes(option) \u0026amp;\u0026amp; setResult(\u0026#34;Asia\u0026#34;); 6 Europe.includes(option) \u0026amp;\u0026amp; setResult(\u0026#34;Europe\u0026#34;); 7 return result; 8}; 9 10console.log(getContinent(\u0026#34;China\u0026#34;)); ","date":"2022-06-15","img":"","permalink":"https://bajie.dev/posts/20220615-javascripts_if/","series":null,"tags":null,"title":"Javascripts中if的优化"},{"categories":null,"content":"Shell脚本的进阶技巧：\n一、String 类的技巧 String类当作数组来使用，注意范围的下标，0:4，4最后要被踢掉，实际是0-3，共4个字符：\n1string=\u0026#34;Bash is great!\u0026#34; 2 3echo ${string:8} # great! 4echo ${string:0:4} # Bash 5echo ${string:8:-1} # great (-1: upto 1st element from right) 数组的长度：\n1string=\u0026#34;Bash is great!\u0026#34;; 2len=${#string} 3echo \u0026#34;len = $len\u0026#34; 字符串的替换：\n1string=\u0026#34;Bash is great!!\u0026#34; 2echo ${string/Bash/GNU Bash} # GNU Bash is great!! 3echo ${string//!/.} # Bash is great.. 大小写替换:\n1string=\u0026#34;Bash\u0026#34; 2echo ${string^^} # BASH 3echo ${string,,} # bash 按index存取数组中字符在shell中不可用，但是可以用function变相实现：\n1function charAt() { 2 string=$1 3 i=$2 4 echo ${string:i:1} 5} 6echo $(charAt \u0026#34;Hello\u0026#34; 2) # l 7echo $(charAt \u0026#34;Hello\u0026#34; 1) # e 二、数组和字典 定义一个数组并且遍历它：\n1A=(2 4 5 6 4) 2for i in ${A[@]}; do 3 echo -n \u0026#34;${i} \u0026#34; # 2 4 5 6 4 4done 同样对数组进行切割：\n1A=(2 4 5 6 4) 2echo ${A[@]:2:3} # 5 6 4 3echo ${A[@]:0:1} # 2 Bash 中的数组需要使用 [@] 符号来引用整个数组，可以用简洁的语法对数组增加元素\n1A=(1 2 3) 2A+=(4) 3echo ${A[@]} # 1 2 3 4 4A+=(5 6) 5echo ${A[@]} # 1 2 3 4 5 6 字典也一样\n1declare -A D 2D=([\u0026#34;one\u0026#34;]=1 [\u0026#34;two\u0026#34;]=2 [\u0026#34;three\u0026#34;]=3) 3for i in ${!D[@]}; do 4 echo \u0026#34;$i -\u0026gt; ${D[$i]}\u0026#34; # one -\u0026gt; 1 .... 5done 三、数学计算 简单的数学计算可以用 $((...)) 来完成：\n1a=10 2b=5 3 4echo \u0026#34;$a + $b = $((a + b))\u0026#34; # 10 + 5 = 15 5echo \u0026#34;$a - $b = $((a - b))\u0026#34; # 10 - 5 = 5 6echo \u0026#34;$a x $b = $((a * b))\u0026#34; # 10 x 5 = 50 7echo \u0026#34;$a / $b = $((a / b))\u0026#34; # 10 / 5 = 2 8echo \u0026#34;$a % $b = $((a % b))\u0026#34; # 10 % 5 = 0 或者借助复杂一点的 bc 来完成：\n1a=10 2b=5.7 3 4echo \u0026#34;$a / $b = $(echo \u0026#34;scale = 2; $a / $b\u0026#34; | bc)\u0026#34; # 10 / 5.7 = 1.75 随机数的产生：\n1echo $(($RANDOM % 10)) # Random number between 0 and 10 四、输入 控制输入变量：\n1read -p \u0026#34;Enter your name: \u0026#34; name 2echo \u0026#34;Hello, $name\u0026#34; 如果要输入数组：\n1read -p \u0026#34;Enter numbers: \u0026#34; -a A 2echo ${A[@]} 借助上面的技巧，我们可以写出一些复杂的 shell 脚本了\n","date":"2022-06-09","img":"","permalink":"https://bajie.dev/posts/20220609-shell_tips/","series":null,"tags":null,"title":"Shell进阶技巧"},{"categories":null,"content":"一、基本概念\n1. 异步\n所谓 \u0026quot; 异步 \u0026ldquo;，简单说就是一个任务分成两段， 先执行第一段， 然后转而执行其他任务去， 等做好了准备， 再回过头执行第二段。\n比如， 有一个任务是读取文件进行处理， 任务的第一段是向操作系统发出请求， 要求读取文件。 然后， 程序执行其他任务， 等到操作系统返回文件，再接着执行任务的第二段（ 处理文件）。 这种不连续的执行， 就叫做异步。\n相应地， 连续的执行就叫做同步。 由于是连续执行， 不能插入其他任务， 所以操作系统从硬盘读取文件的这段时间， 程序只能干等着什么也做不了。\n2. 回调函数\njavascript 语言对异步编程的实现， 就是回调函数。 所谓回调函数， 就是把任务的第二段单独写在一个函数里面， 等到准备好了，继续执行的时候， 就直接调用这个函数。 它的英语名字 callback， 直译过来就是 \u0026quot; 回调 \u0026ldquo;。\n读取文件进行处理， 是这样的，readfile 是异步的，readfileSync 是同步的。\n1fs.readfile(\u0026#39;/etc/passwd\u0026#39;, function(err, data) { 2 if(err) throw err; 3 console.log(data); 4}); 上面代码中， readfile 函数的第二个参数， 就是回调函数， 也就是任务的第二段。 等到操作系统返回了 / etc / passwd这个文件以后， 回调函数才会执行。\n==一个有趣的问题是， 为什么 node.js 约定， 回调函数的第一个参数， 必须是错误对象 err（ 如果没有错误， 该参数就是 null）？ 原因是执行分成两段， 在这两段之间抛出的错误， 程序无法捕捉， 只能当作参数， 传入第二段。==\n3. promise\n“想像一下你是个小孩，你妈妈 promise 承诺你下星期给你一部新手机”\n只有下周来临的时候，你才会知道你真的得到一部手机，或者是骗你玩的。\n这就是 promise. 一个 promise 有三种状态：\nPending: 待定，你不知道你是否能得到一部手机 Resolved: 妈妈很高兴，你得到一部手机 Rejected: 妈妈不高兴，你没有得到一部手机 状态图如下：\n语法很简单：\n1// promise syntax look like this 2new Promise(function (resolve, reject) { ... } ); 再看一下上面问题的解决方法：\n首先确定 Mom 的状态是 unhappy，然后建立一个 Promise 来确定是否得到手机，最后用一个函数 askMom 来调用这个 Promise\nresolve() 中可以放置一个参数用于向下一个 then 传递一个值，then 中的函数也可以返回一个值传递给 then。但是，如果 then 中返回的是一个 Promise 对象，那么下一个 then 将相当于对这个返回的 Promise 进行操作。\nreject() 参数中一般会传递一个异常给之后的 catch 函数用于处理异常。\n但是请注意以下两点：\nresolve 和 reject 的作用域只有起始函数，不包括 then 以及其他序列； resolve 和 reject 并不能够使起始函数停止运行，别忘了 return。 1var isMomHappy = false; 2 3// Promise 4var willIGetNewPhone = new Promise( 5 function (resolve, reject) { 6 if (isMomHappy) { 7 var phone = { 8 brand: \u0026#39;Samsung\u0026#39;, 9 color: \u0026#39;black\u0026#39; 10 }; 11 resolve(phone); // resolved 12 } else { 13 var reason = new Error(\u0026#39;mom is not happy\u0026#39;); 14 reject(reason); // rejected 15 } 16 17 } 18); 19 20var askMom = function () { 21 willIGetNewPhone 22 .then(function (fulfilled) { 23 // yay, you got a new phone 24 console.log(fulfilled); 25 // output: { brand: \u0026#39;Samsung\u0026#39;, color: \u0026#39;black\u0026#39; } 26 }) 27 .catch(function (error) { 28 // oops, mom didn\u0026#39;t buy it 29 console.log(error.message); 30 // output: \u0026#39;mom is not happy\u0026#39; 31 }); 32}; 33 34askMom(); 很有点意思吧。抽象了，那就实际点。\njavascripts 中的 fetch 函数就是基于Promise范式的，promise 的 resolves 绑在了 Response 上。Promise 类有 .then() .catch() 和 .finally() 三个方法，这三个方法的参数都是一个函数，.then() 可以将参数中的函数添加到当前 Promise 的正常执行序列，.catch() 则是设定 Promise 的异常处理序列，.finally() 是在 Promise 执行的最后一定会执行的序列。 .then() 传入的函数会按顺序依次执行，有任何异常都会直接跳到 catch 序列：\n1 fetch(\u0026#34;http://192.168.1.6/graph_image_hubot.php?action=view\u0026amp;local_graph_id=13619\u0026amp;rra_id=5\u0026#34;) 2 .then(response =\u0026gt; { 3 if (response.ok) { 4 return response.buffer(); 5 } 6 throw new Error(\u0026#39;Network response was not ok.\u0026#39;); 7 }) 8 .then(buffer =\u0026gt; { 9 const formData = new FormData(); 10 formData.append( \u0026#39;media\u0026#39;, buffer, {filename: \u0026#39;bandwidth.png\u0026#39;, contentType: \u0026#39;image/png\u0026#39; } ); 11 robot.wwork.sendImageMessage(owner, formData); 12 }); 上面的程序实际上是从 cacti 服务器拿到一张流量图片，然后 fetch 的结果 anyway，response总是有东西的，都会是成功的，所以我们必须再用 response.ok 来判断，成功就继续得到 buffer 流，最终送到 hubot 中去。三个过程用 then 串了起来，每一步都成功才会进行下一步。\n这样写避免了回调地狱，看起来也比较舒服。\n那什么时候用 promise 呢，它是异步的，有大IO读写硬盘、读写网络的时候用，上面是读写网络。下面再来一个读写磁盘的：\n1const {promise: {readFile, writeFile}} = require(\u0026#39;fs\u0026#39;); 2(async () =\u0026gt; { 3 let content = await readFile(\u0026#39;./data.txt\u0026#39;, \u0026#39;utf8\u0026#39;); 4 await writeFile(\u0026#39;2.txt\u0026#39;, content, \u0026#39;utf8\u0026#39;); 5 console.log(\u0026#39;ok\u0026#39;); 6})(); 上面的语法是 ES7 的，它更尽了一步，首先声明一个匿名的函数是 async 异步的，然后用await来进行等待，将读和写两个大操作都放到 await 的一步操作中去，这样程序看起来就变成同步的一步步等待了。跟 ES6 的 promise 比起来，更进了一步。\n","date":"2022-05-24","img":"","permalink":"https://bajie.dev/posts/20220524-javascripts_promise/","series":null,"tags":null,"title":"Javascripts中的promise"},{"categories":null,"content":"Javascripts 中的 map\n说到map，如果我们有循环的话，不停遍历，会把时间搞成 O(n)。如果有map，时间就变成了O(1)，所以还是相当有效率的。\nmap 是 ES2015 中新增加的，是一种新的 Object 类型，允许存放各种 key-value 对。\nmap 最大的特性是，key 可以是任何类型，包括 object 和 function；可以调用 size 得到 map 的大小；迭代的时候，是严格按照添加的顺序进行迭代的。\nmap 的方法如下： set, get, size, has, delete, clear:\n1let things = new Map(); 2 3const myFunc = () =\u0026gt; \u0026#39;🍕\u0026#39;; 4 5things.set(\u0026#39;🚗\u0026#39;, \u0026#39;Car\u0026#39;); 6things.set(\u0026#39;🏠\u0026#39;, \u0026#39;House\u0026#39;); 7things.set(\u0026#39;✈️\u0026#39;, \u0026#39;Airplane\u0026#39;); 8things.set(myFunc, \u0026#39;😄 Key is a function!\u0026#39;); 9 10things.size; // 4 11 12things.has(\u0026#39;🚗\u0026#39;); // true 13 14things.has(myFunc) // true 15things.has(() =\u0026gt; \u0026#39;🍕\u0026#39;); // false, not the same reference 16things.get(myFunc); // \u0026#39;😄 Key is a function!\u0026#39; 17 18things.delete(\u0026#39;✈️\u0026#39;); 19things.has(\u0026#39;✈️\u0026#39;); // false 20 21things.clear(); 22things.size; // 0 23 24// setting key-value pairs is chainable 25things.set(\u0026#39;🔧\u0026#39;, \u0026#39;Wrench\u0026#39;) 26 .set(\u0026#39;🎸\u0026#39;, \u0026#39;Guitar\u0026#39;) 27 .set(\u0026#39;🕹\u0026#39;, \u0026#39;Joystick\u0026#39;); 28 29const myMap = new Map(); 30 31// Even another map can be a key 32things.set(myMap, \u0026#39;Oh gosh!\u0026#39;); 33things.size; // 4 34things.get(myMap); // \u0026#39;Oh gosh!\u0026#39; 迭代它的方法，可以用 for \u0026hellip; of，顺序是严格按照你插入的顺序来的：\n1let activities = new Map(); 2 3activities.set(1, \u0026#39;🏂\u0026#39;); 4activities.set(2, \u0026#39;🏎\u0026#39;); 5activities.set(3, \u0026#39;🚣\u0026#39;); 6activities.set(4, \u0026#39;🤾\u0026#39;); 7 8for (let [nb, activity] of activities) { 9 console.log(`Activity ${nb} is ${activity}`); 10} 11 12// Activity 1 is 🏂 13// Activity 2 is 🏎 14// Activity 3 is 🚣 15// Activity 4 is 🤾 也可以用 forEach 来迭代，注意一点即可，forEach 的 callback 函数，第一个参数是 value，第二个参数是 key，跟 for \u0026hellip; of 是相反的：\n1activities.forEach((value, key) =\u0026gt; { 2 console.log(`Activity ${key} is ${value}`); 3}); ","date":"2022-05-23","img":"","permalink":"https://bajie.dev/posts/20220523-javascripts_map/","series":null,"tags":null,"title":"Javascripts中的map"},{"categories":null,"content":"千里之行始于足下，javascript 模块漫天飞，能做的事也是五花八门。我们来实践一下\n假设我们有一个文本文件，内容如下：\n里面是一行行数据，我们要做的就是把所有值取整求和，文件中有某些空行\n很简单，程序如下：\n1var fs = require(\u0026#39;fs\u0026#39;); 2 3calculate = () =\u0026gt; { 4 fs.readFile(\u0026#39;data.txt\u0026#39;, \u0026#39;utf8\u0026#39;, (err, data) =\u0026gt; { 5 if (err) { 6 throw new Error(err) 7 } 8 9 const arr = data.split(\u0026#39;\\r\\n\u0026#39;); 10 const result = arr 11 .filter(e =\u0026gt; e) 12 .map(parseFloat) 13 .reduce((curr, next) =\u0026gt; curr + next); 14 console.log(\u0026#39;RESULT: \u0026#39;, result); 15 }); 16} 超级简单吧\n关键就是上面的链式调用\nsplit 用来分割每一行 filter 用来去掉空行 map 用来把每一行都转化成整数 reduce 用来求和 ","date":"2022-05-22","img":"","permalink":"https://bajie.dev/posts/20220522-javascirpts_fs/","series":null,"tags":null,"title":"Javascript的实际应用-Fs模块"},{"categories":null,"content":"JavaScript Module 是什么 一个 JavaScript module 模块准确的说就是一个文件，它允许你把代码 export 导出来复用，这样别的 JavaScript 文件可以 import 这个文件把它作为库文件来使用了。\n模块主要是用在工程文件中，用来把代码共享给其他文件用，javascipt 的模块可以说是满天飞。\n这里呢讨论的是前端的部分，即是运行在浏览器中的 javascripts module 部分，而不是运行在后端的 Node.js 的部分，后端现在已经向前端靠拢了。\nES6标准发布后，module成为标准，标准使用是以export指令导出接口，以import引入模块，但是在我们一贯的node模块中，我们依然采用的是CommonJS规范，使用require引入模块，使用module.exports导出接口。这点也理解清楚。\n我们从零来一步步开始吧\n如何把一个 JavaScript 文件转换为 ES 的 module 1: 创建一个 project 目录 这个目录用来放 HTML 和 JavaScript 文件\n2: 建立你的 code 代码文件 在 project 文件夹中建立2个文件:\nindex.html index.js 3: 添加 JavaScript 文件的引用到 HTML 文件中 打开index.html 编辑它：\n1\u0026lt;!DOCTYPE html\u0026gt; 2\u0026lt;html\u0026gt; 3 \u0026lt;head\u0026gt; 4 \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; 5 \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1.0\u0026#34;\u0026gt; 6 \u0026lt;title\u0026gt;ES Module - zhang ranrui\u0026lt;/title\u0026gt; 7 \u0026lt;/head\u0026gt; 8 \u0026lt;body\u0026gt; 9 \u0026lt;h1\u0026gt;ES Module Tutorial\u0026lt;/h1\u0026gt; 10 11 \u0026lt;!-- Add the \u0026#34;index.js\u0026#34; JavaScript file to this HTML document --\u0026gt; 12 \u0026lt;script type=\u0026#34;module\u0026#34; src=\u0026#34;index.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; 13 \u0026lt;/body\u0026gt; 14\u0026lt;/html\u0026gt; 上面的代码中，我们引用了 index.js , 并定义它 type=\u0026ldquo;module\u0026rdquo;，这显式指明了 index.js 是一个 ES 的module。\n那么，如果有了一个 ES 的 module，如何使用它呢，也举个例子：\n如何使用一个 ES 的 Module 1: 建立一个 project 目录 同样是放 html 和 module 文件的。\n2: 建立 code 文件 在 project 文件夹下建立如下文件:\nindex.html module-1.js module-2.js 3: 增加 modules 文件的引用到 HTML 文件 编辑 index.html 文件，增加对 module 的引用:\n1\u0026lt;!DOCTYPE html\u0026gt; 2\u0026lt;html\u0026gt; 3 \u0026lt;head\u0026gt; 4 \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; 5 \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1.0\u0026#34;\u0026gt; 6 \u0026lt;title\u0026gt;ES Module - zhang ranrui\u0026lt;/title\u0026gt; 7 \u0026lt;/head\u0026gt; 8 \u0026lt;body\u0026gt; 9 \u0026lt;h1\u0026gt;ES Module Tutorial\u0026lt;/h1\u0026gt; 10 \u0026lt;h2\u0026gt;Check the console\u0026lt;/h2\u0026gt; 11 12 \u0026lt;script type=\u0026#34;module\u0026#34; src=\u0026#34;module-1.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; 13 \u0026lt;script type=\u0026#34;module\u0026#34; src=\u0026#34;module-2.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; 14 \u0026lt;/body\u0026gt; 15\u0026lt;/html\u0026gt; 4: 浏览你的 index.html 注意，这里只能在 localhost 的服务器上浏览，直接浏览本地文件会报 cors 的错。\n怎么 Export 导出一个 Module 中的代码 有两种使用方法\n在代码前使用 export 关键字 使用 export 声明语句 一、在代码前使用 export 关键字 注意：是在正式代码之前\n1// module-1.js 2 3// Export the \u0026#34;bestClub\u0026#34; variable: 4export const bestClub = \u0026#34;Your Club\u0026#34;; 另外一个例子：\n1// Export the \u0026#34;multiply\u0026#34; function: 2export function multiply(x, y) { 3 return x * y; 4} 二、使用 export 声明语句 另一种方法是使用 export 声明语句，这种方式用 {} 包裹 要导出的变量，中间的变量名用逗号来隔开。\n1// Create a variable named \u0026#34;bestClub\u0026#34;: 2const bestClub = \u0026#34;Your Club\u0026#34;; 3 4// Create a function named \u0026#34;multiply\u0026#34;: 5function multiply(x, y) { 6 return x * y; 7} 8 9// Create an array named \u0026#34;fruits\u0026#34;: 10const fruits = [\u0026#34;Mango\u0026#34;, \u0026#34;Apple\u0026#34;, \u0026#34;Orange\u0026#34;, \u0026#34;Lemon\u0026#34;]; 11 12// Export the three statements above: 13export { bestClub, multiply, fruits }; 导出有了，怎样导入别的模块 Exported 的代码呢？ 接上的例子，只导入一个变量：\n1// module-2.js 2 3import { bestClub } from \u0026#34;./module-1.js\u0026#34;; 导入所有变量\n1// Import three items from the module-1.js file: 2import { bestClub, multiply, fruits } from \u0026#34;./module-1.js\u0026#34;; 注意./是表示当前路径，如果路径不对，有可能需要全路径导入\n1// Import three items from the module-1.js file: 2import { bestClub, multiply, fruits } from \u0026#34;/codesweetly/blog/notes/modular-javascript/es-modules/module-1.js\u0026#34;; 注意在后端 Node.js 和 module bundlers 中，是允许你不写文件名后缀的，比如下面\n1// Import three items from the module-1.js file: 2import { bestClub, multiply, fruits } from \u0026#34;module-1\u0026#34;; 但是，在 ES modules 中，不允许省略文件后缀。\n如何使用从别的模块 Imported 导入的代码 1// module-2.js 2 3import { bestClub } from \u0026#34;./module-1.js\u0026#34;; 4 5const myBestClub = bestClub + \u0026#34; \u0026#34; + \u0026#34;is my best club.\u0026#34;; 6 7console.log(myBestClub); 注意:\nimport 关键字只能出现在 modules 文件中，不能出现在通常的 JavaScript 程序文件中。 一个被 imported 导入的模块只能作用于本文件中，而不是全局可用。你只能在这个文件中使用。 JavaScript 会自动提升 import 语句的作用域，所以你可以在任何地方使用 import ，甚至在使用其中代码之后。 Imported 导入的模块缺省处于 strict 严格模式下。 如何将 Exports 和 Imports 导入导出的模块重命名 用 as 即可：\n1// module-1.js 2 3// Create a variable named \u0026#34;bestClub\u0026#34;: 4const bestClub = \u0026#34;Your Club\u0026#34;; 5 6// Export the bestClub variable as \u0026#34;favoriteTeam\u0026#34;: 7export { bestClub as favoriteTeam }; 接下来使用就不能是 bestclub , 而是 favoriteTeam 了：\n1// module-2.js 2 3import { favoriteTeam } from \u0026#34;./module-1.js\u0026#34;; 4 5const myBestClub = favoriteTeam + \u0026#34; \u0026#34; + \u0026#34;is my best club.\u0026#34;; 6 7console.log(myBestClub); 同样可以在 import 的时候使用 as\n1// module-1.js 2// Create a variable named \u0026#34;bestClub\u0026#34;: 3const bestClub = \u0026#34;Your Club\u0026#34;; 4// Export the bestClub variable: 5export { bestClub }; 6 7// module-2.js 8import { bestClub as favoriteTeam } from \u0026#34;./module-1.js\u0026#34;; 9const myBestClub = favoriteTeam + \u0026#34; \u0026#34; + \u0026#34;is my best club.\u0026#34;; 10console.log(myBestClub); 如何在 ES Module 中重命名多个 Exports 导出 很简单，用as加上逗号分割符号\n1// module-1.js 2 3const bestClub = \u0026#34;Your Club\u0026#34;; 4const fruits = [\u0026#34;Grape\u0026#34;, \u0026#34;Apple\u0026#34;, \u0026#34;Pineapple\u0026#34;, \u0026#34;Lemon\u0026#34;]; 5 6function multiply(x, y) { 7 return x * y; 8} 9 10// Export the three statements above: 11export { 12 bestClub as favoriteTeam, 13 fruits as crops, 14 multiply as product 15}; 16// module-2.js 17 18import { favoriteTeam, crops, product } from \u0026#34;./module-1.js\u0026#34;; 19 20const bestClub = `I bought ${product(2, 11)} ${crops[2]}s at ${favoriteTeam}.`; 21 22console.log(bestClub); 如何在 ES Module 中重命名多个 Import 导入 跟上面一样：\n1// module-1.js 2 3const bestClub = \u0026#34;Your Club\u0026#34;; 4const fruits = [\u0026#34;Grape\u0026#34;, \u0026#34;Apple\u0026#34;, \u0026#34;Pineapple\u0026#34;, \u0026#34;Lemon\u0026#34;]; 5function multiply(x, y) { 6 return x * y; 7} 8 9// Export the three statements above: 10export { bestClub, fruits, multiply }; 11// module-2.js 12 13import { 14 bestClub as favoriteTeam, 15 fruits as crops, 16 multiply as product 17} from \u0026#34;./module-1.js\u0026#34;; 18 19const bestClub = `I bought ${product(2, 11)} ${crops[2]}s at ${favoriteTeam}.`; 20 21console.log(bestClub); 如何在 ES Module 一句话 import 导入所有导出变量 用*号：\n1// Import all exportable features from the \u0026#34;countries.js\u0026#34; module: 2import * as allCountries from \u0026#34;./countries.js\u0026#34;; 例子如下：\n1// module-1.js 2 3const bestClub = \u0026#34;Your Club\u0026#34;; 4const fruits = [\u0026#34;Grape\u0026#34;, \u0026#34;Apple\u0026#34;, \u0026#34;Pineapple\u0026#34;, \u0026#34;Lemon\u0026#34;]; 5function multiply(x, y) { 6 return x * y; 7} 8 9// Export the three statements above: 10export { bestClub, fruits, multiply }; 11// module-2.js 12 13import * as firstModule from \u0026#34;./module-1.js\u0026#34;; 14 15const bestClub = `I bought ${firstModule.multiply(2, 11)} ${firstModule.fruits[2]}s at ${firstModule.bestClub}.`; 16 17console.log(bestClub); 如何在 ES Module 中 Export 指定 default 缺省导出 如果导出的时候指定 default ，就是缺省导出。Default export\n1// module-1.js 2 3const bestClub = \u0026#34;Your Club\u0026#34;; 4 5// Export the bestClub variable as a default export: 6export default bestClub; 注意：default 两边没有花括号，因为 default 只能有一个，而且缺省导出不能是 var , let 或者 const\n如何 import 导入缺省导出 依然两种方法：\n使用 default as 的语法 直接命名缺省导出 一、使用 default as 语法 1import { default as newName } from \u0026#34;./module-relative-path.js\u0026#34;; 例子：\n1// module-1.js 2 3// Export the string value as a default export: 4export default \u0026#34;Your Club\u0026#34;; 5// module-2.js 6 7import { default as favoriteTeam } from \u0026#34;./module-1.js\u0026#34;; 8 9const bestClub = favoriteTeam + \u0026#34; \u0026#34; + \u0026#34;is my best club.\u0026#34;; 10 11console.log(bestClub); 二、直接命名缺省导出 这种方式，可以看作是第一种去掉花括号，去掉 default as 后的简略写法：\n1import newName from \u0026#34;./module-relative-path.js\u0026#34;; 例子：\n1// module-1.js 2 3// Export the string value as a default export: 4export default \u0026#34;Your Club\u0026#34;; 5// module-2.js 6 7import favoriteTeam from \u0026#34;./module-1.js\u0026#34;; 8 9const bestClub = favoriteTeam + \u0026#34; \u0026#34; + \u0026#34;is my best club.\u0026#34;; 10 11console.log(bestClub); over，这么多，大家应该会了吧。回头会另开一篇说说如何把模块都压到一起，那就是 webpack 的功能了。\n","date":"2022-05-21","img":"","permalink":"https://bajie.dev/posts/20220521-javascripts_module/","series":null,"tags":null,"title":"Javascripts中的module"},{"categories":null,"content":"研究一下 javascripts 的 reduce ，reduce 是既能改变 array 的 size，又能改变数值的函数，filter 是只能改变size，不能改变数值；而 map 是不能改变 size，可以改变数值。很拗口吧，三个兄弟。\n简单介绍一下 reduce。\n假设我们有一个数组:\n1[1, 2, 3, 4] 我们要对整个数组求和.\nreduce 实际是按照下列的算式来进行求和的:\n((((1) + 2) + 3) + 4)\n那实际 reduce 函数执行中，你可以按你需求来自定义你自己的 + 操作符。数组的值也可以是其他的任意东西。听起来有点意思吧？\n1、 Reduce 是干嘛的 在一个函数式编程语言中，reduce 其实有很多别的名称，比如 fold（对折）, accumulate（累加器）, aggregate（聚合器）, compress（压缩） 甚至叫 inject（注入）。\n2、 Reduce 的参数 常用用法如下：\n1let myArray = [/* 首先定义一个数组 */]; 2let callbackfn = /* 再定义一个函数 */ ; 3let initialvalue = /* 任意一个初始化的值 */ ; 4 5myArray.reduce(callbackfn) 6myArray.reduce(callbackfn, initialValue) reduce 的参数如下:\ncallbackfn: 必须是一个函数，会在整个数组中反复调用，reduce 调用 callbackfn 的时候有4个参数，我们定义它们是 previousValue, currentElement, index 和 array ，看起来像下面一样:\n1callbackfn(previousValue, currentElement, index, array) 解释一下:\npreviousValue: 这个参数就是一个累加器。 currentElement: 数组中处理的当前元素。 index: 当前元素的索引值。 array: myArray调用的数组. Return value（返回值）: 最后一次调用 callbackfn 的时候，返回值就是整个 reduce 过程的返回值。如果不是最后一次调用，它返回的值会被下次的 callbackFn 的 previousvalue 参数接收。\n我们必须注意，函数就是函数，函数过程中处理的变量要么是外围的 scope 带进来的，要么是函数体内自定义的，所以后面两个 index 和 array 也是必须存在的，只不过是 reduce 函数替你自动处理了。\n3、 用画图来理解 Reduce 看上面的图，reduce 和 reductRight 函数的区别就是方向，一个是从左到右，一个是从右到左。\n关注点如下:\nacc 相当于 previousValue ，累加器. curVal 相当于currentElement，当前处理元素. 数组中的每个元素向下输出到圈 r 就是curVal输出到***r***的具体表现. 包含数组元素的长方形输出到下一个 r 就是 acc 输出到***r***的具体表现. 初始化值在数组外单独表示，它是作为一个单独的 acc 输出到 r 中的. 3、 用流程图来理解 Reduce 下面用20行的伪代码来详细解释整个 reduce 的过程，首先进入 reduce 函数：\nIf initialValue is present, 如果初始化变量不为空 If myArray has no elements, 接着判断如果数组为空 Return initialValue. 那么直接初始化变量作为 reduce 的结果返回 else 初始化变量为空但数组不为空 Let accumulator be initialValue. 把初始变量赋给累加器 If the method is reduce, 如果方法是 reduce Let startIndex be the index of the leftmost element of myArray. 把数组最左边的元素的index值赋予 startIndex else 如果初始化变量为空 If myArray has no elements, 如果数组没有元素 Throw TypeError. 初始化变量为空，数组也为空，直接抛类型错误 Else if myArray has just only one element, 如果数组只有一个元素 Return that element. 那么直接将数组中唯一一个元素作为 reduce 结果返回 Else If the method is reduce,如果方法是 reduce Let accumulator be the leftmost element of myArray. 把数组最左边的第一个元素赋给累加器 If the method is reduce, 如果方法是reduce In left to right order, for each element of myArray such that its index i ≥ startingIndex, 按照从左到右的顺序，遍历数组中的每一个元素，来个大循环 Set accumulator to callbackfn(accumulator, myArray[i], i, myArray). 数组中的每个元素，都逐个设置到callbackfn函数中并运行 Return accumulator. 累加器的值作为 reduce 结果返回 仔细理解，搞完了吧。\n给个实际例子，一群学生，有男有女，先选出女学生，然后计算出她们每个人的平均成绩，最后把她们打印出来：\n1const students = [ 2 { 3 name: \u0026#34;Anna\u0026#34;, 4 sex: \u0026#34;f\u0026#34;, 5 grades: [4.5, 3.5, 4] 6 }, 7 8 { 9 name: \u0026#34;Dennis\u0026#34;, 10 sex: \u0026#34;m\u0026#34;, 11 country: \u0026#34;Germany\u0026#34;, 12 grades: [5, 1.5, 4] 13 }, 14 15 { 16 name: \u0026#34;Martha\u0026#34;, 17 sex: \u0026#34;f\u0026#34;, 18 grades: [5, 4, 2.5, 3] 19 }, 20 21 { 22 name: \u0026#34;Brock\u0026#34;, 23 sex: \u0026#34;m\u0026#34;, 24 grades: [4, 3, 2] 25 } 26]; 27 28 29//TODO: Compute and Return female students results using functional programming. 30 31function studentResult(students){ 32 return students.filter(x =\u0026gt; x.sex==\u0026#34;f\u0026#34;).reduce((init,cur)=\u0026gt;{ 33 cur[\u0026#39;grades\u0026#39;]=cur.grades.reduce((acc,cur) =\u0026gt; acc+cur,0)/cur.grades.length; 34 return init.concat(cur); 35 },[]); 36} 37 38console.log(studentResult(students)); 39//[ { name: \u0026#39;Anna\u0026#39;, sex: \u0026#39;f\u0026#39;, grades: 4 }, { name: \u0026#39;Martha\u0026#39;, sex: \u0026#39;f\u0026#39;, grades: 3.625 } ] 注意一点，reduce 可以动 size，也可以动值，上面实际改变了 students 数组元素的值了，不太好。应该赋一个新值 concat 或者 push 进新数组。\n下面的方法就没有动原数组的数据：\n1function studentResult(students){ 2 return students.filter(x =\u0026gt; x.sex==\u0026#34;f\u0026#34;).reduce((init,cur)=\u0026gt;{ 3 let newarr = {}; 4 newarr[\u0026#39;name\u0026#39;]=cur.name; 5 newarr[\u0026#39;sex\u0026#39;]=cur.sex; 6 newarr[\u0026#39;grades\u0026#39;]=cur.grades.reduce((acc,cur) =\u0026gt; acc+cur,0)/cur.grades.length; 7 init.push(newarr); 8 return init; 9 },[]); 10} ","date":"2022-05-20","img":"","permalink":"https://bajie.dev/posts/20220520-javascripts_reduce/","series":null,"tags":null,"title":"Javascripts之数组的reduce"},{"categories":null,"content":"面试的时候被问到：如何才能让 docker 打出的镜像包尽量小？\n其实在生产已经尽量使用最小化的镜像包了，只是突然被问到还是有点懵圈；因为印象中自己基本是使用 alphine 做底包，Dockerfile 通常就是 copy 一个可执行的程序进去就完事了，如果不行，再开 shell 进去慢慢添加缺少的库文件。\n这里就总结一下，两点：\n一、使用尽量小的底包 以 alphine 为主，使用 alphine 底包的时候，需要注意以下：\n1、替换 apk 的源\n2、更新、更新证书\n3、注意 Timezone 的设置\n4、注意 glibc 库的兼容问题\n二、使用分阶段build 通常类似c、go、rust之类的源代码，都需要经过编译，最后产生可执行文件，那么完整的编译环境其实对最后的镜像来说都是不需要的。\n所以利用分阶段build，甩脱编译环境以及中间产物，就可以缩小最后 build 出镜像的大小，使用也很简单。\nDockerfile 文件内容如下：\n1FROM golang:alpine AS build-env 2WORKDIR /app 3ADD . /app 4RUN cd /app \u0026amp;\u0026amp; go build -o goapp 5 6 7FROM alpine 8RUN apk update \u0026amp;\u0026amp; \\ 9 apk add ca-certificates \u0026amp;\u0026amp; \\ 10 update-ca-certificates \u0026amp;\u0026amp; \\ 11 rm -rf /var/cache/apk/* 12WORKDIR /app 13COPY --from=build-env /app/goapp /app 14EXPOSE 8080 15ENTRYPOINT ./goapp ","date":"2022-04-01","img":"","permalink":"https://bajie.dev/posts/20220401-docker_mini_image/","series":null,"tags":null,"title":"面试之Docker如何打出最小的镜像"},{"categories":null,"content":"面试时被问到：是否了解 Nginx，它使用的 epoll 模式和其他的相比有什么优势？\n直接被问住，实际生产中配过不少的 Nginx，各种 rewrite、regex、正反向代理、php、fast-cgi、限流、证书、jwt、cors；epoll 只大概有印象是下面这行：\n1events { 2 use epoll; 3 worker_connections 1024; 4} 实在是汗颜啊，所以得仔细研究一下这个 epoll。\n首先扔概念 IO多路复用： 多路是指网络连接，复用指的是同一个线程。\nIO多路复用是一种同步IO模型，实现一个线程可以监视多个文件描述符；\n一旦某个描述符就绪（一般是读就绪或者写就绪），就能够通知应用程序进行相应的读写操作；\n没有文件描述符就绪时会阻塞应用程序，交出cpu。\n那么IO多路复用的实现方法有三种： select、poll、epoll select、poll、epoll本质上都是同步I/O，用户进程（这里就是Nginx）负责读写（从内核空间拷贝到用户空间），读写过程中，用户进程是阻塞的。\nselect：\n查询 fd_set 中，是否有就绪的 fd，可以设定一个超时时间，当有 fd (File descripter) 就绪或超时返回；\nfd_set 是一个位集合，大小是在编译内核时的常量，默认大小为 1024\n特点：\n连接数限制，fd_set 可表示的 fd 数量太小了；\n线性扫描：判断 fd 是否就绪，需要遍历一边 fd_set；\n数据复制：从内核空间拷贝到用户空间，复制连接就绪状态信息\npoll：\n解决了连接数限制：\npoll 中将 select 中的 fd_set 替换成了一个 pollfd 数组\n解决 fd 数量过小的问题\n数据复制：从内核空间拷贝到用户空间，复制连接就绪状态信息\nepoll：event 事件驱动\n事件机制：避免线性扫描\n为每个 fd，注册一个监听事件\nfd 变更为就绪时，将 fd 添加到就绪链表\nfd 数量：无限制（OS 级别的限制，单个进程能打开多少个 fd）\n区别在于:\nepoll较灵活，如果有一百万个链接状态同时保持，但是在某个时刻，只有几百个链接是活跃的。epoll的处理是通过epoll_create()创建对象，epoll_ctl()收集所有的套接字添加到epoll对象，epoll_wait()收集所有发生事件也就是所谓的活跃的链接，并收集到一个List链表中，这样只需要遍历这些List链表里的数据，而不用遍历一百万个链接。 而后者select poll则是每次收集事件时，将这一百万个链接都传给操作系统，再由操作系统内核上判断某些链接产生了事件，造成了巨大的资源浪费（大批量的不同态内存复制）。\n为什么epoll效率比较高：\nepoll 在epoll_create 时，就已经建立好了一个文件描述符对应 epoll 对象，同时，在内核 cache 里建立了一个红黑树，用于存储后续epoll_ctl传来的socket连接。再同时，又建立了一个list链表，用于存储准备就绪的事件。 等到 epoll_wait() 调用的时候，只需要观察list链表里有没有数据就可以，有数据就返回，没数据就sleep。等到timeout后，即使没数据，也返回了。所以 epoll_wait 会非常高效\n三者的比较：\nselect poll epoll 数据结构 bitmap 数组 红黑树 最大连接数 1024 无上限 无上限 fd拷贝 每次调用select拷贝 每次调用poll拷贝 fd首次调用epoll_ctl拷贝，每次调用epoll_wait不拷贝 工作效率 轮询：O(n) 轮询：O(n) 回调：O(1) Nginx 的并发处理能力 关于 Nginx 的并发处理能力：\n并发连接数，一般优化后，峰值能保持在 1~3w 左右。（内存和 CPU 核心数不同，会有进一步优化空间） Nginx 的最大连接数：Worker 进程数量 x 单个 Worker 进程的最大连接数 ","date":"2022-04-01","img":"","permalink":"https://bajie.dev/posts/20220401-nginx_epoll/","series":null,"tags":null,"title":"面试之Nginx的epoll的优势"},{"categories":null,"content":"现在各大公有云的 k8s 网络插件基本用的都是 vxlan，我们也需要对这个进行一下详细了解，以便用于公司的正式生产环境。\n一、原理 首先，kubernetes的网络模型：\n包含三要素：\n所有的容器(container)之间能够在不使用 NAT 的情况下互相通讯\n所有的节点(Node)能够在不使用 NAT 的情况下跟所有的容器(container)互相通讯（反之容器访问节点亦然）\n容器(container)中的IP地址，在容器内和容器外面看起来是一样的\n那来到 flannel，它是一种 Overlay network 覆盖网络，盖在原有的 Node 网络基础上： 上图要仔细分析， K8S 中存在三个+一个网络段：\nnode 网络段，上图是 172.20.32.0/19，这是基础网络段 pod 网络段，100.96.0.0/16，2¹⁶(65536)个IP，这是由 flannel 产生的 overlay network，所有的 pod 都站在一个大广场上，互相可见 svc 网络段，上图未提，我们需要知道，想要把 pod 的 ip 给固定下来，就得使用 svc 来分配固定的域名，这个是由 iptables 来维护的 In-Host docker network网络段，这是每个 Node 主机的单独网络段，flannel给每个 Node 主机划分了一个 100.96.x.0/24 段，然后在 etcd 内进行维护，来避免不同的 Node 主机分配IP冲突。 综述：flannel 为每个 Node 分配一个 subnet，容器(container)从此 subnet 中分配 IP，这些 IP 可以在 Node 间路由，容器间无需 NAT 和 port mapping 就可以跨主机通信。\n每个 subnet 都是从一个更大的 IP 池中划分的，flannel 会在每个 Node 上运行一个叫 flanneld 的 agent，其职责就是从池子中分配 subnet。为了在各个主机间共享信息，flannel 用 etcd（与 consul 类似的 key-value 分布式数据库）存放网络配置、已分配的 subnet、host 的 IP 等信息。\n数据包如何在主机间转发是由 backend 实现的。flannel 提供了多种 backend，最常用的有 vxlan 和 host-gw，udp 万万不可使用。\n仔细分析一下两个不同主机上的container跨主机互相通讯的过程：\ncontainer-1 首先建立 IP 包， src: 100.96.1.2 -\u0026gt; dst: 100.96.2.3, 包发到 docker0 网桥，docker0 是 container-1 的缺省 gateway 网关。\n在每个 Node 上，flannel 都会跑一个flanneld的守护进程，它会在 Linux 的 kernel 路由表中建立若干条路由， Node1 的路由表如下:\n1admin@ip-172-20-33-102:~$ ip route 2default via 172.20.32.1 dev eth0 3100.96.0.0/16 dev flannel0 proto kernel scope link src 100.96.1.0 4100.96.1.0/24 dev docker0 proto kernel scope link src 100.96.1.1 5172.20.32.0/19 dev eth0 proto kernel scope link src 172.20.33.102 对照路由表，dst 100.96.2.3 的路由会落到 100.96.0.0/16 这一条上，也就是说会落到 flannel0 这个设备上继续转发出去。\nflannel0 呢本质上是一个由flanneld 进程建立的 TUN 虚拟网卡设备：\n写TUN：当 IP 包写到flannel0后，会转发到 kernel，然后 kernel 查路由表再转发 读TUN：当 IP 包首先到达核心，路由表显示下一跳是flannel0虚拟网卡，kernel会直接把包发到产生这个虚拟网卡的进程去，也就是flanneld进行读包 看上图，IP 包首先到达 Docker0，因为它是网关，然后查内核路由表，到达flannel0虚拟网卡，然后就到达flanneld进程读包， 这时候flanneld会做什么呢？\nflanneld从 etcd 中获得各个主机网段对应的节点信息：\n1admin@ip-172-20-33-102:~$ etcdctl ls /coreos.com/network/subnets 2/coreos.com/network/subnets/100.96.1.0-24 3/coreos.com/network/subnets/100.96.2.0-24 4/coreos.com/network/subnets/100.96.3.0-24 5 6admin@ip-172-20-33-102:~$ etcdctl get /coreos.com/network/subnets/100.96.2.0-24 7{\u0026#34;PublicIP\u0026#34;:\u0026#34;172.20.54.98\u0026#34;} 于是乎 Node1 上面的 flanneld 进程得知 100.96.2.3 IP对应的网段是 100.96.2.0/24 ，再进一步对应到 Node2 的公网IP 172.20.54.98，然后它就会继续封装这个IP包，用UDP或者VXLAN，把这个 IP 包再包裹一层封起来送到 Node2 的 flanneld 进程去。\nNode2：包首先从网卡到达 Node2 的核心 kernel，路由表显示下一跳是flannel0 虚拟网卡，包就转发到 flanneld 进程读包，flanneld接收到包后，就会做拆包，拆完包直接写包到 TUN，然后包到达本地核心路由，再查路由表转发到docker0，最终到达 container-2\n我们同样可以查看 Node2 的路由表，应该显示如下结果：\n1admin@ip-172-20-54-98:~$ ip route 2default via 172.20.32.1 dev eth0 3100.96.0.0/16 dev flannel0 proto kernel scope link src 100.96.2.0 4100.96.2.0/24 dev docker0 proto kernel scope link src 100.96.2.1 5172.20.32.0/19 dev eth0 proto kernel scope link src 172.20.54.98 这样整个过程就明晰了。\n道理明晰了，下面我们就需要来实际操作了。\n二、实战 直接在 k8s 里装就很简单，用 yaml 一步操作就行了，这里我们不做任何介绍。\n我们下面说的是如何单独把 flannel 单独拿出来使用：\n1、首先要装etcd 参照之前的帖子：Etcd单节点应用 这里也给出不用Docker，用systemctl来运行的方案：\n准备工作，关闭selinux，打开包转发：\n1echo 1 \u0026gt; /proc/sys/net/ipv4/ip_forward 2#或者修改/etc/sysctl.conf，然后sysctl -p 3#net.ipv4.ip_forward = 1 4 5systemctl disable firewalld.service 6systemctl stop firewalld.service 7 8iptables -P FORWARD ACCEPT 安装 etcd ：\n1yum install etcd -y 2 3cp /etc/etcd/etcd.conf/etc/etcd/etcd.conf.bak 4 5vi /etc/etcd/etcd.conf 6ETCD_LISTEN_PEER_URLS=\u0026#34;http://172.16.9.110:2380\u0026#34; 7ETCD_LISTEN_CLIENT_URLS=http://172.16.9.110:2379,http://127.0.0.1:2379 8ETCD_NAME=\u0026#34;default\u0026#34; 9ETCD_INITIAL_ADVERTISE_PEER_URLS=\u0026#34;http://172.16.9.110:2380\u0026#34; 10ETCD_ADVERTISE_CLIENT_URLS=http://172.16.9.110:2379 11 12systemctl enable --now etcd 配置 etcd：\n1vi /root/etcd.sh 2{ \u0026#34;Network\u0026#34;: \u0026#34;10.10.0.0/16\u0026#34;,\u0026#34;SubnetLen\u0026#34;: 24,\u0026#34;Backend\u0026#34;: {\u0026#34;Type\u0026#34;:\u0026#34;vxlan\u0026#34;} } 3 4etcdctl --endpoints=http://172.16.9.110:2379 set kubernetes‑cluster/network/config \u0026lt; /root/etcd.sh 5 6etcdctl ls kubernetes‑cluster/network/config 7 8curl -s http://172.16.9.110:2379/v2/keys/kubernetes‑cluster/network/subnets 解释一下：pod 的网段是 10.10.0.0/16，掩码是 /16 ，本地 Node 主机的掩码是/24，也就是说一台宿主机上最多产256台container。\n然后 etcd 的 key 是 kubernetes‑cluster/network\n2、然后装 flannel： 注意 etcd 的配置跟上面一致：\n1yum install flannel -y 2 3cp /etc/sysconfig/flanneld/etc/sysconfig/flanneld.bak 4 5vi/etc/sysconfig/flanneld 6FLANNEL_ETCD_ENDPOINTS=http://172.16.9.110:2379 7FLANNEL_ETCD_PREFIX=\u0026#34;kubernetes‑cluster/network\u0026#34; 8 9systemctl enable --now flanneld 3、重启Docker 编辑docker启动配置文件：\n1cat /run/flannel/subnet.env 2FLANNEL_NETWORK=10.10.0.0/16 3FLANNEL_SUBNET=10.10.1.1/24 4FLANNEL_MTU=1450 5FLANNEL_IPMASQ=false 6 7/usr/lib/systemd/system/docker.service 8 9# Add: --bip= and --mtu= 10 11vi /usr/lib/systemd/system/docker.service 12dockerd --bip=$FLANNEL_SUBNET --mtu=$FLANNEL_MTU 13 14或者 15cat /run/flannel/docker 16DOCKER_OPT_BIP=\u0026#34;‑‑bip=10.10.1.1/24\u0026#34; 17DOCKER_OPT_IPMASQ=\u0026#34;‑‑ip‑masq=true\u0026#34; 18DOCKER_OPT_MTU=\u0026#34;‑‑mtu=1450\u0026#34; 19DOCKER_NETWORK_OPTIONS=\u0026#34; ‑‑bip=10.10.1.1/24 ‑‑ip‑masq=false ‑‑mtu=1450 \u0026#34; 20 21cat /etc/systemd/system/docker.service.d/docker.conf 22ServiceEnvironmentFile=‑/etc/sysconfig/docker 23EnvironmentFile=‑/etc/sysconfig/docker‑storage 24EnvironmentFile=‑/etc/sysconfig/docker‑network 25EnvironmentFile=‑/run/flannel/docker 26ExecStart= 27ExecStart=/usr/bin/dockerd $OPTIONS 28 $DOCKER_STORAGE_OPTIONS 29 $DOCKER_NETWORK_OPTIONS 30 $BLOCK_REGISTRY 31 $INSECURE_REGISTRY 然后就可以了。\n那么为什么 flannel 不用 UDP 呢？\n看上图，包从用户空间到内核空间的流入流出，会经过1、2、3的来回拷贝翻转，性能损失较大，所以 UDP 只能用在测试环境。\n最后 flannel 的 VXLAN 和 HOST-GW 又有什么区别呢？\n与 vxlan 不同，host-gw 不会封装数据包，而是在主机的路由表中创建到其他主机 subnet 的路由条目，从而实现容器跨主机通信 host-gw 把每个主机都配置成网关，主机知道其他主机的 subnet 和转发地址。 vxlan 则在主机间建立隧道，不同主机的容器都在一个大的网段内（比如 10.2.0.0/16）。 虽然 vxlan 与 host-gw 使用不同的机制建立主机之间连接，但对于容器则无需任何改变。 由于 vxlan 需要对数据进行额外打包和拆包，性能会稍逊于 host-gw。 ","date":"2022-03-17","img":"","permalink":"https://bajie.dev/posts/20220317-kubernetes_flannel/","series":null,"tags":null,"title":"Kubernetes下Flannel网络"},{"categories":null,"content":"有 V2EX 的坛友问到如何静态编译 keepalived 的问题，实际上 keepalived 确实配置比较麻烦。那还有一个简单易行的 ucarp，生产也可以用这个。\nucarp 跟 keepalived 一样，都是用于高可用的 IP 漂移\n但是比 keepalived 配置简单，而且 opnsense 就是用的这个做的高可用，opnsense 是个非常可靠的防火墙软件。\n首先有两台虚机，172.18.19.1和172.18.19.2，虚拟ip是172.18.19.3\n先配置172.18.19.1\n1yum install epel-release 2yum install psmisc 3yum install ucarp 4 5cat\u0026lt;\u0026lt; EOF \u0026gt;\u0026gt; /etc/systemd/system/ucarp.service 6[Unit] 7Description=UCARP virtual interface 8After=network.target 9 10[Service] 11Type=simple 12ExecStart=/usr/local/bin/ucarp.sh 13RemainAfterExit=true 14ExecStop=/usr/bin/killall -SIGTERM ucarp 15ExecStop=/bin/sleep 10 16TimeoutStopSec=30 17StandardOutput=journal 18 19[Install] 20WantedBy=multi-user.target 21EOF 22 23注意，上面我们的systemd是直接调用了ucarp.sh来启动，这样更简单。 24三个脚本，注意权限是755 25 26cat\u0026lt;\u0026lt; EOF \u0026gt;\u0026gt; /usr/local/bin/ucarp.sh 27#!/bin/bash\u0026#39; 28/usr/sbin/ucarp -i eth0 -B -p nb1Dshiwode -v 001 -a 172.18.19.3 -s 172.18.19.1 --shutdown --preempt -u /usr/local/bin/vip-up.sh -d /usr/local/bin/vip-down.sh 29EOF 30 31cat\u0026lt;\u0026lt; EOF \u0026gt;\u0026gt; /usr/local/bin/vip-up.sh 32#!/bin/sh 33/sbin/ip addr add ${2}/24 dev ${1} 34/sbin/ip neigh flush dev ${1} 35EOF 36 37cat\u0026lt;\u0026lt; EOF \u0026gt;\u0026gt; /usr/local/bin/vip-down.sh 38#!/bin/sh 39/sbin/ip addr del ${2}/24 dev ${1} 40/usr/sbin/arp -d ${2} 41EOF 42 43chmod 755 /usr/local/bin/ucarp.sh /usr/local/bin/vip-up.sh /usr/local/bin/vip-down.sh ok，第一台机器就配置好了\n所有的东西都在ucarp.sh的那一行的参数上，直接用ucarp -h来查看\n配置第二台172.18.19.2，其他的东西都一样，唯一不同的就是ucapr.sh的一处不一样\n1cat\u0026lt;\u0026lt; EOF \u0026gt;\u0026gt; /usr/local/bin/ucarp.sh 2#!/bin/bash\u0026#39; 3/usr/sbin/ucarp -i eth0 -B -p nb1Dshiwode -v 001 -a 172.18.19.3 -s 172.18.19.2 --shutdown --preempt -u /usr/local/bin/vip-up.sh -d /usr/local/bin/vip-down.sh 4EOF 5 6# -s 172.18.19.2 源ip和19.1不同 ok，两台都配置完毕, 分别执行启动\n1systemctl daemon-reload 2systemctl enable --now ucarp 可以再找一台机器，一直长 ping 172.18.19.3，然后随机杀掉 19.1 和 19.2 上面的 ucarp 进程，可以看到 vip 172.18.19.3 会来回飘\n注意：如果主掉了，从接管了 vip 变成 master，那么主再起来的时候不会去抢从的 master。\n","date":"2022-02-25","img":"","permalink":"https://bajie.dev/posts/20220225-ucarp/","series":null,"tags":null,"title":"Ucarp的安装配置"},{"categories":null,"content":"用过了好几个 tunnel 工具软件，各有长处。\nghostunnel 是支持自定义 tls 证书加密的，这回用个更厉害的，支持各种代理链条的，gost\n项目地址：https://github.com/ginuerzh/gost\n里面的概念有点意思，其实就两条，-L 本地监听，-F 链条转发。\n注意的就一点，-F 的时候实际是一个中转服务器。这个中转服务器要转发的规则需要在 -L 参数里指定。\n那么举个具体的例子：\n中转服务器开了一个明文端口 9999，这个端口跑的是 ghostunnel client，加密数据后传到另外一个ghostunnel server服务器，然后数据明文穿出，访问具体的目的网站的端口。\n客户端的服务器呢无外网访问权限，需要访问中转服务器 192.168.1.1 的 9999 端口，然后数据通过 ghostunnel 加密穿出到外网。\n中转服务器配置如下：\n1# ghostunnel 监听本地环回地址的9999端口，TLS 加密穿越到外网110.114.5.19:9999，然后根据外网 server 里面的配置，到达最终目的地 ip 和 端口 2ghostunnel client --listen localhost:9999 --target 110.114.5.19:9999 --keystore client.pem --cacert ca.pem --unsafe-listen 3 4# gost 监听本地网卡192.168.1.1的9999端口 5gost-linux-amd64-2.11.1 -L=tls://192.168.1.1:9999 客户端配置如下：\n1gost-linux-amd64-2.11.1 -L=tcp://localhost:9999/localhost:9999 -F=tls://192.168.1.1:9999 这样客户端的程序，只要连接 localhost:9999 的 tcp 端口，就可以接力中转服务器穿出去了。\n上面的配置非常怪异吧，仔细解释一下：\n中转服务器呢有两个地址，localhost的环回地址，以及本地网卡192.168.1.1\nlocalhost:9999 是用 ghostunnel 加密 tcp 流量，tls 穿到外网 110.114.5.19 的 9999 端口，然后外网再转发。\n中转服务器同时监听了一个 192.168.1.1:9999 端口 tls 的加密代理端口。\n中转服务器的部分就完了。\n而客户端配置 -L = tcp://localhost:9999/localhost:9999\n第一个 localhost:9999 指的是本机的地址，第二个 localhost:9999 指的是中转机上面的转发地址，这个地方非常让人迷惑。\n强调一下：中转代理的转发配置是在客户端设置，而不是中转代理上设置。 然后 -F 指定了中转服务器，就可以了。\n最后一个要注意的地方，代理协议无论是 tls 、kcp 等，都自带缺省的证书，如果不满意，可以自己指定的。这个工具不用特意生成证书来使用。\n","date":"2022-02-24","img":"","permalink":"https://bajie.dev/posts/20220224-gost_tunnel/","series":null,"tags":null,"title":"Gost Tunnel的使用"},{"categories":null,"content":"这个需求也很有点意思，DBA 要求做 MySQL 的卸载从库，数据量会很大，硬盘空间后期需要扩容，但是 cpu 反倒占的不多。\n单独 MySQL 是无法限制其 CPU 核使用的，这样的话，最好就是做个虚机来控制 MySQL 总 CPU 核数的使用，然后硬盘扩容的话，比如要拉伸虚机的附加第二块硬盘，如果是 QEMU 格式，会花费很长时间，所以干脆把宿主机的目录直接透传进虚机，之后如果要扩容加硬盘，直接把新的大硬盘 mount 出来再透进去即可，新旧硬盘拷贝数据也比拉伸虚机硬盘快。\nCentOS7 下的做法如下：\n两个大前提：\n一、宿主机的 KVM qemu 系统需要使用新的 rpm 包，需要编译\n二、虚机的内核需要升级，mount 命令需要支持 -t p9 的新格式\n我们做好准备，就可以开始了\n一、编译宿主机的qemu新包 现在已经是2022年了，所以编译的方式也发生变化了，最佳编译方式是干脆启动一个 Docker 虚拟机，来编译出来 rpm 包，也不污染环境。\n首先克隆下来项目：\n1git clone https://github.com/AlekseyChudov/qemu-kvm-virtfs.git 2 3cd qemu-kvm-virtfs 看一下最后的 build 脚本，有一个地方需要修改：\n现在的 CentOS 最新版是 7.9.2009 ，这个版本树里是没有 qemu 的 Source Code 的，需要修改 baseurl，降低到 7.8.2003 才有 Source 的 repo\n1baseurl=https://mirrors.tripadvisor.com/centos-vault/centos/\\$releasever/virt/Source/kvm-common/ 2 3改成： 4 5baseurl=https://mirrors.tripadvisor.com/centos-vault/centos/7.8.2003/virt/Source/kvm-common/ 改好后 build 脚本如下\n1#!/bin/bash 2 3set -ex 4 5yum -y update 6 7yum -y install \\ 8 \u0026#34;@Development Tools\u0026#34; \\ 9 centos-release-qemu-ev \\ 10 glusterfs-api-devel \\ 11 glusterfs-devel \\ 12 iasl \\ 13 kernel-devel \\ 14 libcacard-devel \\ 15 libepoxy-devel \\ 16 mesa-libgbm-devel \\ 17 nss-devel \\ 18 spice-protocol \\ 19 spice-server-devel \\ 20 usbredir-devel 21 22cat \u0026lt;\u0026lt; EOF \u0026gt;\u0026gt; /etc/yum.repos.d/CentOS-QEMU-EV.repo 23[centos-qemu-ev-source] 24name=CentOS-\\$releasever - QEMU EV Sources 25baseurl=https://mirrors.tripadvisor.com/centos-vault/centos/7.8.2003/virt/Source/kvm-common/ 26gpgcheck=1 27enabled=0 28gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-SIG-Virtualization 29EOF 30 31yum-builddep -y qemu-kvm-ev 32 33yumdownloader --source qemu-kvm-ev 34 35rpm -Uvh qemu-kvm-ev-*.src.rpm 36 37sed -i \u0026#39;s/--disable-virtfs/--enable-virtfs/\u0026#39; /root/rpmbuild/SOURCES/build_configure.sh 38 39sed -i -e \u0026#39;/^%files -n qemu-kvm-common/,/^$/s/^$/%{_bindir}\\/virtfs-proxy-helper\\n%{_mandir}\\/man1\\/virtfs-proxy-helper.1.gz\\n/\u0026#39; \\ 40 -e \u0026#39;/^%if %{rhev}$/,/^%else$/s/pkgsuffix -ev/pkgsuffix -virtfs/\u0026#39; \\ 41 -e \u0026#39;/%define rhel_rhev_conflicts()/ a Provides: %1-ev = %{epoch}:%{version}-%{release} \\\\\\nObsoletes: %1-ev \u0026lt; %{obsoletes_version} \\\\\u0026#39; \\ 42 /root/rpmbuild/SPECS/qemu-kvm.spec 43 44rpmbuild -ba --clean /root/rpmbuild/SPECS/qemu-kvm.spec 然后一句话执行 Build：\n1cd qemu-kvm-virtfs 2 3docker run -it --privileged -v \u0026#34;$PWD/rpmbuild:/root/rpmbuild\u0026#34; \\ 4 docker.io/centos:7 /root/rpmbuild/build 5 6会得到如下的包，版本是 2.12.0-44.1.el7_8.1 ： 7 8rpmbuild/RPMS/x86_64/qemu-img-virtfs-2.12.0-44.1.el7_8.1.x86_64.rpm 9rpmbuild/RPMS/x86_64/qemu-kvm-common-virtfs-2.12.0-44.1.el7_8.1.x86_64.rpm 10rpmbuild/RPMS/x86_64/qemu-kvm-tools-virtfs-2.12.0-44.1.el7_8.1.x86_64.rpm 11rpmbuild/RPMS/x86_64/qemu-kvm-virtfs-2.12.0-44.1.el7_8.1.x86_64.rpm 12rpmbuild/RPMS/x86_64/qemu-kvm-virtfs-debuginfo-2.12.0-44.1.el7_8.1.x86_64.rpm 13rpmbuild/SRPMS/qemu-kvm-virtfs-2.12.0-44.1.el7_8.1.src.rpm 我们在宿主机上安装替换新的 qemu 包，并重启 libvirtd\n1yum -y install \\ 2 cloud-utils \\ 3 libvirt-client \\ 4 libvirt-daemon-config-network \\ 5 libvirt-daemon-config-nwfilter \\ 6 libvirt-daemon-driver-interface \\ 7 libvirt-daemon-driver-qemu \\ 8 virt-install \\ 9 rpmbuild/RPMS/x86_64/qemu-img-virtfs-2.12.0-44.1.el7_8.1.x86_64.rpm \\ 10 rpmbuild/RPMS/x86_64/qemu-kvm-common-virtfs-2.12.0-44.1.el7_8.1.x86_64.rpm \\ 11 rpmbuild/RPMS/x86_64/qemu-kvm-virtfs-2.12.0-44.1.el7_8.1.x86_64.rpm 12 13systemctl restart libvirtd 这样宿主机就准备好了。\n二、生产虚机，做好passthrough 生产虚机的时候，需要做好内外文件目录的直通，宿主机目录是 /opt/test，对应虚机目录是 /test\n1\u0026lt;domains ...\u0026gt; 2 ... 3 \u0026lt;devices ...\u0026gt; 4 \u0026lt;filesystem type=\u0026#39;mount\u0026#39; accessmode=\u0026#39;passthrough\u0026#39;\u0026gt; 5 \u0026lt;source dir=\u0026#39;/opt/test\u0026#39;/\u0026gt; 6 \u0026lt;target dir=\u0026#39;/test\u0026#39;/\u0026gt; 7 \u0026lt;/filesystem\u0026gt; 8 \u0026lt;/devices\u0026gt; 9\u0026lt;/domains\u0026gt; 虚机启动后，需要升级核心，支持新的 mount 选项\n1yum -y update 2yum -y --enablerepo centosplus install kernel-plus 3reboot 4 5# 把宿主机提供的 /test 点 mount 出来 6mount -t 9p -o trans=virtio,version=9p2000.L /test /opt/test 也可以把挂载点写进虚机的 fstab，下次就会自动 mount 了\n1/test /opt/test 9p trans=virtio,version=9p2000.L,nofail,_netdev,x-mount.mkdir 0 0 ","date":"2022-02-23","img":"","permalink":"https://bajie.dev/posts/20220223-kvm_passthrouth/","series":null,"tags":null,"title":"KVM下宿主机的目录直通到虚机"},{"categories":null,"content":"这是个严肃话题，正经挖以太币的话，很多的矿厂都是如果直接挖到以太币地址，那么必须挖到完整一个才允许提现，手续费还特高。\n但是如果挖到以太链Polygon的地址的话，就可以 0.005 ETH提币了，如下图所示\n但是千万记住，Polygon提出的所谓 0.005 ETH，看下图，只是个ERC-20的Token，实际是wETH Token，必须经过转换才能提到 ETH 去。\n从 ERC-20 转换到其它代币呢，需要 MATIC 代币来付账，这玩意也必须得有啊！废话不多说，直接放上挖MATIC教程，我的机器是 CentoOS\n首先需要有个完全自主的的 ETH 钱包地址，这个我就不教大家了\n一、下载xmrig挖矿软件 下载地址：https://github.com/xmrig/xmrig/releases\n我们选择最近的下载就好\n二、做好加密通道 我们需要做好一条加密tcp通道\n用 ghostunnel, localhost:9012 \u0026mdash;\u0026gt; vps:9012 \u0026mdash;\u0026gt; rx.unmineable.com:3333\n三、用screen后台开挖 1screen 2 3#./xmrig -o localhost:9999 -a rx -k -u DOGE:MATIC地址.矿工名 4./xmrig -o localhost:9012 -u MATIC:0x64358C7ddC96001697aBbA7ed431BADB6ABAaec5.cpu01 -p x --cpu-no-yield 5 6ctrl+a+d 四、查看挖了多少 查看地址：https://unmineable.com/coins/MATIC/address/ 五、兑换 挖到了足够的 MATIC，就可以愉快的兑换了。\n","date":"2022-02-22","img":"","permalink":"https://bajie.dev/posts/20220222-eth_matic/","series":null,"tags":null,"title":"如何用CPU挖Polygon网络的MATIC币"},{"categories":null,"content":"公司要做阿里的小程序接入，需要通过测试，测试呢需要提供硬盘的监控报告，比如 iops 。\n同事从网上找了一下，iops 监控原文如下：监控磁盘的 iops ，利用 linux 的 /proc/diskstats 的第四个字段和第八字段可监控读和写的 iops，第四个记录是记录所有读的次数，第八个字段是记录所有写的次数。通过 zabbix 上的差速率即可监控磁盘的 iops。\n文章链接：https://cloud.tencent.com/developer/article/1519113?ivk_sa=1024320u 仔细研究了一下上面的文章，看了它提供了两张监控图，分析一下：\n第一张图：\n有两个指标，绿色的是硬盘每秒的 io 读次数，红色的是硬盘每秒的 io 写次数。\n第二张图：\n同样两个指标，绿色的是硬盘每秒的 io 读 Bytes，红色的是硬盘每秒的 io 写 Bytes。\n知道了指标具体的含义，这样就好办了。\n我们用的是 prometheus 和 node_exporter\n首先去看看 node_exporter 暴露的指标，搜一搜 node_disk，会看到如下4个指标：\n1# HELP node_disk_reads_completed_total The total number of reads completed successfully. 2# TYPE node_disk_reads_completed_total counter 3node_disk_reads_completed_total{device=\u0026#34;sda\u0026#34;} 4.9530358e+07 4# HELP node_disk_writes_completed_total The total number of writes completed successfully. 5# TYPE node_disk_writes_completed_total counter 6node_disk_writes_completed_total{device=\u0026#34;sda\u0026#34;} 1.4449267304e+10 7 8# HELP node_disk_read_bytes_total The total number of bytes read successfully. 9# TYPE node_disk_read_bytes_total counter 10node_disk_read_bytes_total{device=\u0026#34;sda\u0026#34;} 6.4101677568e+11 11# HELP node_disk_written_bytes_total The total number of bytes written successfully. 12# TYPE node_disk_written_bytes_total counter 13node_disk_written_bytes_total{device=\u0026#34;sda\u0026#34;} 1.15483858333184e+14 可以看出是上面 4 个指标，这四个指标都是 counter 计数器类型的，都是只增不减的。\n然后去 prometheus ，画个图试试，query 分别如下（注意我们的 instance 即 node_exporter，是跑在了 50000 端口，是非标准的）：\n1node_disk_reads_completed_total{instance=\u0026#34;192.168.1.1:50000\u0026#34;} 2node_disk_writes_completed_total{instance=\u0026#34;192.168.1.1:50000\u0026#34;} 3 4node_disk_read_bytes_total{instance=\u0026#34;192.168.1.1:50000\u0026#34;} 5node_disk_written_bytes_total{instance=\u0026#34;192.18.1.1:50000\u0026#34;} 大家看到了 counter 类型，必然是一条斜线直冲天际。\n好了，我们然后去 grafana 里增加面板：\n选 Add Query\n先选数据源，选择系统中已经配好的 prometheus，怎么配这里就不说了：\n然后在 Query 的 Metrics 里填入 node_disk_written_bytes_total{instance=\u0026quot;192.168.1.1:50000\u0026quot;}：\n在 Legend 的空白处随便点一下，大折线出现了，而且给出了提示：Time series is monotonically increasing. Try applying a rate() function.\n听人劝，吃饱饭。我们改一下 Metrics 的查询语句，因为我们是5分钟抓一次数据，所以改成如下格式： rate(node_disk_written_bytes_total{instance=\u0026quot;192.18.1.1:50000\u0026quot;}[5m])\n再增加一个查询，Add Query 同时把 read bytes 和 write bytes 放进一张图去：\n最后修正一下：\n1A: 2Metrics: rate(node_disk_read_bytes_total{instance=\u0026#34;192.168.1.1:50000\u0026#34;}[5m]) 3Legend: sda read per second 4 5B: 6Metrics: rate(node_disk_written_bytes_total{instance=\u0026#34;192.168.1.1:50000\u0026#34;}[5m]) 7Legend: sda write per second 再把最大、最小、平均、当前给出来：\n这样图就做好了，最后完工的两张图：\n所有 prometheus 从各种 exporter 收上来的数据都可以这么图形化，以后画图就丰简由人了。\n","date":"2022-01-18","img":"","permalink":"https://bajie.dev/posts/20220118-grafana_prometheus/","series":null,"tags":null,"title":"Grafana画出prometheus的图"},{"categories":null,"content":"最近开始弄hubot，顺带也开始学习 javascript ，首先从单独一个 nodejs 项目开始：\nnvm 和 node 的安装使用在前面已经说过了： Nodejs多版本的安装与管理 新建一个项目目录，build01\n然后在其下建立如下目录结构：\n1mkdir build01 2 3cd build01 4mkdir es6 dist 5 6mkdir public 7cd public 8mkdir es6 dist 解释一下，es6 下放的是服务器端的源代码，dist 下放的是服务器端处理过的源代码。\npublic/es6 放的是浏览器端的源代码，public/dist 放的是浏览器端处理过的源代码。\n一、安装gulp和babel babel 是用来做 js 格式转换的，注意，原有的 babel-preset-es2015 已经不适用了。\n1npm install --save-dev gulp gulp-babel babel-preset-env@next 生成一个.babelrc 文件，内容就一行\n1{ \u0026#34;presets\u0026#34;: [\u0026#34;env\u0026#34;] } 在 es6 目录下，建立一个测试的 test.js 文件，内容如下：\n1\u0026#39;use strict\u0026#39;; 2 3const sentences = [ 4 { subject: \u0026#39;JavaScript\u0026#39;, verb: \u0026#39;is\u0026#39;, object: \u0026#39;great\u0026#39; }, 5 { subject: \u0026#39;Elephants\u0026#39;, verb: \u0026#39;are\u0026#39;, object: \u0026#39;large\u0026#39; }, 6]; 7 8function say({ subject, verb, object }) { 9 console.log(`${subject} ${verb} ${object}`); 10} 11 12for ( let s of sentences) { 13 say(s); 14} 仔细看，上面这个文件用到了 es6 的语法，对象的解构，of语法。\n我们来用 babel 把它转换一下，变成 es5 的语法\n在build01目录下，生成gulpfile.js，内容如下：\n1const gulp = require(\u0026#39;gulp\u0026#39;); 2const babel = require(\u0026#39;gulp-babel\u0026#39;); 3 4 5gulp.task(\u0026#39;default\u0026#39;, done =\u0026gt;{ 6 7 gulp.src(\u0026#34;es6/**/*.js\u0026#34;) 8 .pipe(babel()) 9 .pipe(gulp.dest(\u0026#34;dist\u0026#34;)); 10 11 gulp.src(\u0026#34;public/es6/**/*.js\u0026#34;) 12 .pipe(babel()) 13 .pipe(gulp.dest(\u0026#34;public/dist\u0026#34;)); 14 15 done(); 16 17}); 注意上面，**表示洞穿所有子目录。运行 gulp ，就会在 dist 目录下生成新的 test.js ，内容如下：\n1\u0026#39;use strict\u0026#39;; 2 3var sentences = [{ 4 subject: \u0026#39;JavaScript\u0026#39;, 5 verb: \u0026#39;is\u0026#39;, 6 object: \u0026#39;great\u0026#39; 7}, { 8 subject: \u0026#39;Elephants\u0026#39;, 9 verb: \u0026#39;are\u0026#39;, 10 object: \u0026#39;large\u0026#39; 11}]; 12 13function say(_ref) { 14 var subject = _ref.subject, 15 verb = _ref.verb, 16 object = _ref.object; 17 console.log(\u0026#34;\u0026#34;.concat(subject, \u0026#34; \u0026#34;).concat(verb, \u0026#34; \u0026#34;).concat(object)); 18} 19 20for (var _i = 0; _i \u0026lt; sentences.length; _i++) { 21 var s = sentences[_i]; 22 say(s); 23} 仔细看，es6 的语法 变成 es5 的语法了。\n二、安装eslint babel 是对语法进行转换，那么 eslint 就是对语法作出规范，很多人吃 airbnb 那一套规则。\n1npm install --save-dev eslint gulp-eslint gulp-if 安装完成后执行命令进行初始化：\n1eslint --init 会提问一堆问题：\n1How would you like to use ESLint? … 2▸ To check syntax, find problems, and enforce code style 3 4What type of modules does your project use? … 5▸ JavaScript modules (import/export) 6 7Which framework does your project use? … 8 React 9 Vue.js 10▸ None of these 11 12Does your project use TypeScript? ‣ No / Yes 13 14#两个都选哦 15Where does your code run? … (Press \u0026lt;space\u0026gt; to select, \u0026lt;a\u0026gt; to toggle all, \u0026lt;i\u0026gt; to invert selection) 16✔ Browser 17✔ Node 18 19How would you like to define a style for your project? … 20▸ Answer questions about your style 21 22✔ What format do you want your config file to be in? · JSON 23✔ What style of indentation do you use? · 4 24✔ What quotes do you use for strings? · single 25✔ What line endings do you use? · unix 26✔ Do you require semicolons? · Yes 最后得到文件 .eslintrc.json\n1{ 2 \u0026#34;env\u0026#34;: { 3 \u0026#34;browser\u0026#34;: true, 4 \u0026#34;es2021\u0026#34;: true, 5 \u0026#34;node\u0026#34;: true 6 }, 7 \u0026#34;extends\u0026#34;: \u0026#34;eslint:recommended\u0026#34;, 8 \u0026#34;parserOptions\u0026#34;: { 9 \u0026#34;ecmaVersion\u0026#34;: 13, 10 \u0026#34;sourceType\u0026#34;: \u0026#34;module\u0026#34; 11 }, 12 \u0026#34;rules\u0026#34;: { 13 \u0026#34;indent\u0026#34;: [ 14 \u0026#34;error\u0026#34;, 15 4 16 ], 17 \u0026#34;linebreak-style\u0026#34;: [ 18 \u0026#34;error\u0026#34;, 19 \u0026#34;unix\u0026#34; 20 ], 21 \u0026#34;quotes\u0026#34;: [ 22 \u0026#34;error\u0026#34;, 23 \u0026#34;single\u0026#34; 24 ], 25 \u0026#34;semi\u0026#34;: [ 26 \u0026#34;error\u0026#34;, 27 \u0026#34;always\u0026#34; 28 ] 29 } 30} 这里注意，要改两个地方：\n1\u0026#34;es2021\u0026#34;: true, 2改成 3\u0026#34;es6\u0026#34;: true, 4 5\u0026#34;ecmaVersion\u0026#34;: 13, 6改成 7\u0026#34;ecmaVersion\u0026#34;: 10, 然后我们改写一下 gulpfile.js，增加 eslint 的部分\n1const gulp = require(\u0026#39;gulp\u0026#39;); 2const babel = require(\u0026#39;gulp-babel\u0026#39;); 3const eslint = require(\u0026#39;gulp-eslint\u0026#39;); 4const gulpIf = require(\u0026#39;gulp-if\u0026#39;); 5 6function isFixed(file) { 7 return file.eslint !== null \u0026amp;\u0026amp; file.eslint.fixed; 8} 9 10gulp.task(\u0026#39;default\u0026#39;, done =\u0026gt;{ 11 12 gulp.src([\u0026#34;es6/**/*.js\u0026#34;, \u0026#34;public/**/*.js\u0026#34;]) 13 .pipe(eslint({fix: true})) 14 .pipe(eslint.format()) 15 .pipe(gulpIf(isFixed, gulp.dest(file =\u0026gt; file.base))) 16 .pipe(eslint.failAfterError()); 17 18 19 gulp.src(\u0026#34;es6/**/*.js\u0026#34;) 20 .pipe(babel()) 21 .pipe(gulp.dest(\u0026#34;dist\u0026#34;)); 22 23 gulp.src(\u0026#34;public/es6/**/*.js\u0026#34;) 24 .pipe(babel()) 25 .pipe(gulp.dest(\u0026#34;public/dist\u0026#34;)); 26 27 done(); 28 29}); 再运行 gulp ，会把 es6/test.js 给格式化好，对比一下原文件就知道了，ident 被调整成4个空格了：\n1\u0026#39;use strict\u0026#39;; 2 3const sentences = [ 4 { subject: \u0026#39;JavaScript\u0026#39;, verb: \u0026#39;is\u0026#39;, object: \u0026#39;great\u0026#39; }, 5 { subject: \u0026#39;Elephants\u0026#39;, verb: \u0026#39;are\u0026#39;, object: \u0026#39;large\u0026#39; }, 6]; 7 8function say({ subject, verb, object }) { 9 console.log(`${subject} ${verb} ${object}`); 10} 11 12for ( let s of sentences) { 13 say(s); 14} ","date":"2022-01-04","img":"","permalink":"https://bajie.dev/posts/20220104-javascript_babel_eslint/","series":null,"tags":null,"title":"Javascript的项目与babel和eslint"},{"categories":null,"content":"Hubot是一个通用的聊天机器人，能和很多聊天软件集成，比如slack、rockchat、telegram、企业微信、IRC等。\n公司用的是企业微信，之前用群机器人和 prometheus 集成，发送各种告警信息，但是群机器人无法接收消息；同时我们想集成进去很多自定义命令，比如发送指令就开始 jenkins build，发送 ansible 指令执行，发送流量图就自动把机房的流量图发过来，这个群机器人就用不成了。所以必须再直接一些，构建一个企业微信的应用。\n一、企业微信安装前准备 安装前需要在企业微信中拿到4个信息：\nWECHATWORK_CORP_ID WECHATWORK_APP_AGENT_ID WECHATWORK_APP_SECRET WECHATWORK_AES_KEY 首先联系企业微信管理员新增一个应用：\n在“管理后台-我的企业”中可以获得地一个信息，企业 ID 信息，CORP_ID ：\n拿拿 在“管理后台-应用管理-点击某具体应用-详情”页中可以获得二、三个信息， APP_AGENT_ID 和 APP_SECRET\n第四个 AES_KEY 就比较麻烦了，我们进入这个应用，点击配置的图标：\n然后进入，点击启用 API 接收\n然后设置一下\nurl 填写 https://hubot.rendoumi.com/wechatwork/webhook Token 点随机获取 EncodingAESKey 点随机获取，这就是具体的 AESKey 了 然后点击保存，这里必然会失败！没有关系，因为我们还没有装hubot呢。我们把这三个地方先记录下来，随后再来点击保存。\n二、Hubot 安装前准备 1、注意上面地址：URL 地址是 https ，然后 hubot 的缺省地址是 http://xxx:8080 ，这样如果前面没有证书卸载的设备，就得搭建一个Nginx（haproxy、traefik）来代理 https 443端口到8080。\n2、Hubot 是支持 Coffeescript 和 nodejs 的，从它的 Adapters 网页看\nhttps://hubot.github.com/docs/adapters/development/ 里面都是 coffee 味的单箭头函数，导致不太熟悉的八戒以为在 wechatwork module导出的时候也应该用单箭头函数，结果就悲剧了。\n所以务必清楚，Adapter 的语法是 js，普通自建自动回复script可以用coffeescript，后面会详细说这一点。\n3、安装Hubot必须新建一个普通用户，不能用 root 装\n用 root 会导致 nodejs 一堆报错，八戒新建了一个用户 bot，家目录/home/bot，然后再在这下面建立/home/bot/myhubot，在这个目录下安装。\n理由是这个如果直接装在这个用户的 home 目录下，会导致这个用户只能有这一个用途了，目录结构太浅，一堆文件\n4、Hubot的变量引入都是通过环境变量引入的\n所以无论什么wechatwork还是jenkins，都需要通过环境变量导入变量\n三、安装Hubot 首先安装nvm，参看文章 Nodejs多版本的安装与管理 1curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.0/install.sh | bash 安装的时候最好加个 https_proxy 代理，因为这个 shell 脚本不翻墙很难下下来。\n然后务必关了代理，退出终端并重新进入，安装 nodejs 的最新版本\n1nvm ls-remote 2nvm install v16.13.1 3nvm list 确认一下，nodejs 的版本缺省是 v16.13.1\n这里千万不要手欠把 npm 的源换成国内的腾讯等，就是国外源\n然后安装hubot，并初始化\n1su - bot 2npm install -g yo generator-hubot 3 4mkdir /home/bot/myhubot 5cd /home/bot/myhubot 6yo hubot 会问几个问题：\nOwer：写自己邮箱就行 Bot name: bot Description: rendoumi corp Bot adapter: campfire 注意最后一个，选campfire就好，我们稍后再装wechatwork\n1 _____________________________ 2 / \\ 3 //\\ | Extracting input for | 4 ////\\ _____ | self-replication process | 5 //////\\ /_____\\ \\ / 6 ======= |[^_/\\_]| /---------------------------- 7 | | _|___@@__|__ 8 +===+/ /// \\_\\ 9 | |_\\ /// HUBOT/\\\\ 10 |___/\\// / \\\\ 11 \\ / +---+ 12 \\____/ | | 13 | //| +===+ 14 \\// |xx| 15 16? Owner zhangranrui@rendoumi.com 17? Bot name bot 18? Description rendoumi corp 19? Bot adapter campfire 然后就会是漫长的等待：\n最后会报几个警告，不用管，忽略即可。\n1npm notice created a lockfile as package-lock.json. You should commit this file. 2+ hubot-help@1.0.1 3+ hubot-google-images@0.2.7 4+ hubot-rules@1.0.0 5+ hubot-shipit@0.2.1 6+ hubot-redis-brain@1.0.0 7+ hubot-google-translate@0.2.1 8+ hubot-diagnostics@1.0.0 9+ hubot-scripts@2.17.2 10+ hubot@3.3.2 11+ hubot-maps@0.0.3 12+ hubot-heroku-keepalive@1.0.3 13+ hubot-pugme@0.1.1 14added 99 packages from 53 contributors and audited 99 packages in 20.176s 15 162 packages are looking for funding 17 run `npm fund` for details 18 19found 1 low severity vulnerability 20 run `npm audit fix` to fix them, or `npm audit` for details 我们先跑一下，简单测一下功能，我们的bot name输入的是bot\n1cd /home/bot/myhubot 2 3./bin/hubot 4 5#进入后 6bot ping 7 8#得到回应 9PONG 这样就装好了 hubot，我们做一下修改，去掉 heroku 和 redis，否则会不停丢出报错信息，但是不要用 npm 去删除这两个包，放着那里备用也不占多少空间的\n1cd /home/bot/myhubot 2 3vi external-scripts.json 4[ 5 \u0026#34;hubot-diagnostics\u0026#34;, 6 \u0026#34;hubot-help\u0026#34;, 7 \u0026#34;hubot-heroku-keepalive\u0026#34;, 8 \u0026#34;hubot-google-images\u0026#34;, 9 \u0026#34;hubot-google-translate\u0026#34;, 10 \u0026#34;hubot-pugme\u0026#34;, 11 \u0026#34;hubot-maps\u0026#34;, 12 \u0026#34;hubot-redis-brain\u0026#34;, 13 \u0026#34;hubot-rules\u0026#34;, 14 \u0026#34;hubot-shipit\u0026#34; 15] 16 17#去掉下面两行 18hubot-heroku-keepalive 19hubot-redis-brain 三、安装微信 Adapter Adapter 插件，hubot是一套系统，通过各种插件跟各种聊天软件进行交互\n微信插件原始地址：https://github.com/billtt/hubot-wechatwork\n八戒稍微改了一下：https://github.com/zhangrr/hubot-wechatwork\n主要是添加了个自动回应和添加了一些调试信息，否则不知道加载成不成功。\n1#先装那个老的 2cd /home/bot/myhubot 3npm install hubot-wechatwork 4 5#覆盖掉老的模块 6cd /home/bot/myhubot/node_modules 7git clone https://github.com/zhangrr/hubot-wechatwork 启动之前普及一下 hubot 基本知识，hubot 中变量的传入都是通过环境变量导入的，启动测试一下wechatwork\n1export WECHATWORK_CORP_ID=aaaa 2export WECHATWORK_APP_AGENT_ID=bbbb 3export WECHATWORK_APP_SECRET=cccc 4export WECHATWORK_AES_KEY=dddd 5 6cd /home/bot/myhubot 7./bin/hubot -a wechatwork 看到信息，INFO Yunwei: loading wechatwork Adapter，说明启动没啥毛病\n然后我们回到企业微信这个应用的应用管理，点击保存接收消息服务器配置，这次就应该能成功了。\n测试一下，我们在企业微信中，对这个应用机器人私聊，发个 hi：\n再发个help，会冒出一堆信息\n然后我们可以试试指令，再发个ping，机器人回PONG，这就算正常工作了\n同时我们在终端也可以看到接收到的消息\n要注意的命令： wechatwork 有一条命令 chat create CHATID USER1,USER2,USER3,\u0026hellip;\n解释一下，是用来建立一个企业微信群聊的，群主默认是第一个人：\nchatid 是指一个群聊的 id 号，格式是字母+数字，比如yunwei001，这个一旦创建就无法销毁，会永远保存在企业微信那里，切切记住！！！\nuser1,user2 这种是员工企业微信的id，通常是用户名全称，比如 zhangrenren,liudaqiang\n所以说的时候务必要小心，万万记住 chatid 一旦建立就无法收回。\n之后可以调用 sendChatMessage 来给群聊发送消息。\n四、准备systemd启动hubot 这样不能每次用个终端启动 hubot 吧，写个 systemd 文件来处理吧\n由于我们用到了nvm，这里确实需要一些技巧，cat /etc/systemd/system/hubot.service\n1[Unit] 2Description=Hubot 3Requires=network.target 4After=network.target 5 6[Service] 7Type=simple 8WorkingDirectory=/home/bot/myhubot 9User=bot 10 11Restart=always 12TimeoutStartSec=10 13RestartSec=10 14 15; Configure Hubot environment variables, use quotes around vars with whitespace as shown below. 16; Environment=\u0026#34;HUBOT_SLACK_TOKEN=SLACK_TOKEN\u0026#34; 17; Environment=\u0026#34;HUBOT_JENKINS_AUTH=pe:x837491lkaflajksdf7\u0026#34; 18; Environment=\u0026#34;HUBOT_JENKINS_URL=http://192.168.1.90:8080/\u0026#34; 19Environment=\u0026#34;NODE_VERSION=default\u0026#34; 20Environment=\u0026#34;WECHATWORK_CORP_ID=aaaa\u0026#34; 21Environment=\u0026#34;WECHATWORK_APP_AGENT_ID=bbbb\u0026#34; 22Environment=\u0026#34;WECHATWORK_APP_SECRET=cccc\u0026#34; 23Environment=\u0026#34;WECHATWORK_AES_KEY=dddd\u0026#34; 24 25ExecStart=/home/bot/.nvm/nvm-exec /home/bot/myhubot/bin/hubot --adapter wechatwork 26 27[Install] 28WantedBy=multi-user.target 注意上面，我们指定了环境变量 NODE_VERSION=default，以及最下的 ExecStart，确定用 nvm-exec 引动 node\n另外把 wechatwork 所需的4个参数也放进环境变量中，这样重启就可以了\n1systemctl daemon-reload 2systemctl start hubot 五、集成jenkins 准备知识，我们必须在 jenkins 拿到用户名和 token（token是用来代替密码的），用来访问 jenkins\n首先登录 jenkins，访问网址：$JENKINS_URL/me/configure，新建一个 API Token，记录下来\n用法很简单：token替代密码用于 url 认证即可。\n1cd /home/bot/myhubot 2npm install hubot-jenkins-optimised 3 4#编辑external-scripts.json 5vi external-scripts.json 6[ 7 \u0026#34;hubot-diagnostics\u0026#34;, 8 \u0026#34;hubot-help\u0026#34;, 9 \u0026#34;hubot-google-images\u0026#34;, 10 \u0026#34;hubot-google-translate\u0026#34;, 11 \u0026#34;hubot-pugme\u0026#34;, 12 \u0026#34;hubot-maps\u0026#34;, 13 \u0026#34;hubot-rules\u0026#34;, 14 \u0026#34;hubot-shipit\u0026#34;, 15 \u0026#34;hubot-jenkins-optimised\u0026#34; 16] 17 18#上面增加了一行 19hubot-jenkins-optimised 20 21#编辑 /etc/systemd/system/hubot.service ，添加 jenkins 的环境变量 22#jenkins的用户名是pe，所有auth是 \u0026#34;pe:x837491lkaflajksdf7\u0026#34; 23...... 24Environment=\u0026#34;HUBOT_JENKINS_AUTH=pe:x837491lkaflajksdf7\u0026#34; 25Environment=\u0026#34;HUBOT_JENKINS_URL=http://192.168.1.90:8080/\u0026#34; 26...... 27 28# 重启 hubot 29systemctl daemon-reload 30systemctl restart hubot 然后我们私聊这个机器人，对它说: jenkins list，就可以看到项目了， jenkins b 3 就可以开始build第三个项目了，所有命令可以通过对机器人说 help 来获得\n六、集成ansible 集成 ansible，其实就是集成一个 shell，也很简单\n1cd /home/bot/myhubot 2npm install hubot-script-shellcmd 3 4#编辑external-scripts.json 5vi external-scripts.json 6[ 7 \u0026#34;hubot-diagnostics\u0026#34;, 8 \u0026#34;hubot-help\u0026#34;, 9 \u0026#34;hubot-google-images\u0026#34;, 10 \u0026#34;hubot-google-translate\u0026#34;, 11 \u0026#34;hubot-pugme\u0026#34;, 12 \u0026#34;hubot-maps\u0026#34;, 13 \u0026#34;hubot-rules\u0026#34;, 14 \u0026#34;hubot-shipit\u0026#34;, 15 \u0026#34;hubot-jenkins-optimised\u0026#34;, 16 \u0026#34;hubot-script-shellcmd\u0026#34; 17] 18 19#上面增加了一行 20hubot-script-shellcmd 21 22#把shellcmd目录下的bash目录，复制一份到/home/bot/myhubot/bash 23cp -R node_modules/hubot-script-shellcmd/bash ./ 24 25#我们把ansible的yml都放到/homebot/myhubot/bash/ansible目录下 26cd /home/bot/myhubot/bash 27mkdir ansible 28 29#把ansible-playbook的yml都放进去 30.... 31 32#然后编写命令脚本 33cd /home/bot/myhubot/bash/handles 34 35cat \u0026lt; EOF \u0026gt;\u0026gt; fuse 36#!/bin/bash 37 38if [ -n \u0026#34;$1\u0026#34; ] ; then 39 if [ -n \u0026#34;$2\u0026#34; ] ; then 40 ansible-playbook /home/bot/myhubot/bash/ansible/check-disk.yml -e \u0026#34;ip=$1 fs=$2\u0026#34; 41 else 42 ansible-playbook /home/bot/myhubot/bash/ansible/check-disk.yml -e \u0026#34;ip=$1 fs=/\u0026#34; 43 fi 44fi 45 46exit 0 47EOF 48 49#给执行权 50chmod 755 /home/bot/myhubot/bash/handles/fuse 51 52#重启hubot 53systemctl restart hubot check-disk.yml 的真身，是在远程机器上调用本机才有的rust dust命令，来算出远程机器的某个目录的使用情况\n1--- 2- name: check machine disk space 3 hosts: \u0026#34;{{ ip }}\u0026#34; 4 gather_facts: false 5 vars: 6 - ip: \u0026#34;{{ ip }}\u0026#34; 7 - fs: \u0026#34;{{ fs }}\u0026#34; 8 - ansible_ssh_user: \u0026#34;root\u0026#34; 9 - ansible_ssh_pass: \u0026#34;Fuck2021!\u0026#34; 10 - ansible_ssh_common_args: \u0026#34;-o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null\u0026#34; 11 - host_key_checking: False 12 tasks: 13 - name: Run a script with arguments (using \u0026#39;cmd\u0026#39; parameter) 14 ansible.builtin.script: 15 cmd: /usr/local/bin/dust -bcr {{ fs }} 16 register: output 17 18 - debug: var=output.stdout_lines 我们在企业微信对机器说 shell ，就会得到所有命令列表\n对它说 shell fuse 172.19.16.1 /export ，就会把本机才有的 dust 命令，拷贝到172.19.16.1的机器上，并对 /export 执行，得到占磁盘空间最大的文件和目录。\n参考资料： https://qydev.weixin.qq.com/wiki/index.php?title=%E5%8F%91%E9%80%81%E6%8E%A5%E5%8F%A3%E8%AF%B4%E6%98%8E http://sd.blackball.lv/library/Automation_and_Monitoring_with_Hubot_(2014).pdf ","date":"2021-12-22","img":"","permalink":"https://bajie.dev/posts/20211222-hubot_wechat/","series":null,"tags":null,"title":"Hubot集成企业微信+jenkins+ansible"},{"categories":null,"content":"最近在搞 JavaScripts，nodejs的版本满天飞，之前用的 nvm 管理的方法突然不能用了。\n这里记录一下，比较新的 nvm 的安装使用方法：\n下载安装：\n1curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.0/install.sh | bash 以上的脚本安装到底做了什么呢？\n一、它 clone 了 nvm 到 $HOME/.nvm目录\n二、它在 .bashrc 中添加了三行\n事实上它是根据当前的 bash 选择在 .bash_profile、.zshrc、.profile、.bashrc中的某一个进行添加\n然后我们退出终端，并重新进入，看看\n1nvm list 2nvm ls-remote 然后选择最新的版本安装：\n1nvm install v16.13.1 如果想使用不同版本\n1nvm use 14.0.0 设置缺省版本或者取消\n1nvm alias defaut \u0026lt;your_nodejs_default_version\u0026gt; 2 3nvm alias default 14.0.0 4node -v # prints 14.0.0 参考地址：https://github.com/nvm-sh/nvm\n","date":"2021-12-17","img":"","permalink":"https://bajie.dev/posts/20211217-nodejs_nvm/","series":null,"tags":null,"title":"Nodejs多版本的安装与管理"},{"categories":null,"content":"S3 的桶存储可以用 minio 来模拟。\n网上有个 Backblaze 的网站，提供 10GB 的免费空间，且如果是公网访问，1GB流量是免费的，如果前面套了 Cloudflare 的CDN，那基本是全免了。\n说下使用方式：\n首先去 https://www.backblaze.com/ 注册个帐号：\n然后直接建个桶（Create a Bucket）\n桶文件类型选择 public，否则无法公网访问：\n然后 gen 出 master app key 来并记录好：\n随后最大的问题就来了，如果你用它页面自带的上传下载工具，彻底完蛋，拖进去的文件夹会变成扁平的，完全丧失目录结构。\n必须要找一个好用的上传工具了，推荐 b2 。\nb2 工具下载：\nhttps://github.com/Backblaze/B2_Command_Line_Tool 下载后直接改名为 b2 ，然后放到 /usr/local/bin 中\n1wget -O /usr/local/bin/b2 https://github.com/Backblaze/B2_Command_Line_Tool/releases/download/v3.1.0/b2-linux 2chmod 755 /usr/lcoal/b2 运行一下，有很多命令参数：\n详细的使用文档：\nhttps://b2-command-line-tool.readthedocs.io/en/master/ 先去配置帐号，输入applicationKeyId和applicationKey\n1b2 authorize-account 然后就可以往桶里传东西了，把当前目录下的东西传到 rendoumi 这个桶里，并且不要传 .git 目录\n1b2 sync --excludeDirRegex .git . b2://rendoumi/ 套 Cloudflare 就随意了。full ssl，建个 CNAME 且用 Page rule 做 url 转发就可以了。\n","date":"2021-12-08","img":"","permalink":"https://bajie.dev/posts/20211208-backblaze_usage/","series":null,"tags":null,"title":"Backblaze类S3免费免备案对象存储"},{"categories":null,"content":"kubernetes下定制 http 50x 以及 40x 服务器返回信息的话，如果用 Nginx 做 ingress，大家会很自然想到直接用 nginx 来定制：\n1 error_page 500 502 503 504 /50x.html; 2 location = /50x.html { 3 root /export/html; 4 } 这样做是不行的，因为 Nginx 作为 ingress 来使用，就不能落地，只能做转发；如果你落地了，访问了本地文件，就违背了初衷。而且在 ingress 的 pod 上放页面，会导致 ingress 的配置错乱。\n那么正确 50x 重定位的方法如下：首先建立一个 deploy 和对应的 svc，就是建立一个 web 服务器，能提供服务返回 50x 和 40x 的定制页面；然后在 ingress\n内指定 custom-http-errors 和 default-backend 指向它就可以了。\n一、建立miniserve的deploy和svc 我们选用 rust 的 miniserve 作为 web 服务器，准备好定制好的 index.html，写个 Dockerfile\n网址：https://github.com/svenstaro/miniserve\n1FROM alpine:3.12 2RUN apk add --update bash \u0026amp;\u0026amp; rm -rf /var/cache/apk/* 3 4COPY . /data/ 5 6RUN rm -rf /data/Dockerfile 7 8WORKDIR /data 9EXPOSE 8080 10 11CMD [\u0026#34;/data/miniserve\u0026#34;,\u0026#34;--index\u0026#34;,\u0026#34;index.html\u0026#34;] 12#CMD /bin/sh -c \u0026#34;while true; do echo hi; sleep 10; done\u0026#34; 作出镜像后推送到阿里镜像：registry.cn-shanghai.aliyuncs.com/rendoumi/miniserve:lastest\n然后准备好 deploy和 svc，部署到 kubernetes 里\n1--- 2apiVersion: apps/v1 3kind: Deployment 4metadata: 5 name: miniserve-deploy 6 labels: 7 app: miniserve 8spec: 9 replicas: 1 10 selector: 11 matchLabels: 12 app: miniserve 13 template: 14 metadata: 15 labels: 16 app: miniserve 17 spec: 18 containers: 19 - name: miniserve 20 image: registry.cn-shanghai.aliyuncs.com/rendoumi/miniserve:lastest 21 imagePullPolicy: Always 22 ports: 23 - containerPort: 8080 24--- 25apiVersion: v1 26kind: Service 27metadata: 28 name: miniserve-svc 29 labels: 30 app: miniserve 31spec: 32 ports: 33 - name: http 34 protocol: TCP 35 port: 80 36 targetPort: 8080 37 selector: 38 app: miniserve 39 type: ClusterIP 二、修改ingress 修改的地方就是 annotations 的最后两行：\n1apiVersion: networking.k8s.io/v1beta1 2kind: Ingress 3metadata: 4 name: kala-com-ingress 5 annotations: 6 kubernetes.io/ingress.class: \u0026#34;nginx\u0026#34; 7 nginx.ingress.kubernetes.io/limit-connections: \u0026#34;2\u0026#34; 8 nginx.ingress.kubernetes.io/limit-rpm: \u0026#34;2\u0026#34; 9 nginx.ingress.kubernetes.io/limit-rps: \u0026#34;1\u0026#34; 10 nginx.ingress.kubernetes.io/app-root: \u0026#34;/webui/\u0026#34; 11 nginx.ingress.kubernetes.io/auth-type: basic 12 nginx.ingress.kubernetes.io/auth-secret: kala-auth 13 nginx.ingress.kubernetes.io/auth-realm: \u0026#39;Authentication Required - Kala\u0026#39; 14 nginx.ingress.kubernetes.io/default-backend: \u0026#34;miniserve-svc\u0026#34; 15 nginx.ingress.kubernetes.io/custom-http-errors: \u0026#34;403,404,500,502,503,504\u0026#34; 16spec: 17 rules: 18 - host: kala.rendoumi.com 19 http: 20 paths: 21 - path: / 22 backend: 23 serviceName: kala-svc 24 servicePort: 80 测试的话用 wrk 压住，然后curl再测就503转到miniserve-svc\n1wrk -c 3 -t 3 -d 10 http://kala.rendoumi.com/webui/ --latency 2 3curl -v -H \u0026#34;Host: kala.rendoumi.com\u0026#34; http://kala.rendoumi.com/webui/ ","date":"2021-11-29","img":"","permalink":"https://bajie.dev/posts/20211129-kubernetes_custom_503/","series":null,"tags":null,"title":"Kubernetes下定制服务器503以及其他的403消息"},{"categories":null,"content":"在 kubernetes 中，ingress 负责转发、融断和限流。\n我们以 Nginx ingress 为例，讨论一下这方面的问题。\n一、Nginx ingress黑白名单 这个很简单了，丢进黑名单或者白名单，在入口前拦一刀。\n我们只要在 annotations 声明即可：\n1apiVersion: networking.k8s.io/v1beta1 2kind: Ingress 3metadata: 4name: www-com-ingress 5annotations: 6 kubernetes.io/ingress.class: nginx 7 nginx.ingress.kubernetes.io/ssl-redirect: \u0026#34;true\u0026#34; 8 nginx.ingress.kubernetes.io/block-cidrs: a.b.c.d/32 9 #nginx.ingress.kubernetes.io/whitelist-source-range: a.b.c.d/32 10spec: 11tls: 12- hosts: 13 - www.huabbao.com 14 secretName: www-huabbao-com-cert 15rules: 16- host: www.huabbao.com 17 http: 18 paths: 19 - path: / 20 backend: 21 serviceName: nginx-svc 注意，多个ip的话，之间用逗号隔开（该值是逗号分隔的CIDR列表） ：\n1nginx.ingress.kubernetes.io/whitelist-source-range: \u0026#39;58.246.36.130,180.167.74.98,114.85.176.38’ 二、Nginx ingress限速 限速这个问题比较复杂，首先我们需要考虑到用户体验，如果很鲁棒性的强制限制客户端流量是10KB，那么如果一开始的加载页面如果较大，客户直接就离场了。所以首先要考虑要有初始字节量，这个量需要能让客户顺畅的打开首页。然后我们再设定请求速率，再考虑单个IP地址的并发连接量（并发量×速率=客户端实际速率），第四个再考虑每分钟或每秒的最大请求数。\n以下注释定义按顺序排列，可以设置单个客户端IP地址打开的连接的限制，可用于缓解DDoS攻击。\n一、nginx.ingress.kubernetes.io/limit-rate-after：设置初始字节量，在此之后，进一步传输将受到limit-rate速率限制。 二、nginx.ingress.kubernetes.io/limit-rate：每秒接受的请求速率（每秒字节数）。 三、nginx.ingress.kubernetes.io/limit-connections：来自单个IP地址的并发连接数。（可以建立最大连接数） 四、nginx.ingress.kubernetes.io/limit-rps：每秒可从给定IP接受的连接数。（每个源 IP 每个源 IP 每秒最大请求次数） 五、nginx.ingress.kubernetes.io/limit-rpm：每分钟可从给定IP接受的连接数。（每个源 IP 每分钟最大请求次数） 可以指定 nginx.ingress.kubernetes.io/limit-whitelist 设置从速率限制中排除的客户端IP源范围，该值也是逗号分隔的CIDR列表。\n如果在单个Ingress规则中设置了多个注释 limit-rpm，则 limit-rps 优先。\n1apiVersion: extensions/v1beta1 2kind: Ingress 3metadata: 4 name: my-ingress 5 annotations: 6 nginx.ingress.kubernetes.io/limit-rate-after: 500k 7 nginx.ingress.kubernetes.io/limit-rate: 50k 8 nginx.ingress.kubernetes.io/limit-connections: \u0026#34;3\u0026#34; 9 nginx.ingress.kubernetes.io/limit-rps: \u0026#34;1\u0026#34; 10 nginx.ingress.kubernetes.io/limit-rpm: \u0026#34;3\u0026#34; 11 nginx.ingress.kubernetes.io/limit-whitelist: \u0026#34;192.168.7.0/24\u0026#34; 12spec: 13 rules: 14 - host: my.test.com 15 http: 16 paths: 17 - path: / 18 backend: 19 serviceName: nginx 20 servicePort: 80 解释一下：\n首先设置了初始字节量是500k\n然后初始字节量用完之后，限制用户传输速率是50k/s\n随后并发连接设置为3，那么客户端如果同时打开3个连接，总速率是 50k×3=150k/s\n第四步进一步设置了每秒最大请求次数是1，下面又设置每分钟最大请求次数是3。\n第五步设置了 192.168.7.0/24 这段不受限速的影响\n设置好以后压测试一下：\n超限返回的代码是503\n1wrk -c 3 -t 3 -d 10 http://m.test.com --latency 2 3 4curl -v -H \u0026#34;Host: m.test.com\u0026#34; http://m.test.com 5\u0026lt; HTTP/1.1 503 Service Temporarily Unavailable 6\u0026lt; Date: Wed, 28 Jul 2021 04:32:03 GMT 7\u0026lt; Content-Type: text/html 8\u0026lt; Content-Length: 190 9\u0026lt; Connection: keep-alive 10HTTP/1.1 503 Service Temporarily Unavailable ","date":"2021-11-29","img":"","permalink":"https://bajie.dev/posts/20211129-kubernetes_nginx_ingress_limit/","series":null,"tags":null,"title":"Kubernetes下nginx Ingress的限制"},{"categories":null,"content":"阿里的云的 OSS 并不是完全版本的 AWS S3 兼容。\n我们如果需要用 S3 协议访问 OSS，就比较麻烦了。\n所以搭建一个 minio 来做网关，代理OSS，minio 是基本兼容S3的，所以这样曲线救国，通过 S3 协议访问 minio 来访问最后端的 OSS\n这里还有一段故事：\nMinio 中间有一版是支持 oss 的，但是后来 oss 改了协议，所以现在的最新版本 minio 反而是不支持代理 oss 的，我们必须手动作出镜像，放到镜像库里，然后阿里 ACK 再使用\n首先去下载那一版直接支持oss的\n1wget http://dl.minio.org.cn/server/minio/release/linux-amd64/archive/minio.RELEASE.2020-04-15T19-42-18Z 2 3chmod 755 minio.RELEASE.2020-04-15T19-42-18Z 这个文件比较宝贵，给个本地备份链接下载：\nminio.RELEASE.2020-04-15T19-42-18Z 然后在当前目录编辑 Dockerfile ，因为 K8S 和 OSS 同一地域，所以用 OSS 私网域名：\n1FROM alpine:3.12 2 3RUN apk add --update bash \u0026amp;\u0026amp; rm -rf /var/cache/apk/* 4 5COPY minio.RELEASE.2020-04-15T19-42-18Z /data/minio.RELEASE.2020-04-15T19-42-18Z 6 7ENV MINIO_ACCESS_KEY=LTAI5tFFTbsxxxxxuLb 8ENV MINIO_SECRET_KEY=t78PyGnHZilxxxxxdxBCjvNgtVC5Y 9 10WORKDIR /data 11EXPOSE 9000 12 13CMD [\u0026#34;/data/minio.RELEASE.2020-04-15T19-42-18Z\u0026#34;,\u0026#34;gateway\u0026#34;,\u0026#34;oss\u0026#34;,\u0026#34;http://oss-cn-shanghai-internal.aliyuncs.com\u0026#34;] 14# CMD /bin/sh -c \u0026#34;while true; do echo hi; sleep 10; done\u0026#34; 注意上面，MINIO_ACCESS_KEY 和 MINIO_SECRET_KEY 对应的是阿里云 OSS的 AccessKey ID 和 AccessKey Secret\n打开阿里云网址，新建AccessKey ID 和 AccessKey Secret ，注意这东西只能看见一次，之后再也不能明文看了，所以第一次务必保存好！！！\nhttps://ram.console.aliyun.com/manage/ak 然后还有 Dockerfile 的最后一行，八戒的习惯是保留一个死循环 shell，如果镜像 CMD 有问题，无法启动，就换成这个先启动，然后再进去调试。（经常有什么库错、链接搞不好需要修改）\ndocker build -t registry.cn-shanghai.aliyuncs.com/rendoumi/minio . 然后 push 上去\ndocker push registry.cn-shanghai.aliyuncs.com/rendoumi/minio 编写好Deployment和svc，如果想公开还可以写 ingress 向外暴露，自己公司用还是 port-forward 更安全\n1--- 2apiVersion: apps/v1 3kind: Deployment 4metadata: 5 name: minio-deploy 6 labels: 7 app: minio 8spec: 9 replicas: 1 10 selector: 11 matchLabels: 12 app: minio 13 template: 14 metadata: 15 labels: 16 app: minio 17 spec: 18 containers: 19 - name: minio 20 image: registry.cn-shanghai.aliyuncs.com/rendoumi/minio:latest 21 ports: 22 - containerPort: 9000 23--- 24apiVersion: v1 25kind: Service 26metadata: 27 name: minio-svc 28 labels: 29 app: minio 30spec: 31 ports: 32 - name: http 33 protocol: TCP 34 port: 9000 35 targetPort: 9000 36 selector: 37 app: minio 38 type: ClusterIP 然后开转发：\n1kubectl port-forward svc/minio-svc 9000:9000 \u0026amp; 用浏览器访问 http://localhost:9000 就可以了，还得输入一遍密码\n这样就可以看到 OSS 的所有桶了\nMinio的官方命令行客户端是 mc，使用方法如下：\n1wget https://dl.min.io/client/mc/release/linux-amd64/mc 2chmod 755 mc 3 4./mc alias set minio http://minio-svc:9000 MINIO_ACCESS_KEY 和 MINIO_SECRET_KEY 5 6./mc ls minio minio 跑在容器外进行 OSS 代理的方法，注意不同地方，OSS 的域名改为公网的了：\n1#!/bin/sh 2export MINIO_ACCESS_KEY=LTAI5tFFTbsxxxxxuLb 3export MINIO_SECRET_KEY=t78PyGnHZilxxxxxdxBCjvNgtVC5Y 4nohup ./minio.RELEASE.2020-04-15T19-42-18Z gateway oss http://oss-cn-shanghai.aliyuncs.com \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 \u0026amp; ","date":"2021-11-26","img":"","permalink":"https://bajie.dev/posts/20211126-kubernetes_minio_oss_gateway/","series":null,"tags":null,"title":"Kubernetes搭建minio作为阿里OSS的Gateway"},{"categories":null,"content":"本来 k8s 的 crontab 是启动一个容器来运行的，很简单，如下：\n1apiVersion: batch/v1beta1 2kind: CronJob 3metadata: 4 name: curl-cron 5spec: 6 schedule: \u0026#34;*/1 * * * *\u0026#34; 7 jobTemplate: 8 spec: 9 template: 10 spec: 11 containers: 12 - name: curl-cron 13 image: radial/busyboxplus:curl 14 imagePullPolicy: IfNotPresent 15 command: 16 - /bin/sh 17 - -c 18 - date;echo \u0026#34;run crontab\u0026#34;;curl http://www.baidu.com 19 restartPolicy: OnFailure 20 successfulJobsHistoryLimit: 2 21 failedJobsHistoryLimit: 2 上面就跑了一个 busybox 的 pod，每分钟去访问百度，然后在 stdout 输出结果\n这个没什么，注意上面文件的最后两行。限制成功以及失败 job 的 History 数量，如果不加限制，kubectl get pods 会看到无穷无尽的completed 状态的 curl-cron pod。\n分割线，部署 crontab 的 yaml 很简单。但是，现在产品部有个需求，他们要在某一天进行促销活动，大概2天，期间会流量大增，于是想应用 hpa 来伸缩 pod，活动结束后关闭 hpa。之后他们还会有不断的促销，研发不知道什么时候开始，什么时候结束；但是研发又不想产品部看到 yaml，并且随时配合部署 hpa。\n还有个问题，如果用crontab，那么活动开始运行一次，活动结束一次，最后研发还得清理删除掉这两个 crontab 来恢复正常。\n这下麻烦了，有什么页面管理 crontab 的神器，能让产品部自己运行，然后权限分离，而且能指定再未来的某天运行一次或多次，且不用手工清理就好了。\n还真的有一个，kala 就是了！！！\nKala 是一个cron管理配置工具，项目地址：\nhttps://github.com/ajvb/kala 卡拉是基于 Airbnb 的 Chronos 翻写的 Go 程序。\n它比 crontab 更好一些的是，可以指定未来某天的一次性执行任务，有个使用界面\nKala 可以执行本地的命令，也可以发起 http 的请求。\n注意，kala 的源代码里有一个地方需要修改，否则参数bolt-path不生效，我们改掉它自己编译个2进制程序出来。\n1Modify cmd/server.go 2 3switch viper.GetString(\u0026#34;jobdb\u0026#34;) { 4case \u0026#34;boltdb\u0026#34;: 5db = boltdb.GetBoltDB(viper.GetString(\u0026#34;boltpath\u0026#34;)) 6 7Change to 8db = boltdb.GetBoltDB(viper.GetString(\u0026#34;bolt-path\u0026#34;)) 9 10Run it : 11./kala serve --jobdb=boltdb --bolt-path=/data/db 我们用 Dockerfile 造一个镜像出来：\n1FROM alpine:3.12 2RUN apk add --update bash \u0026amp;\u0026amp; rm -rf /var/cache/apk/* 3 4COPY . /data/ 5 6RUN mkdir /lib64 \u0026amp;\u0026amp; \\ 7 ln -s /lib/libc.musl-x86_64.so.1 /lib64/ld-linux-x86-64.so.2 \u0026amp;\u0026amp; \\ 8 mv /data/kubectl /bin \u0026amp;\u0026amp; \\ 9 mv /data/.kube /root \u0026amp;\u0026amp; \\ 10 rm -rf /data/Dockerfile 11 12WORKDIR /data 13EXPOSE 8000 14 15CMD [\u0026#34;/data/kala\u0026#34;, \u0026#34;serve\u0026#34;,\u0026#34;--jobdb=boltdb\u0026#34;,\u0026#34;--bolt-path=/data/db\u0026#34;] 16#CMD /bin/sh -c \u0026#34;while true; do echo hi; sleep 10; done\u0026#34; 注意上面我们把 kubectl 和 .kube 配置一起放进了镜像，当然一些 yaml 和脚本也可以都打进去，然后推到阿里云镜像仓库去（注意安全问题）。\n备注 ：kala默认的端口是8000，访问 url 是 /webui\n然后我们生成一个 secret 放进 k8s\n1htpasswd -c auth kala-auth 2New password: 3Re-type new password: 4Adding password for user kala 5 6kubectl create secret generic kala-auth --from-file=kala-auth 7secret \u0026#34;kala-auth\u0026#34; created 8 9kubectl get secret kala-auth -o yaml 然后我们定义一系列的资源文件，Deployment、SVC、Ingress，其中 ingress 引用了 kala-auth 的 secret，并且指定了 app-root 是 /webui/。\n另外 kala 使用了 k8s-kala-5g 的持久化卷，务必事先准备好 pv 和 pvc，不持久化的话 pod 一重启，之前保存的配置信息就全没了。\n1--- 2apiVersion: apps/v1 3kind: Deployment 4metadata: 5 name: kala-deploy 6 labels: 7 app: kala 8spec: 9 replicas: 1 10 selector: 11 matchLabels: 12 app: kala 13 template: 14 metadata: 15 labels: 16 app: kala 17 spec: 18 containers: 19 - name: kala 20 image: registry.cn-shanghai.aliyun.com/xxx/kala:latest 21 imagePullPolicy: Always 22 ports: 23 - containerPort: 8000 24 volumeMounts: 25 - mountPath: /data/db 26 name: kala-data 27 volumes: 28 - name: kala-data 29 persistentVolumeClaim: 30 claimName: k8s-kala-5g 31--- 32apiVersion: v1 33kind: Service 34metadata: 35 name: kala-svc 36 labels: 37 app: kala 38spec: 39 ports: 40 - name: http 41 protocol: TCP 42 port: 8000 43 targetPort: 8000 44 selector: 45 app: kala 46 type: ClusterIP 47--- 48apiVersion: networking.k8s.io/v1beta1 49kind: Ingress 50metadata: 51 name: kala-com-ingress 52 annotations: 53 kubernetes.io/ingress.class: nginx 54 nginx.ingress.kubernetes.io/block-cidrs: 111.201.134.93/32 55 nginx.ingress.kubernetes.io/auth-type: basic 56 nginx.ingress.kubernetes.io/auth-secret: kala-auth 57 nginx.ingress.kubernetes.io/auth-realm: \u0026#39;Authentication Required - Kala\u0026#39; 58 nginx.ingress.kubernetes.io/app-root: \u0026#39;/webui/\u0026#39; 59spec: 60 rules: 61 - host: kala.rendoumi.com 62 http: 63 paths: 64 - path: / 65 backend: 66 serviceName: kala-svc 67 servicePort: 80 这样 kala 就可以从公网访问了，http://kala.rendoumi.com:8000/ ，且需要提供密码才能进入。\n下面说一下具体的用法，还是有一些技巧性的，登录进去后的页面：\n进入后左上角点击 Create 建立新任务\n要填写内容如下：\nJob name：起个名字，叫：增大HPA\nCommand：要运行的脚本：/data/resize.sh 35 (固定扩大ECI的脚本)\nSchedule：R0/2021-07-07T06:00:00+08:00/PT0S\n​ 这个最重要\n​ 这行的格式是这样的：Number of times to repeat/Start Datetime/Interval Between Runs\n​ R0 重复0次，即只执行一次，不重复\n​ 2021-07-06T06:00:00+8:00 时间戳，注意时区中国+8:00\n​ PT0S 无延迟0S启动\n​ 合起来就是\n​ R0/2021-07-07T06:00:00+08:00/PT0S 就是在2021年7月7日中国时间6点无任何延迟执行一次脚本 resize.sh\nReset Form 这个一定要选掉，保持空，否则下次进来上次的数据都没了，还得手动输入，麻烦 其他都不填写，然后 Create 就行了\n然后我们可以再建一个恢复的任务\n我们去界面就可以看到多了两个任务，增大HPA和恢复HPA，这样就 OK 了\n注意：JOB一旦建立，以后就可以直接从界面上手动执行，Run Manually\n这东西真是个神器，产品部的同事可以随时修改hpa和恢复，而不用通过研发部了，善莫大焉。\n","date":"2021-11-25","img":"","permalink":"https://bajie.dev/posts/20211125-kubernetes_crontab_kala/","series":null,"tags":null,"title":"替代kubernetes Crontab的神器kala"},{"categories":null,"content":"kubernetes 的动态伸缩 HPA 是非常有用的特性。\n我们的服务器托管在阿里云的 ACK 上，k8s 根据 cpu 或者 内存的使用情况，会自动伸缩关键 pod 的数量，以应对大流量的情形。而且更妙的是，动态扩展的 pod 并不是使用自己的固定服务器，而是使用阿里动态的 ECI 虚拟节点服务器，这样就真的是即开即用，用完即毁。有多大流量付多少钱，物尽其用。\n我们先明确一下概念：\nk8s 的资源指标获取是通过 api 接口来获得的，有两种 api，一种是核心指标，一种是自定义指标。\n核心指标：Core metrics，由metrics-server提供API，即 metrics.k8s.io，仅提供Node和Pod的CPU和内存使用情况。api 是 metrics.k8s.io\n自定义指标：Custom Metrics，由Prometheus Adapter提供API，即 custom.metrics.k8s.io，由此可支持任意Prometheus采集到的自定义指标。api 是 custom.metrics.k8s.io 和 external.metrics.k8s.io\n一、核心指标metrics-server 阿里的 ACK 缺省是装了 metrics-server 的，看一下，系统里有一个metrics-server\n1kubectl get pods -n kube-system 再看看 api 的核心指标能拿到什么，先看 Node 的指标：\n1kubectl get --raw \u0026#34;/apis/metrics.k8s.io\u0026#34; | jq . 2kubectl get --raw \u0026#34;/apis/metrics.k8s.io/v1beta1\u0026#34; | jq . 3kubectl get --raw \u0026#34;/apis/metrics.k8s.io/v1beta1/nodes\u0026#34; | jq . 可以看到阿里 eci 虚拟节点的 cpu 和 memory 资源。\n再看看 Pod 的指标：\n1kubectl get --raw \u0026#34;/apis/metrics.k8s.io/v1beta1/pods\u0026#34; | jq . 可以清楚得看到调度到虚拟节点上的 Pod 的 cpu 和 memory 资源使用情况。大家看到只有 cpu 和 memory，这就够了。\n给个全部使用自己 Work Node 实体节点伸缩的例子：\nphp-hpa.yaml (这里定义了平均cpu使用到达80%或者内存平均使用到200M就伸缩）：\nphp-hpa 规范了 php-deploy 如何伸缩，最小2，最大10：\n1apiVersion: autoscaling/v2beta1 2kind: HorizontalPodAutoscaler 3metadata: 4 name: php-hpa 5 namespace: default 6spec: 7 scaleTargetRef: 8 apiVersion: extensions/v1beta1 9 kind: Deployment 10 name: php-deploy 11 minReplicas: 2 12 maxReplicas: 10 13 metrics: 14 - type: Resource 15 resource: 16 name: cpu 17 targetAverageUtilization: 80 18 - type: Resource 19 resource: 20 name: memory 21 targetAverageValue: 200Mi 再给个阿里 ACK 使用 ECI 虚拟节点伸缩的例子：\n我们首先定义一个正常的 deployment ，php-deploy，这个就正常定义跟别的没区别。\n然后再定义扩展到 eci 节点的 ElasticWorload，elastic-php，这个用来控制 php-deploy 拓展到 eci 虚拟节点去\n下面是固定6个，动态24个，合计30个\n1apiVersion: autoscaling.alibabacloud.com/v1beta1 2kind: ElasticWorkload 3metadata: 4 name: elastic-php 5spec: 6 sourceTarget: 7 name: php-deploy 8 kind: Deployment 9 apiVersion: apps/v1 10 min: 0 11 max: 6 12 elasticUnit: 13 - name: virtual-kubelet 14 labels: 15 virtual-kubelet: \u0026#34;true\u0026#34; 16 alibabacloud.com/eci: \u0026#34;true\u0026#34; 17 annotations: 18 virtual-kubelet: \u0026#34;true\u0026#34; 19 nodeSelector: 20 type: \u0026#34;virtual-kubelet\u0026#34; 21 tolerations: 22 - key: \u0026#34;virtual-kubelet.io/provider\u0026#34; 23 operator: \u0026#34;Exists\u0026#34; 24 min: 0 25 max: 24 26 replicas: 30 然后定义 HPA php-hpa 来控制 elastic-php 的自动伸缩\n1apiVersion: autoscaling/v2beta2 2kind: HorizontalPodAutoscaler 3metadata: 4 name: php-hpa 5 namespace: default 6spec: 7 scaleTargetRef: 8 apiVersion: autoscaling.alibabacloud.com/v1beta1 9 kind: ElasticWorkload 10 name: elastic-php 11 minReplicas: 6 12 maxReplicas: 30 13 metrics: 14 - type: Resource 15 resource: 16 name: cpu 17 target: 18 type: Utilization 19 averageUtilization: 90 20 behavior: 21 scaleUp: 22 policies: 23 #- type: percent 24 # value: 500% 25 - type: Pods 26 value: 5 27 periodSeconds: 180 28 scaleDown: 29 policies: 30 - type: Pods 31 value: 1 32 periodSeconds: 600 上面的 ElasticWorkload 需要仔细解释一下，php-deploy 定义的 pod 副本固定是6个，这6个都是在我们自己节点上不用再付费，然后 ECI 的 pod 副本数是0个到24个，那么总体pod数量就是 6+24 = 30 个，其中 24个是可以在虚拟节点上伸缩的。而 php-hpa 定义了伸缩范围是 6-30，那就意味着平时流量小的时候用的都是自己服务器上那6个固定 pod，如果流量大了，就会扩大到 eci 虚拟节点上，虚拟节点最大量是24个，如果流量降下来了，就会缩回去，缩到自己服务器的6个pod 上去。这样可以精确控制成本。\nphp-hpa 定义的最下面，扩大的时候如果 cpu 到了 90%，那么一次性扩大5个pod，缩小的时候一个一个缩，这样避免带来流量的毛刺。\n二、自定义指标Prometheus 如上其实已经可以满足大多数要求了，但是想更进一步，比如想从 Prometheus 拿到的指标来进行 hpa 伸缩。\n那就比较麻烦了\n看上图，Prometheus Operator 通过 http 拉取 pod 的 metric 指标，Prometheus Adaptor 再拉取 Prometheus Operator 存储的数据并且暴露给 Custom API 使用。为啥要弄这二个东西呢？因为 Prometheus 采集到的 metrics 数据并不能直接给 k8s 用，因为两者数据格式不兼容，还需要另外一个组件(kube-state-metrics)，将prometheus 的 metrics 数据格式转换成 k8s API 接口能识别的格式，转换以后，因为是自定义API，所以还需要用 Kubernetes aggregator 在主 API 服务器中注册，以便其他程序直接通过 /apis/ 来访问。\n我们首先来看看如何安装这二个东西：\n首先装 Prometheus Operator，这家伙会自动装一捆东西， Pormetheus、Grafana、Alert manager等，所以最好给它单独弄一个命名空间\nprometheus-operator prometheus alertmanager node-exporter kube-state-metrics grafana 1#安装 2helm install --name prometheus --namespace monitoring stable/prometheus-operator 3 4#开个端口本地访问 prometheus 的面板，curl http://localhost:9090 5kubectl port-forward --namespace monitoring svc/prometheus-operator-prometheus 9090:9090 6 7 8#看看都有什么pod 9kubectl get pod -n monitoring 10NAME READY STATUS RESTARTS AGE 11pod/alertmanager-prometheus-operator-alertmanager-0 2/2 Running 0 98m 12pod/prometheus-operator-grafana-857dfc5fc8-vdnff 2/2 Running 0 99m 13pod/prometheus-operator-kube-state-metrics-66b4c95cd9-mz8nt 1/1 Running 0 99m 14pod/prometheus-operator-operator-56964458-8sspk 2/2 Running 0 99m 15pod/prometheus-operator-prometheus-node-exporter-dcf5p 1/1 Running 0 99m 16pod/prometheus-operator-prometheus-node-exporter-nv6ph 1/1 Running 0 99m 17pod/prometheus-prometheus-operator-prometheus-0 3/3 Running 1 98m 18 19#看看都有什么svc 20kubectl get svc -n monitoring 21NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE 22alertmanager-operated ClusterIP None \u0026lt;none\u0026gt; 9093/TCP,9094/TCP,9094/UDP 100m 23prometheus-operated ClusterIP None \u0026lt;none\u0026gt; 9090/TCP 100m 24prometheus-operator-alertmanager NodePort 10.1.238.78 \u0026lt;none\u0026gt; 9093:31765/TCP 102m 25prometheus-operator-grafana NodePort 10.1.125.228 \u0026lt;none\u0026gt; 80:30284/TCP 102m 26prometheus-operator-kube-state-metrics ClusterIP 10.1.187.129 \u0026lt;none\u0026gt; 8080/TCP 102m 27prometheus-operator-operator ClusterIP 10.1.242.61 \u0026lt;none\u0026gt; 8080/TCP,443/TCP 102m 28prometheus-operator-prometheus NodePort 10.1.156.181 \u0026lt;none\u0026gt; 9090:30268/TCP 102m 29prometheus-operator-prometheus-node-exporter ClusterIP 10.1.226.134 \u0026lt;none\u0026gt; 9100/TCP 102m 我们看到有 prometheus-operated 这个ClusterIP svc，注意 k8s 的 coredns 域名解析方式，集群的内部域名是 hbb.local，那么这个 svc 的全 hostname 就是 prometheus-operated.monitoring.svc.hbb.local，集群中可以取舍到 prometheus-operated.monitoring 或者 prometheus-operated.monitoring.svc 来访问。\n然后再装 Prometheus Adaptor，我们要根据上面的具体情况来设置 prometheus.url：\n1helm install --name prometheus-adapter stable/prometheus-adapter --set prometheus.url=\u0026#34;http://prometheus-operated.monitoring.svc\u0026#34;,prometheus.port=\u0026#34;9090\u0026#34; --set image.tag=\u0026#34;v0.4.1\u0026#34; --set rbac.create=\u0026#34;true\u0026#34; --namespace custom-metrics 访问 external 和 custom 验证一下：\n1kubectl get --raw \u0026#34;/apis/external.metrics.k8s.io/v1beta1\u0026#34; | jq 2{ 3 \u0026#34;kind\u0026#34;: \u0026#34;APIResourceList\u0026#34;, 4 \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, 5 \u0026#34;groupVersion\u0026#34;: \u0026#34;external.metrics.k8s.io/v1beta1\u0026#34;, 6 \u0026#34;resources\u0026#34;: [] 7} 8 9kubectl get --raw \u0026#34;/apis/custom.metrics.k8s.io/v1beta1\u0026#34; | jq 10{ 11 \u0026#34;kind\u0026#34;: \u0026#34;APIResourceList\u0026#34;, 12 \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, 13 \u0026#34;groupVersion\u0026#34;: \u0026#34;custom.metrics.k8s.io/v1beta1\u0026#34;, 14 \u0026#34;resources\u0026#34;: [ 15 { 16 \u0026#34;name\u0026#34;: \u0026#34;*/agent.googleapis.com|agent|api_request_count\u0026#34;, 17 \u0026#34;singularName\u0026#34;: \u0026#34;\u0026#34;, 18 \u0026#34;namespaced\u0026#34;: true, 19 \u0026#34;kind\u0026#34;: \u0026#34;MetricValueList\u0026#34;, 20 \u0026#34;verbs\u0026#34;: [ 21 \u0026#34;get\u0026#34; 22 ] 23 }, 24[...lots more metrics...] 25 { 26 \u0026#34;name\u0026#34;: \u0026#34;*/vpn.googleapis.com|tunnel_established\u0026#34;, 27 \u0026#34;singularName\u0026#34;: \u0026#34;\u0026#34;, 28 \u0026#34;namespaced\u0026#34;: true, 29 \u0026#34;kind\u0026#34;: \u0026#34;MetricValueList\u0026#34;, 30 \u0026#34;verbs\u0026#34;: [ 31 \u0026#34;get\u0026#34; 32 ] 33 } 34 ] 35} 重头戏在下面，其实 Prometheus Adaptor 从 Prometheus 拿的指标也是有限的，如果有自定义指标，或者想多拿些，就得继续拓展！！！\n最快的方式是编辑 namespace 是 custom-metrics 下的 configmap ，名字是 Prometheus-adapter，增加seriesQuery\nseriesQuery长这样子，下面是统计了所有 app=shopping-kart 的 pod 5分钟之内的变化速率总和：\n1apiVersion: v1 2kind: ConfigMap 3metadata: 4 labels: 5 app: prometheus-adapter 6 chart: prometheus-adapter-v1.2.0 7 heritage: Tiller 8 release: prometheus-adapter 9 name: prometheus-adapter 10data: 11 config.yaml: | 12- seriesQuery: \u0026#39;{app=\u0026#34;shopping-kart\u0026#34;,kubernetes_namespace!=\u0026#34;\u0026#34;,kubernetes_pod_name!=\u0026#34;\u0026#34;}\u0026#39; 13 seriesFilters: [] 14 resources: 15 overrides: 16 kubernetes_namespace: 17 resource: namespace 18 kubernetes_pod_name: 19 resource: pod 20 name: 21 matches: \u0026#34;\u0026#34; 22 as: \u0026#34;\u0026#34; 23 metricsQuery: sum(rate(\u0026lt;\u0026lt;.Series\u0026gt;\u0026gt;{\u0026lt;\u0026lt;.LabelMatchers\u0026gt;\u0026gt;}[5m])) by (\u0026lt;\u0026lt;.GroupBy\u0026gt;\u0026gt;) Adapter configmap的配置如下：\nseriesQuery tells the Prometheus Metric name to the adapter（要去prometheus拿什么指标）\nresources tells which Kubernetes resources each metric is associated with or which labels does the metric include, e.g., namespace, pod etc.（关联的资源，最常用的就是 pod 和 namespace）\nmetricsQuery is the actual Prometheus query that needs to be performed to calculate the actual values.（叠加 seriesQuery 后发送给prometheus的实际查询，用于得出最终的指标值）\nname with which the metric should be exposed to the custom metrics API（暴露给API的指标名）\n举个例子：如果我们要计算 container_network_receive_packets_total，在Prometheus UI里我们要输入以下行来查询:\nsum(rate(container_network_receive_packets_total{namespace=\u0026ldquo;default\u0026rdquo;,pod=~\u0026ldquo;php-deploy.*\u0026rdquo;}[10m])) by (pod)*\n转换成 Adapter 的 metricsQuery 就变成这样了，很难懂：\n*metricsQuery: \u0026lsquo;sum(rate(\u0026laquo;.series\u0026raquo;{\u0026laquo;.labelmatchers\u0026raquo;}10m])) by (\u0026laquo;.groupby\u0026raquo;)\u0026rsquo;\u0026lt;/.groupby\u0026gt;\u0026lt;/.labelmatchers\u0026gt;\u0026lt;/.series\u0026gt;*\n再给个例子：\n1rate(gorush_total_push_count{instance=\u0026#34;push.server.com:80\u0026#34;,job=\u0026#34;push-server\u0026#34;}[5m]) 变成 adapter 的 configmap\n1apiVersion: v1 2data: 3 config.yaml: | 4 rules: 5 - seriesQuery: \u0026#39;{__name__=~\u0026#34;gorush_total_push_count\u0026#34;}\u0026#39; 6 seriesFilters: [] 7 resources: 8 overrides: 9 namespace: 10 resource: namespace 11 pod: 12 resource: pod 13 name: 14 matches: \u0026#34;\u0026#34; 15 as: \u0026#34;gorush_push_per_second\u0026#34; 16 metricsQuery: rate(\u0026lt;\u0026lt;.Series\u0026gt;\u0026gt;{\u0026lt;\u0026lt;.LabelMatchers\u0026gt;\u0026gt;}[5m]) 修改了configmap，必须重启prometheus-adapter的pod重新加载配置！！！\n在hpa中应用的例子：\n1apiVersion: autoscaling/v2beta1 2kind: HorizontalPodAutoscaler 3metadata: 4 name: gorush-hpa 5spec: 6 scaleTargetRef: 7 apiVersion: apps/v1 8 kind: Deployment 9 name: gorush 10 minReplicas: 1 11 maxReplicas: 5 12 metrics: 13 - type: Pods 14 pods: 15 metricName: gorush_push_per_second 16 targetAverageValue: 1m 再来一个，prometheus 函数名是 myapp_client_connected：\n1apiVersion: v1 2data: 3 config.yaml: | 4 rules: 5 - seriesQuery: \u0026#39;{__name__= \u0026#34;myapp_client_connected\u0026#34;}\u0026#39; 6 seriesFilters: [] 7 resources: 8 overrides: 9 k8s_namespace: 10 resource: namespace 11 k8s_pod_name: 12 resource: pod 13 name: 14 matches: \u0026#34;myapp_client_connected\u0026#34; 15 as: \u0026#34;\u0026#34; 16 metricsQuery: \u0026lt;\u0026lt;.Series\u0026gt;\u0026gt;{\u0026lt;\u0026lt;.LabelMatchers\u0026gt;\u0026gt;,container_name!=\u0026#34;POD\u0026#34;} hpa的使用\n1apiVersion: autoscaling/v2beta1 2kind: HorizontalPodAutoscaler 3metadata: 4 name: hpa-sim 5 namespace: default 6spec: 7 scaleTargetRef: 8 apiVersion: apps/v1 9 kind: Deployment 10 name: hpa-sim 11 minReplicas: 1 12 maxReplicas: 10 13 metrics: 14 - type: Pods 15 pods: 16 metricName: myapp_client_connected 17 targetAverageValue: 20 很复杂吧。我们下面给个详细例子\n三、自定义指标全套例子 我们先定义一个 deployment，运行一个 nginx-vts 的 pod，这个镜像其实已经自己暴露出了 metric 指标\n1apiVersion: apps/v1 2kind: Deployment 3metadata: 4 name: nginx-deploy 5 annotations: 6 prometheus.io/scrape: \u0026#34;true\u0026#34; 7 prometheus.io/port: \u0026#34;80\u0026#34; 8 prometheus.io/path: \u0026#34;/status/format/prometheus\u0026#34; 9spec: 10 selector: 11 matchLabels: 12 app: nginx-deploy 13 template: 14 metadata: 15 labels: 16 app: nginx-deploy 17 spec: 18 containers: 19 - name: nginx-deploy 20 image: cnych/nginx-vts:v1.0 21 resources: 22 limits: 23 cpu: 50m 24 requests: 25 cpu: 50m 26 ports: 27 - containerPort: 80 28 name: http 然后定义个 svc，把80端口暴露出去\n1apiVersion: v1 2kind: Service 3metadata: 4 name: nginx-svc 5spec: 6 ports: 7 - port: 80 8 targetPort: 80 9 name: http 10 selector: 11 app: nginx-deploy 12 type: ClusterIP prometheus 是自动发现的，所以 annotations 就会触发 prometheus 自动开始收集这些 nginx metric指标\n集群内起个shell，访问看看\n1$ curl nginx-svc.default.svc.hbb.local/status/format/prometheus 2# HELP nginx_vts_info Nginx info 3# TYPE nginx_vts_info gauge 4nginx_vts_info{hostname=\u0026#34;nginx-deployment-65d8df7488-c578v\u0026#34;,version=\u0026#34;1.13.12\u0026#34;} 1 5# HELP nginx_vts_start_time_seconds Nginx start time 6# TYPE nginx_vts_start_time_seconds gauge 7nginx_vts_start_time_seconds 1574283147.043 8# HELP nginx_vts_main_connections Nginx connections 9# TYPE nginx_vts_main_connections gauge 10nginx_vts_main_connections{status=\u0026#34;accepted\u0026#34;} 215 11nginx_vts_main_connections{status=\u0026#34;active\u0026#34;} 4 12nginx_vts_main_connections{status=\u0026#34;handled\u0026#34;} 215 13nginx_vts_main_connections{status=\u0026#34;reading\u0026#34;} 0 14nginx_vts_main_connections{status=\u0026#34;requests\u0026#34;} 15577 15nginx_vts_main_connections{status=\u0026#34;waiting\u0026#34;} 3 16nginx_vts_main_connections{status=\u0026#34;writing\u0026#34;} 1 17# HELP nginx_vts_main_shm_usage_bytes Shared memory [ngx_http_vhost_traffic_status] info 18# TYPE nginx_vts_main_shm_usage_bytes gauge 19nginx_vts_main_shm_usage_bytes{shared=\u0026#34;max_size\u0026#34;} 1048575 20nginx_vts_main_shm_usage_bytes{shared=\u0026#34;used_size\u0026#34;} 3510 21nginx_vts_main_shm_usage_bytes{shared=\u0026#34;used_node\u0026#34;} 1 22# HELP nginx_vts_server_bytes_total The request/response bytes 23# TYPE nginx_vts_server_bytes_total counter 24# HELP nginx_vts_server_requests_total The requests counter 25# TYPE nginx_vts_server_requests_total counter 26# HELP nginx_vts_server_request_seconds_total The request processing time in seconds 27# TYPE nginx_vts_server_request_seconds_total counter 28# HELP nginx_vts_server_request_seconds The average of request processing times in seconds 29# TYPE nginx_vts_server_request_seconds gauge 30# HELP nginx_vts_server_request_duration_seconds The histogram of request processing time 31# TYPE nginx_vts_server_request_duration_seconds histogram 32# HELP nginx_vts_server_cache_total The requests cache counter 33# TYPE nginx_vts_server_cache_total counter 34nginx_vts_server_bytes_total{host=\u0026#34;_\u0026#34;,direction=\u0026#34;in\u0026#34;} 3303449 35nginx_vts_server_bytes_total{host=\u0026#34;_\u0026#34;,direction=\u0026#34;out\u0026#34;} 61641572 36nginx_vts_server_requests_total{host=\u0026#34;_\u0026#34;,code=\u0026#34;1xx\u0026#34;} 0 37nginx_vts_server_requests_total{host=\u0026#34;_\u0026#34;,code=\u0026#34;2xx\u0026#34;} 15574 38nginx_vts_server_requests_total{host=\u0026#34;_\u0026#34;,code=\u0026#34;3xx\u0026#34;} 0 39nginx_vts_server_requests_total{host=\u0026#34;_\u0026#34;,code=\u0026#34;4xx\u0026#34;} 2 40nginx_vts_server_requests_total{host=\u0026#34;_\u0026#34;,code=\u0026#34;5xx\u0026#34;} 0 41nginx_vts_server_requests_total{host=\u0026#34;_\u0026#34;,code=\u0026#34;total\u0026#34;} 15576 42nginx_vts_server_request_seconds_total{host=\u0026#34;_\u0026#34;} 0.000 43nginx_vts_server_request_seconds{host=\u0026#34;_\u0026#34;} 0.000 44nginx_vts_server_cache_total{host=\u0026#34;_\u0026#34;,status=\u0026#34;miss\u0026#34;} 0 45nginx_vts_server_cache_total{host=\u0026#34;_\u0026#34;,status=\u0026#34;bypass\u0026#34;} 0 46nginx_vts_server_cache_total{host=\u0026#34;_\u0026#34;,status=\u0026#34;expired\u0026#34;} 0 47nginx_vts_server_cache_total{host=\u0026#34;_\u0026#34;,status=\u0026#34;stale\u0026#34;} 0 48nginx_vts_server_cache_total{host=\u0026#34;_\u0026#34;,status=\u0026#34;updating\u0026#34;} 0 49nginx_vts_server_cache_total{host=\u0026#34;_\u0026#34;,status=\u0026#34;revalidated\u0026#34;} 0 50nginx_vts_server_cache_total{host=\u0026#34;_\u0026#34;,status=\u0026#34;hit\u0026#34;} 0 51nginx_vts_server_cache_total{host=\u0026#34;_\u0026#34;,status=\u0026#34;scarce\u0026#34;} 0 52nginx_vts_server_bytes_total{host=\u0026#34;*\u0026#34;,direction=\u0026#34;in\u0026#34;} 3303449 53nginx_vts_server_bytes_total{host=\u0026#34;*\u0026#34;,direction=\u0026#34;out\u0026#34;} 61641572 54nginx_vts_server_requests_total{host=\u0026#34;*\u0026#34;,code=\u0026#34;1xx\u0026#34;} 0 55nginx_vts_server_requests_total{host=\u0026#34;*\u0026#34;,code=\u0026#34;2xx\u0026#34;} 15574 56nginx_vts_server_requests_total{host=\u0026#34;*\u0026#34;,code=\u0026#34;3xx\u0026#34;} 0 57nginx_vts_server_requests_total{host=\u0026#34;*\u0026#34;,code=\u0026#34;4xx\u0026#34;} 2 58nginx_vts_server_requests_total{host=\u0026#34;*\u0026#34;,code=\u0026#34;5xx\u0026#34;} 0 59nginx_vts_server_requests_total{host=\u0026#34;*\u0026#34;,code=\u0026#34;total\u0026#34;} 15576 60nginx_vts_server_request_seconds_total{host=\u0026#34;*\u0026#34;} 0.000 61nginx_vts_server_request_seconds{host=\u0026#34;*\u0026#34;} 0.000 62nginx_vts_server_cache_total{host=\u0026#34;*\u0026#34;,status=\u0026#34;miss\u0026#34;} 0 63nginx_vts_server_cache_total{host=\u0026#34;*\u0026#34;,status=\u0026#34;bypass\u0026#34;} 0 64nginx_vts_server_cache_total{host=\u0026#34;*\u0026#34;,status=\u0026#34;expired\u0026#34;} 0 65nginx_vts_server_cache_total{host=\u0026#34;*\u0026#34;,status=\u0026#34;stale\u0026#34;} 0 66nginx_vts_server_cache_total{host=\u0026#34;*\u0026#34;,status=\u0026#34;updating\u0026#34;} 0 67nginx_vts_server_cache_total{host=\u0026#34;*\u0026#34;,status=\u0026#34;revalidated\u0026#34;} 0 68nginx_vts_server_cache_total{host=\u0026#34;*\u0026#34;,status=\u0026#34;hit\u0026#34;} 0 69nginx_vts_server_cache_total{host=\u0026#34;*\u0026#34;,status=\u0026#34;scarce\u0026#34;} 0 然后用 wrk 随机发狂发请求压一把，我们去 prometheus 的面板看看指标被收集到没有\n很疯狂啊。我们编辑 Prometheus-Adapter 的 configmap ，加上如下内容\n1rules: 2- seriesQuery: \u0026#39;nginx_vts_server_requests_total\u0026#39; 3 seriesFilters: [] 4 resources: 5 overrides: 6 kubernetes_namespace: 7 resource: namespace 8 kubernetes_pod_name: 9 resource: pod 10 name: 11 matches: \u0026#34;^(.*)_total\u0026#34; 12 as: \u0026#34;${1}_per_second\u0026#34; 13 metricsQuery: (sum(rate(\u0026lt;\u0026lt;.Series\u0026gt;\u0026gt;{\u0026lt;\u0026lt;.LabelMatchers\u0026gt;\u0026gt;}[1m])) by (\u0026lt;\u0026lt;.GroupBy\u0026gt;\u0026gt;)) 然后杀了 Prometheus-Adapter 的 Pod 让它重启重新加载配置，过段时间访问一下，看看值，是527m\n1kubectl get --raw \u0026#34;/apis/custom.metrics.k8s.io/v1beta1/namespaces/default/pods/*/nginx_vts_server_requests_per_second\u0026#34; | jq . 2{ 3 \u0026#34;kind\u0026#34;: \u0026#34;MetricValueList\u0026#34;, 4 \u0026#34;apiVersion\u0026#34;: \u0026#34;custom.metrics.k8s.io/v1beta1\u0026#34;, 5 \u0026#34;metadata\u0026#34;: { 6 \u0026#34;selfLink\u0026#34;: \u0026#34;/apis/custom.metrics.k8s.io/v1beta1/namespaces/default/pods/%2A/nginx_vts_server_requests_per_second\u0026#34; 7 }, 8 \u0026#34;items\u0026#34;: [ 9 { 10 \u0026#34;describedObject\u0026#34;: { 11 \u0026#34;kind\u0026#34;: \u0026#34;Pod\u0026#34;, 12 \u0026#34;namespace\u0026#34;: \u0026#34;default\u0026#34;, 13 \u0026#34;name\u0026#34;: \u0026#34;hpa-prom-demo-755bb56f85-lvksr\u0026#34;, 14 \u0026#34;apiVersion\u0026#34;: \u0026#34;/v1\u0026#34; 15 }, 16 \u0026#34;metricName\u0026#34;: \u0026#34;nginx_vts_server_requests_per_second\u0026#34;, 17 \u0026#34;timestamp\u0026#34;: \u0026#34;2020-04-07T09:45:45Z\u0026#34;, 18 \u0026#34;value\u0026#34;: \u0026#34;527m\u0026#34;, 19 \u0026#34;selector\u0026#34;: null 20 } 21 ] 22} ok，没问题，我们定义个hpa，根据这个指标来伸缩\n1apiVersion: autoscaling/v2beta1 2kind: HorizontalPodAutoscaler 3metadata: 4 name: nginx-hpa 5spec: 6 scaleTargetRef: 7 apiVersion: apps/v1 8 kind: Deployment 9 name: nginx-deploy 10 minReplicas: 2 11 maxReplicas: 5 12 metrics: 13 - type: Pods 14 pods: 15 metricName: nginx_vts_server_requests_per_second 16 targetAverageValue: 10 这样就好了。\n如果 pod 本身不能暴露 metric ，我们可以在 sidecar 里安装 exporter 来收集数据并暴露出去就可以了。\n","date":"2021-11-25","img":"","permalink":"https://bajie.dev/posts/20211125-kubernetes_hpa/","series":null,"tags":null,"title":"Kubernetes的hpa和自定义指标hpa"},{"categories":null,"content":"公司已经由 saltstack 全面转向了 ansible 。\n用 ansible-playbook 执行各种任务的时候，需要登录主机，就必然涉及到主机 ssh 密码的输入。\n最早我们是在 inventory 里做了定义：\n1[deqin:vars] 2ansible_ssh_common_args=\u0026#34;-o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null\u0026#34; 3host_key_checking=False 4ansible_ssh_user=\u0026#34;peadmin\u0026#34; 5ansible_ssh_pass=\u0026#34;Fuck2021!\u0026#34; 6 7[deqin] 8192.168.1.19 太直白了，所有看到这文件内容的人都会知道密码了。完全没有安全性，这样行不通啊！\n好在 ansible-vault 提供了一种方法来解决：那就是生成一个密文放进去，然后解开它必须再输入一个密码。这样看到的人也不知道实际的密码到底是什么\n具体的做法如下，首先生成 key \u0026ndash;\u0026gt; 加密字符串的键值对：\n1ansible-vault encrypt_string \u0026#39;Fuck2021!\u0026#39; --name \u0026#39;ansible_ssh_pass\u0026#39; 输入密码，会得到下面一串字符\n1ansible_ssh_pass: !vault | 2 $ANSIBLE_VAULT;1.1;AES256 3 37393235646234613332646366306233346330656666623862313339313861393239646261366237 4 6663343263363161643634653266343466356634656539650a393834663938636165336431656433 5 66333761643538623434363334316661653035313166333137373562363436613636366162353239 6 3661623733323933350a373164626131646235616361356638653733646534616163393362373135 7 6139 这个就是密文了，必须用输入的密码才能解开。\n注意：这里的键值 name 不可改变，如果你想把字符串拷贝下来，改掉 ansible_ssh_pass 的名字，改成别的，想改名引用，是不行的。\n这一大长串密文有以下两种用法：\n一、ini格式的inventory引用 最原始的 inventory.ini 内容如下：\n1[deqin:vars] 2ansible_ssh_common_args=\u0026#34;-o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null\u0026#34; 3host_key_checking=False 4ansible_ssh_user=\u0026#34;peadmin\u0026#34; 5 6[deqin] 7192.168.1.19 我们定义 playbook 文件 shenji.yml：\n1- hosts: deqin 2 become: yes 3 vars_files: 4 - pass.yml 5 vars: 6 ansible_ssh_pass: \u0026#39;{{ ansible_ssh_pass }}\u0026#39; 7 8 tasks: 9 - name: mkdirs 10 file: path=\u0026#34;{{ item }}\u0026#34; state=directory 11 with_items: 12 - \u0026#34;OS.05\u0026#34; 13 - \u0026#34;OS.06\u0026#34; 把密文放进 pass.yml 文件\n1cat \u0026lt;\u0026lt; EOF \u0026gt;\u0026gt; pass.yml 2ansible_ssh_pass: !vault | 3 $ANSIBLE_VAULT;1.1;AES256 4 37393235646234613332646366306233346330656666623862313339313861393239646261366237 5 6663343263363161643634653266343466356634656539650a393834663938636165336431656433 6 66333761643538623434363334316661653035313166333137373562363436613636366162353239 7 3661623733323933350a373164626131646235616361356638653733646534616163393362373135 8 6139 9EOF 运行 playbook：\n1ansible-playbook --ask-vault-pass -i inventory.ini shenji.yml -vvv 为什么这样呢？因为 ansible-vault 加密过的字符串是 yaml 格式的，在 ini 里无法直接引用。\n所以在 playbook 的 yaml 文件中引入它，然后再跟从 inventory.ini 中获取的变量合作一起。\n二、yaml格式的inventory引用 上面我们看到了必须间接引用才可以，为了避免掉 pass.yml 文件，那么干脆把 inventroy 用 yaml 格式来写，那不就可以了么\n如下即可：\n1cat \u0026lt;\u0026lt; EOF \u0026gt;\u0026gt; inventory.yml 2--- 3all: 4 hosts: 5 deqin: 6 ansible_host: 192.168.1.19 7 vars: 8 host_key_checking: \u0026#34;False\u0026#34; 9 ansible_ssh_common_args: \u0026#34;-o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null\u0026#34; 10 ansible_ssh_pass: !vault | 11 $ANSIBLE_VAULT;1.1;AES256 12 37393235646234613332646366306233346330656666623862313339313861393239646261366237 13 6663343263363161643634653266343466356634656539650a393834663938636165336431656433 14 66333761643538623434363334316661653035313166333137373562363436613636366162353239 15 3661623733323933350a373164626131646235616361356638653733646534616163393362373135 16 6139 17EOF 然后运行就可以了：\n1ansible-playbook --ask-vault-pass -i inventory.yml shenji.yml -vvv ","date":"2021-11-24","img":"","permalink":"https://bajie.dev/posts/20211124-ansible_vault/","series":null,"tags":null,"title":"Ansible Vault加密的使用"},{"categories":null,"content":"审计啊审计，公司使用的华为防火墙需要配置双因子登录认证，这下麻烦了。\n查了一下华为手册，支持 Radius 认证，那么没办法，最省钱的办法就是用 FreeIPA 和 FreeRadius 搭一套 OTP 双因子认证了。\n系统是 CentOS 7 ，已关闭防火墙服务，方法如下：\n一、搭建FreeIPA 首先设置 hostname\n1hostnamectl set-hostname freeipa.rendoumi.local 2 3echo \u0026#34;192.168.1.5 freeipa.rendoumi.local\u0026#34; \u0026gt;\u0026gt; /etc/hosts 然后安装 FreeIPA，注意要回答的几个问题\n不装bind，无论是 dnsmasq 或 coredns，都比 bind 轻，要装也装那两个。 server hostname 是 freeipa.rendoumi.local domian name 是 rendoumi.local realm name 是大写的 RENDOUMI.LOCAL 有两个密码，第一个是 LDAP 的密码，第二个是 IPA 的密码 1yum -y install deltarpm 2yum update 3 4yum -y install freeipa-server 5 6sysctl net.ipv6.conf.all.disable_ipv6=0 7 8ipa-server-install 9 10This program will set up the IPA Server. 11 12This includes: 13 * Configure a stand-alone CA (dogtag) for certificate management 14 * Configure the Network Time Daemon (ntpd) 15 * Create and configure an instance of Directory Server 16 * Create and configure a Kerberos Key Distribution Center (KDC) 17 * Configure Apache (httpd) 18 19To accept the default shown in brackets, press the Enter key. 20 21WARNING: conflicting time\u0026amp;date synchronization service \u0026#39;chronyd\u0026#39; will be disabled in favor of ntpd 22 23Do you want to configure integrated DNS (BIND)? [no]:no 24 25Server host name [freeipa.rendoumi.local]: 26 27Please confirm the domain name [rendoumi.local]: 28 29Please provide a realm name [RENDOUMI.LOCAL]: 30 31Directory Manager password: 32Password (confirm): 33... 34IPA admin password: 35Password (confirm): 36 37The IPA Master Server will be configured with: 38Hostname: freeipa.rendoumi.local 39IP address(es): 192.168.1.5 40Domain name: rendoumi.local 41Realm name: RENDOUMI.LOCAL 42 43Continue to configure the system with these values? [no]: yes 44 45The following operations may take some minutes to complete. 46Please wait until the prompt is returned. 47 48Configuring NTP daemon (ntpd) 49 50... 51 52Setup complete 53 54Next steps: 55 1. You must make sure these network ports are open: 56 TCP Ports: 57 * 80, 443: HTTP/HTTPS 58 * 389, 636: LDAP/LDAPS 59 * 88, 464: kerberos 60 UDP Ports: 61 * 88, 464: kerberos 62 * 123: ntp 63 64 2. You can now obtain a kerberos ticket using the command: \u0026#39;kinit admin\u0026#39; 65 This ticket will allow you to use the IPA tools (e.g., ipa user-add) 66 and the web user interface. 67 68Be sure to back up the CA certificate stored in /root/cacert.p12 69This file is required to create replicas. The password for this file is the Directory Manager password 以上，就装好了 FreeIPA，配置文件在 /etc/ipa/default.conf\n验证一下：\n1# 输入ipa密码 2kinit admin 3klist 4 5ipactl status 6# sn 输入 01 7ipa cert-show 登录： http://freeipa.rendoumi.com ，（注意你访问的机器必须能解析到这个域名）用户名 admin ，密码是上面填入的 ipa 密码，建立一个新用户\n然后给这个用户添加 OTP Token：\n缺省什么都不用填，直接选 Add：\n会蹦出来一个二维码，建议是用 FreeOTP 扫描：\n我们在手机上装上 FreeOTP 软件，扫描添加：\n这样就ok了。下次登录的时候密码就是预设密码+FreeOTP密码合在一起。中间没有加号哦\n比如预设密码是 Fuck，otp密码是762405，合在一起就是 Fuck762405，一起输入即可。\n那 FreeIPA 的部分就完成了。\n二、搭建FreeRadius 上面的部分其实是 FreeIPA 充当了用户数据库，用 LDAP 存放数据，而 Radius 需要从 IPA 拿到用户信息。\n安装：\n1yum -y install freeradius freeradius-utils freeradius-ldap freeradius-krb5 Radius 的配置都在 /etc/raddb 目录下：\n编辑 /etc/raddb/client.conf ，增加一个网段的认证，允许 172.0.0.0/8 访问\n1client localnet { 2 ipaddr = 172.0.0.0/8 3 proto = * 4 secret = Fuck2021 5 nas_type = other 6 limit { 7 max_connections = 16 8 lifetime = 0 9 idle_timeout = 30 10 } 11} 同时修改下面的 clinet localhost 部分，修改 secret，之后我们要从本地登录做测试\n1client localhost { 2 secret = ChinaBank2021 3 再修改 /etc/raddb/sites-enabled/default and /etc/raddb/sites-enabled/inner-tunnel ，支持 LDAP，有二处地方\n把\n1 # 2 # The ldap module reads passwords from the LDAP database. 3 -ldap 换成：\n1 # 2 # The ldap module reads passwords from the LDAP database. 3 ldap 4 if ((ok || updated) \u0026amp;\u0026amp; User-Password) { 5 update { 6 control:Auth-Type := ldap 7 } 8 } 把\n1# Auth-Type LDAP { 2# ldap 3# } 换成：\n1 Auth-Type LDAP { 2 ldap 3 } 然后 ldap 模块配置一下\n1ln -s /etc/raddb/mods-available/ldap /etc/raddb/mods-enabled/ 我们先用 ldapsearch 搜索一下，看看具体的 dn 信息，这里输入之前设置的 ldap 密码\n1ldapsearch -x -v -W -D \u0026#39;cn=Directory Manager\u0026#39; uid=test|grep test 2ldap_initialize( \u0026lt;DEFAULT\u0026gt; ) 3Enter LDAP Password: 4filter: uid=test 5requesting: All userApplication attributes 6memberOf: cn=test,cn=groups,cn=accounts,dc=rendoumi,dc=local 得到 cn=accounts,dc=rendoumi,dc=local\n再去修改 /etc/raddb/mods-enabled/ldap 文件，修改 server 和 base_dn 与之对应：\n1 server = \u0026#39;freeipa.rendoumi.local\u0026#39; 2 base_dn = \u0026#39;cn=accounts,dc=rendoumi,dc=local\u0026#39; 注意，上面我们没装 bind，所以必须在 /etc/hosts 存在记录，否则本地就访问不到了\n启动 radiusd 的调试模式：\n1radiusd –X 2... 3Listening on auth address * port 1812 as server default 4Listening on acct address * port 1813 as server default 5Listening on auth address :: port 1812 as server default 6Listening on acct address :: port 1813 as server default 7Listening on auth address 127.0.0.1 port 18120 as server inner-tunnel 8Opening new proxy socket \u0026#39;proxy address * port 0\u0026#39; 9Listening on proxy address * port 36752 10Ready to process requests 再开一个终端测试一下，注意，我们是从本地(127.0.0.1)发起测试的，所以对应要用到上面设置的 secret，用 admin 登录，就避免要用到 freeotp 的口令，这里 xxxxxxxx 是 admin 的密码：\n1radtest admin xxxxxxxx freeipa.rendoumi.local 1812 ChinaBank2021 2Sent Access-Request Id 57 from 0.0.0.0:45247 to 172.18.31.41:1812 length 75 3 User-Name = \u0026#34;admin\u0026#34; 4 User-Password = \u0026#34;xxxxxxxx\u0026#34; 5 NAS-IP-Address = 172.18.31.41 6 NAS-Port = 1812 7 Message-Authenticator = 0x00 8 Cleartext-Password = \u0026#34;xxxxxxxx\u0026#34; 9Received Access-Accept Id 57 from 172.18.31.41:1812 to 0.0.0.0:0 length 20 看到上面 Access-Accept 就ok了，ctrl-c 终止 radiusd 的运行，开启 radiusd 服务。\n1systemctl enable --now radiusd 然后在华为防火墙设置这个 radiusd 服务器就可以了。\n参考资料：\nhttps://www.freeipa.org/page/Using_FreeIPA_and_FreeRadius_as_a_RADIUS_based_software_token_OTP_system_with_CentOS/RedHat_7 ","date":"2021-11-23","img":"","permalink":"https://bajie.dev/posts/20211123-freeipa_freeradius/","series":null,"tags":null,"title":"使用FreeIPA和FreeRadius搭建双因子认证服务器"},{"categories":null,"content":"这个比较有意思，同事要存放 2TB 的数据，但是系统是 1.7TB 的 4 块 600G 盘组成的 Raid10。\n很明显盘空间不够了，去库房找两块 10TB 的大盘组成 Raid1 给他用好了。\n问题来了，系统是 KVM 虚机，怎样把这个 10TB 的大盘给送进虚机呢？\n这里面还真有要注意的问题：\nImportant\nGuest virtual machines should not be given write access to whole disks or block devices (for example, /dev/sdb). Guest virtual machines with access to whole block devices may be able to modify volume labels, which can be used to compromise the host physical machine system. Use partitions (for example, /dev/sdb1) or LVM volumes to prevent this issue.\n注意上面的说明，是不建议将整个硬盘送进虚机里面去的，建议是送分区进去，避免会写坏整个盘。\n所以先把盘的分区做好，格式化并挂载，以 /dev/sdb 为例\n1parted -s /dev/sdb mklabel gpt mkpart primary 0% 100% 2mkfs.xfs /dev/sdb1 3mount -t xfs /dev/sdb1 /mnt/sdb1 然后到实体机的磁盘目录\n1# cd /dev/disk/ 2# ls 3by-id by-path by-uuid 4 5# cd by-id 6# ll 7total 0 8lrwxrwxrwx 1 root root 9 3月 26 2018 scsi-36b083fe0e4f71a001c36c36716543d4d -\u0026gt; ../../sda 9lrwxrwxrwx 1 root root 10 3月 26 2018 scsi-36b083fe0e4f71a001c36c36716543d4d-part1 -\u0026gt; ../../sda1 10lrwxrwxrwx 1 root root 10 3月 26 2018 scsi-36b083fe0e4f71a001c36c36716543d4d-part2 -\u0026gt; ../../sda2 11lrwxrwxrwx 1 root root 10 3月 26 2018 scsi-36b083fe0e4f71a001c36c36716543d4d-part3 -\u0026gt; ../../sda3 12lrwxrwxrwx 1 root root 9 11月 18 16:02 scsi-36b083fe0e4f71a002928bf63ef284827 -\u0026gt; ../../sdb 13lrwxrwxrwx 1 root root 10 11月 18 16:24 scsi-36b083fe0e4f71a002928bf63ef284827-part1 -\u0026gt; ../../sdb1 14lrwxrwxrwx 1 root root 9 3月 26 2018 wwn-0x6b083fe0e4f71a001c36c36716543d4d -\u0026gt; ../../sda 15lrwxrwxrwx 1 root root 10 3月 26 2018 wwn-0x6b083fe0e4f71a001c36c36716543d4d-part1 -\u0026gt; ../../sda1 16lrwxrwxrwx 1 root root 10 3月 26 2018 wwn-0x6b083fe0e4f71a001c36c36716543d4d-part2 -\u0026gt; ../../sda2 17lrwxrwxrwx 1 root root 10 3月 26 2018 wwn-0x6b083fe0e4f71a001c36c36716543d4d-part3 -\u0026gt; ../../sda3 18lrwxrwxrwx 1 root root 9 11月 18 16:02 wwn-0x6b083fe0e4f71a002928bf63ef284827 -\u0026gt; ../../sdb 19lrwxrwxrwx 1 root root 10 11月 18 16:24 wwn-0x6b083fe0e4f71a002928bf63ef284827-part1 -\u0026gt; ../../sdb1 看中间一行，scsi-36b083fe0e4f71a002928bf63ef284827-part1 指向 /dev/sdb1，记录下来\n然后 virsh-edit 编辑 KVM 虚机文件，增加硬盘部分：\n1 \u0026lt;disk type=\u0026#39;block\u0026#39; device=\u0026#39;disk\u0026#39;\u0026gt; 2 \u0026lt;driver name=\u0026#39;qemu\u0026#39; type=\u0026#39;raw\u0026#39;/\u0026gt; 3 \u0026lt;source dev=\u0026#39;/dev/disk/by-id/scsi-36b083fe0e4f71a002928bf63ef284827-part1\u0026#39;/\u0026gt; 4 \u0026lt;target dev=\u0026#39;vda\u0026#39; bus=\u0026#39;virtio\u0026#39;/\u0026gt; 5 \u0026lt;/disk\u0026gt; 注：上面我们是使用了 by-id，当然也可以使用 by-path 和 by-uuid 来指定源盘\n重启虚机并进入查看，我们能看到这块盘盘符是 /dev/vda\n1fdisk -l 2 3Disk /dev/vda: 11755.9 GB, 11755860262912 bytes, 22960664576 sectors 4Units = sectors of 1 * 512 = 512 bytes 5Sector size (logical/physical): 512 bytes / 512 bytes 6I/O size (minimum/optimal): 512 bytes / 512 bytes 接下来就比较怪异了，这个盘分明没有分区，我们却可以直接 mount\n1mount -t xfs /dev/vda /material 然后 df -h 再看\n能认出来，正常使用就行了。\n非常古怪是吧。\n大家还要注意实体机的挂载路径是 /mnt/sdb1，虚机内的挂载路径是 /material，这两个路径都是指向同一块盘，可以共通。\n","date":"2021-11-19","img":"","permalink":"https://bajie.dev/posts/20211119-kvm_disk_passthrough/","series":null,"tags":null,"title":"KVM下附加硬盘的passthrough直通"},{"categories":null,"content":"说到 CentOS7 的紧急模式与救援模式，网上可以搜到漫天飞的帖子，说一下区别\nRESCUE 救援模式： 救援模式启动的系统没有挂载硬盘，可以将硬盘 mount 出然后拷出数据。\nEMERGENCY 紧急模式： 紧急模式启动的系统是一个最小的环境。根目录档案系统将会被挂载为仅能读取，而且将不会做任何的设定。\n当然进入的方法也很简单，进入系统的时候按 e 修改 grub 菜单参数，就可以进入不同的模式\n本文讨论的重点不是怎么进去，而是那两句命令，在紧急状态下反正我是记不住的\n1systemd.unit=rescue.target 2systemd.unit=emergency.target 都没有之前的 single 简单，也完全记不住，既然记不住，那就干脆做到菜单里好了，这才是本文的重点。\n现在都是使用 grub2 了，而不是 grub，这很重要。grub2的配置文件是 /boot/grub2/grub.cfg。\n修改 grub2 有两个工具，grub2-mkconfig 和 grubby，不要同时使用这两个工具修改，会覆盖的\ngrub2-mkconfig 会去搜索 /boot 目录下的内核文件，有多少个内核就会生成多少个启动项。那么如果是同一个内核，想修改不同的启动参数，做多个启动项就完蛋，他不能自动生成单内核的多个启动项 grubby 很灵活，可以根据当前 grub2 的配置，生成一个内核，多个不同启动参数的多个启动项。 那么我们要加进去两个只是启动参数不同，内核其实一样的启动项，用 grubby 就好了\n1grubby --add-kernel=\\$(ls -1cat /boot/vmlinuz*|grep rescue) --title=\u0026#34;RESCUE BOOT\u0026#34; --initrd=\\$(ls -1cat /boot/initramfs*|grep rescue) --args=\u0026#34;systemd.unit=rescue.target\u0026#34; --copy-default 2 3grubby --add-kernel=\\$(ls -1cat /boot/vmlinuz*|grep rescue) --title=\u0026#34;EMERGENCY BOOT\u0026#34; --initrd=\\$(ls -1cat /boot/initramfs*|grep rescue) --args=\u0026#34;systemd.unit=emergency.target\u0026#34; --copy-default 切忌我们之后不能运行\n1grub2-mkconfig -o /boot/grub2/grub.cfg 否则上面的两个启动项菜单会消失，因为 grub2-mkconfig 配置的话一个内核只能有一个启动项\ngrub2-mkconfig 也有自己的强项，如果要修改缺省的菜单超时时间，grubby 就做不到了\n1sed -i \u0026#39;/^GRUB_TIMEOUT=/s/^.*$/GRUB_TIMEOUT=10/\u0026#39; /etc/default/grub 2grub2-mkconfig -o /boot/grub2/grub.cfg ","date":"2021-11-19","img":"","permalink":"https://bajie.dev/posts/20211119-linux_rescue/","series":null,"tags":null,"title":"CentOS7的救援模式和紧急模式"},{"categories":null,"content":"说实在话，这个场景非常怪异，客户在 linux 下要动态根据 url 选择代理：\n看图，中间的是前端代理，地址是 192.168.1.1:8080，然后客户设置使用这个代理\n1export http_proxy=http://192.168.1.1:8080 2export https_proxy=http://192.168.1.1:8080 然后对应后端有三个代理，两个 http 代理，一个 socks 代理\n1http 192.168.2.1:3128 2socks 192.168.2.2:1080 3http 192.168.2.3:3128 我们要根据客户的请求 URL 来决定具体要使用后端的哪个代理\n这个如果在浏览器上设置非常容易，设置 PAC 即可。但是偏偏客户端不是浏览器，而是一个程序，那么麻烦就来了。怎么设置呢？\n步骤很简单：\n一、安装 pacproxy 网址：https://github.com/williambailey/pacproxy\n1wget https://github.com/williambailey/pacproxy/releases/download/v.2.0.4/pacproxy_2.0.4_linux_amd64.tar.gz 2tar zxvf pacproxy_2.0.4_linux_amd64.tar.gz 拷出来 pacproxy 备用\n二、生成配置文件 最主要的就是 PAC 文件的生成\n我们给个例子：\n1cat \u0026lt;\u0026lt; EOF \u0026gt;\u0026gt; pac 2function FindProxyForURL(url, host) 3{ 4 if (host == \u0026#34;www.baidu.com\u0026#34;) { 5 return \u0026#34;PROXY 192.168.2.1:3128\u0026#34;; 6 } 7 else if (host == \u0026#34;www.sina.com.cn\u0026#34;) { 8 return \u0026#34;SOCKS 192.168.2.2:1080\u0026#34;; 9 } 10 else if (host == \u0026#34;www.sohu.com\u0026#34;) { 11 return \u0026#34;SOCKS 192.168.2.3:1080\u0026#34;; 12 } 13 else { 14 return \u0026#34;DIRECT\u0026#34;; 15 } 16} 17EOF 其实 PAC 文件的内容就是一段 javascript，用来返回代理的地址\n运行并测试：\n1pacproxy -c pac -l 0.0.0.0:8080 -v 2 3 4export http_proxy=http://192.168.1.1:8080 5export https_proxy=http://192.168.1.1:8080 6curl -I www.baidu.com 这样就在 Linux 搭建了一个动态的 PAC 代理\nPAC文件的参考资料：\nhttps://web.archive.org/web/20070602031929/http://wp.netscape.com/eng/mozilla/2.0/relnotes/demo/proxy-live.html https://developer.mozilla.org/zh-CN/docs/Web/HTTP/Proxy_servers_and_tunneling/Proxy_Auto-Configuration_PAC_file https://www.websense.com/content/support/library/web/v76/pac_file_best_practices/PAC_file_sample.aspx ","date":"2021-11-16","img":"","permalink":"https://bajie.dev/posts/20211116-pacproxy/","series":null,"tags":null,"title":"Linux下web Pacproxy的用法"},{"categories":null,"content":"Tomcat 是一个 HTTP server。同时，它也是一个 serverlet 容器，可以执行 java 的 Servlet，也可以把 JavaServer Pages（JSP）和 JavaServerFaces（JSF）编译成 Java Servlet。它的模型图如下：\n给个生产环境 server.xml 的例子：\n1\u0026lt;?xml version=\u0026#39;1.0\u0026#39; encoding=\u0026#39;utf-8\u0026#39;?\u0026gt; 2\u0026lt;Server port=\u0026#34;-1\u0026#34; shutdown=\u0026#34;SHUTDOWN\u0026#34;\u0026gt; 3 \u0026lt;Listener className=\u0026#34;org.apache.catalina.startup.VersionLoggerListener\u0026#34; /\u0026gt; 4 \u0026lt;Listener className=\u0026#34;org.apache.catalina.core.AprLifecycleListener\u0026#34; SSLEngine=\u0026#34;on\u0026#34; /\u0026gt; 5 \u0026lt;Listener className=\u0026#34;org.apache.catalina.core.JreMemoryLeakPreventionListener\u0026#34; /\u0026gt; 6 \u0026lt;Listener className=\u0026#34;org.apache.catalina.mbeans.GlobalResourcesLifecycleListener\u0026#34; /\u0026gt; 7 \u0026lt;Listener className=\u0026#34;org.apache.catalina.core.ThreadLocalLeakPreventionListener\u0026#34; /\u0026gt; 8 9 \u0026lt;GlobalNamingResources\u0026gt; 10 \u0026lt;Resource name=\u0026#34;UserDatabase\u0026#34; auth=\u0026#34;Container\u0026#34; 11 type=\u0026#34;org.apache.catalina.UserDatabase\u0026#34; 12 description=\u0026#34;User database that can be updated and saved\u0026#34; 13 factory=\u0026#34;org.apache.catalina.users.MemoryUserDatabaseFactory\u0026#34; 14 pathname=\u0026#34;conf/tomcat-users.xml\u0026#34; /\u0026gt; 15 \u0026lt;/GlobalNamingResources\u0026gt; 16 17 \u0026lt;Service name=\u0026#34;Catalina\u0026#34;\u0026gt; 18 \u0026lt;Connector port=\u0026#34;8080\u0026#34; 19 server=\u0026#34;www.rendoumi.com\u0026#34; 20 protocol=\u0026#34;org.apache.coyote.http11.Http11NioProtocol\u0026#34; 21 maxHttpHeaderSize=\u0026#34;8192\u0026#34; 22 acceptCount=\u0026#34;500\u0026#34; 23 maxThreads=\u0026#34;1000\u0026#34; 24 minSpareThreads=\u0026#34;200\u0026#34; 25 enableLookups=\u0026#34;false\u0026#34; 26 redirectPort=\u0026#34;8443\u0026#34; 27 connectionTimeout=\u0026#34;20000\u0026#34; 28 relaxedQueryChars=\u0026#34;[]|{}@!$*()+\u0026#39;.,;^\\`\u0026amp;quot;\u0026amp;lt;\u0026amp;gt;\u0026#34; 29 disableUploadTimeout=\u0026#34;true\u0026#34; 30 allowTrace=\u0026#34;false\u0026#34; 31 URIEncoding=\u0026#34;UTF-8\u0026#34; 32 useBodyEncodingForURI=\u0026#34;true\u0026#34; /\u0026gt; 33 34 \u0026lt;Engine name=\u0026#34;Catalina\u0026#34; defaultHost=\u0026#34;localhost\u0026#34;\u0026gt; 35 \u0026lt;Realm className=\u0026#34;org.apache.catalina.realm.LockOutRealm\u0026#34;\u0026gt; 36 \u0026lt;Realm className=\u0026#34;org.apache.catalina.realm.UserDatabaseRealm\u0026#34; 37 resourceName=\u0026#34;UserDatabase\u0026#34;/\u0026gt; 38 \u0026lt;/Realm\u0026gt; 39 \u0026lt;Host name=\u0026#34;localhost\u0026#34; appBase=\u0026#34;webapps\u0026#34; unpackWARs=\u0026#34;false\u0026#34; autoDeploy=\u0026#34;false\u0026#34;\u0026gt; 40 \u0026lt;Valve className=\u0026#34;org.apache.catalina.valves.AccessLogValve\u0026#34; directory=\u0026#34;logs\u0026#34; prefix=\u0026#34;access_log.\u0026#34; suffix=\u0026#34;.txt\u0026#34; 41 fileDateFormat=\u0026#34;yyyy-MM-dd\u0026#34; 42 pattern=\u0026#34;%a|%A|%T|%{X-Forwarded-For}i|%l|%u|%t|%r|%s|%b|%{Referer}i|%{User-Agent}i \u0026#34; resolveHosts=\u0026#34;false\u0026#34;/\u0026gt; 43 \u0026lt;Context path=\u0026#34;\u0026#34; docBase=\u0026#34;/export/servers/tomcat/webapps/web\u0026#34; /\u0026gt; 44 \u0026lt;/Host\u0026gt; 45 46 \u0026lt;/Engine\u0026gt; 47 \u0026lt;/Service\u0026gt; 48\u0026lt;/Server\u0026gt; 分解开来一部分一部分的看：\nServer Server 第2行，是最大且必须唯一的组件。表示一个 Tomcat 的实例，它可以包含多个 Services，而每个 Service 都可以有自己的 Engine 和 connectors。\n1\u0026lt;Server port=\u0026#34;8005\u0026#34; shutdown=\u0026#34;SHUTDOWN\u0026#34;\u0026gt; ...... \u0026lt;/Server\u0026gt; 注意上面，我们把缺省的 port=\u0026ldquo;8005\u0026rdquo; 改成了 “-1”，这样防止从8005重启，避免安全问题。\nListeners 一个 Server 容器拥有若干 Listeners (行 3-7)。一个 Listener 监听并回应特定的事件.\n例如 GlobalResourcesLifecycleListener 就设置了 global resources，这样就可以使用 JNDI 来存取像数据库这种资源。\n1\u0026lt;Listener className=\u0026#34;org.apache.catalina.mbeans.GlobalResourcesLifecycleListener\u0026#34; /\u0026gt; Global Naming Resources \u0026lt;GlobalNamingResources\u0026gt; (行 9-15) 定义了 JNDI (Java Naming and Directory Interface) 资源，这个是 LDAP 的东西，允许 java 软件通过目录发现对象和属性值。\n缺省定义了一个 UserDatabase 的 JNDI 资源（行 10-14），这个对象其实是一个内存数据库，保存着从 conf/tomcat-users.xml 中加载上来的用户认证数据。\n1\u0026lt;GlobalNamingResources\u0026gt; 2 \u0026lt;Resource name=\u0026#34;UserDatabase\u0026#34; auth=\u0026#34;Container\u0026#34; 3 type=\u0026#34;org.apache.catalina.UserDatabase\u0026#34; 4 description=\u0026#34;User database that can be updated and saved\u0026#34; 5 factory=\u0026#34;org.apache.catalina.users.MemoryUserDatabaseFactory\u0026#34; 6 pathname=\u0026#34;conf/tomcat-users.xml\u0026#34; /\u0026gt; 7\u0026lt;/GlobalNamingResources\u0026gt; 我们可以再定义一个 Mysql 之类的 JNDI 来保存 mysql 数据库连接信息，用来实现连接池\nServices Service 用来关联多个 Connectors 到 Engine。缺省的配置是配了一个 叫做 Catalina 的 Service ，Catalinna 缺省配了两个 Connectors，一个是8080的HTTP，一个是8009的AJP。\n1\u0026lt;Service name=\u0026#34;Catalina\u0026#34;\u0026gt; 2 \u0026lt;Connector port=\u0026#34;8080\u0026#34; protocol=\u0026#34;HTTP/1.1\u0026#34; 3 connectionTimeout=\u0026#34;20000\u0026#34; 4 redirectPort=\u0026#34;8443\u0026#34; /\u0026gt; 5 \u0026lt;Connector port=\u0026#34;8009\u0026#34; protocol=\u0026#34;AJP/1.3\u0026#34; redirectPort=\u0026#34;8443\u0026#34; /\u0026gt; 6\u0026lt;/Service\u0026gt; 注意最上面的配置，我们取消了8009的AJP，这个协议现在基本没人用，取消掉也避免安全问题。\n下面给出一个 SSL 8443 的 connectors 的例子\n1 \u0026lt;Connector port=\u0026#34;8443\u0026#34; maxHttpHeaderSize=\u0026#34;8192\u0026#34; SSLEnabled=\u0026#34;true\u0026#34; server=\u0026#34;Rendoumi\u0026#34; 2 maxThreads=\u0026#34;150\u0026#34; minSpareThreads=\u0026#34;25\u0026#34; maxSpareThreads=\u0026#34;75\u0026#34; 3 enableLookups=\u0026#34;false\u0026#34; disableUploadTimeout=\u0026#34;true\u0026#34; 4 acceptCount=\u0026#34;100\u0026#34; scheme=\u0026#34;https\u0026#34; secure=\u0026#34;true\u0026#34; clientAuth=\u0026#34;false\u0026#34; 5 URIEncoding=\u0026#34;UTF-8\u0026#34; 6 sslProtocol=\u0026#34;SSL\u0026#34; 7 ciphers=\u0026#34;SSL_RSA_WITH_RC4_128_MD5, SSL_RSA_WITH_RC4_128_SHA\u0026#34; 8 keystoreFile=\u0026#34;/export/servers/tomcat/rendoumi.keystore\u0026#34; 9 keystorePass=\u0026#34;Fuck2020\u0026#34; /\u0026gt; Containers 概念 Tomcat 的容器概念，Tomcat认为 Engine, Host, Context 和 Cluster处在同一个容器中. 最顶层的是 Engine; 最底层是Context。\n另外像 Realm 和 Valve，也可以放在容器中\nEngine Engine 是容器的顶端，可以包含有若干 Hosts。我们可以配置 tomcat 服务多个虚拟主机。下面配置就是服务本机的一个服务。\n1\u0026lt;Engine name=\u0026#34;Catalina\u0026#34; defaultHost=\u0026#34;localhost\u0026#34;\u0026gt; Catalina Engine 从 HTTP connector 处接收到客户端的 HTTP 请求，根据请求头中 Host: xxx 的内容，把请求路由到某个具体的 Host上。\nRealm Realm 本质是一个数据库，是用来保存用户名、密码、认证角色的东西。\n1\u0026lt;Realm className=\u0026#34;org.apache.catalina.realm.LockOutRealm\u0026#34;\u0026gt; 2 \u0026lt;Realm className=\u0026#34;org.apache.catalina.realm.UserDatabaseRealm\u0026#34; resourceName=\u0026#34;UserDatabase\u0026#34;/\u0026gt; 3\u0026lt;/Realm\u0026gt; Hosts Host 定义了虚拟主机，可以定义多个虚拟主机。其中 appBase 千万不能为空，否则就可以读到 tomcat 的所有文件，就出线上事故大 Bug 了！！！\n1\u0026lt;Host name=\u0026#34;localhost\u0026#34; appBase=\u0026#34;webapps\u0026#34; unpackWARs=\u0026#34;true\u0026#34; autoDeploy=\u0026#34;true\u0026#34;\u0026gt; 我们最上面的生产配置，不自解压，也不自动部署。\nValve Valve 是用来拦截 HTTP requests的，做预处理后再交给下一个组件，它能在 Engine, Host, Context 中被定义。下面的例子就是在 Engine 中做了拦截，定义了日志格式，交给下一个日志处理组件做处理。\n缺省配置\n1\u0026lt;Valve className=\u0026#34;org.apache.catalina.valves.AccessLogValve\u0026#34; directory=\u0026#34;logs\u0026#34; 2 prefix=\u0026#34;localhost_access_log.\u0026#34; suffix=\u0026#34;.txt\u0026#34; 3 pattern=\u0026#34;%h %l %u %t \u0026amp;quot;%r\u0026amp;quot; %s %b\u0026#34; /\u0026gt; 我们的配置修改了缺省的日志格式，多了很多明细的内容，改变了分割符：\n1\u0026lt;Valve className=\u0026#34;org.apache.catalina.valves.AccessLogValve\u0026#34; directory=\u0026#34;logs\u0026#34; 2 prefix=\u0026#34;access_log.\u0026#34; suffix=\u0026#34;.txt\u0026#34; 3 fileDateFormat=\u0026#34;yyyy-MM-dd\u0026#34; 4 pattern=\u0026#34;%a|%A|%T|%{X-Forwarded-For}i|%l|%u|%t|%r|%s|%b|%{Referer}i|%{User-Agent}i \u0026#34; resolveHosts=\u0026#34;false\u0026#34;/\u0026gt; Context Context定义了具体的访问路径，一定要指定 docBase ！！！大家一定要记住 appBase 和 docBase 不同为空！！！\n1\u0026lt;Context path=\u0026#34;\u0026#34; docBase=\u0026#34;/export/servers/tomcat/webapps/web\u0026#34; /\u0026gt; 如上，我们就配好了一个生产环境的 server.xml ，注意，这个 xml 文件是可以放在解开的tomcat压缩包 apache-tomcat-8.5.64 目录之外的。\n我们再另外编写一个启动文件 start.sh：\n1#!/bin/sh 2export JAVA_OPTS=\u0026#34;-server -Xms1024m -Xmx2048m -Djava.awt.headless=true -Dsun.net.client.defaultConnectTimeout=60000 -Dsun.net.client.defaultReadTimeout=60000 -Djmagick.systemclassloader=no -Dnetworkaddress.cache.ttl=300 -Dsun.net.inetaddr.ttl=300\u0026#34; 3export CATALINA_HOME=/export/servers/tomcat/apache-tomcat-8.5.64 4 5cd /export/servers/tomcat/ 6$CATALINA_HOME/bin/catalina.sh start -config \u0026#34;/export/servers/tomcat/server.xml\u0026#34; 这样，我们的启动文件和配置文件，就和 tomcat 目录分离了，这样做的好处是如果 tomcat 需要升级，那么直接解压换目录即可。\n我们可以做个软链接 tomcat 链接到 apache-tomcat-8.5.64，这样更方便操作。\n","date":"2021-11-15","img":"","permalink":"https://bajie.dev/posts/20211115-tomcat_config/","series":null,"tags":null,"title":"Tomcat Server.xml配置详细解释"},{"categories":null,"content":"公司用了6年的GlusterFS终于到了要调整卷参数的地步了。\n小文件已经多到要影响 IO 的地步了。\n首先说一下结论，GlusterFS 安装完成后，基本不需要调整任何参数。生产系统千万不可盲目！\n然后我们这是有特殊情况，所以调优步骤如下（以卷名为 esign-vol 为例）：\n1#必须关掉NFS 2gluster volume set esign-vol nfs.disable on 3 4#必须保留10%的空间，避免塞爆卷空间 5gluster volume set esign-vol cluster.min-free-disk 10% 6 7#本机有256G的内存，所以设置25G的读缓存 8gluster volume set esign-vol performance.cache-size 25GB 9#读缓存中，单个文件的缓存，最大文件size是128MB，大于128MB的单个文件不缓存 10gluster volume set esign-vol performance.cache-max-file-size 128MB 11 12#设置每个客户端都允许多线程，缺省是2，多个小文件增加为4 13gluster volume set esign-vol client.event-threads 4 14#设置服务器端对特定的卷允许多线程，缺省是1,多个小文件增加为4 15gluster volume set esign-vol server.event-threads 4 16 17#分割线，以下参数不要调整，除非明确知道后果 18 19#设置 io 线程数量，这个值缺省是16，已经很大了，足够用 20gluster volume set esign-vol performance.io-thread-count 16 21#设置写缓冲区，这个值缺省是1M，弄大了如果停电什么的，会丢数据 22gluster volume set esign-vol performance.write-behind-window-size: 1M 以上就可以了。还有个参数 global-threading ，缺省是 off，不要设置为 on，有使用条件的，弄错了反而会导致性能降低。\n","date":"2021-11-15","img":"","permalink":"https://bajie.dev/posts/20211115-gluster_tuning/","series":null,"tags":null,"title":"GlusterFS文件系统的优化"},{"categories":null,"content":"在实际中遇到这样一个问题，公司软件发布上线自动化。\n说简单点，就是需要去登录一个上线的内部网站，然后爬下所有的上线数据。\n然后根据爬下来的数据整理好，可以一起上线的，就并发多线程，其实就是去传参数点击一个链接等返回。\n不能并发的就单线程点链接。\n那这个事情必须更有效率，单线程的没问题，用 python 的 request 就可以实现了。\n我们仔细研究一下协程，先讲一下历史：\n使用Python的人往往纠结在多线程、多进程，哪个效率更高？到底用哪个好呢？\n其实 Python 的多进程和多线程，相对于别家的协程和异步处理机制，都不行，线程之间切换耗费 CPU 和寄存器，OS 的调度不可控，多进程之间通讯也不便。性能根本不行。\n后来呢 Python 改进了语法，出现了 yiled from 充当协程调度，有人就根据这个特性开发了第三方的协程框架，Tornado，Gevent等。\n官方也不能坐视不理啊，任凭别人出风头，于是 Python 之父深入简出3年，苦心钻研自家的协程，async/await 和 asyncio 库，并放到 Python3.5 后成为官方原生的协程。\n对于 http请求、读写文件、读写数据库这种高延时的 IO 操作，协程是个大杀器，优点非常多；它可以在预料到一个阻塞将发生时，挂起当前协程，跑去执行其它协程，同时把事件注册到循环中，实现了多协程并发，其实这玩意是跟 Nodejs 的回调学的。\n看下图，详细解释下，左边我们有100个网页请求，并发100个协程请求（其实也是1个1个发），当需要等待长时间回应回应时，挂起当前协程，并注册一个回调函数到事件循环（Event Loop）中，执行下一个协程，当有协程事件完成再通过回调函数唤醒挂起的协程，然后返回结果。\n这个跟 nodejs 的回调函数基本一样，我们必须注意主进程和协程的关系，如果我在一个主进程中，触发协程函数，有100个协程，那么必须等待100个协程都结束后，才能回到正常的那个主进程中。当然，主进程也可能也是一个协程。\n那么协程的基本用法\nasync f(n) 声明一个函数是协程的\nawait f(n) 挂起当前协程，把控制权交回 event loop，并且执行f(n)和注册之后的f(n)回调。\n举个例子：如果在 g() 这个函数中执行了 await f()，那么g()函数会被挂起，并等待 f() 函数有结果结束，然后返回 g() 继续执行。\n1async def get(url): 2 async with aiohttp.ClientSession() as session: 3 async with session.get(url) as response: 4 return await response.text() 最后一行 await 是挂起命令，挂起当前函数 get() ，并执行 response.text() 和注册回调，等待 response.text() 执行完成后重新激活当前函數get()继续执行，返回。\n所以 await 只叫做挂起是不太对的，感觉应该叫做 挂起并注册回调 比较合适。\n看以下程序，在 Python 3.7 之前，协程是这么用的：\n1import time 2import asyncio 3 4now = lambda : time.time() 5 6async def do_some_work(x): 7 print(\u0026#39;Waiting: \u0026#39;, x) 8 9start = now() 10coroutine = do_some_work(2) 11loop = asyncio.get_event_loop() 12loop.run_until_complete(coroutine) 13print(\u0026#39;TIME: \u0026#39;, now() - start) 我们指定了一个协程 coroutine ，然后定义了一个事件循环 loop，loop 是需要 run_until_complete 所有的协程，然后交出控制权，返回正常的主进程。\n跟上图完全匹配。\n在 Python 3.7 之后，简化了用法，一句 asyncio.run 就可以了：\n1asyncio.run(do_some_work(2)) 上面程序就变了，省了好多，但是副作用是第一次看到的人会不明白它是怎么进化过来的：\n1import time 2import asyncio 3 4now = lambda : time.time() 5 6async def do_some_work(x): 7 print(\u0026#39;Waiting: \u0026#39;, x) 8 9start = now() 10asyncio.run(do_some_work(2)) 11print(\u0026#39;TIME: \u0026#39;, now() - start) 如果我们要访问一个网站的100个网页，单线程的做法是：请求一次，回来一次，然后进行下一个\n1for url in urls： 2 response=get(url) 3 results=parse(response) 这样效率很低，协程呢，做法就不同了，一次发起100个请求（准确的说也是一个一个发），不同的是协程不会死等返回，而是发一个请求，挂起，再发一个再挂起，发起100个，就挂起100个，然后注册并等待100个返回，效率提升了100倍。可以理解为同时做了100件事，做到由自己调度而不是交给CPU，程序的并发由自己来控制，而不是交由 OS 去调度，效率极大的提高了。\n进化到协程，我们把费 IO 的 get 函数抽出来放到协程里：\n1async def get(url:str): 2 my_conn = aiohttp.TCPConnector(limit=10) 3 async with aiohttp.ClientSession(connector=my_conn) as session: 4 async with session.get(url) as resp: 5 return await resp.text() 6 7 8for url in urls： 9 response=asyncio.run(get(url)) 10 results=parse(response) 11 具体到我们的项目，我们首先要登录一个网页拿到 cookie，这个过程其实就一个协程，没人会登录个几百次吧。然后把放了 cookie 的 session 取出来，供后面的协程再复用就可以了，示例代码如下：\n1import aiohttp 2import asyncio 3 4async def login(): 5 my_conn = aiohttp.TCPConnector(limit=10) 6 async with aiohttp.ClientSession(connector=my_conn) as session: 7 data = {\u0026#39;loginname\u0026#39;:\u0026#39;wangbadan\u0026#39;,\u0026#39;password\u0026#39;:\u0026#39;Fuckyouall\u0026#39;} 8 async with session.post(\u0026#39;http://192.168.1.3/user/login\u0026#39;,data=data) as resp: 9 print(resp.url) 10 print(resp.status) 11 print(await resp.text()) 12 return session 13 14session = asyncio.run(login()) 15print(f\u0026#34;{session}\u0026#34;) 再给一个完全版的主函数是进程，下载是协程的例子，注意里面的 aiohttp.TCPConnector(limit=10)，限制一下并发是10个，否则会被服务器 Ban 掉：\n1import asyncio 2import time 3import aiohttp 4from aiohttp.client import ClientSession 5 6async def download_link(url:str,session:ClientSession): 7 async with session.get(url) as response: 8 result = await response.text() 9 print(f\u0026#39;Read {len(result)} from {url}\u0026#39;) 10 11async def download_all(urls:list): 12 my_conn = aiohttp.TCPConnector(limit=10) 13 async with aiohttp.ClientSession(connector=my_conn) as session: 14 tasks = [] 15 for url in urls: 16 task = asyncio.ensure_future(download_link(url=url,session=session)) 17 tasks.append(task) 18 await asyncio.gather(*tasks,return_exceptions=True) # the await must be nest inside of the session 19 20url_list = [\u0026#34;https://www.google.com\u0026#34;,\u0026#34;https://www.bing.com\u0026#34;]*50 21print(url_list) 22start = time.time() 23asyncio.run(download_all(url_list)) 24end = time.time() 25print(f\u0026#39;download {len(url_list)} links in {end - start} seconds\u0026#39;) 协程里的 session 也有很多种用法，参考下面的链接就好：\nhttps://blog.csdn.net/weixin_39643613/article/details/109171090 我们也给出简单易用的线程池版，说不定以后会用上：\n1import requests 2from requests.sessions import Session 3import time 4from concurrent.futures import ThreadPoolExecutor 5from threading import Thread,local 6 7url_list = [\u0026#34;https://www.google.com/\u0026#34;,\u0026#34;https://www.bing.com\u0026#34;]*50 8thread_local = local() 9 10def get_session() -\u0026gt; Session: 11 if not hasattr(thread_local,\u0026#39;session\u0026#39;): 12 thread_local.session = requests.Session() 13 return thread_local.session 14 15def download_link(url:str): 16 session = get_session() 17 with session.get(url) as response: 18 print(f\u0026#39;Read {len(response.content)} from {url}\u0026#39;) 19 20def download_all(urls:list) -\u0026gt; None: 21 with ThreadPoolExecutor(max_workers=10) as executor: 22 executor.map(download_link,url_list) 23 24start = time.time() 25download_all(url_list) 26end = time.time() 27print(f\u0026#39;download {len(url_list)} links in {end - start} seconds\u0026#39;) ","date":"2021-11-12","img":"","permalink":"https://bajie.dev/posts/20211112-python_aiohttp/","series":null,"tags":null,"title":"Python的协程详细解释"},{"categories":null,"content":"我们介绍了如何在 kubernetes 环境中使用 filebeat sidecar 方式收集日志\n使用的是 filebeat 的 moudle 模块，但凡是常用的软件，基本都有对应的模块可用，所以我们首先应该使用模块来收集日志。\n那对于一些我们自己写的 Go 软件呢，或者根本不是标准的，那该怎么办呢？\n那就自定义 filebeat.inputs，网上的 filebeat 例子，其实大多都是这样的\n很简单，给个例子\n1[beat-logstash-some-name-832-2015.11.28] IndexNotFoundException[no such index] 2 at org.elasticsearch.cluster.metadata.(IndexNameExpressionResolver.java:566) 3 at org.elasticsearch.cluster.metadata.(IndexNameExpressionResolver.java:133) 4 at org.elasticsearch.cluster.metadata.(IndexNameExpressionResolver.java:77) 5 at org.elasticsearch.action.admin..checkBlock(TransportDeleteIndexAction.java:75) 看如上日志，如果不用模式匹配的话，那么会送5条记录到 ES， Kibana 看起来就十分割裂了。\n所以要用正则把底下的4行和上面的第1行合在一起，合并成一条就记录推送到 ES 去\n仔细观察，开头一行是以 [ 开始的，所以正则就是 \u0026lsquo;^\\[\u0026rsquo;，我们要匹配的是底下的4行，必须反转一下模式。\nmultiline 的 negate 属性，这个是指定是否反转匹配到的内容，这里如果是 true 的话，反转，那就是选择不以 [ 开头的行。 netgate 属性的缺省值是 false\nmultiline 的 match 属性，after 指追加到上一条事件（向上合并），before 指合并到下一条（向下合并）\n于是，我们就可以写出以下的匹配模式，这样就把匹配到条目追加到上一条，合并在一起了：\n1filebeat.inputs: 2 - type: log 3 enabled: true 4 paths: 5 - /Users/liuxg/data/multiline/multiline.log 6 multiline.type: pattern 7 multiline.pattern: \u0026#39;^\\[\u0026#39; 8 multiline.negate: true 9 multiline.match: after 10 11output.elasticsearch: 12 hosts: [\u0026#34;localhost:9200\u0026#34;] 13 index: \u0026#34;multiline\u0026#34; 再给个例子，java 的 stack 调用，日志格式如下：\n1Exception in thread \u0026#34;main\u0026#34; java.lang.NullPointerException 2 at com.example.myproject.Book.getTitle(Book.java:16) 3 at com.example.myproject.Author.getBookTitles(Author.java:25) 4 at com.example.myproject.Bootstrap.main(Bootstrap.java:14) 分析一下：\n匹配到开头是空格的那些行，正则 \u0026lsquo;^[[:space]]\u0026rsquo; 匹配的行不反转，就是要那些空格开头的行，所以 negate 是缺省的 false 匹配的空格行追加到上一个事件，向上合并，所以 match 是 after 1filebeat.inputs: 2 - type: log 3 enabled: true 4 paths: 5 - /Users/liuxg/data/multiline/multiline.log 6 multiline.type: pattern 7 multiline.pattern: \u0026#39;^[[:space:]]\u0026#39; 8 multiline.negate: false 9 multiline.match: after 再给个例子，假如你在 Go 程序里定义了输出日志的格式，以 Start new event 开头，以 End event 结尾\n1[2015-08-24 11:49:14,389] Start new event 2[2015-08-24 11:49:14,395] Content of processing something 3[2015-08-24 11:49:14,399] End event 4[2015-08-24 11:50:14,389] Some other log 5[2015-08-24 11:50:14,395] Some other log 6[2015-08-24 11:50:14,399] Some other log 7[2015-08-24 11:51:14,389] Start new event 8[2015-08-24 11:51:14,395] Content of processing something 9[2015-08-24 11:51:14,399] End event 我们就用到 filebeat multiline 的另一个开关，flush_pattern，来控制如何结束，注意 negate 是 true ，匹配到的是 Start 和 End 中间的部分：\n1multiline.type: pattern 2multiline.pattern: \u0026#39;Start new event\u0026#39; 3multiline.negate: true 4multiline.match: after 5multiline.flush_pattern: \u0026#39;End event\u0026#39; 6 7filebeat.inputs: 8 - type: log 9 enabled: true 10 paths: 11 - /Users/liuxg/data/multiline/multiline.log 12 multiline.type: pattern 13 multiline.pattern: \u0026#39;Start new event\u0026#39; 14 multiline.negate: true 15 multiline.match: after 16 multiline.flush_pattern: \u0026#39;End event\u0026#39; 给两篇参考，大家一定要先看那篇原版英文的，中文的那篇翻得不太好；最好两篇对照着看。\n参考：\nhttps://www.elastic.co/guide/en/beats/filebeat/current/multiline-examples.html https://developer.aliyun.com/article/764544 ","date":"2021-11-11","img":"","permalink":"https://bajie.dev/posts/20211111-k8s_filebeat/","series":null,"tags":null,"title":"Kubernetes使用filebeat Multiline自定义收集日志"},{"categories":null,"content":"在生产环境使用 Kubernetes ，绕不过去的一个问题就是持久化卷。\n如果是使用阿里 ACK 托管平台的话，可以用 OSS 来持久化卷，如果是自搭的 kubernetes，那么存储就需要仔细考虑了。\nceph比较复杂，容易出故障。nfs 也不可用，毛病多多。minio倒是可以。\n这种情况下使用双副本的 GlusterFS 就是不错的选择。\n生产环境就不能随意了，最好不要使用 Heketi，因为凡是要持久化的东西，都是比较重要的东西，最好都有 yaml 记录。\nGlusterFS 的搭建就不说了。说说实际使用过程：\n一、装GFS，生产新卷 安装就不说了，我们的GFS有两个节点，172.19.20.18 和 172.19.20.36，我们强制建立一个两副本的卷： kuaijian-vol\n1gluster volume create kuaijian-vol replica 2 transport tcp 172.19.20.18:/glusterfs/kuaijian-vol 172.19.20.36:/glusterfs/kuaijian-vol force 二、为k8s产生GFS的endingpoint和service 1cat \u0026lt;\u0026lt; EOF \u0026gt;\u0026gt; ep-svc.yaml 2--- 3apiVersion: v1 4kind: Service 5metadata: 6 name: gfs-cluster_svc 7spec: 8 ports: 9 - port: 1 10--- 11apiVersion: v1 12kind: Endpoints 13metadata: 14 name: gfs-cluster_svc 15subsets: 16 - addresses: 17 - ip: 172.19.20.18 18 ports: 19 - port: 1 20 - addresses: 21 - ip: 172.19.20.36 22 ports: 23 - port: 1 24EOF 25 26kubectl apply -f ep-svc.yaml 这里要提一个概念，通常情况下 service 是通过 selector 标签来选择对应的 pod 来增加 endingpoint 的。如下：\n1apiVersion: v1 2kind: Service 3metadata: 4 name: go-api_svc 5spec: 6 ports: 7 - port: 8080 8 protocol: TCP 9 targetPort: 8080 10 selector: 11 app: go-api 12 type: ClusterIP 而上面，我们没有通过标签，而是让 endingpoint 和 svc 同名而手动增加 endingpoint 到 svc 的。\n三、为k8s生产创建 PV和PVC 静态环境不使用 Storageclass 持久化卷的图解如下，pod做pvc声明，pvc连接到pv，pv从GFS中拿到卷。\n首先声明一个 pv：\n1cat \u0026lt;\u0026lt; EOF \u0026gt;\u0026gt; kuaijian-pv.yaml 2apiVersion: v1 3kind: PersistentVolume 4metadata: 5 name: gfs-kuaijian-50G_pv 6 labels: 7 name: gfs-kuaijian-50G_pv 8spec: 9 capacity: 10 storage: 50Gi 11 accessModes: 12 - ReadWriteMany 13 glusterfs: 14 endpoints: gfs-cluster_svc 15 path: kuaijian-vol 16 readOnly: false 17 persistentVolumeReclaimPolicy: Retain 18EOF 19 20kubectl apply -f kuaijian-pv.yaml 然后声明一个 pvc 通过 matchLabels 来跟之前的 pv 绑定。\n1cat \u0026lt;\u0026lt; EOF \u0026gt;\u0026gt; kuaijian-pvc.yaml 2kind: PersistentVolumeClaim 3apiVersion: v1 4metadata: 5 name: gfs-kuaijian-50G_pvc 6spec: 7 accessModes: 8 - ReadWriteMany 9 resources: 10 requests: 11 storage: 50Gi 12 selector: 13 matchLabels: 14 name: gfs-kuaijian-50G_pv 15EOF 16 17kubectl apply -f kuaijian-pvc.yaml 注意上面的 pvc，我们一下子申请了50G，把整个 pv 空间全用光了；当然我们也可以只申请个 10Gi，下个 pvc 再 10Gi，这样也是行的通的。\n四、POD使用PVC 声明一个 Nginx 的 Deployment 来使用这个 pvc\n1apiVersion: apps/v1 2kind: Deployment 3metadata: 4 name: nginx-deployment 5 labels: 6 app: nginx 7spec: 8 replicas: 2 9 selector: 10 matchLabels: 11 app: nginx 12 template: 13 metadata: 14 labels: 15 app: nginx 16 spec: 17 containers: 18 - name: nginx 19 image: nginx 20 ports: 21 - containerPort: 80 22 volumeMounts: 23 - name: data-www 24 mountPath: /data/www 25 volumes: 26 - name: data-www 27 persistentVolumeClaim: 28 claimName: gfs-kuaijian-50G_pvc 这样 kubernetes 的存储部分就搞定了。GFS 用于生产非常稳定，基本跑了 7 年了没有大毛病。\n","date":"2021-11-10","img":"","permalink":"https://bajie.dev/posts/20211110-k8s_gfs/","series":null,"tags":null,"title":"生产环境kubernetes使用持久化卷GlusterFS"},{"categories":null,"content":"在生产环境中，ES 通常是不会在 k8s 集群中存在的，一般 MySQL 和 Elasticsearch 都是独立在 k8s 之外。\n那么无论哪种 pod，要甩日志到 ES，最轻量的方案肯定是用 filebeat 甩过去了。\n当然，如果是阿里的 ACK，logtail 和 logstore 配搭已经非常不错了，根本用不到 filebeat 和 ES。\n可但是，我们不想为阿里 sls、logstore 出钱买单，就只能用 filebeat + ES 了\n说一下 filebeat 的 sidecar 边车（僚机）用法：\n如上图所示，简单说就是起一个 filebeat 的 logging-agent 边车（僚机），边车和主应用之间共享某个文件夹（mountPath），达到收集主应用日志并发送到 ES，而不用动 app-container 分毫。\n我们以部署一个 Tomcat 应用为例来说明：\n一、打造 filebeat 边车镜像 首先准备 Dockerfile\n1FROM alpine:3.12 2 3ARG VERSION=7.15.1 4 5COPY docker-entrypoint.sh / 6 7RUN set -x \\ 8 \u0026amp;\u0026amp; cd /tmp \\ 9 \u0026amp;\u0026amp; wget https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-${VERSION}-linux-x86_64.tar.gz \\ 10 \u0026amp;\u0026amp; tar xzvf filebeat-${VERSION}-linux-x86_64.tar.gz \\ 11 \u0026amp;\u0026amp; mv filebeat-${VERSION}-linux-x86_64 /opt \\ 12 \u0026amp;\u0026amp; rm /tmp/* \\ 13 \u0026amp;\u0026amp; chmod +x /docker-entrypoint.sh 14 15 16ENV PATH $PATH:/opt/filebeat-${VERSION}-linux-x86_64 17 18WORKDIR /opt/filebeat-${VERSION}-linux-x86_64 19 20ENTRYPOINT [\u0026#34;/docker-entrypoint.sh\u0026#34;] 我们以 alphine:3.12 为底版，然后下载 filebeat 7.15.1的二进制包并释放到 /opt 下，最后指定入口文件 /docker-entrypoint.sh\n奥妙全在这个 docker-entrypoint.sh 中了\n1#!/bin/bash 2 3cat \u0026gt; /etc/filebeat.yaml \u0026lt;\u0026lt; EOF 4filebeat.config.modules: 5 path: /opt/filebeat-7.15.1-linux-x86_64/modules.d/*.yml 6 reload.enabled: true 7 8# 加入自定义的字段 9fields_under_root: true 10fields: 11 project: kuaijian-tomcat 12 pod_ip: ${POD_IP} 13 pod_name: ${POD_NAME} 14 node_name: ${NODE_NAME} 15 pod_namespace: ${POD_NAMESPACE} 16 17# 收集云厂商的数据和docker的变量 18processors: 19 - add_cloud_metadata: ~ 20 - add_docker_metadata: ~ 21 22filebeat.modules: 23 - module: apache 24 access: 25 enabled: true 26 var.paths: 27 - \u0026#34;/usr/local/tomcat/logs/localhost_access_log.*.txt\u0026#34; 28 error: 29 enabled: true 30 var.paths: 31 - \u0026#34;/usr/local/tomcat/logs/application.log*\u0026#34; 32 - \u0026#34;/usr/local/tomcat/logs/catalina.*.log\u0026#34; 33 - \u0026#34;/usr/local/tomcat/logs/host-manager.*.log\u0026#34; 34 - \u0026#34;/usr/local/tomcat/logs/localhost.*.log\u0026#34; 35 - \u0026#34;/usr/local/tomcat/logs/manager.*.log\u0026#34; 36 37setup.template.name: \u0026#34;tomcat-logs\u0026#34; 38setup.template.pattern: \u0026#34;tomcat-logs-*\u0026#34; 39output.elasticsearch: 40 hosts: [\u0026#34;172.19.20.xxx:9200\u0026#34;,\u0026#34;172.19.20.xxx:9200\u0026#34;] 41 index: \u0026#34;tomcat-logs-%{+yyyy.MM}\u0026#34; 42EOF 43 44set -xe 45 46# If user don\u0026#39;t provide any command 47# Run filebeat 48if [[ \u0026#34;$1\u0026#34; == \u0026#34;\u0026#34; ]]; then 49 exec /opt/filebeat-7.15.1-linux-x86_64/filebeat -c /etc/filebeat.yaml 50else 51 # Else allow the user to run arbitrarily commands like bash 52 exec \u0026#34;$@\u0026#34; 53fi 我们为什么不在 k8s 里用 configmap 来配置 filebeat.yml 呢？\n理由是收集日志文件多且路径、类型各不相同，这么一堆的配置都放在 configmap 里会让人癫狂的。所以干脆放到镜像里，便于调试也便于修改。\n上面我们也充分利用了 filebeat 的 module，有 module 可用就必须用 module，而不是手动指定 filebeat.inputs ，可用的 mudole 实在太多了，一定要善用！！！另外 tomcat 和 apache 的日志格式是一样的。\n我们在最后执行的时候，也加了 exec $@ 便于调试，如果没有指定 CMD，就启动 filebeat，如果指定了比如 /bin/bash，就进入调试状态。\n我们打好镜像就 push 到 harbor 里待用\n附录：https://www.elastic.co/guide/en/beats/filebeat/current/configuration-general-options.html filebeat的配置列表\n二、sidecar部署 我们写一个 k8s 的 tomcat deployment文件：\n1apiVersion: apps/v1 2kind: Deployment 3metadata: 4 name: tomcat 5 labels: 6 app: tomcat 7spec: 8 replicas: 1 9 selector: 10 matchLabels: 11 app: tomcat 12 template: 13 metadata: 14 labels: 15 app: tomcat 16 spec: 17 containers: 18 - name: filebeat-sidecar 19 image: xxxx.xxxx.xxx/filebeat:xxx 20 env: 21 - name: POD_NAMESPACE 22 valueFrom: 23 fieldRef: 24 apiVersion: v1 25 fieldPath: metadata.namespace 26 - name: NODE_NAME 27 valueFrom: 28 fieldRef: 29 apiVersion: v1 30 fieldPath: spec.nodeName 31 - name: POD_IP 32 valueFrom: 33 fieldRef: 34 apiVersion: v1 35 fieldPath: status.podIP 36 - name: POD_NAME 37 valueFrom: 38 fieldRef: 39 apiVersion: v1 40 fieldPath: metadata.name 41 volumeMounts: 42 - name: logs-volume 43 mountPath: /usr/local/tomcat/logs 44 - name: tomcat 45 image: tomcat 46 ports: 47 - containerPort: 8080 48 volumeMounts: 49 - name: logs-volume 50 mountPath: /usr/local/tomcat/logs 51 volumes: 52 - name: logs-volume 53 emptyDir: {} 可以看到我们在这个 deployment 里定义了 pod 是单副本，里面跑了两个 container，一个是 filebeat，一个是 tocmat，两者通过同一个 volume 连接在一起，这样就可以做到不修改 tomcat container 而拿到里面的日志了。\n这样就把 tomcat 应用的日志收到 ES 去了。\n","date":"2021-11-10","img":"","permalink":"https://bajie.dev/posts/20211110-k8s_sidecar/","series":null,"tags":null,"title":"Kubernetes生产环境使用filebeat Sidecar收集日志"},{"categories":null,"content":"我们选择 haproxy 1.8 版本以上的，编译安装到路径 /export/servers/haproxy\n1make TARGET=linux2628 PREFIX=/export/servers/haproxy USE_GETADDRINFO=1 USE_ZLIB=1 USE_REGPARM=1 USE_OPENSSL=1 \\ 2 USE_SYSTEMD=1 USE_PCRE=1 USE_PCRE_JIT=1 USE_NS=1 3 4make install PREFIX=/export/servers/haproxy 编辑 haproxy.conf 配置文件：\n1global 2 maxconn 5120 3 chroot /export/servers/haproxy 4 daemon 5 quiet 6 nbproc 2 7 pidfile /tmp/haproxy.pid 8 9defaults 10 timeout connect 5s 11 timeout client 50s 12 timeout server 20s 13 14listen http 15 bind :80 16 timeout client 1h 17 tcp-request inspect-delay 2s 18 acl is_http req_proto_http 19 tcp-request content accept if is_http 20 server server-http :8080 21 use_backend ssh if !is_http 22 23backend ssh 24 mode tcp 25 timeout server 1h 26 server server-ssh :22 解释一下：我们在 8080 端口开了 http 服务，在 22 端口开了 ssh 服务，80端口由 haproxy 做代理转发，首先判断客户端请求是否是 http 请求，如果是就转发到 8080 端口，如果不是，就转发到 22 端口，这样就实现了 80 端口同时跑 http 和 ssh 两个服务。\n","date":"2021-11-10","img":"","permalink":"https://bajie.dev/posts/20211110-haproxy_multiple_port/","series":null,"tags":null,"title":"Haproxy一个端口跑多个服务"},{"categories":null,"content":"Ucloud的机器在两会期间干脆端口全灭，firewall设置进来的端口全关闭！！！！！！\n还好有个Global ssh的服务，可以 ssh ubuntu@111.129.37.89.ipssh.net 登录上去，注意，直接ssh ubuntu@111.129.37.89 是不通的。\n那我们就搭建一个SSLH服务，可以把ssh和openvpn以及ssl服务统统塞到一个端口22里\n动手吧\n1、修改openssh的端口 从22端口改成2222，千万别重启，22这会先得归ssh用\n2、安装sslh\n1sudo apt install sslh 2vi /etc/default/sslh 3 4找到Run=no 5改成Run=yes 6 7然后到下面，按需配置（不跑443的话可以不配） 8DAEMON_OPTS=\u0026#34;--user sslh --listen 0.0.0.0:22 --ssh 127.0.0.1:2222 --ssl 127.0.0.1:443 --openvpn 127.0.0.1:1194 --pidfile /var/run/sslh/sslh.pid --timeout 5\u0026#34; 3、配置sslh并且重启服务器\n1sudo systemctl enable sslh 2sudo reboot 就搞定了\n","date":"2021-11-10","img":"","permalink":"https://bajie.dev/posts/20211110-sslh_multiple_port/","series":null,"tags":null,"title":"Sslh的一个端口同时跑多个服务"},{"categories":null,"content":"这个要求挺古怪的，背景是防火墙只开了 nginx 443 端口。我也想同时 ssh 登录进去，但是F5没开IP\n就只能这么干了，让 Nginx 一个端口跑多个服务\n在 nginx.conf 加一段，stream 配置，nginx 的 ip 是 192.168.8.110：\n1 2#Multi Ports 3stream { 4 upstream ssh { 5 server 192.168.8.112:22; 6 } 7 8 upstream https { 9 server 192.168.8.111:443; 10 } 11 12 map $ssl_preread_protocol $upstream { 13 default ssh; 14 \u0026#34;TLSv1.2\u0026#34; https; 15 \u0026#34;TLSv1.3\u0026#34; https; 16 \u0026#34;TLSv1.1\u0026#34; https; 17 \u0026#34;TLSv1.0\u0026#34; https; 18 } 19 20 # SSH and SSL on the same port 21 server { 22 listen 443; 23 24 proxy_pass $upstream; 25 ssl_preread on; 26 } 27} 测试一下：\n1curl -v https://192.168.8.110 2 3ssh 192.168.8.110 -p 443 这样就可以了。\n","date":"2021-11-09","img":"","permalink":"https://bajie.dev/posts/20211109-nginx_multiple_port/","series":null,"tags":null,"title":"Nginx的一个端口同时跑SSH和HTTPS服务"},{"categories":null,"content":"kubernetes 装好正常运行一段时间后，会出现要把研发和运维权限分开的场景：\n比如：\n给某个用户某一指定名称空间下的管理权限 给用户赋予集群的只读权限 … 非常麻烦，我们这里不讨论过多的概念，从运维的角度出发，简单实用化\n我们需要明确三个RBAC最基本的概念\nRole: 角色，它定义了一组规则，定义了一组对Kubernetes API对象的操作权限 RoleBinding: 定义了\u0026quot;被作用者\u0026quot;和\u0026quot;角色\u0026quot;的绑定关系 Subject: 被作用者，既可以是\u0026quot;人\u0026quot;，也可以是机器，当然也可以是 Kubernetes 中定义的用户(ServiceAccount主要负责kubernetes内置用户) 我们的操作过程流程如下，首先创建客户端证书；其次创建Role角色；再创建RoleBinding，把Subject和Role绑定，就完事了；最后一步是生成 kubectl 的配置文件。\n一、创建客户端证书 我们以已建好的阿里 ACK 为例，或者自建好的 Kubernetes 也行；确定已经有了 .kube/config 配置文件，拥有集群最高权限，并且可以正常执行 kubectl 命令。\n首先是生成证书，并向集群提出证书请求并签发，脚本如下：\n1#!/bin/sh 2 3useraccount=reader 4 5openssl req -new -newkey rsa:4096 -nodes -keyout $useraccount-k8s.key -out $useraccount-k8s.csr -subj \u0026#34;/CN=$useraccount/O=devops\u0026#34; 6csr=$(cat $useraccount-k8s.csr | base64 | tr -d \u0026#39;\\n\u0026#39;) 7cat \u0026lt;\u0026lt; EOF \u0026gt; k8s-csr.yaml 8apiVersion: certificates.k8s.io/v1beta1 9kind: CertificateSigningRequest 10metadata: 11 name: $useraccount-k8s-access 12spec: 13 groups: 14 - system:authenticated 15 request: $csr 16 usages: 17 - client auth 18EOF 19 20 21kubectl create -f k8s-csr.yaml 22kubectl certificate approve $useraccount-k8s-access 23 24kubectl get csr $useraccount-k8s-access -o jsonpath=\u0026#39;{.status.certificate}\u0026#39; | base64 --decode \u0026gt; $useraccount-k8s-access.crt 25kubectl config view -o jsonpath=\u0026#39;{.clusters[0].cluster.certificate-authority-data}\u0026#39; --raw | base64 --decode - \u0026gt; k8s-ca.crt 解释一下：我们定义了一个用户CN=reader，然后向集群发送了证书请求并签发，最终从集群获得了 reader-k8s-access.crt 的客户端证书和 k8s-ca.crt 的 CA 证书。\nk8s-csr.yaml 和 reader-k8s.csr 都是中间产物，最终我们有了客户端私钥 reader-k8s.key ，客户端证书 reader-k8s-access.crt ，CA 证书 k8s-ca.crt 这三个有用的文件。\n二、创建 Role 角色 1cat \u0026lt;\u0026lt; EOF \u0026gt;\u0026gt; role.yaml 2apiVersion: rbac.authorization.k8s.io/v1 3kind: Role 4metadata: 5 name: role-pods_reader 6 namespace: default 7rules: 8- apiGroups: 9 - \u0026#34;\u0026#34; 10 resources: 11 - pods 12 - pods/log 13 verbs: 14 - get 15 - list 16 - watch 17EOF 18 19kubectl apply -f role.yaml 解释：我们创建了一个 role 角色，名字叫做 role-pods_reader，所属命名空间是 default ，它对 pods 和 pods/log 有 get 、list、watch的权限，也就是说role-pods_reader 可以查看 default 空间的 pods 和 pods 的日志。\n三、创建 Rolebinding 1cat \u0026lt;\u0026lt; EOF \u0026gt;\u0026gt; rolebinding.yaml 2apiVersion: rbac.authorization.k8s.io/v1 3kind: RoleBinding 4metadata: 5 name: rolebinding-default_pods_reader 6 namespace: default 7roleRef: 8 apiGroup: rbac.authorization.k8s.io 9 kind: Role 10 name: role-pods_reader 11subjects: 12- apiGroup: rbac.authorization.k8s.io 13 kind: User 14 name: reader 15EOF 16 17kubectl apply -f rolebinding.yaml 解释：我们创建了一个 rolebinding，名字叫做 rolebinding-default_pods_reader，同样所属命名空间是 default，它绑了两个东西，一个是 role，就是上面第二步创建的 role-pods_reader；另一个是 subject，对应了一个用户，就是我们第一步创建的那个 reader。\n四、生成 kubectl 配置文件 1#!/bin/sh 2useraccount=reader 3namespace=default 4 5kubectl config set-cluster $(kubectl config view -o jsonpath=\u0026#39;{.clusters[0].name}\u0026#39;) --server=$(kubectl config view -o jsonpath=\u0026#39;{.clusters[0].cluster.server}\u0026#39;) --certificate-authority=k8s-ca.crt --embed-certs --kubeconfig=$useraccount-k8s-config 6 7kubectl config set-credentials $useraccount --client-certificate=$useraccount-k8s-access.crt --client-key=$useraccount-k8s.key --embed-certs --kubeconfig=$useraccount-k8s-config 8 9kubectl config set-context $useraccount --cluster=$(kubectl config view -o jsonpath=\u0026#39;{.clusters[0].name}\u0026#39;) --namespace=$namespace --user=$useraccount --kubeconfig=$useraccount-k8s-config 10 11kubectl config use-context $useraccount --kubeconfig=$useraccount-k8s-config 解释：上面看起来很复杂，其实就是四步，在配置文件里设置 cluster 、设置 credentials 证书、设置 context 上下文、设置当前上下文。\n完事后会产生一个完整的 reader-k8s-config 文件，如下：\n五、测试 我们验证一下：\n1KUBECONFIG=reader-k8s-config kubectl get pods 2 3KUBECONFIG=reader-k8s-config kubectl auth can-i delete pods 4 5KUBECONFIG=reader-k8s-config kubectl auth can-i delete svc 这样一个对 default 空间的只读用户就建立好了\n六、集群只读用户 我们这里给出集群只读用户的 role，命名 role-cluster_reader\n1kubectl apply -f - \u0026lt;\u0026lt;EOF 2apiVersion: rbac.authorization.k8s.io/v1 3kind: ClusterRole 4metadata: 5 name: role-cluster_reader 6rules: 7- apiGroups: 8 - \u0026#34;\u0026#34; 9 resources: 10 - nodes 11 - pods 12 - pods/exec 13 - pods/log 14 - services 15 - configmaps 16 - secrets 17 - serviceaccounts 18 - endpoints 19 verbs: 20 - get 21 - list 22 - watch 23- apiGroups: 24 - apps 25 resources: 26 - deployments 27 - replicasets 28 - daemonsets 29 - statefulsets 30 verbs: 31 - get 32 - list 33 - watch 34- apiGroups: 35 - batch 36 resources: 37 - jobs 38 - cronjobs 39 verbs: 40 - get 41 - list 42 - watch 43EOF 以及 rolebinding，还是绑到第一步的用户 reader 的例子\n1kubectl apply -f - \u0026lt;\u0026lt;EOF 2apiVersion: rbac.authorization.k8s.io/v1 3kind: ClusterRoleBinding 4metadata: 5 name: rolebinding-cluster_reader 6roleRef: 7 apiGroup: rbac.authorization.k8s.io 8 kind: ClusterRole 9 name: role-cluster_reader 10subjects: 11- apiGroup: rbac.authorization.k8s.io 12 kind: User 13 name: reader 14EOF 生成配置文件的步骤跟上一步是一样的。\n从上面大家可以看到，其实最主要的就是 role 的 yaml 文件，里面控制着到底有怎么样的权限。\n","date":"2021-11-09","img":"","permalink":"https://bajie.dev/posts/20211109-k8s_rbac/","series":null,"tags":null,"title":"Kubernetes创建普通账号"},{"categories":null,"content":"没啊办法，翻墙翻墙还是翻墙。\n上游有若干 trojian 、v2ray 、sock5 、http各种各样的代理，这样多种的选择，那么就装一个 clash 客户端就可以全接管了。\n说下我们的做法：找个小Linux做旁路由，DNS和网关都设置在这台机器上，局域网内的机器都通过这台上网。\n我们用到的是 clash 的 Tproxy redir-host 和 udp-proxy 模式，这种模式比较强大。用就用最强大的。\n安装很简单，操作系统 centos 或者 ubuntu 都行，项目地址：\nhttps://github.com/Dreamacro/clash 说明书：\nhttps://lancellc.gitbook.io/clash/clash-config-file/proxy-groups/load-balance 首先下载二进制文件，现在版本是 v1.7.1，解压后放到 /usr/local/bin 目录下\n1wget https://github.com/Dreamacro/clash/releases/download/v1.7.1/clash-linux-amd64-v1.7.1.gz 2gzip -d clash-linux-amd64-v1.7.1.gz 3chmod 755 clash-linux-amd64-v1.7.1 4mv clash-linux-amd64-v1.7.1 /usr/local/bin 然后生成 clash.service\n1cat \u0026lt;\u0026lt; EOF \u0026gt;\u0026gt; /etc/systemd/system/clash.service 2[Unit] 3Description=clash service 4After=network.target 5 6[Service] 7Type=simple 8User=root 9ExecStart=/usr/local/bin/clash-linux-amd64-v1.7.1 10Restart=on-failure # or always, on-abort, etc 11 12[Install] 13WantedBy=multi-user.target 14EOF 然后最重要的，就是配置文件了\n我这里这个旁路由的设备 IP 地址是 192.168.2.2，网卡设备是 enp2s0\n下卖弄\n1# http的代理端口 2port: 7890 3#mixed-port: 7890 4socks-port: 7891 5redir-port: 7892 6tproxy-port: 7893 7 8ipv6: false 9 10allow-lan: true 11bind-address: \u0026#39;192.168.2.2\u0026#39; 12interface-name: enp2s0 13 14mode: rule 15log-level: info 16external-controller: 0.0.0.0:9090 17secret: \u0026#34;Fuck2021\u0026#34; 18external-ui: dashboard 19 20profile: 21 store-selected: false 22 tracing: true 23 24 25hosts: 26 # 把cantv的域名解析屏蔽掉，禁止它自动升级 27 \u0026#39;tms.can.cibntv.net\u0026#39;: 0.0.0.0 28 29dns: 30 enable: true 31 listen: 0.0.0.0:1053 32 enhanced-mode: redir-host # or fake-ip 33 nameserver: 34 - \u0026#39;114.114.114.114\u0026#39; 35 - \u0026#39;223.5.5.5\u0026#39; 36 fallback: 37 - 208.67.220.220:5353 38 - 208.67.222.222:5353 39 - 101.6.6.6:5353 40 41proxies: 42 - name: \u0026#34;trojan1\u0026#34; 43 type: trojan 44 server: www.linuxboy.net 45 port: 443 46 password: Fuck2021 47 sni: www.linuxboy.net 48 skip-cert-verify: true 49 50 - name: \u0026#34;vmess1\u0026#34; 51 type: vmess 52 server: 101.59.201.93 53 port: 41555 54 uuid: 7a17ae5e-fb86-42e2-abd4-b8c33cfabcd 55 alterId: 64 56 cipher: auto 57 58proxy-groups: 59 - name: Proxy 60 type: select 61 proxies: 62 - trojan 63 64 - name: \u0026#34;auto\u0026#34; 65 type: url-test 66 proxies: 67 - vmess1 68 - trojan1 69 url: \u0026#39;http://www.gstatic.com/generate_204\u0026#39; 70 interval: 300 71 72rules: 73 - DOMAIN-SUFFIX,v2ex.com,Proxy 74 - DOMAIN-SUFFIX,t66y.com,Proxy 75 - DOMAIN-SUFFIX,ycombinator.com,Proxy 76 - DOMAIN-SUFFIX,reddit.com,Proxy 77 - DOMAIN-KEYWORD,amazon,Proxy 78 - DOMAIN-KEYWORD,google,Proxy 79 - DOMAIN-KEYWORD,gmail,Proxy 80 - DOMAIN-KEYWORD,youtube,Proxy 81 - DOMAIN-KEYWORD,facebook,Proxy 82 - DOMAIN-SUFFIX,fb.me,Proxy 83 - DOMAIN-SUFFIX,fbcdn.net,Proxy 84 - DOMAIN-KEYWORD,twitter,Proxy 85 - DOMAIN-KEYWORD,instagram,Proxy 86 - DOMAIN-KEYWORD,dropbox,Proxy 87 - DOMAIN-SUFFIX,twimg.com,Proxy 88 - DOMAIN-KEYWORD,blogspot,Proxy 89 - DOMAIN-SUFFIX,youtu.be,Proxy 90 - DOMAIN-KEYWORD,whatsapp,Proxy 91 - SRC-IP-CIDR,192.168.1.0/32,DIRECT 92 - SRC-IP-CIDR,192.168.2.0/32,DIRECT 93 - IP-CIDR,127.0.0.0/8,DIRECT 94 - GEOIP,CN,DIRECT 95 - MATCH,Proxy 解释一下：proxy 定义了两个代理，一个是 trojan，一个是 v2ray。然后再集合成组，一个组叫 Proxy， 显式指定用 trojan；另一个组叫 auto，根据 vmess1 和 trojan1 访问 http://www.gstatic.com/generate_204 的页面速度，谁快就用谁，缺省300秒会访问一次这个页面来决定哪个代理快。\n剩下的 rules 就很简单，把自己知道要访问的域名放到代理中去，然后把局域网的 IP 段放进 DIRECT 直接访问，最后 GEO IP 不是中国的由 Proxy 兜底。\n网上有一大堆规则，八戒的建议是不要去学，规则越多越慢，你自己知道要访问什么网站需要翻墙，加进去就好了。弄一堆，自己看着都头蒙\n最后我们在 rc.local 放入以下 iptable 内容，就可以了\n1### 2#clash 3ip rule add fwmark 1 table 100 4ip route add local 0.0.0.0/0 dev lo table 100 5 6# CREATE TABLE 7iptables -t mangle -N clash 8 9# RETURN LOCAL AND LANS 10iptables -t mangle -A clash -d 0.0.0.0/8 -j RETURN 11iptables -t mangle -A clash -d 10.0.0.0/8 -j RETURN 12iptables -t mangle -A clash -d 100.64.0.0/10 -j RETURN 13iptables -t mangle -A clash -d 127.0.0.0/8 -j RETURN 14iptables -t mangle -A clash -d 169.254.0.0/16 -j RETURN 15iptables -t mangle -A clash -d 172.16.0.0/12 -j RETURN 16iptables -t mangle -A clash -d 192.168.0.0/16 -j RETURN 17iptables -t mangle -A clash -d 224.0.0.0/4 -j RETURN 18iptables -t mangle -A clash -d 240.0.0.0/4 -j RETURN 19 20# whitelist China ip. 21# iptables -t mangle -A clash -m set --match-set china dst -j RETURN 22 23# FORWARD ALL 24iptables -t mangle -A clash -p udp -j TPROXY --on-port 7893 --tproxy-mark 1 25iptables -t mangle -A clash -p tcp -j TPROXY --on-port 7893 --tproxy-mark 1 26 27# REDIRECT 28iptables -t mangle -A PREROUTING -j clash 29 30# hijack DNS to Clash 31iptables -t nat -N CLASH_DNS 32iptables -t nat -F CLASH_DNS 33iptables -t nat -A CLASH_DNS -p udp -j REDIRECT --to-port 1053 34iptables -t nat -I PREROUTING -p udp --dport 53 -j CLASH_DNS 最后启动clash\n1systemctl start clash ","date":"2021-11-08","img":"","permalink":"https://bajie.dev/posts/20211108-clash/","series":null,"tags":null,"title":"Clash的搭建教程"},{"categories":null,"content":"上篇简单介绍了 onedev ，这篇我们具体拿个 java spring 的项目来实际编译一下\n首先必须确认环境：\nonedev 和 agent 都是用 root 安装运行的，然后已经安装了 docker，且 selinux 设置为 disabled，否则会出权限麻烦。\n我们用的例子是 spring 的 petclinic，正常的 build 的步骤如下：\n1git clone https://github.com/spring-projects/spring-petclinic.git 2cd spring-petclinic 3./mvnw package 我们首先在 onedev 的 projects 新建一个项目 spring-boot\n然后到 clone 下来的源代码目录下\n1cd spring-petclinic 2 3git init 4git add . 5git commit -m \u0026#34;Spring boot demo project\u0026#34; 6 7git remote add origin http://192.168.86.101:6610/spring-boot 8git push --set-upstream origin master 这样就可以在 spring-boot 里看到代码了\n然后看上图，有个紫色灯泡，Enable build support by adding.onedev-buildspec.yml，点那个链接\n我们增加一个 build 的步骤，里面再增加两个 job，一个是 Get code ，一个是 build\n第一步肯定是把代码拿下来，选择 Checkout Code ，命名为 Get code ，然后其他保持缺省配置，保存\n第二步就是 build 代码，选择 Execute Shell/Batch Commands，然后实际是启动了一个 docker 镜像来执行 build 的过程\nImage 填入：maven:3.5-jdk-8-alphine\nCommands 填入：\n1unset MAVEN_CONFIG 2cat \u0026lt;\u0026lt; EOF \u0026gt;\u0026gt; /root/.m2/settings.xml 3\u0026lt;settings\u0026gt; 4 \u0026lt;proxies\u0026gt; 5 \u0026lt;proxy\u0026gt; 6 \u0026lt;id\u0026gt;default\u0026lt;/id\u0026gt; 7 \u0026lt;active\u0026gt;true\u0026lt;/active\u0026gt; 8 \u0026lt;protocol\u0026gt;http\u0026lt;/protocol\u0026gt; 9 \u0026lt;host\u0026gt;192.168.1.10\u0026lt;/host\u0026gt; 10 \u0026lt;port\u0026gt;3128\u0026lt;/port\u0026gt; 11 \u0026lt;/proxy\u0026gt; 12 \u0026lt;/proxies\u0026gt; 13\u0026lt;/settings\u0026gt; 14EOF 15./mvnw package 解释一下，maven:3.5-jdk-8-alphine 这个镜像，如果在里面执行 mvnw package ，会报错 repository 的错，所以必须把 MAVEN_CONFIG 的环境变量给删除，另外整个 build 过程会去拉 N 多包和配置，大概200多兆，如果不翻墙，基本是失败。所以这里强制 mvn 使用了代理！！！否则 build 一天都不会成功。\n第三步是配置 mvn repository的缓存，大家不想每次build都去下一遍依赖包吧，在 More Settings 设置\n在Caches里填入：\nkey: maven-cache\npath: /root/.m2/repository\n然后执行 Build , 然后等待，第一次时间会很长，终于 Successful 了\n接下来的步骤就可以用 Dockerfile 生成镜像，然后推到 Harbor，再下载或者 gitops下载来各种 yaml 文件，拉 kubectl 和配置文件，就可以推送到 kubernetes 了。\n补充一下，在别的地方突然看到有 maven 的阿里镜像地址，记录一下：\n1 \u0026lt;mirror\u0026gt; 2 \u0026lt;id\u0026gt;alimaven\u0026lt;/id\u0026gt; 3 \u0026lt;name\u0026gt;aliyun maven\u0026lt;/name\u0026gt; 4 \u0026lt;url\u0026gt;http://maven.aliyun.com/nexus/content/groups/public/\u0026lt;/url\u0026gt; 5 \u0026lt;mirrorOf\u0026gt;*\u0026lt;/mirrorOf\u0026gt; 6 \u0026lt;/mirror\u0026gt; ","date":"2021-11-08","img":"","permalink":"https://bajie.dev/posts/20211108-onedev_maven/","series":null,"tags":null,"title":"Onedev构建一个实际java Spring应用"},{"categories":null,"content":"vpn的搭建一直是一个难解的题目，openvpn、ipsec、tinc 都在用，都不够简洁。\nn2n 是一种 peer to peer 端到端的 vpn，搭建起来非常简单方便。必备利器\n首先了解一下概念\nn2n 的 vpn 分为两种角色，Supernode 超级节点和 Edgenode 边缘节点。\n很简单，Supernode 最好是公网地址，需要开放端口供其它节点连接上来；剩余的 Edgenode 可以没有公网地址，各个 Edgenode 边缘节点之间会尝试绕过 Supernode 直接连接。这大大提高了节点之间通讯的效率。通讯是加密的，非常安全。\n安装的话完全不用安装，直接一个命令就搞定了。\nhttps://github.com/ntop/n2n 弄出两个可执行文件 supernode 和 edge 就好了\n超级节点：\n1/usr/local/bin/supernode -p 11111 -v 边缘节点：\n1/usr/local/bin/edge -d n2n0 -c ThisisaSecret2012 -k Fuck2020 -a 192.168.0.2 -l 41.22.59.112:11111 -f 参数解释：\n-d 生成的虚拟网卡的命名 -c 某组vpn节点的口令。大家看到启动 supernode 的时候没有任何参数，supdernode 只负责转发。supernode 支持多组不同的 vpn。这里就是用来区分不同的 vpn 组的。 -k 节点之间通讯是用的 twofish 加密算法，这里指定的是密钥 -a 节点的ip -l supernode的ip和端口 -f 放到后台 daemon 执行 这样就建立好了，系统中会多出一张 n2n0 的网卡。\n","date":"2021-11-05","img":"","permalink":"https://bajie.dev/posts/20211105-n2n_vpn/","series":null,"tags":null,"title":"N2n一种peer to Peer的VPN的使用"},{"categories":null,"content":"其实一直在用 github、gitlab、jenkins，但是 github 时不时的抽风， gitlab 的 runner 套 Docker in docker 的方法委实很难用。\n所以 CI/CD 这一块反倒挺喜欢阿里云效这种简便易行的。但确实找不到其他合适且类似的软件。\n从 V2EX 上看到一个老哥发的 onedev，是一个一站式的开源软件，这不就试试先\n以 centos7 为例，安装过程如下：\n一、安装 java 1.8 版本 1rpm -ivh jdk-8u201-linux-x64.rpm 二、安装 git 高版本 缺省 centos7 和 epel 带的 git 版本太低，不符合要求，得加个新的源装新版本\n1yum -y install https://packages.endpoint.com/rhel/7/os/x86_64/endpoint-repo-1.7-1.x86_64.rpm 2yum -y install git curl 三、安装配置 onedev https://github.com/theonedev/onedev 下载压缩包，然后解压运行 bin/server.sh start ，很简洁，不错！\n运行完打开 http://192.168.86.101:6610 进行初始配置，就两步就 ok 了。\n三、例子 我们是要在正式生产环境用的，所以在 projects 新建一个项目 spring-boot，以 spring-petclinic 为例：\n然后到源代码目录下\n1git clone https://github.com/spring-projects/spring-petclinic.git 2cd spring-petclinic 3./mvnw package 4 5cd spring-petclinic 6 7git init 8git add . 9git commit -m \u0026#34;Spring boot demo project\u0026#34; 10 11git remote add origin http://192.168.86.101:6610/spring-boot 12git push --set-upstream origin master 这样就可以在 spring-boot 里看到代码了\n然后看上图，有个紫色灯泡，Enable build support by adding.onedev-buildspec.yml，点那个链接就可以进入一系列 build 、push、deploy 的 pipeline 了。\n友情提示，单机的话，机器需要装 onedev 的 agent，然后还有 docker，就可以build了。用起来感觉跟云效差不多，很不错。\n","date":"2021-11-05","img":"","permalink":"https://bajie.dev/posts/20211105-onedev/","series":null,"tags":null,"title":"一站式Git软件onedev的安装使用"},{"categories":null,"content":"Nginx 可以用 kill -HUP 来重启，不会丢失已建连接。Haproxy 如何做才能做到 zero downtime 无缝重载呢？\n做法如下：\n一、配置 编译harpoxy的时候带上参数 USE_SYSTEMD，选择 Haproxy 1.8 以上版本\n1make TARGET=linux2628 USE_GETADDRINFO=1 USE_ZLIB=1 USE_REGPARM=1 USE_OPENSSL=1 \\ 2 USE_SYSTEMD=1 USE_PCRE=1 USE_PCRE_JIT=1 USE_NS=1 3make install Haproxy无缝重载技术：\n旧进程当前管理的连接根据 file descriptor 文件描述符通过 socket 套接字传输到新进程。\n在这个过程中，文件socket（unix socket）的连接没有断开。\n新进程在充当 master-worker 主工作者的同时执行此任务。\n综上所述，通过使用unix socket来维护连接状态并在旧进程和新进程之间传递，防止了连接丢失。\nhaproxy 的运行使用 Systemd 重载，使用 -Ws 方式。 （此外，在构建时必须启用 USE_SYSTEMD）\n1 -D : start as a daemon. The process detaches from the current terminal after 2 forking, and errors are not reported anymore in the terminal. It is 3 equivalent to the \u0026#34;daemon\u0026#34; keyword in the \u0026#34;global\u0026#34; section of the 4 configuration. It is recommended to always force it in any init script so 5 that a faulty configuration doesn\u0026#39;t prevent the system from booting. 6 7 -W : master-worker mode. It is equivalent to the \u0026#34;master-worker\u0026#34; keyword in 8 the \u0026#34;global\u0026#34; section of the configuration. This mode will launch a \u0026#34;master\u0026#34; 9 which will monitor the \u0026#34;workers\u0026#34;. Using this mode, you can reload HAProxy 10 directly by sending a SIGUSR2 signal to the master. The master-worker mode 11 is compatible either with the foreground or daemon mode. It is 12 recommended to use this mode with multiprocess and systemd. 13 14 -Ws : master-worker mode with support of `notify` type of systemd service. 15 This option is only available when HAProxy was built with `USE_SYSTEMD` 16 build option enabled. 具体的启动脚本：/etc/systemd/system/haproxy.service\n1[Unit] 2Description=HAProxy Load Balancer 3After=network-online.target 4Wants=network-online.target 5 6[Service] 7Environment=\u0026#34;CONFIG=/etc/haproxy/haproxy.cfg\u0026#34; \u0026#34;PIDFILE=/run/haproxy.pid\u0026#34; 8ExecStartPre=/usr/sbin/haproxy -f $CONFIG -c -q 9ExecStart=/usr/sbin/haproxy -Ws -f $CONFIG -p $PIDFILE 10ExecReload=/usr/sbin/haproxy -f $CONFIG -c -q 11ExecReload=/bin/kill -USR2 $MAINPID 12KillMode=mixed 13Restart=always 14SuccessExitStatus=143 15Type=notify 16 17# The following lines leverage SystemD\u0026#39;s sandboxing options to provide 18# defense in depth protection at the expense of restricting some flexibility 19# in your setup (e.g. placement of your configuration files) or possibly 20# reduced performance. See systemd.service(5) and systemd.exec(5) for further 21# information. 22 23# NoNewPrivileges=true 24# ProtectHome=true 25# If you want to use \u0026#39;ProtectSystem=strict\u0026#39; you should whitelist the PIDFILE, 26# any state files and any other files written using \u0026#39;ReadWritePaths\u0026#39; or 27# \u0026#39;RuntimeDirectory\u0026#39;. 28# ProtectSystem=true 29# ProtectKernelTunables=true 30# ProtectKernelModules=true 31# ProtectControlGroups=true 32# If your SystemD version supports them, you can add: @reboot, @swap, @sync 33# SystemCallFilter=~@cpu-emulation @keyring @module @obsolete @raw-io 34 35[Install] 36WantedBy=multi-user.target 二、验证 验证是否真的是无缝重载的步骤如下：\n在haproxy.cfg的global段落中加入stat的配置：\n1stats socket /var/run/haproxy.sock level admin expose-fd listeners process 1 运行一个不断循环重启的脚本：\n1while true ; do systemctl reload haproxy ; sleep 3 ; done 用 apache 的压测工具 ab 来压一下。\nSend request while service is reloading:\n1ab -r -c 20 -n 100000 http://127.0.0.1/ 最后检查结果中 \u0026ldquo;Failed requests\u0026rdquo; 是否为零就可以了\n","date":"2021-11-04","img":"","permalink":"https://bajie.dev/posts/20211104-haproxy_restart/","series":null,"tags":null,"title":"Haproxy的Zero Downtime重启如何做"},{"categories":null,"content":"用 alphine 镜像的一些常用技巧：\n会随时增加：\n一、修改源，用国外的源非常慢，替换成国内的中科大源 1sed -i \u0026#39;s/dl-cdn.alpinelinux.org/mirrors.ustc.edu.cn/g\u0026#39; /etc/apk/repositories 二、更新apk库 1apk update 三、安装软件 1#安装telnet 2apk add busybox-extras 3 4#安装curl 5apk add curl 6 7#安装时间组件 8apk add tzdata 9 10#更新并且安装软件 11apk add --update tzdata 四、进入容器一步执行换源、更新、安装 1sed -i \u0026#39;s/dl-cdn.alpinelinux.org/mirrors.ustc.edu.cn/g\u0026#39; /etc/apk/repositories \u0026amp;\u0026amp; apk update \u0026amp;\u0026amp; apk add curl \u0026amp;\u0026amp; apk add busybox-extras 五、解决缺少glibc库的问题 如果不ln会报错，原因是缺少glibc库！！！解决方法如下：\n1RUN mkdir /lib64 \u0026amp;\u0026amp; \\ 2 ln -s /lib/libc.musl-x86_64.so.1 /lib64/ld-linux-x86-64.so.2 六、一些调试的CMD 1CMD [\u0026#34;/bin/bash\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;while true; do echo hi; sleep 10; done\u0026#34;] 2 3kubectl run curlpod --image=radial/busyboxplus:curl --command -- /bin/sh -c \u0026#34;while true; do echo hi; sleep 10; done\u0026#34; 七、pod的等待技巧 这儿里启动正式的pod之前，先临时起了两个容器等待其他服务的完成\n1 containers: 2 - name: myapp-container 3 image: busybox 4 command: [\u0026#39;sh\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#39;echo The app is running! \u0026amp;\u0026amp; sleep 3600\u0026#39;] 5 initContainers: 6 - name: init-myservice 7 image: busybox 8 command: [\u0026#39;sh\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#39;until nslookup myservice; do echo waiting for myservice; sleep 2; done;\u0026#39;] 9 - name: init-mydb 10 image: busybox 11 command: [\u0026#39;sh\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#39;until nslookup mydb; do echo waiting for mydb; sleep 2; done;\u0026#39;] ","date":"2021-11-04","img":"","permalink":"https://bajie.dev/posts/20211104-alphine_usage/","series":null,"tags":null,"title":"Alphine镜像的使用技巧"},{"categories":null,"content":"我们都喜欢用 alphine 的镜像做底包，来生产自己的镜像\nalphine 的底包的时间设定就非常重要了\n直接给出 Dockerfile\n1FROM alpine:3.12 2 3# latest certs 4RUN apk add ca-certificates --no-cache \u0026amp;\u0026amp; update-ca-certificates 5 6# timezone support 7ENV TZ=Asia/Shanghai 8RUN apk add --update tzdata --no-cache \u0026amp;\u0026amp;\\ 9 cp /usr/share/zoneinfo/${TZ} /etc/localtime \u0026amp;\u0026amp;\\ 10 echo $TZ \u0026gt; /etc/timezone 11 12# install chrony and place default conf which can be overridden with volume 13RUN apk add --no-cache chrony \u0026amp;\u0026amp; mkdir -p /etc/chrony 14COPY chrony.conf /etc/chrony/. 15 16# port exposed 17EXPOSE 123/udp 18 19# start 20CMD [ \u0026#34;/usr/sbin/chronyd\u0026#34;, \u0026#34;-d\u0026#34;, \u0026#34;-s\u0026#34;] 时间设定重要的就是下面这几行\n1# timezone support 2ENV TZ=Asia/Shanghai 3RUN apk add --update tzdata --no-cache \u0026amp;\u0026amp;\\ 4 cp /usr/share/zoneinfo/${TZ} /etc/localtime \u0026amp;\u0026amp;\\ 5 echo $TZ \u0026gt; /etc/timezone 还有如果在 image: maven:3.5-jdk-8-alpine 这种镜像里，无论怎么改时间都不是东八区，可以试试下面的命令：\n1export COMMIT_TIME=$(TZ=CST-8 date +%F-%H-%M) ","date":"2021-11-03","img":"","permalink":"https://bajie.dev/posts/20211103-alphine_timezone/","series":null,"tags":null,"title":"Alphine镜像中timezone的设定"},{"categories":null,"content":"其实我们的生产环境一直是 KVM ，然后用 shell 脚本控制虚机的生成，也是用到了 Cloud-init 的标准镜像。\n听说 Proxmox 也很不错，于是想看看能否也在生产环境中用上\n如果在生产环境中用，必须要让 proxmox 支持 cloud-init ，否则无意义，下面也说一下跑在生产的注意事项\n首先我们用光盘安装：\n然后第一个注意的地方就是硬盘，选 Options 后：\n会冒出一堆选项，公司的生产环境，服务器如果没有 raid 卡是很奇怪的，所以 zfs 反而不是标配，因为我们会事先在 raid 卡上划分好硬盘，生产环境基本必然是 raid10 ，接下来就是 ext4 和 xfs 二选一了，八戒选 ext4 ，因为坏了好修理，xfs_repair 用起来相当龟毛：\n那么，选定了 ext4 ，接下来就比较重要了\nhdsize 1116.0 ，单位是G，这个是自动收集上来的，不用改\nswapsize，交换分区大小，这个给 8 G（最大8G）\nmaxroot，这个分区是第一个分区，存放 iso 和 template 的，需要给够，100 G\nminfree，第一个分区最小留多大，给 10 G（缺省16G）\nmaxvz，这个分区是第二个分区，存放实际的虚机文件，全都用上，什么也不填写\n然后继续，国家选 china，Hostname 填写 proxmox-168-86-103.local，再填写好其他信息，就安装成功了。\n打开网页，我们可以看到一个 local，100G，对应上面的 maxroot\n然后 local-lvm ，就是剩余放虚机的空间\nssh登录系统，首先换成中科大的 apt 源，并升级一下系统：\n1sed -i \u0026#39;s|^deb http://ftp.debian.org|deb https://mirrors.ustc.edu.cn|g\u0026#39; /etc/apt/sources.list 2sed -i \u0026#39;s|^deb http://security.debian.org|deb https://mirrors.ustc.edu.cn/debian-security|g\u0026#39; /etc/apt/sources.list 3CODENAME=`cat /etc/os-release |grep CODENAME |cut -f 2 -d \u0026#34;=\u0026#34;` 4echo \u0026#34;deb https://mirrors.ustc.edu.cn/proxmox/debian $CODENAME pve-no-subscription\u0026#34; \u0026gt; /etc/apt/sources.list.d/pve-no-subscription.list 5cat /etc/apt/sources.list.d/pve-no-subscription.list 6rm /etc/apt/sources.list.d/pve-enterprise.list 7apt update 8apt upgrade 那生产使用，是必须用 Cloud-init 的标准化镜像的。我们需要造出一个 template 。\n以 Centos7 为例子\n1wget http://cloud.centos.org/centos/7/images/CentOS-7-x86_64-GenericCloud.qcow2 2apt-get install libguestfs-tools 然后准备脚本 modify.sh ：\n1#!/bin/sh 2image_name=CentOS-7-x86_64-GenericCloud.qcow2 3# virt-edit -a ${image_name} /etc/cloud/cloud.cfg 4virt-edit -a ${image_name} /etc/cloud/cloud.cfg -e \u0026#39;s/disable_root: [Tt]rue/disable_root: False/\u0026#39; 5virt-edit -a ${image_name} /etc/cloud/cloud.cfg -e \u0026#39;s/disable_root: 1/disable_root: 0/\u0026#39; 6virt-edit -a ${image_name} /etc/cloud/cloud.cfg -e \u0026#39;s/lock_passwd: [Tt]rue/lock_passwd: False/\u0026#39; 7virt-edit -a ${image_name} /etc/cloud/cloud.cfg -e \u0026#39;s/lock_passwd: 1/lock_passwd: 0/\u0026#39; 8virt-edit -a ${image_name} /etc/cloud/cloud.cfg -e \u0026#39;s/ssh_pwauth: 0/ssh_pwauth: 1/\u0026#39; 9virt-edit -a ${image_name} /etc/ssh/sshd_config -e \u0026#39;s/PasswordAuthentication no/PasswordAuthentication yes/\u0026#39; 10virt-edit -a ${image_name} /etc/ssh/sshd_config -e \u0026#39;s/PermitRootLogin [Nn]o/PermitRootLogin yes/\u0026#39; 11virt-edit -a ${image_name} /etc/ssh/sshd_config -e \u0026#39;s/#PermitRootLogin [Yy]es/PermitRootLogin yes/\u0026#39; 12virt-edit -a ${image_name} /etc/ssh/sshd_config -e \u0026#39;s/#PermitRootLogin prohibit-password/PermitRootLogin yes/\u0026#39; 13virt-edit -a ${image_name} /etc/ssh/sshd_config -e \u0026#39;s/[#M]axAuthTries 6/MaxAuthTries 20/\u0026#39; 14virt-customize --install cloud-init,atop,htop,nano,vim,qemu-guest-agent,curl,wget,telnet,lsof,screen -a ${image_name} 运行它，以上命令其实是侵入镜像，修改 sshd_config 允许 root 用 password 登录，然后又安了几个常用软件，大家可以按需修改。\n最后生成 template , 脚本： vm.sh 1#!/bin/sh 2vm_id=9999 3image_name=CentOS-7-x86_64-GenericCloud.qcow2 4 5qm create ${vm_id} --memory 8196 --net0 virtio,bridge=vmbr0 6qm importdisk ${vm_id} ${image_name} local-lvm 7qm set ${vm_id} --scsihw virtio-scsi-pci --scsi0 local-lvm:vm-${vm_id}-disk-0 8qm set ${vm_id} --ide2 local-lvm:cloudinit 9qm set ${vm_id} --boot c --bootdisk scsi0 10qm set ${vm_id} --serial0 socket --vga serial0 11qm template ${vm_id} cloud-init 技术的核心其实就是用配置文件，在虚机启动的时候动态修改，这里把配置放到了 ide2 的一个虚拟 cdrom 中\n最终会生成一个 id 为 9999 的 template\n我们还需要改两处：\n一是 CPU、MEMORY、硬盘大小，缺省是 8G，我们生产的镜像标配是80G，需要 resize , 加 72G，合计80G\n二是 cloud-init 部分，用户名、密码、DNS、IP、MASK、GATEWAY\n这样这个 template 就做好了，在生产的时候，只需要 clone 这个模板（模式要选 Full Clone），然后记得修改为不同的IP，就可以了。\n总体来说，这个东西偏小白，对于习惯了 KVM 的人来说，反而不如脚本来的快。\n相关文档：https://whattheserver.com/proxmox-cloud-init-os-template-creation/\n","date":"2021-11-03","img":"","permalink":"https://bajie.dev/posts/20211103-proxmox/","series":null,"tags":null,"title":"生产环境Proxmox 7.02的安装和配置"},{"categories":null,"content":"kubernetes 中 nginx ingress 的优化分两部分\n一、系统sysctl部分优化 首先是对nginx启动前的系统性能进行优化，这部分调整网络的缓冲区，减小闲置 socket 关闭的时间\n以阿里 ACK 为例，我们可以编辑 deployments 的 nginx-ingress-controller\n1 initContainers: 2 - command: 3 - /bin/sh 4 - -c 5 - | 6 mount -o remount rw /proc/sys 7 sysctl -w net.core.somaxconn=65535 8 sysctl -w net.ipv4.ip_local_port_range=\u0026#34;1024 65535\u0026#34; 9 sysctl -w net.ipv4.tcp_tw_reuse=1 10 sysctl -w fs.file-max=1048576 11 sysctl -w net.ipv4.tcp_keepalive_time = 300 12 sysctl -w net.ipv4.tcp_keepalive_probes = 5 13 sysctl -w net.ipv4.tcp_keepalive_intvl = 15 14 二、nginx ingress 参数优化 大家制动，nginx ingree 其实是做为一个中间代理，所以上下游的socket参数也需要优化\n同样以阿里ACK为例，我们可以编辑 configmaps 的 nginx-configuration\n1apiVersion: v1 2data: 3 allow-backend-server-header: \u0026#34;true\u0026#34; 4 enable-underscores-in-headers: \u0026#34;true\u0026#34; 5 generate-request-id: \u0026#34;true\u0026#34; 6 ignore-invalid-headers: \u0026#34;true\u0026#34; 7 log-format-upstream: $remote_addr - [$remote_addr] - $remote_user [$time_local] 8 \u0026#34;$request\u0026#34; $status $body_bytes_sent \u0026#34;$http_referer\u0026#34; \u0026#34;$http_user_agent\u0026#34; $request_length 9 $request_time [$proxy_upstream_name] $upstream_addr $upstream_response_length 10 $upstream_response_time $upstream_status $req_id $host [$proxy_alternative_upstream_name] 11 proxy-body-size: 20m 12 proxy-connect-timeout: \u0026#34;10\u0026#34; 13 reuse-port: \u0026#34;true\u0026#34; 14 server-tokens: \u0026#34;false\u0026#34; 15 ssl-redirect: \u0026#34;false\u0026#34; 16 17 upstream-keepalive-timeout: \u0026#34;900\u0026#34; 18 keep-alive-requests: \u0026#34;10000\u0026#34; 19 upstream-keepalive-connections: \u0026#34;500\u0026#34; 20 max-worker-connections: \u0026#34;65536\u0026#34; 21 22 worker-cpu-affinity: auto 23kind: ConfigMap ","date":"2021-11-02","img":"","permalink":"https://bajie.dev/posts/20211102-ingress_nginx/","series":null,"tags":null,"title":"K8s中nginx Ingress的性能优化"},{"categories":null,"content":"这篇是纯配置篇，解释都在配置里了，是生产服务器 sysctl.conf 的配置\n1### KERNEL ### 2 3# Reboot after 10sec. on kernel panic 4kernel.panic = 10 5 6### IMPROVE SYSTEM MEMORY MANAGEMENT ### 7 8# Increase size of file handles and inode cache 9fs.file-max = 2097152 10 11# Insure we always have enough memory 12vm.min_free_kbytes = 8192 13 14# Do less swapping 15vm.swappiness = 10 16vm.dirty_ratio = 10 17vm.dirty_background_ratio = 2 18 19 20### GENERAL NETWORK SECURITY OPTIONS ### 21 22# Avoid a smurf attack 23net.ipv4.icmp_echo_ignore_broadcasts = 1 24 25# Turn on protection for bad icmp error messages 26net.ipv4.icmp_ignore_bogus_error_responses = 1 27 28# Turn on syncookies for SYN flood attack protection 29net.ipv4.tcp_syncookies = 1 30net.ipv4.tcp_max_syn_backlog = 8192 31 32 33# Turn on timestamping 34net.ipv4.tcp_timestamps = 1 35 36# Turn on and log spoofed, source routed, and redirect packets 37net.ipv4.conf.all.log_martians = 1 38net.ipv4.conf.default.log_martians = 1 39 40# No source routed packets here 41net.ipv4.conf.all.accept_source_route = 0 42net.ipv4.conf.default.accept_source_route = 0 43 44# Turn on reverse path filtering 45net.ipv4.conf.all.rp_filter = 1 46net.ipv4.conf.default.rp_filter = 1 47 48# Make sure no one can alter the routing tables 49net.ipv4.conf.all.accept_redirects = 0 50net.ipv4.conf.default.accept_redirects = 0 51net.ipv4.conf.all.secure_redirects = 0 52net.ipv4.conf.default.secure_redirects = 0 53 54# Don\u0026#39;t act as a router 55net.ipv4.ip_forward = 0 56net.ipv4.conf.all.send_redirects = 0 57net.ipv4.conf.default.send_redirects = 0 58 59# Number of times SYNACKs for passive TCP connection. 60net.ipv4.tcp_synack_retries = 2 61 62# Allowed local port range 63net.ipv4.ip_local_port_range = 1024 65000 64 65# Protect Against TCP Time-Wait 66net.ipv4.tcp_rfc1337 = 1 67 68# Decrease the time default value for tcp_fin_timeout connection 69net.ipv4.tcp_fin_timeout = 15 70 71# Decrease the time default value for connections to keep alive 72net.ipv4.tcp_keepalive_time = 300 73net.ipv4.tcp_keepalive_probes = 5 74net.ipv4.tcp_keepalive_intvl = 15 75# This means that the keepalive process waits 300 seconds for socket 76# activity before sending the first keepalive probe, and then resend 77# it every 15 seconds. If no ACK response is received for 5 consecutive 78# times (75s in this case), the connection is marked as broken. 79 80### TUNING NETWORK PERFORMANCE ### 81 82# Disable IPv6 83net.ipv6.conf.all.disable_ipv6 = 1 84net.ipv6.conf.default.disable_ipv6 = 1 85net.ipv6.conf.lo.disable_ipv6 = 1 86 87# Default Socket Receive Buffer 88net.core.rmem_default = 31457280 89 90# Maximum Socket Receive Buffer 91net.core.rmem_max = 12582912 92 93# Default Socket Send Buffer 94net.core.wmem_default = 31457280 95 96# Maximum Socket Send Buffer 97net.core.wmem_max = 12582912 98 99# Increase number of incoming connections 100net.core.somaxconn = 5000 101 102# Increase number of incoming connections backlog 103net.core.netdev_max_backlog = 65536 104 105# Enable TCP window scaling 106net.ipv4.tcp_window_scaling = 1 107 108# Increase the maximum amount of option memory buffers 109net.core.optmem_max = 25165824 110 111 112# Increase the maximum total buffer-space allocatable 113# This is measured in units of pages (4096 bytes) 114net.ipv4.tcp_mem = 65536 131072 262144 115net.ipv4.udp_mem = 65536 131072 262144 116 117# Increase the read-buffer space allocatable 118net.ipv4.tcp_rmem = 8192 87380 16777216 119net.ipv4.udp_rmem_min = 16384 120 121# Increase the write-buffer-space allocatable 122net.ipv4.tcp_wmem = 8192 65536 16777216 123net.ipv4.udp_wmem_min = 16384 124 125 126# Increase the tcp-time-wait buckets pool size to prevent simple DOS attacks 127net.ipv4.tcp_max_tw_buckets = 1800000 128 129# TIME_WAIT socket policy 130# Note: if both enabled then disable 131# net.ipv4.tcp_timestamps for servers 132# behind NAT to prevent dropped incoming connections 133#net.ipv4.tcp_tw_recycle = 1 134net.ipv4.tcp_tw_reuse = 1 135 136# Enable TCP MTU probing (in case of Jumbo Frames enabled) 137#net.ipv4.tcp_mtu_probing = 1 138 139# Speedup retrans (Google recommended) 140net.ipv4.tcp_slow_start_after_idle = 0 141net.ipv4.tcp_early_retrans = 1 142 143# Conntrack 144# 288bytes x 131072 = 37748736 (~38MB) max memory usage 145#net.netfilter.nf_conntrack_max = 131072 146#net.netfilter.nf_conntrack_tcp_loose = 1 147 148#TCP的直接拥塞通告(tcp_ecn)关掉 149net.ipv4.tcp_ecn = 0 150 151#路由缓存刷新频率，当一个路由失败后多长时间跳到另一个路由，默认是300。 152net.ipv4.route.gc_timeout = 100 153 154#设定系统中最多允许在多少TCP套接字不被关联到任何一个用户文件句柄上。 155#如果超过这个数字，没有与用户文件句柄关联的TCP 套接字将立即被复位 156#防简单Dos 157net.ipv4.tcp_max_orphans = 655360 158 159# NOTE: Enable this if machine support it 160# -- 10gbe tuning from Intel ixgb driver README -- # 161# turn off selective ACK and timestamps 162#net.ipv4.tcp_sack = 0 163#net.ipv4.tcp_timestamps = 1 ** 注意，net.ipv4.tcp_tw_recycle 不要打开，在 NAT 环境中会出错，而且在 K8S 中也会因 NAT 导致 pod 出错，切记！！！**\n","date":"2021-11-02","img":"","permalink":"https://bajie.dev/posts/20211102-sysctl_conf/","series":null,"tags":null,"title":"Linux内核sysctl内核参数优化"},{"categories":null,"content":"Custom Configuration of TCP Socket Keep-Alive Timeouts 这是个古老的话题，我们在机器的优化中，需要设置 TCP Socket 的 Timeout 参数\n用来加快 TCP 关闭无用闲置连接的时间\nLinux 内核中有三个缺省参数:\n1 tcp_keepalive_time 缺省是 7200 秒 1 tcp_keepalive_probes 缺省是 9 1 tcp_keepalive_intvl 缺省是 75 秒 处理流程如下：\n一、客户端打开一个 TCP socket 连接，开始跟服务器通讯\n二、如果这条 socket 连接空闲没有任何数据传输，静默了 tcp_keepalive_time 秒后，那么客户端会主动发送一个空的 ACK 包到服务器\n三、那么，根据服务器是否回应了一个相应的 ACK 包来判断\n1ACK 未回应 等待 tcp_keepalive_intvl 秒，然后再发一个 ACK 包 重复以上等待并发送 ACK 包的过程，直到次数等于 tcp_keepalive_probes 如果第2步做完还收不到任何回应，发送一个 RST 包并关闭连接 回应了: 回到上述第二步 那么缺省情况下，7200+75×9，一个没有任何数据传输的 socket 才会被关闭，大概是2小时11分钟。\n这个时间太长了。需要优化一下：\n1net.ipv4.tcp_keepalive_time = 300 2net.ipv4.tcp_keepalive_probes = 5 3net.ipv4.tcp_keepalive_intvl = 15 上面的时间是 300 + 5x15，大概是6分钟，大大缩短释放空闲 socket 的时间\n","date":"2021-11-02","img":"","permalink":"https://bajie.dev/posts/20211102-tcp_keealive/","series":null,"tags":null,"title":"Linux内核TCP连接Keep-Alive Timeout的配置"},{"categories":null,"content":"这属于Shell的高级技巧了，我们可能需要在 bash 中并发 wget rsync 文件，下面就讨论一下这个问题。\n首先从简单的单线程开始：\n1$ for i in $(seq 1 2); do echo $i; done 21 32 可以看到是顺序执行的，下面变多线程：\n1$ for i in $(seq 1 2); do echo $i \u0026amp; done 2[1] 245505 31 4[2] 245506 52 6[1] Done echo $i 7[2] Done echo $i 可以看到我们只把 ; 号改成了 \u0026amp; 号，程序就变成了多线程执行。\n区别在于 ; 号会等待之前的命令执行完毕再执行下一条，而 \u0026amp; 不等待，直接继续执行下一条；相当于后台运行了前一条命令。\n下面说说 find 的单线程和多线程：\nfind 的 exec 用法\n1$ find /path [args] -exec [cmd] {} \\; {} 占位符号，存放find找到的记录 ; 对于每一条找到的单独记录，执行的cmd是一条一条单独执行的 执行的顺序如下: cmd result1; cmd result2; \u0026hellip;; cmd result N 1$ find /path [args] -exec [cmd] {} \\+ {} 占位符号，存放find找到的记录 + 对于找到的所有记录，执行的cmd是合并了所有记录集执行的 执行顺序如下: cmd result1 result2 \u0026hellip; result N 多个exec可以串起来：\n1$ find /tmp/dir1/ -type f -exec grep howtouselinux {} \\; -exec echo {} \\; | sed \u0026#39;s/howtouselinux/deep/g\u0026#39; 至此，find 也还是单线程执行的，并没有并发。\nfind 要并发，就只能跟 xargs 结合在一起：\nxargs 通常配合管道使用，将前面命令产生的参数，逐个传入后续命令，作为参数。xargs 传来的参数，默认位于 xargs 后面命令的最后，如果要改变位置，需要用**-I**参数。xargs 如果不带命令，缺省是 echo\n-d 分隔符\n1$ echo -e \u0026#34;a\\tb\\tc\u0026#34; | xargs -d \u0026#34;\\t\u0026#34; echo 2a b c -I{} 指定占位符，-I %那就是 % 替代从之前管道取得的参数\n1$ find . -type d | xargs -I % -0 rsync -auvPR % 192.168.1.38::new/ -0 跟find的-print0配合，find命令有一个特别的参数-print0，指定输出的文件列表以null分隔。然后，xargs命令的-0参数表示用null当作分隔符。\n1$ find ./new -mindepth 6 -maxdepth 6 -type d -print0 | xargs -I % -0 rsync -auvPR % 172.18.34.38::new/ -P 最大并发线程数，下面是并发30线程\n1$ find ./new -mindepth 6 -maxdepth 6 -type d -print0 | xargs -P 30 -I % -0 rsync -auvPR % 172.18.34.38::new/ -n 选项限制单个命令行的参数个数，下面是 rsync 一行命令传带60个文件，30个进程那就是30个 rsync，每个 rsync 同时传60个文件。\n1$ find ./new -mindepth 6 -maxdepth 6 -type d -print0 | xargs -P 30 -n 60 -I % -0 rsync -auvPR % 172.18.34.38::new/ 2 3$ echo {0..10} | xargs -I{} -n 2 40 1 52 3 64 5 76 7 88 9 910 使用 bash -c 并发的例子：\n1$ time for i in $(seq 1 5); do echo $[$RANDOM % 5 + 1]; done | xargs -I{} echo \u0026#34;sleep {}; echo \u0026#39;Done! {}\u0026#39;\u0026#34; | xargs -P5 -I{} bash -c \u0026#34;{}\u0026#34; ","date":"2021-10-29","img":"","permalink":"https://bajie.dev/posts/20211029-bash_multithread/","series":null,"tags":null,"title":"Shell以及find的多线程执行"},{"categories":null,"content":"之前讲过如何对 opnvpn 总体限速，这次来了一个更严格的程序限速需求：\n场景如下：\n两个机房间有一条专线 100M\n两个机房间需要同步数据，同步需要限制到60M，给别的程序留出带宽空间\n传输是多个文件，用 rsync 并发传送\n分析了一下脚本的核心部分\n1find . -type f | grep $(date -d\u0026#34;-1 day\u0026#34; +\u0026#39;%Y%m%d\u0026#39;) |xargs -I % -P 30 rsync -auvPR % 192.168.9.17::mysql 发现是利用 xargs 的并发，-P 30 最大并发30个，启动了 rsync 同步\nrsync 没有限速，这就麻烦了。\n一、单文件单独限速 首先是使用 rsync 的 \u0026ndash;bwlimit=600 参数，把速度限制为 600KB/s ，600×8=4800，单进程基本是5M的速度，最多只能跑12个了，就会跑到60M。\n这样也不太对，尤其是 rsync 并发进程逐渐减少，少于12个的时候，这样就出现跑不满60M的现象。\n二、多文件整体限速 那么 rsync 支持多文件传输 ，使用如下格式整体限速\n1rsync --bwlimit=7200 -auvPR 文件1 文件2 文件3 192.168.9.17::mysql 问题又来了，文件1 文件2 文件3 的路径非常长，而文件个数不定，有撑爆命令行单行长度限制的可能，也不可行\n三、tc 使用 tc 可以控制源ip或者目的ip的带宽，但是本机网卡是万兆光卡，生产环境，每时每刻都有数据读写。\n一旦错了，就直接完蛋了。也不太可行\n四、杀器trickle 寻找了半天，终于找到了个大杀器trickle，可以对程序单独限速，也可以对一堆程序整体限速\n安装：\n1$ yum install -y epel-release 2$ yum install trickle 参数解释：\n-u 上载速度 KB/s ，乘以8换算成网络速度 -d 下载速度 KB/s ，乘以8换算成网络速度 -s standalone独立模式，不参与 trickled 的整体模式 如果要对一个程序单独限速，10KB\n1trickle -s -u 10 -d 10 wget http://mirrors.163.com/rocky/8/isos/x86_64/Rocky-8.4-x86_64-dvd1.iso wget 的下载完全被限制在了10KB\n如过要对使用trickle的所有程序做整体限速\n1# 首先启动守护进程 trickled 2$ trickled -u 7200 -d 7200 3 4# 然后所有用trickle执行的命令就会整理限速在7200KB/s （网络速度60M） 5find . -type f | grep $(date -d\u0026#34;-1 day\u0026#34; +\u0026#39;%Y%m%d\u0026#39;) |xargs -I % -P 30 trickle rsync -auvPR % 192.168.9.17::mysql ","date":"2021-10-28","img":"","permalink":"https://bajie.dev/posts/20211028-trickle/","series":null,"tags":null,"title":"Linux下的程序限速软件Trickle"},{"categories":null,"content":"Dockerfile 是造出镜像的基础，是必须熟知并了解的知识：\n一、编写Dockerfile 先给个例子，是 minio 代理访问阿里的 OSS\n1FROM alpine:3.12 2 3RUN apk add --update bash \u0026amp;\u0026amp; rm -rf /var/cache/apk/* 4 5COPY minio.RELEASE.2020-04-15T19-42-18Z /data/minio.RELEASE.2020-04-15T19-42-18Z 6 7ENV MINIO_ACCESS_KEY=LTAI5tFFTbsxxxxxuLb 8ENV MINIO_SECRET_KEY=t78PyGnHZilxxxxxdxBCjvNgtVC5Y 9 10WORKDIR /data 11EXPOSE 9000 12 13CMD [\u0026#34;/data/minio.RELEASE.2020-04-15T19-42-18Z\u0026#34;,\u0026#34;gateway\u0026#34;,\u0026#34;oss\u0026#34;,\u0026#34;http://oss-cn-shanghai-internal.aliyuncs.com\u0026#34;] 14# CMD /bin/sh -c \u0026#34;while true; do echo hi; sleep 10; done\u0026#34; 详细解释每一条语句：\nFROM\n基板，alpine 3.12 是个比较微小的版本，注意它的毛病，/bin/sh其实是busybox，没有/bin/bash，某些bash的函数功能支持不全，比如for循环\nRUN\n在容器中运行命令，上例中我们添加了 bash ，并清理了缓存。命令间用 \u0026amp;\u0026amp; 可以避免镜像过多分层。\nRUN分两种模式shell和exec模式:\n我们只用 exec 模式，因为在 image 里装入多个 shell，没什么意义。\nCOPY 和 ADD\n作用都是将文件或目录复制到 Dockerfile 构建的镜像中\n我们只用COPY，如果遇到要把URL的文件放进去，可以先wget，然后放；如果要解压tarr包放进去，那就先解压再放。\n注意源文件路径都使用相对路径，目标路径使用绝对路径。\n如果dest不指定绝对路径，则是想对于WORDIR的相对路径\nCMD 入口\n用 [] 分割， 把所有 \u0026quot;\u0026quot; 的部分合并为一行，中间用空格隔开执行；或者直接一行没任何分割符。\n所以上面的例子就是执行了一句：\n1/data/minio.RELEASE.2020-04-15T19-42-18Z gateway oss http://oss-cn-shanghai-internal.aliyuncs.com 技巧：\n把几个命令合在一起执行\n()表示在当前shell合并执行\n{}表示派生出一个子shell，在子shell中合并执行，{ echo \u0026ldquo;aaa\u0026rdquo; }必须有空格\n\u0026amp;表示后台运行\n命令之间使用 \u0026amp;\u0026amp; 连接，实现逻辑与的功能。\n只有在 \u0026amp;\u0026amp; 左边的命令返回真（命令返回值 $? == 0），\u0026amp;\u0026amp; 右边的命令才会被执行。 只要有一个命令返回假（命令返回值 $? == 1），后面的命令就不会被执行。 \u0026amp;\u0026amp;左边的命令（命令1）返回真(即返回0，成功被执行）后，\u0026amp;\u0026amp;右边的命令（命令2）才能够被执行；换句话说，“如果这个命令执行成功\u0026amp;\u0026amp;那么执行第二个命令”。\n最下的语法用了seq而不是for循环，是因为busybox的sh不支持for语法\n所以才有如此怪异的语法，在容器中后台跑10个php think queue，1个crond，前台跑一个php-fpm：\n1CMD for i in $(seq 10); do (php think queue \u0026amp;) ; done \u0026amp; crond \u0026amp;\u0026amp; php-fpm ARG 参数\nARG VERSION=7.6.1\n定义后可以用${VERSION}引用，build的时候可以加\u0026ndash;build-arg 传参数进去\n1docker build --build-arg VERSION=${LATEST} -t $(ORG)/$(NAME):$(BUILD) . 还有很多 Build 的技巧，如果造一个 go 语言编译环境的中间层镜像，然后造最终镜像。\n但是八戒还是推荐直接造出二进制可执行文件，然后直接拷贝进去就好，不要弄的过于麻烦，中间层那种适用于用源码 CI/CD 中无编译环境的情况。\n二、调试容器 很多情况下我们造好了 image ，一跑就掉下来了，也不知道是什么情况\n这个时候，我们把 CMD 换成一个 sh 执行一个死循环\n1CMD /bin/sh -c \u0026#34;while true; do echo hi; sleep 10; done\u0026#34; 然后进入容器，然后再执行之前的 CMD 命令，看看报错信息是什么，就可以调试了\n1$ docker exec -it 89174asklja /bin/sh 三、pod的等待技巧 这里启动正式的pod之前，先临时起了两个容器等待其他服务的完成\n1 containers: 2 - name: myapp-container 3 image: busybox 4 command: [\u0026#39;sh\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#39;echo The app is running! \u0026amp;\u0026amp; sleep 3600\u0026#39;] 5 initContainers: 6 - name: init-myservice 7 image: busybox 8 command: [\u0026#39;sh\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#39;until nslookup myservice; do echo waiting for myservice; sleep 2; done;\u0026#39;] 9 - name: init-mydb 10 image: busybox 11 command: [\u0026#39;sh\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#39;until nslookup mydb; do echo waiting for mydb; sleep 2; done;\u0026#39;] ","date":"2021-10-28","img":"","permalink":"https://bajie.dev/posts/20211028-dockerfile/","series":null,"tags":null,"title":"Dockerfile的编写与调试技巧"},{"categories":null,"content":"给同事做了个 PHP 接口，转发发送短信的请求，同时要把发送记录发送到远程的 cacti 的 syslog 去\n很简单，但是也不简单\n首先是 PHP 服务器，是最简化编译的，php -m 查了一下\n1php -m 2[PHP Modules] 3Core 4ctype 5curl 6date 7dom 8fileinfo 9filter 10gettext 11hash 12iconv 13json 14libxml 15openssl 16pcre 17PDO 18pdo_sqlite 19Phar 20posix 21Reflection 22session 23SimpleXML 24SPL 25sqlite3 26standard 27tokenizer 28xml 29xmlreader 30xmlwriter 31 32[Zend Modules] 居然没有 socket 模块，没办法，找到源代码，编译一个安装，原有的 php 安装路径是 /export/servers/php\n1$ tar zxvf php-7.4.0.tar.gz 2$ cd php-7.4.0/sockets 3$ /export/servers/php/bin/phpize 4$ ./configure --enable-sockets --with-php-config=/export/servers/php/bin/php-config 5$ make 6$ make install 又看了一眼，是 php-fpm，居然没有 php.ini ，得，再生成一个，放在 /export/servers/php/lib/php.ini\n1extension_dir = \u0026#34;/export/servers/php740/lib/php/extensions/no-debug-non-zts-20190902/\u0026#34; 2extension = sockets.so 然后重启 php-fpm ，重新 php -m 检查，发现有 socket 模块就 ok 了。\n接下来就是 php 源代码了\n1\u0026lt;?php 2date_default_timezone_set(\u0026#39;Asia/Shanghai\u0026#39;); 3 4function send_remote_syslog($message) { 5 $sock = socket_create(AF_INET, SOCK_DGRAM, SOL_UDP); 6 foreach(explode(\u0026#34;\\n\u0026#34;, $message) as $line) { 7 $syslog_message = \u0026#34;\u0026lt;22\u0026gt;\u0026#34; . date(\u0026#39;M d H:i:s \u0026#39;) . \u0026#39;Qi_an_xin sms_log: \u0026#39; . $line; 8 socket_sendto($sock, $syslog_message, strlen($syslog_message), 0, \u0026#39;172.18.31.6\u0026#39;, 514); 9 } 10 socket_close($sock); 11} 12 13 send_remote_syslog(\u0026#34;ABC 验证码: 39792192\u0026#34;); 14?\u0026gt; 这样就可以了，上面代码比较难理解的是\u0026lt;22\u0026gt;，那是报错级别的计算方法，Facility + Severity：\n1 * Facility values: 2 * 0 kernel messages 3 * 1 user-level messages 4 * 2 mail system 5 * 3 system daemons 6 * 4 security/authorization messages 7 * 5 messages generated internally by syslogd 8 * 6 line printer subsystem 9 * 7 network news subsystem 10 * 8 UUCP subsystem 11 * 9 clock daemon 12 * 10 security/authorization messages 13 * 11 FTP daemon 14 * 12 NTP subsystem 15 * 13 log audit 16 * 14 log alert 17 * 15 clock daemon 18 * 16 local user 0 (local0) (default value) 19 * 17 local user 1 (local1) 20 * 18 local user 2 (local2) 21 * 19 local user 3 (local3) 22 * 20 local user 4 (local4) 23 * 21 local user 5 (local5) 24 * 22 local user 6 (local6) 25 * 23 local user 7 (local7) 26 * 27 * Severity values: 28 * 0 Emergency: system is unusable 29 * 1 Alert: action must be taken immediately 30 * 2 Critical: critical conditions 31 * 3 Error: error conditions 32 * 4 Warning: warning conditions 33 * 5 Notice: normal but significant condition (default value) 34 * 6 Informational: informational messages 35 * 7 Debug: debug-level messages 计算方法就是 （facility*8 + severity），这里的22+0，可以理解成 local6 ，就是级别6\n如果发了一个 \u0026ldquo;local use 4\u0026rdquo; 和 Serverity = 5 的消息，那么就是 20×8+5=165 ，包头就是 \u0026lt;165\u0026gt;\n","date":"2021-10-28","img":"","permalink":"https://bajie.dev/posts/20211028-php_syslog/","series":null,"tags":null,"title":"PHP程序如何发送syslog到远程服务器"},{"categories":null,"content":"用 kubernetes 越多，用 docker 越多，就愈发感觉到好处多多。\n简简单单的一个可执行文件，用 docker 基板 alphine 封装，就可以运行起一个 pod ，然后指定 deployment、svc、ingress，就可以将服务暴露出去。\n其实很多情况下单可执行文件 + systemd也是不错的选择。\n这不就遇到个问题，ghostunnel 这个软件，github 只释放出了源代码以及 windows 、linux 和 mac 的三个可执行版本。\n可我的执行环境是 nanopi ，是个 arm7 的架构，就无计可施了。\n无奈下，在 nanopi 上装了 go ，编译了个 arm7 的出来。\n但是遇到 vaultwanden ，rust 的，就没法弄了，vps 太弱，根本无法用 cargo 编译。\n那怎么办呢？方法如下，不安装 Docker ，也可以把镜像中的文件抽取出来\n1$ mkdir vm 2$ wget https://raw.githubusercontent.com/jjlin/docker-image-extract/main/docker-image-extract 3$ chmod +x docker-image-extract 4$ ./docker-image-extract vaultwarden/server:alpine 5Getting API token... 6Getting image manifest for vaultwarden/server:alpine... 7Fetching and extracting layer a0d0a0d46f8b52473982a3c466318f479767577551a53ffc9074c9fa7035982e... 8Fetching and extracting layer 3a9a529931676767ec84d35ab19774b24bd94e20f6fff7e6bda57ef5f2a66cfc... 9Fetching and extracting layer f9dcfa9aefe67ce52ab2a73e515ea715d242348b8fc338dbe4ca72a853ea0318... 10Fetching and extracting layer 4249d8cece35148b5faca2c6a98d566a271a1996b127b14480793ee8825e43c0... 11Fetching and extracting layer 72f4873a62cc82eaf28905077df3791e3b235bf5d17670e7aff6d5fb5e280739... 12Fetching and extracting layer 8eb772c524f9d998c8c7c92acc5ba96e3e9ebfb175dbb2441fe6e7b7598874f5... 13Fetching and extracting layer 663794f103b44abb8a90e1376dce14735905e2f938b4ca7e0ff379b09cbf6148... 14Image contents extracted into ./output. 这样我们就可以在 output 目录下得到 vaultwarden 和 web-vault\n如果我们要拿到 arm7 的镜像，还需要再费点劲，以 ghostunnel 为例：\n访问： https://hub.docker.com/r/ghostunnel/ghostunnel/tags 找到 linux/arm/v7 的 tag，40247f4b49c3 点开后，找到 DIGEST: ，复制sha256以及后面的字串，sha256:40247f4b49c364046ebf47696dbd31752b4db2ae2b9edf1fd4c52c2573f06e04\n按如下方法释放即可：\n1$ ./docker-image-extract ghostunnel/ghostunnel:sha256:40247f4b49c364046ebf47696dbd31752b4db2ae2b9edf1fd4c52c2573f06e04 不用担心目录结构，释放出来的文件会放到当前 output 目录下。\n","date":"2021-10-27","img":"","permalink":"https://bajie.dev/posts/20211027-docker_extract_file/","series":null,"tags":null,"title":"没有装Docker如何从镜像中释放出文件"},{"categories":null,"content":"为什么会有非 Docker 环境这个怪字眼呢？\n无他，因为满网搜索到的教程都是在 Docker 环境下安装使用。\n但是穷啊，八戒的 vps 是个单核 500m 的 justhost 机器，便宜的很，这种廉价机器来跑 Docker，基本要占100M，跑不太动。\n这种一穷二白的环境，就只能把 Bitwarden 从容器里拆出来用。\n好在 Bitwarden_rs 是一个 rust 程序，占内存(16M左右)和cpu极少，本身就适合在 systemd 环境下跑。\n这里就利用 vaultwarden 和 traefik，在一台老破小服务器上运行。\n系统环境是 CentOS 7.9\n步骤如下：\n一、下载bitwarden(vaultwarden) 1wget https://github.com/dani-garcia/vaultwarden/archive/refs/tags/1.23.0.tar.gz 二、安装cargo并编译（可选） 1yum install -y epel-release 2yum install -y openssl-devel cargo 3 4cd vaultwarden-1.23.0 5cargo build --release --features sqlite 直接爆错啊，小小的 vps 连编译都过不去，编译进程都被 kill 掉了\n三、下载vaultwarden主文件 编译不通，就只能想别的办法了。Faint\n找一台有 docker 机器，从里面把文件都解析出来好了\n1docker pull vaultwarden/server:alpine 2docker create --name vw vaultwarden/server:alpine 3docker cp vw:/vaultwarden . 4docker cp vw:/web-vault . 5docker rm vw 这样会得到一个可执行文件 vaultwarden 和一个目录 web-vault\n我们把这两个东西都挪到 /opt/vaultwarden 目录下，并且建立 data 文件夹，用来存放要生成的 sqlite3 数据文件。\n1mkdir -p /opt/vaultwarden/data 2mv vaultwarden /opt/vaultwarden 3mv web-vault /opt/vaultwarden 四、生成systemd启动文件 注意，下面我们设置了 vaultwarden ROCKET_ADDRESS 的监听地址是 127.0.0.1 ，一是为了安全，二是为了下一步我们搭建 traefik，来反代 vaultwarden 用的；因为访问 vaultwarden 必须要加证书，而它本身是没有这个功能的，必须前置一个 nginx 或者 haproxy 或者 traefik 或者 carddy。\n1cat \u0026lt;\u0026lt; EOF \u0026gt;\u0026gt; /etc/systemd/system/vaultwarden.service 2[Unit] 3Description=Bitwarden 4 5[Service] 6Type=simple 7Restart=always 8Environment=\u0026#34;ROCKET_ADDRESS=127.0.0.1\u0026#34; 9WorkingDirectory=/opt/vaultwarden 10ExecStart=/opt/vaultwarden/vaultwarden 11 12[Install] 13WantedBy=local.target 14EOF 五、配置traefik 1wget https://github.com/traefik/traefik/releases/download/v2.4.8/traefik_v2.4.8_linux_amd64.tar.gz 2tar zxvf traefik_v2.4.8_linux_amd64.tar.gz 3 4mkdir -p /opt/traefik/dynamic 5mv traefik /opt/traefik 生成traefik配置文件，利用 traefik 自动申请 Let\u0026rsquo;s encrypt 证书\n1cat \u0026lt;\u0026lt; EOF \u0026gt;\u0026gt; /opt/traefik/traefik.yml 2log: 3 level: DEBUG 4 5api: 6 insecure: false 7 dashboard: true 8 9entryPoints: 10 http: 11 address: \u0026#34;:80\u0026#34; 12 http: 13 redirections: 14 entryPoint: 15 to: https 16 scheme: https 17 https: 18 address: \u0026#34;:443\u0026#34; 19 20certificatesResolvers: 21 letsEncrypt: 22 acme: 23 storage: /opt/traefik/acme.json 24 email: zhangranrui@gmail.com 25 tlsChallenge: {} 26 httpChallenge: 27 entryPoint: http 28 29providers: 30 file: 31 directory: /opt/traefik/dynamic 32 watch: true 配置 vaultwarden 代理\n1cat \u0026lt;\u0026lt; EOF \u0026gt;\u0026gt; /opt/traefik/dynamic/pass.yml 2http: 3 routers: 4 https_01: 5 rule: \u0026#34;Host(`xxx.rendoumi.com`)\u0026#34; 6 service: svc_01 7 tls: 8 certresolver: letsEncrypt 9 http_01: 10 rule: \u0026#34;Host(`xxx.rendoumi.com`)\u0026#34; 11 service: svc_01 12 entryPoints: 13 - http 14 services: 15 svc_01: 16 loadBalancer: 17 servers: 18 - url: \u0026#34;http://localhost:8000\u0026#34; 19EOF 设置 traefik 的 systemd 启动文件\n1cat \u0026lt;\u0026lt; EOF \u0026gt;\u0026gt; /etc/systemd/system/traefik.service 2[Unit] 3Description=traefik 4 5[Service] 6Type=simple 7Restart=always 8WorkingDirectory=/export/servers/traefik 9ExecStart=/export/servers/traefik/traefik 10 11[Install] 12WantedBy=local.target 13EOF 五、启动vaultwarden和traefik 1systemctl daemon-reload 2systemctl enable --now vaultwarden 3systemctl enable --now traefik 打开页面，我们就成功的用一台老破小搭建了自己的密码管理服务器！！！\n","date":"2021-10-27","img":"","permalink":"https://bajie.dev/posts/20211027-bitwarden/","series":null,"tags":null,"title":"Bitwarden（vaultwarden）如何在非Docker环境下安装使用"},{"categories":null,"content":"在生产环境来创建阿里ACK托管k8s集群的过程：\n完全用于生产，不是搭建来做测试用的。\n授公司委托，给的RAM用户，所以阿里云RAM第一次登录后，强制修改密码\n然后授权资源管理， 正式开始建立过程\n一、准备条件 两台及以上ecs服务器\n阿里云账户余额100元以上（阿里云要求）\n阿里云oss一个（oss和ecs在一个区域最好）\n首先阿里云创建k8s集群要求至少有两台ecs服务器，可以创建集群的时候再购买，不要预先购买。\n二、下面开始创建： 阿里云最左上角的菜单（新版本首页）-\u0026gt;产品与服务-\u0026gt;容器服务kubernetes版本\n第一次创建会让开启ram授权，正常点击授权就可以\n点击创建集群\n点击后如下\n各个选项的详细说明：\n第一部分：\n集群版本： 最上面可以选择ACK托管版，和其他4个版本，着重说一下专有版和托管版的区别\n专有版本：master和worker都需要自己创建，如果需要高可用，那么master需要至少三个，也就是说，如果你不想把master和worker放在同一台服务器上，就要多使用三台服务器。\nACK托管版：master由阿里云给创建，自己只需要购买worker服务器。\n集群名称： k8s-hbb\n地域： 请选择自己ecs和rds等资源所在区域，这里是华东2（上海）\nKubernetes版本：阿里云已经做好充分的测试了，所以选择默认的即可。这里是 1.18.8-aliyun.1\n容器运行时： Docker 19.03.5\n第二部分：\n专有网络: 专有网络选择和ecs，rds同一个专有网络，这里是vpc-uf6pcr7nvp3dqmx86yyk0，网段是172.19.0.0/16\n虚拟交换机： 同一个专有网络下面的交换机是可以互通的，这里新建一个虚拟交换机，网段是172.19.240.0/20\n网络插件： 选Flannel，除去阿里云自己的区别描述，还有一点 如果使用flannel插件，则worker端对外，访问外网（比如短信接口等）使用的是worder所在ecs自己的eip或者如果使用的是snat模式，就是snat绑定的eip。如果使用的terway插件则走的就是snat的eip。注意，创建集群成功后，会为集群创建一个对外服务的ingress的slb，worker内部的容器直接对外访问，使用的不是这个slb的ip。slb只是进来的通道。\npod网络 CIDR：为统一起见，10.240.0.0/20\nservice CIDR：为统一起见，192.168.240.0/20\n注意以上 建立好了三个网段，三个网段中均有240字段，便于记忆\nECS(2台)：172.19.240.0/20\nPOD网段：10.240.0.0/20\nService网段：192.168.240.0/20 ​\n节点IP数量：256，指单个节点可运行 Pod 数量的上限。 一定要拉到最大量256 ，弄到16的话，一个节点本身要跑10多个system的pod，就无法跑应用pod了。\n第三部分:\n配置SNAT：必选配置SNAT，对外主动访问的时候IP需要一致。解释：如果ecs没有访问外网能力，则必须使用snat，snat就是把vpc绑定一个eip，然后给内部的ecs使用nat方式主动外出访问用的，比如主动反问第三方的接口等。如果ecs自己已经绑定了eip或者自带ip带宽，可以不选择。\nAPISERVER访问：必选公网EIP暴露，这个绑定以后ip不收费，可以使用流量包，管理master用的。如果要使用『云效』必选。\n默认不选中使用EIP暴露API Server。 API Server提供了各类资源对象（Pod，Service等）的增删改查及watch等HTTP Rest接口。 - 如果选择开放，会创建一个EIP，并挂载到内网SLB上。此时，Master节点的6443端口（对应API Server）暴露出来，用户可以在外网通过kubeconfig连接并操作集群。 - 如果选择不开放，则不会创建EIP，您只能在VPC内部用kubeconfig连接并操作集群。\nRDS白名单：这个注意选择，目前阿里云显示出来的只有普通的mysql-rds，redis的和polardb的都不显示\n安全组：选择企业类型就可以，后期可以修改规则。\n第四部分\nKube-proxy模式：选IPVS，比IPTABLES性能高\n集群本地域名： hbb.local，.local结尾的统统是本地域名\n下一步，增加worker\n必须选新增实例，不要选择现有实例\n新增实例：就是新购买ecs，要注意自己选择vpc和交换机\n选择已有实例：可以选择现有的服务器，注意：现有服务器会被更换硬盘，硬盘内容会被清空。\n企业级实例规格族\n实例规格族名称格式为ecs.\u0026lt;规格族\u0026gt;，实例规格名称为ecs.\u0026lt;规格族\u0026gt;.large。\necs：云服务器ECS的产品代号。 \u0026lt;规格族\u0026gt;：由小写字母加数字组成。 小写字母为某个单词的缩写，并标志着规格族的性能领域。部分小写字母的含义如下所示。 c：一般表示计算型（computational） g：一般表示通用型（general） r：一般表示内存型（ram） ne：一般表示网络增强型（network enhanced） 数字一般区别同类型规格族间的发布时间。更大的数字代表新一代规格族，拥有更高的性价比，价格低性能好。 large：n越大，vCPU核数越多。 例如，ecs.g6.2xlarge表示通用型g6规格族中的一个实例规格，拥有8个vCPU核。相比于g5规格族，g6为新一代通用型实例规格族。\n按上面选ecs.c6.large，费用0.95/时，似乎比ecs.n1.medium 1.34/时好。\n节点数量：最少是2个，无法减到0，没办法。\n系统盘：选ESSD，速度快，40G即可。不要开云盘备份，会要求设置snapshot策略，要收钱。\n操作系统：选CentOS 7.9，不要选aliyun的自定义版本。更加标准化，便于升级。\n这里选择操作系统的时候，只有两个系统可以选择，一个是centos7，一个是阿里云linux。为什么操作系统不能选很多种？因为阿里云要使用Cloud-init自动安装docker的各种工具包进作为worker角色的ecs，所以他对系统的要求更高，否则很可能出现各种各样的问题。这里才会有这种限制。\n密钥对选则新建一个k8s-ssh。\n下一步组件配置\n安装ingress组件：需要对外服务，这个必选，会给分配一个slb负载均衡\n如果想K8S集群的服务直接提供服务给用户访问，可以选择『公网』，它会创建一个SLB并用EIP暴露公网，后端是k8s-ingress入口。\n负载类型： 公网，对外服务就要写公网。\n存储插件： 必选CSI，对之后创建数据卷语法没影响\n监控插件： 基础版是免费的，可以放心使用\n日志服务： 日志服务可以加，尤其是以后如果想采集内部doker里面的日志，这里还是推荐加一下最好，他会自动创建标记采集，后面使用这个标记可以方便的自动添加日志节点。\nslb费用： 0.66/小时,带宽费用0.8/g\n下一步确认配置\n核对一下是否和自己选择的一样\n核对无误后点击创建集群。注意：创建集群的时候，会检测一些权限，如果权限未开通，可以令开页面进行开通授权，比如ess弹性伸缩，开通后点小按钮刷新状态， 状态都ok以后，点击创建集群。\n等待十分钟左右集群创建成功。到此创建集群已经完成。\n然后到控制台可以查看，这样ACK集群就创建好了。\n后记：\n毁掉ACK的时候，切忌去删除arms-prom的helm，再删除ack集群，否则会清不干净东西。\n下次重建的时候会装不上prometheus\n","date":"2021-10-26","img":"","permalink":"https://bajie.dev/posts/20211026-ack_build/","series":null,"tags":null,"title":"阿里云ACK完全生产环境规划和搭建"},{"categories":null,"content":"之前介绍过如何制作一个 centos live cdrom 系统\n那么，某些情况下我们可能无法弄一个 pxe 系统，而只能通过 idrac 挂载 iso 的方式安装系统\n该如何去做呢？\n步骤如下：\n一、下载Centos的minimal安装光盘 1wget http://mirrors.163.com/centos/7.9.2009/isos/x86_64/CentOS-7-x86_64-Minimal-2009.iso 2yum install -y mkisofs 二、准备kickstart安装文件 下载： centos7.ks 1text 2skipx 3install 4 5auth --useshadow --enablemd5 6authconfig --enableshadow --passalgo=sha512 7 8firstboot --disable 9keyboard us 10lang en_US.UTF-8 11reboot 12cdrom 13 14firewall --disable 15selinux --disabled 16 17services --enabled=\u0026#34;chronyd\u0026#34; 18logging level=info 19 20 21#ignoredisk --only-use=vda 22ignoredisk --only-use=sda 23#bootloader --location=mbr --append=\u0026#34;net.ifnames=0 biosdevname=0 crashkernel=auto\u0026#34; 24bootloader --location=mbr --append=\u0026#34;crashkernel=auto\u0026#34; 25 26rootpw --plaintext Renren2021! 27timezone Asia/Shanghai --isUtc 28 29network --device=lo --hostname=localhost.localdomain 30user --name=supdev --gid=511 --groups=\u0026#34;supdev\u0026#34; --uid=511 --password=\u0026#34;Renren2021!\u0026#34; 31 32zerombr 33clearpart --all --initlabel 34 35part biosboot --fstype=biosboot --size=1 36part /boot --fstype ext4 --size=2048 37part swap --asprimary --size=8192 38part / --fstype ext4 --size=1 --grow 39 40#part biosboot --fstype=biosboot --size=1 41#part /boot --fstype ext2 --size 250 42#part pv.01 --size 1 --grow 43#volgroup vg pv.01 44#logvol / --vgname=vg --size=1 --grow --fstype ext4 --fsoptions=discard,noatime --name=root 45#logvol /tmp --vgname=vg --size=1024 --fstype ext4 --fsoptions=discard,noatime --name=tmp 46#logvol swap --vgname=vg --recommended --name=swap 47 48#uefi 49#partition /boot/efi --asprimary --fstype=vfat --label EFI --size=200 50#partition /boot --asprimary --fstype=ext4 --label BOOT --size=500 51#partition / --asprimary --fstype=ext4 --label ROOT --size=4096 --grow 52 53 54services --enabled=network 55 56reboot 57 58%pre 59parted -s /dev/sda mklabel gpt 60%end 61 62%packages 63@core 64@system-admin-tools 65@additional-devel 66@virtualization-client 67@virtualization-platform 68@virtualization-tools 69libguestfs-tools-c 70perl-Sys-Virt 71qemu-guest-agent 72qemu-kvm-tools 73curl 74dstat 75expect 76openssl 77initscripts 78ipmitool 79lrzsz 80lsof 81mtools 82nc 83nmap 84perl 85perl-CPAN 86procps 87python 88screen 89sysstat 90systemtap 91systemtap-client 92systemtap-devel 93tcpdump 94telnet 95vim 96wget 97wsmancli 98zip 99chrony 100kexec-tools 101net-tools 102ntp 103ntpdate 104man 105acpid 106chrony 107telnet 108%end 三、准备生成iso的脚本 下载： makeiso.sh 1#!/bin/bash 2rm -rf /tmp/bootiso /tmp/bootcustom /tmp/boot.iso 3mkdir /tmp/bootiso 4mount -o loop CentOS-7-x86_64-Minimal-2009.iso /tmp/bootiso 5 6mkdir /tmp/bootcustom 7cp -r /tmp/bootiso/* /tmp/bootcustom 8umount /tmp/bootiso 9rmdir /tmp/bootiso 10 11 12chmod -R u+w /tmp/bootcustom 13 14cp centos7.ks /tmp/bootcustom/isolinux/ks.cfg 15 16sed -i \u0026#39;/menu\\ default/d\u0026#39; /tmp/bootcustom/isolinux/isolinux.cfg 17sed -i \u0026#39;s/^timeout\\ .*/timeout 10/g\u0026#39; /tmp/bootcustom/isolinux/isolinux.cfg 18sed -i \u0026#39;/^label\\ linux/i label\\ kickstart\u0026#39; /tmp/bootcustom/isolinux/isolinux.cfg 19sed -i \u0026#39;/^label\\ linux/i \\ \\ menu\\ label\\ ^Install\\ Using\\ Kickstart\\ CentOS 7\u0026#39; /tmp/bootcustom/isolinux/isolinux.cfg 20sed -i \u0026#39;/^label\\ linux/i \\ \\ menu\\ default\u0026#39; /tmp/bootcustom/isolinux/isolinux.cfg 21sed -i \u0026#39;/^label\\ linux/i \\ \\ kernel\\ vmlinuz\\ biosdevname=0\u0026#39; /tmp/bootcustom/isolinux/isolinux.cfg 22sed -i \u0026#39;/^label\\ linux/i \\ \\ append\\ initrd=initrd.img\\ ks=cdrom:\\/ks.cfg\u0026#39; /tmp/bootcustom/isolinux/isolinux.cfg 23sed -i \u0026#39;/^label\\ linux/i \\\\n\u0026#39; /tmp/bootcustom/isolinux/isolinux.cfg 24 25cd /tmp/bootcustom 26mkisofs -o /tmp/boot.iso -b isolinux.bin -c boot.cat -no-emul-boot -boot-load-size 4 -boot-info-table -V \u0026#34;CentOS 7 x86_64\u0026#34; -R -J -v -T isolinux/. . 生成的自动安装光盘文件在 /tmp/boot.iso ，在 idrac 中 mount 出来，就可以用 virtual CD-ROM 自动安装了\n","date":"2021-10-25","img":"","permalink":"https://bajie.dev/posts/20211025-autoinstall_cd/","series":null,"tags":null,"title":"Centos Auto Install Cdrom自动安装cdrom的制作"},{"categories":null,"content":"为了研发方便就给他们在内网开通了 vsftpd 的服务。\n结果 java 直接有封好的 ftp library 可用，大家就直接用了。\n导致任何单独的一个文件上传都会起一个 ftp 实例，没有复用 ftp 的 socket 链接 。系统挤压了大量的socket连接。\n烦恼啊，出了事就麻烦。需要把日志都详细记下来\n做法如下：\n1vi /etc/vsftp/vsftpd.conf 2...... 3dual_log_enable=YES 4log_ftp_protocol=YES 5xferlog_enable=YES 6xferlog_std_format=NO 7...... 解释一下：\ndual_log_enable \u0026mdash; 和 xferlog_enable 协同，会写两份日志，一份到/var/log/xferlog，一份到/var/log/vsftpd.log\nlog_ftp_protocol \u0026mdash; 和 xferlog_enable 协同，同时xferlog_std_format需要设置为NO，这样所有的 FTP 命令都会记录下来。\n这样所有人的操作都会被记录下来，就后顾无忧了。\n","date":"2021-10-25","img":"","permalink":"https://bajie.dev/posts/20211025-vsftpd/","series":null,"tags":null,"title":"Vsftpd的日志设置"},{"categories":null,"content":"一般来说，我们要搭建一个正式的pxe自动装机系统，需要装 dnsmasq 做 dhcp + tftp ，需要编译 ipxe 来获得 undionly.kpxe ，需要 http 服务器来提供资源下载，repo 同步服务来提供 repo。组件非常多，也比较麻烦。\n当然，这么多也是有必要的，因为可以持续提供一个稳定的装机系统。\n场景一换，如果我们在本地机房里，什么都没有，想搭一套环境的步骤就比较繁复了。\nPyPXE 就是非常简单的一个程序，居然自己实现了用于 PXE 的 dhcp、tftp 和 http 全部的功能，而且支持 iPXE。\n太牛逼了，前提啊，PyPXE 是基于 Python 2.7 的，Python 3.x是运行不了的。\n想让它跑起来还必须做一定的修改，步骤如下：\n一、下载PyPXE 1git clone https://github.com/pypxe/PyPXE.git 2cd PyPXE 下载就行了，不用安装。\n二、手动生成config.json配置文件 1{ 2 \u0026#34;DHCP_SERVER_IP\u0026#34;: \u0026#34;192.168.85.27\u0026#34;, 3 \u0026#34;DHCP_FILESERVER\u0026#34;: \u0026#34;192.168.85.27\u0026#34;, 4 5 \u0026#34;DHCP_OFFER_BEGIN\u0026#34;: \u0026#34;192.168.85.200\u0026#34;, 6 \u0026#34;DHCP_OFFER_END\u0026#34;: \u0026#34;192.168.85.250\u0026#34;, 7 \u0026#34;DHCP_SUBNET\u0026#34;: \u0026#34;255.255.255.0\u0026#34;, 8 \u0026#34;DHCP_ROUTER\u0026#34;: \u0026#34;192.168.85.1\u0026#34;, 9 \u0026#34;DHCP_DNS\u0026#34;: \u0026#34;114.114.114.114\u0026#34;, 10 11 \u0026#34;DHCP_SERVER_PORT\u0026#34;: 67, 12 \u0026#34;DHCP_BROADCAST\u0026#34;: \u0026#34;\u0026#34;, 13 \u0026#34;DHCP_MODE_PROXY\u0026#34;: false, 14 \u0026#34;DHCP_WHITELIST\u0026#34;: false, 15 \u0026#34;HTTP_PORT\u0026#34;: 80, 16 \u0026#34;LEASES_FILE\u0026#34;: \u0026#34;\u0026#34;, 17 \u0026#34;MODE_DEBUG\u0026#34;: \u0026#34;dhcp\u0026#34;, 18 \u0026#34;MODE_VERBOSE\u0026#34;: \u0026#34;\u0026#34;, 19 \u0026#34;NBD_BLOCK_DEVICE\u0026#34;: \u0026#34;\u0026#34;, 20 \u0026#34;NBD_COPY_TO_RAM\u0026#34;: false, 21 \u0026#34;NBD_COW\u0026#34;: true, 22 \u0026#34;NBD_COW_IN_MEM\u0026#34;: false, 23 \u0026#34;NBD_PORT\u0026#34;: 10809, 24 \u0026#34;NBD_SERVER_IP\u0026#34;: \u0026#34;0.0.0.0\u0026#34;, 25 \u0026#34;NBD_WRITE\u0026#34;: false, 26 \u0026#34;NETBOOT_DIR\u0026#34;: \u0026#34;netboot\u0026#34;, 27 \u0026#34;NETBOOT_FILE\u0026#34;: \u0026#34;boot.http.ipxe\u0026#34;, 28 \u0026#34;STATIC_CONFIG\u0026#34;: \u0026#34;\u0026#34;, 29 \u0026#34;SYSLOG_PORT\u0026#34;: 514, 30 \u0026#34;SYSLOG_SERVER\u0026#34;: null, 31 \u0026#34;USE_DHCP\u0026#34;: true, 32 \u0026#34;USE_HTTP\u0026#34;: true, 33 \u0026#34;USE_IPXE\u0026#34;: true, 34 \u0026#34;USE_TFTP\u0026#34;: true 35} 上面json文件无法加注解，我们把它分三部分\n本机配置，本机的地址都是 192.168.85.27\ndhcp 的配置，开始192.168.85.200，结束192.68.85.250，掩码255.255.255.0，网关192.168.85.1，DNS114.114.114.114\n第三部分不用动\n三、下载ISO并修改ipxe脚本 1cd netboot 2wget http://mirrors.163.com/rocky/8/isos/x86_64/Rocky-8.4-x86_64-dvd1.iso 3mkdir rocky8.iso 4mount -o loop Rocky-8.4-x86_64-dvd1.iso rocky8.iso 5 6cat \u0026lt;\u0026lt; EOF \u0026gt;\u0026gt; boot.http.ipxe 7#!ipxe 8 9:start 10menu PXE Boot Options 11item shell iPXE shell 12item Rocky8 Install rocky8 13item exit Exit to BIOS 14 15choose --default rocky8 --timeout 5000 option \u0026amp;\u0026amp; goto ${option} 16:shell 17shell 18 19 20:rocky8 21set root http://192.168.85.27/rocky8.iso 22initrd ${root}/images/pxeboot/initrd.img 23kernel ${root}/images/pxeboot/vmlinuz inst.repo=${root}/ initrd=initrd.img ip=dhcp 24boot 25 26 27:exit 28exit 29EOF 三、修改源代码 运行一下：\n1python -m pypxe.server --config config.json --debug all --verbose all 如果我们起一台机器或者虚机，会报第一个错：\nUnicodeDecodeError: \u0026lsquo;ascii\u0026rsquo; codec can\u0026rsquo;t decode byte 0xc0 in position 0: ordinal not in range(128)\n这个是代码报错，我们需要修改一下\n1vi pypxe/dhcp.py 2 3 def tlv_encode(self, tag, value): 4 \u0026#39;\u0026#39;\u0026#39;Encode a TLV option.\u0026#39;\u0026#39;\u0026#39; 5 6 # 注释掉下面的两行，我们不需要打印出我们一定能看懂的字符，都按bytes处理即可 7 #if type(value) is str: 8 # value = value.encode(\u0026#39;ascii\u0026#39;) 9 value = bytes(value) 10 return struct.pack(\u0026#39;BB\u0026#39;, tag, len(value)) + value 然后我们需要修改第二个地方，理由是这个 PyPXE 会判断 Client 发过来的 dhcp 请求，它只实现了针对PXE-Client的 Vendor-class：\n所以我们也要屏蔽一下，否则按照正常过程\n客户端dhcp \u0026ndash;\u0026gt; PyPXE 后，PyPXE 送回客户 ipxe 脚本，然后客户安装，当加载了vmlinuz和initrd之后会进入anaconda-linux进行系统安装，过程中会再次向DHCP服务器申请IP地址， 这个时候他向DHCP Server发出的discover申请是得不到回复的，因此安装过程将被打断。\n1vi pypxe/dhcp.py 2 3 def validate_req(self, client_mac): 4 # client request is valid only if contains Vendor-Class = PXEClient 5 \u0026#39;\u0026#39;\u0026#39;代码整个注释掉，直接返回 True 6 if self.whitelist and self.get_mac(client_mac) not in self.get_namespaced_static(\u0026#39;dhcp.binding\u0026#39;): 7 self.logger.info(\u0026#39;Non-whitelisted client request received from {0}\u0026#39;.format(self.get_mac(client_mac))) 8 return False 9 if 60 in self.options[client_mac] and \u0026#39;PXEClient\u0026#39;.encode() in self.options[client_mac][60][0]: 10 self.logger.info(\u0026#39;PXE client request received from {0}\u0026#39;.format(self.get_mac(client_mac))) 11 return True 12 self.logger.info(\u0026#39;Non-PXE client request received from {0}\u0026#39;.format(self.get_mac(client_mac))) 13 return False 14 \u0026#39;\u0026#39;\u0026#39; 15 return True 这样修改后，就可以正常安装了。\n服务器启动：\n客户端启动pxe开始安装，看下面，系统的ipxe dhcp一次，然后chainload.kpxe 又一次，anaconda 又一次，最少会发三次或更多的dhcp请求。\n用 VNC 连进去可以看到安装画面，如果是 kickstart 就是全自动安装了。\n","date":"2021-10-22","img":"","permalink":"https://bajie.dev/posts/20211022-pypxe/","series":null,"tags":null,"title":"PyPXE-一个牛逼的一站式PXE安装包"},{"categories":null,"content":"上一篇文章我们介绍了 ETCD 的容器化，搞这件事情的主要目的其实是要动态更新 Nginx 的配置\n这一章我们就来配置 confd 和 Nginx，来达到动态更新 Nginx 配置的目的\n一、安装配置confd 下载并安装：\n1wget https://github.com/kelseyhightower/confd/releases/download/v0.16.0/confd-0.16.0-linux-amd64 2mv confd-0.16.0-linux-amd64 /usr/sbin/confd 3chmod +x /usr/sbin/confd 生成配置文件：\n我们在 etcd 中存放的格式如下\n1etcdctl set /nginx/app01/subdomain app1 2etcdctl set /nginx/app01/upstream/app01_1 \u0026#34;192.168.0.1:5601\u0026#34; 3 4/nginx/app01/subdomain \u0026#34;app01\u0026#34; 5/nginx/app01/upstream/app01_1 \u0026#34;192.168.0.1:5601\u0026#34; 6/nginx/app01/upstream/app01_2 \u0026#34;192.168.0.2:5601\u0026#34; 那么，我们先生成 confd 的配置文件：\n1mkdir -p /etc/confd/{conf.d,templates} 2 3cat \u0026lt;\u0026lt;EOF\u0026gt;\u0026gt;/etc/confd/conf.d/nginx.toml 4[template] 5src = \u0026#34;nginx.conf.tmpl\u0026#34; 6dest = \u0026#34;/etc/nginx/conf.d/nginx-auto.conf\u0026#34; 7keys = [ 8 \u0026#34;/nginx/app01/subdomain\u0026#34;, 9 \u0026#34;/nginx/app01/upstream\u0026#34;, 10] 11check_cmd = \u0026#34;/usr/sbin/nginx -t\u0026#34; 12reload_cmd = \u0026#34;/usr/sbin/nginx -s reload\u0026#34; 13EOF 14 15cat \u0026lt;\u0026lt;EOT\u0026gt;\u0026gt;/etc/confd/templates/nginx.conf.tmpl 16upstream {{getv \u0026#34;/nginx/app01/subdomain\u0026#34;}} { 17{{range getvs \u0026#34;/nginx/app01/upstream/*\u0026#34;}} 18 server {{.}}; 19{{end}} 20} 21 22server { 23 server_name {{getv \u0026#34;/nginx/app01/subdomain\u0026#34;}}.example.com; 24 location / { 25 proxy_pass http://{{getv \u0026#34;/nginx/app01/subdomain\u0026#34;}}; 26 proxy_redirect off; 27 proxy_set_header Host $host; 28 proxy_set_header X-Real-IP $remote_addr; 29 proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; 30 } 31} 32EOT confd 会根据 etcd 的值，结合 nginx.conf.tmpl ，生成 nginx-auto.conf，然后 nginx -t 验证通过后，执行 nginx -s rolad。\n注意：nginx的配置中必须有 include /etc/nginx/conf.d/*.conf;\n二、运行confd 1# 只处理一次 2confd -onetime -backend etcd -node http://etcd-svc.default:2379 3 4# 按时间轮询 5confd -interval=60 -backend etcd -node http://etcd-svc.default:2379 \u0026amp; 这样就可以动态更新 Nginx 了。\n","date":"2021-10-21","img":"","permalink":"https://bajie.dev/posts/20211021-etcd_confd_nginx/","series":null,"tags":null,"title":"ETCD + CONFD + NGINX的配置"},{"categories":null,"content":"由于使用到了阿里的 K8S 托管集群 ACK，于是想占便宜。想用到托管 master node 的 etcd 来保存数据。\n结果是，未遂！！无法使用。\n阿里有单独的配置管理服务，复杂化了，不想用。\n那么解决方案就是，启动只有一个节点副本的 etcd pod，然后数据持久化到 OSS 的 S3 桶中。\n一、实现etcd的单节点docker化 首先我们只想在测试环境中跑一个单节点的 etcd，还没有用到 k8s，做法如下：\n1#!/bin/bash 2 3NODE1=172.18.31.33 4REGISTRY=quay.io/coreos/etcd 5# available from v3.2.5 6#REGISTRY=gcr.io/etcd-development/etcd 7 8docker run \\ 9 -p 2379:2379 \\ 10 -p 2380:2380 \\ 11 --volume=/data/etcd:/etcd-data \\ 12 --name etcd ${REGISTRY}:latest \\ 13 /usr/local/bin/etcd \\ 14 --data-dir=/etcd-data --name node1 \\ 15 --initial-advertise-peer-urls http://${NODE1}:2380 --listen-peer-urls http://0.0.0.0:2380 \\ 16 --advertise-client-urls http://${NODE1}:2379 --listen-client-urls http://0.0.0.0:2379 \\ 17 --initial-cluster node1=http://${NODE1}:2380 如上就可以了，容器跑起来以后进入容器测试一下：\n1docker exec -it 425f26903466 /bin/sh 2 3etcdctl -C http://127.0.0.1:2379 member list 4c3511611548b7c7c: name=node1 peerURLs=http://172.18.31.33:2380 clientURLs=http://172.18.31.33:2379 isLeader=true 5 6etcdctl ls --recursive / 这样一个单节点的 etcd 就弄好了，对外暴露的是 2379 和 2380 端口\n二、实现 etcd 的单节点 k8s 化 首先编写一个deployment文件etcd-deploy.yaml：\n下载：etcd-deploy.yaml 1apiVersion: apps/v1 2kind: Deployment 3metadata: 4 name: etcd-deploy 5 labels: 6 app: etcd 7spec: 8 replicas: 1 9 selector: 10 matchLabels: 11 app: etcd 12 template: 13 metadata: 14 labels: 15 app: etcd 16 spec: 17 containers: 18 - name: etcd 19 image: quay.io/coreos/etcd:latest 20 ports: 21 - containerPort: 2379 22 name: client 23 protocol: TCP 24 - containerPort: 2380 25 name: server 26 protocol: TCP 27 command: 28 - /usr/local/bin/etcd 29 - --name 30 - etcd 31 - --initial-advertise-peer-urls 32 - http://etcd:2380 33 - --listen-peer-urls 34 - http://0.0.0.0:2380 35 - --listen-client-urls 36 - http://0.0.0.0:2379 37 - --advertise-client-urls 38 - http://etcd:2379 39 - --initial-cluster 40 - etcd=http://etcd:2380 41 - --data-dir 42 - /etcd-data 43 volumeMounts: 44 - mountPath: /etcd-data 45 name: etcd-data 46 lifecycle: 47 postStart: 48 exec: 49 command: 50 - \u0026#34;sh\u0026#34; 51 - \u0026#34;-c\u0026#34; 52 - \u0026gt; 53 echo \u0026#34;127.0.0.1 etcd\u0026#34; \u0026gt;\u0026gt; /etc/hosts; 54 volumes: 55 - name: etcd-data 56 persistentVolumeClaim: 57 claimName: k8s-etcd-20g 58 restartPolicy: Always 注意上面，我们使用了一个 pvc 卷 k8s-etcd-20g，这个卷挂在 /etcd-data，是由 OSS 建立的，用于持久话数据，省得重启 etcd 的 pod，数据消失不见了。\n然后，我们需要把这个 deployment 作为 svc 服务暴露在集群中，再编写一个etcd-svc.yaml\n下载：etcd-svc.yaml 1apiVersion: v1 2kind: Service 3metadata: 4 name: etcd-svc 5spec: 6 ports: 7 - port: 2379 8 name: tcp2379 9 protocol: TCP 10 targetPort: 2379 11 - port: 2380 12 name: tcp2380 13 protocol: TCP 14 targetPort: 2380 15 selector: 16 app: etcd 17 type: ClusterIP kubectl apply 部署到 k8s 中，这样就可以了。\nk8s测试方法，随便启动一个 busybox pod，进去测试一下：\n1kubectl run curl --image=radial/busyboxplus:curl -i --tty --rm 2 3curl http://etcd-svc:2379/version 4 5curl http://etcd-svc.default:2379/version 6 7curl http://etcd-svc.default:2379/v2/keys 8 9curl http://etcd-svc.default:2379/v2/keys/?recursive=true 10 11curl http://etcd-svc.default:2379/v2/keys/service/nginx 12 13curl http://etcd-svc.default:2379/v2/keys/service/nginx/127.0.0.1 14 15curl --location --request PUT \u0026#39;http://etcd-svc:2379/v2/keys/service/nginx/10.240.0.41\u0026#39; --header \u0026#39;Content-Type: application/x-www-form-urlencoded\u0026#39; --data-urlencode \u0026#39;value=10.240.0.41:9000\u0026#39; 16 17curl http://etcd-svc.default:2379/v2/keys/service/nginx/ ","date":"2021-10-21","img":"","permalink":"https://bajie.dev/posts/20211021-etcd_docker/","series":null,"tags":null,"title":"Etcd单节点应用"},{"categories":null,"content":"在生产环境中我们大量使用了 kvm 的虚拟技术，虚拟机的镜像系统使用的是 Cloud-init 的技术\n不可避免的，虚机会遭到各种损坏，维护的手段就十分必要了\n假设我们有一个虚机文件 vis-16-41-18.qcow2 坏了\n一、安装支持包 1yum install libguestfs libguestfs-tools 二、查看日志 1virt-log -a vis-16-41-18.qcow2 没有什么特殊的报错信息\n三、分析文件系统组成 virt-filesystems和virt-df都可以，用virt-df看的更多一些\n1virt-filesystems -l -a vis-16-41-18.qcow2 2Name Type VFS Label Size Parent 3/dev/sda1 filesystem ext4 - 209715200 - 4/dev/sda2 filesystem ext4 - 214536355840 - 5 6virt-df -a vis-16-41-18.qcow2 7Filesystem 1K-blocks Used Available Use% 8vis-16-41-18.qcow2:/dev/sda1 194241 31706 152295 17% 9vis-16-41-18.qcow2:/dev/sda2 206088704 5639856 189973444 3% 四、挂载文件系统开始修复（方法1） 从上面可以看到 vis-16-41-18.qcow2 里面有两个分区，/dev/sda1 和/dev/sda2\n第一个应该是/boot，第二个是/\n把 / mount 出来\n1mkdir 18 2guestmount -a vis-16-41-18.qcow2 -m /dev/sda2 --rw ./18 或者全自动mount\n1guestmount -a vis-16-41-18.qcow2 -i --rw ./18 这样就可以直接进18目录进行修复操作了\n1cd 18/lib64 2ls libc*.* 发现同事胡乱升级glibc，把libc的基础库弄坏了，少libc.so.6的软链接，建立一个修复即可\n1ln -s libc-2.15.so libc.so.6 五、挂载文件系统开始修复（方法2） 我们可以用 guestmount，也可以直接用 guestfish 。\nguestfish 是个命令行工具。它使用 libguestfs 的所有功能。\n1guestfish 2 3Welcome to guestfish, the libguestfs filesystem interactive shell for 4editing virtual machine filesystems. 5 6Type: \u0026#39;help\u0026#39; for help on commands 7 \u0026#39;man\u0026#39; to read the manual 8 \u0026#39;quit\u0026#39; to quit the shell 9 10\u0026gt;\u0026lt;fs\u0026gt; add vis-16-41-18.qcow2 11\u0026gt;\u0026lt;fs\u0026gt; run 12\u0026gt;\u0026lt;fs\u0026gt; list-filesystems 13/dev/sda1: ext4 14/dev/sda2: ext4 15\u0026gt;\u0026lt;fs\u0026gt; mount /dev/sda2 / 16\u0026gt;\u0026lt;fs\u0026gt; cat /etc/fstab 17 18# 19# /etc/fstab 20# Created by anaconda on Mon Dec 29 15:24:53 2014 21# 22# Accessible filesystems, by reference, are maintained under \u0026#39;/dev/disk\u0026#39; 23# See man pages fstab(5), findfs(8), mount(8) and/or blkid(8) for more info 24# 25UUID=9fdc111c-3042-4527-b3f8-a2961e55077e / ext4 defaults 1 1 26UUID=1855d5e1-18f8-48ea-8c3b-c52cdd512a5e /boot ext4 defaults 1 2 27tmpfs /dev/shm tmpfs defaults 0 0 28devpts /dev/pts devpts gid=5,mode=620 0 0 29sysfs /sys sysfs defaults 0 0 30proc /proc proc defaults 0 0 31 32\u0026gt;\u0026lt;fs\u0026gt; guestfish的常用命令：\n1add vis-16-41-18.qcow2 2run 3list-filesystems 4 5ll / 6ls / 7cat /etc/fstab 8write-append /etc/rc.d/rc.local \u0026#34;service sshd start\u0026#34; 9edit /etc/fstab. 10less /var/log/messages 11mkdir /tmp/a 12touch /tmp/a/b.txt 13write /tmp/a/b.txt 14rm /tmp/a/b.txt 15 16upload Upload a local file to the disk. ###注意：是上载本地文件到镜像文件去！！！ 六、virt对应guestfish的一些命令 1virt-cat vis-16-41-18.qcow2 /home/supdev/.bash_history 2 3virt-copy-in Copy files and directories into a guest. 4virt-copy-out Copy files and directories out of a guest. 5 6virt-edit Edit a file in a guest. 7virt-ls List files and directories in a guest 七、virt-rescue救援模式 如果虚机系统起不来，可以先尝试进入 rescue 救援模式\nvirt-rescue 类似于救援 CD，但用于虚拟机，且无需提供 CD。\nvirt-rescue 为用户提供救援外壳和一些简单的恢复工具，可用于检查和更正虚拟机或磁盘映像中的问题。\n1virt-rescue -a vis-16-41-18.qcow2 2Welcome to virt-rescue, the libguestfs rescue shell. 3 4Note: The contents of / are the rescue appliance. 5You need to mount the guest\u0026#39;s partitions under /sysroot 6before you can examine them. A helper script for that exists: 7mount-rootfs-and-do-chroot.sh /dev/sda2 8 9\u0026gt;\u0026lt;rescue\u0026gt; 10[ 67.194384] EXT4-fs (sda1): mounting ext3 file system 11using the ext4 subsystem 12[ 67.199292] EXT4-fs (sda1): mounted filesystem with ordered data 13mode. Opts: (null) 14mount: /dev/sda1 mounted on /sysroot. 15mount: /dev bound on /sysroot/dev. 16mount: /dev/pts bound on /sysroot/dev/pts. 17mount: /proc bound on /sysroot/proc. 18mount: /sys bound on /sysroot/sys. 19Directory: /root 20Thu Jun 5 13:20:51 UTC 2014 21(none):~ # ","date":"2021-10-21","img":"","permalink":"https://bajie.dev/posts/20211021-libguestfs/","series":null,"tags":null,"title":"Libguestfs的救援手段"},{"categories":null,"content":"这是个娱乐话题，Dogecoin 狗币在马斯克的吹捧鼓动下，冲上云霄\n其实真的用CPU挖币，应该是挖 xmb 门罗币才是对的选择，挖狗币只是娱乐一下\n废话不多说，直接放上教程，我的机器是 CentoOS\n首先需要有个狗币钱包地址，这个我就不教大家了\n一、下载xmrig挖矿软件 下载地址：https://github.com/xmrig/xmrig/releases\n我们选择最近的下载就好\n二、做好加密通道 我们需要做好一条加密tcp通道\n用 ghostunnel, localhost:9999 \u0026mdash;\u0026gt; vps:9999 \u0026mdash;\u0026gt; rx.unmineable.com:3333\n三、用screen后台开挖 1screen 2 3#./xmrig -o localhost:9999 -a rx -k -u DOGE:狗币地址.矿工名#heyt-3711 4./xmrig -o localhost:9999 -a rx -k -u DOGE:DLR3DZGucJiSdahARW1vV5B1h3WYiw454a.work01 5 6ctrl+a+d 四、查看挖了多少 查看地址：https://unmineable.com/coins/DOGE/address/\n","date":"2021-10-21","img":"","permalink":"https://bajie.dev/posts/20211021-dogecoin/","series":null,"tags":null,"title":"如何用CPU挖狗币Dogecoin"},{"categories":null,"content":"chrony 已经成了事实标准，替代了ntp。\n但是，有几个细节，需要非常注意。\n给出我们的配置，/etc/chrony.conf\n1 2# Use public servers from the pool.ntp.org project. 3# Please consider joining the pool (http://www.pool.ntp.org/join.html). 4server 172.10.1.1 iburst prefer minpoll 6 maxpoll 10 5server 172.10.1.2 iburst 6 7# Record the rate at which the system clock gains/losses time. 8driftfile /var/lib/chrony/drift 9 10# Allow the system clock to be stepped in the first three updates 11# if its offset is larger than 1 second. 12makestep 1.0 3 13 14# Enable kernel synchronization of the real-time clock (RTC). 15rtcsync 16 17# Enable hardware timestamping on all interfaces that support it. 18#hwtimestamp * 19 20# Increase the minimum number of selectable sources required to adjust 21# the system clock. 22#minsources 2 23 24# Allow NTP client access from local network. 25#allow 192.168.0.0/16 26 27# Serve time even if not synchronized to a time source. 28#local stratum 10 29 30# Specify file containing keys for NTP authentication. 31#keyfile /etc/chrony.keys 32 33# Specify directory for log files. 34logdir /var/log/chrony 35logchange 0.5 36# Select which information is logged. 37#log measurements statistics tracking rtc 里面有好几个细节，下面逐一解释一下：\n一、server 这里可以添加很多时间服务器，172.10.1.1 和 172.10.1.2 是两台自建的时间服务器。\nibust 会在 chrony 启动的2秒内，去快速poll服务器4次来快速矫正当前系统时间\nprefer 优先使用指定的服务器\nminpoll 6，缺省是6，意思是2的6次方，也就是64秒，最小轮询时间服务器的时间间隔是64秒\nmaxpoll 10，缺省是10，同上，2的10次方，也就是1024秒，最大轮询时间间隔是1024秒\n通常情况下一过minpoll的时间周期，就会触发一次时间同步询问。\n二、makestep 正常情况下如果系统时钟跟时间服务器不一致，chrony调整的方式是慢慢增加，或慢慢减少，不会一步到位，直接去跟时间服务器对齐。\nmakestep 1.0 3，意思就是如果时间服务器跟系统时间相差1秒，那么就在下3个时钟更新中追上时间服务器。\n这样就会立刻快速追平了，这样会带来时间跳跃。\n三、rtcsync 这个是把系统时钟同步到主板的硬件时钟去。\n缺省情况下是11分钟同步一次\n四、logchange logchange 0.5，意思是如果chrony调整的系统时间，超过了0.5秒的时长，就会发一条消息到syslog，这样我们就能在/var/log/messages里看到这条消息了。\n五、验证调试 开发人员会问，什么时候同步的服务器啊，多长时间同步一次，时间到底准不准啊，有没有发生跳跃啊\n我们就用chronyc sources来验证，配置中\n1server 172.10.1.1 iburst prefer minpoll 4 maxpoll 5 2server 172.10.1.2 iburst 解释一下，minpoll 4 maxpoll 5 ，那么最小轮询时间16秒，最大32秒。\n我们可以看到上图 LastRx 就是上次询问时间服务器的间隔时间，14秒、15秒、16秒，然后就变1了，最小间隔16秒后，立即就询问时间服务器，同步时间。\n同样可以看到第二台时间服务器就不受这个限制，缺省minpoll 6，就是64秒。所以上图第二台，63秒、64秒、65秒，变0，才去询问时间服务器。\n总结一下，chrony 调整时间偏差是匀速的，缓慢的。它询问时间服务器的间隔由minpoll来控制。\n我们用logchange来记录大的时间调整，以备追溯和查询。\n","date":"2021-10-20","img":"","permalink":"https://bajie.dev/posts/20211020-chrony/","series":null,"tags":null,"title":"Chrony的几个详细配置细节"},{"categories":null,"content":"很现实的问题，局域网内有态势感知和网络流量分析，这很讨厌！\n那么，如何把某段流量隐藏起来，让态势感知无法分析呢？\n前提条件，你需要有国外的一台 VPS 作为外援，把 TCP 流量通过 TLS 加密送到国外的服务器，然后再转发到正确的目标服务器上，这样就不会被人追踪了。\n这里推荐 Ghostunnel ，这是个 Go 的程序，只有一个执行文件。配合 certik 证书生成，就完美了。\n项目地址：https://github.com/ghostunnel/ghostunnel\n首先我们使用 certik 生成三个证书，ca.pem server.pem 和 client1.pem\n然后ghostunnel以及三个证书文件都放在/usr/local/bin下\n一、在VPS上运行ghostunnel模式server 1/usr/local/bin/ghostunnel server --listen 0.0.0.0:9999 --target tr.dero.herominers.com:1117 --keystore server.pem --cacert ca.pem --allow-cn client1 --unsafe-target 上面监听了端口0.0.0.0:9999（监听0.0.0.0必须加参数\u0026ndash;unsafe-target），远程转发到dero的矿池端口1117，只允许验证过的cn client1连接。\n二、在本地机器上运行ghostunnel模式client 1/usr/local/bin/ghostunnel client --listen localhost:9999 --target 193.42.114.129:9999 --keystore client.pem --cacert ca.pem 本地监听localhost:9999，所以不用加\u0026ndash;unsafe-target参数，然后连接到远程 vps 服务器，ip地址是193.42.114.129，端口是9999\n这样就完成了。可以放心的启动程序，连接到本地端口localhost:9999，TCP流量就会被隐藏起来，不会被分析到.\n","date":"2021-10-19","img":"","permalink":"https://bajie.dev/posts/20211019-ghostunnel/","series":null,"tags":null,"title":"Ghostunnel使用TLS加密TCP流量"},{"categories":null,"content":"我们会有很多时候需要用到TLS证书，一个非常方便、小众的工具就是 certik 。\n这个软件纯由 Go 组成，就一个可执行文件，使用了 etcd 的 boltdb 格式存放所有的证书。\n软件地址：https://github.com/opencoff/certik\n使用：\n一、初始化证书库 1./certik -v tls.db init CA 二、签发一张server证书 1#./certik -v tls.db server -i IP.ADDR.ES server.domain.name 2./certik -v tls.db server -i 193.42.114.129 server 上面我们 server 的 ip 是193.42.114.129，域名简洁起见就叫 server 了 。\n三、签发一张client1证书 1./certik -v tls.db client client1 四、查看证书库 1./certik -v tls.db list 我们就可以看到三个证书了，一个 CA ，一个 server ，一个 client1\n五、导出各个证书 1#导出CA 2./certik -v tls.db export --root-ca 3 4#导出server 5./certik -v tls.db export server 6 7#导出client1 8./certik -v tls.db export client1 以上就可以得到各个东西了。\n那么我们 gen 出证书做什么用呢？当然用作 ghostunnel 的验证用\n","date":"2021-10-19","img":"","permalink":"https://bajie.dev/posts/20211019-certik/","series":null,"tags":null,"title":"Certik 证书签发软件"},{"categories":null,"content":"现在国内都禁止挖币，什么币安、火币、cnspark之类的都不允许国内 IP 访问了。\n如何实现的呢？\n首先需要得到国家IP段，下载地址：http://www.ipdeny.com/ipblocks/。这里以我们国家 cn 为例\n步骤如下：\n一、安装ipset 1#Debian/Ubuntu系统 2apt-get -y install ipset 3 4#CentOS系统 5yum -y install ipset 二、清空iptable规则 1#防止设置不生效，清空之前的防火墙规则 2iptables -P INPUT ACCEPT 3iptables -F 三、创建ipset规则集 1#创建一个名为cnip的规则 2ipset -N cnip hash:net 3 4#下载国家IP段，这里以中国为例 5wget -P . http://www.ipdeny.com/ipblocks/data/countries/cn.zone 6 7#将IP段添加到cnip规则集中 8for i in $(cat /root/cn.zone ); do ipset -A cnip $i; done 四、创建iptable黑名单 1#扔黑名单 2iptables -A INPUT -p tcp --dport 80 -m set --match-set cnip src -j DROP 3iptables -A INPUT -p tcp --dport 443 -m set --match-set cnip src -j DROP 4 5#然后放行其他的 6iptables -P INPUT ACCEPT ","date":"2021-10-18","img":"","permalink":"https://bajie.dev/posts/20211018-ipset_block_cn/","series":null,"tags":null,"title":"使用IPSET封掉某个国家整个的访问"},{"categories":null,"content":"公司安装了 openvpn ，带来方便，但是也有很多不便的地方，机房的总带宽就那么多。\n很多人共用 vpn 的时候，就会抢占带宽。\n那么，我们需要限制一下，限制 openvpn 所能使用的带宽，避免抢占 WEB 的带宽\n做法如下：\n由于我们不是要单独限制某一个 openvpn 用户，而是限制整体，所以简单用 TC 就可以了\n1#!/bin/sh 2tc qdisc del dev tun0 root 3tc qdisc add dev tun0 root handle 1: htb default 1 4tc class add dev tun0 parent 1: classid 1:1 htb rate 30Mbit ceil 30Mbit 解释一下：\n我们 openvpn 启的是 tun0 ，所以限制的对象就是 dev tun0 首先第一行清除 tun0 的根队列 然后第二行建立 tun0 的 root 根队列为 1:0 htb ，缺省是1:1的子队列 最后一行，第三行建立 1:1 的子队列，带宽限制是 30Mbit ，注意这里是大B，就是网络术语中的带宽，换算成小b的话，需要除以8 效果很明显，直接被限制住（41兆而不是30M是因为这台机器是虚机，实体机上还有别的流量）：\n","date":"2021-10-18","img":"","permalink":"https://bajie.dev/posts/20211018-openvpn_limit_bandwidth/","series":null,"tags":null,"title":"OpenVPN 限制流量带宽"},{"categories":null,"content":"本站这个博客的由来：\n源自于 wiredcraft 的面试，这个公司让老八很是向往，可以远程工作，第一次面试是 devops ，老外见面聊了后，由代理中国人面。因为面的是K8S的东西，正好刚给画包包公司做了整体迁往阿里ACK的工程，以为没问题，实际是直接问倒了我。就好比master node上面都跑了什么进程，唉，一言难尽啊，又问到ansible的变量，回答估计也不满意，结果就挂了\n知耻而后勇，后面去恶补了一下ansible和k8s的东西，实际也是实际操作居多，然后第二次面的是 sysadmin，全程老外面。还出了几道题，应该是没问题。但是工资要高了，也被刷了。\n这个博客就是其中一道题，那既然搭建出来了，干脆就好好用吧。\n本身自己对静态的 Blog 系统也比较感兴趣，自己主站 www.rendoumi.com 的 Blog 是由 journey 搭建的，是一个精巧的 go 程序，最妙的是它兼容 Ghost 博客系统，也能使用 Ghost 的 theme ，这样就完美的把自己以前 Ghost 的博客迁移了过去，但是，里面文章有很多过时了，但也不想清理，干脆借着这个机会，从新开始。搭一个自己喜欢的系统继续写新博客\n流行的 Markdown 写作平台有Hexo和Hugo，选Hugo是因为实在是不喜欢node，弄一堆npm，迁移麻烦死。\n这个博客程序基于Hugo，托管在 github，众所周知，github 是托管静态文件的地方，Markdown的最大毛病是图片。图片放在图床也不是好办法，所以图片和静态文件要在一起，下面就说一下搭建过程，我的主机是Ubuntu：\n一、下载Hugo 下载地址： https://github.com/gohugoio/hugo/releases/download/v0.88.1/hugo_0.88.1_Linux-64bit.tar.gz 1wget https://github.com/gohugoio/hugo/releases/download/v0.88.1/hugo_0.88.1_Linux-64bit.tar.gz 2tar zxvf hugo_0.88.1_Linux-64bit.tar.gz 二、初始一个博客写作目录 1./hugo new site MyBlog ​\n三、下载theme 1cd MyBlog 2git clone https://github.com/halogenica/beautifulhugo.git themes/beautifulhugo 3echo theme = \\\u0026#34;beautifulhugo\\\u0026#34; \u0026gt;\u0026gt; config.toml ​\n四、写一篇新文章 1cd MyBlog 2../hugo new posts/my-first-post.md echo \u0026#34;#### This is another Blog\u0026#34; \u0026gt;\u0026gt; content/posts/my-first-post.md 五、运行server，build草稿 1cd MyBlog 2../hugo server --buildDrafts ​\n六、测试一下 1curl http://localhost:1313 ​\n七、推送到github 首先我们要去github开一个xxx.github.io的repo仓库，然后 git 把生成的静态内容推上去就好了\n1cd MyBlog 2 3#生成静态文件 4../hugo --buildDrafts 5 6#文件生成的目录是public 7cd public 8 9#正常git操作就可以了 10git init 11git add . 12git commit -m \u0026#34;first commit\u0026#34; 13git branch -M main 14git remote add origin git@github.com:zhangrr/zhangrr.github.io.git 15git push -u origin main 八、看下结果 打开网页 http://zhangrr.github.io 就能看到网页了\n​\n九、选择写作软件 其实现在开始才是最重要的，用什么软件来写，就用大家推荐的 Typora 来就好了\n1# or use 2# sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys BA300B7755AFCFAE 3wget -qO - https://typora.io/linux/public-key.asc | sudo apt-key add - 4 5# add Typora\u0026#39;s repository 6sudo add-apt-repository \u0026#39;deb https://typora.io/linux ./\u0026#39; 7sudo apt-get update 8 9# install typora 10sudo apt-get install typora 十、选择目录存放格式 这个才是最主要的问题，看下图，post目录是所有文章，下面按目录存放，目录名是日期和文章名，目录里面是index.md和文章附带的图片。\n我觉得这个模式才是符合我的要求的。\n1. 2├── post 3│ ├── 2018-01-11-关联了两款小程序.md 4│ └── index.md 5│ ├── 2018-02-05-一款小小的物流数据产品.md 6│ └── index.md 7│ ├── 2018-03-19-现已加入 Algolia 搜索服务.md 8│ └── index.md 9│ ├── 2018-04-13-我是如何搞砸了本站搜索服务的.md 10│ └── index.md 11│ ├── 2018-04-18-小站构建工具已成功切换到 Hugo.md 12│ └── index.md 13│ ├── 2018-04-19-开始翻译一个文档：Saleor.md 14│ └── index.md 15│ ├── 2018-04-22-Saleor 初稿已翻译完成.md 16│ └── index.md 17│ ├── 2018-04-26-今天全是干货 18│ │ ├── IMG_5991-4755089.jpg 19│ │ ├── IMG_5997-4755064.jpg 20│ │ ├── IMG_5998-4755103.jpg 21│ │ ├── IMG_5999-4755051.jpg 22│ │ ├── IMG_6001-4755073.jpg 23│ │ ├── IMG_6002-4755080.jpg 24│ │ ├── IMG_6003-4755030.jpg 25│ │ ├── IMG_6004-4755009.jpg 26│ │ └── index.md 27│ ├── 2018-05-02-从 Jekyll 到 Hugo 的一些细节.md 28│ └── index.md 29│ └── 2018-05-03-Hugo 的文件管理方案.md 30│ └── index.md 十一、微调 Typora 为了能显示目录结构Outline，所以所有副标题需要用 ctrl+2 的标题文本，这样就能自动生成Outline\n为了能让剪贴板自动把ctrl+v贴上的图片放到目录里面，需要设置Image\n这样就完美了，以后就在这里写工作博客了。\n十二、更新， Typora 的降级 最新消息，Typora 升到了 1.0 ，开始收费了。\n免费的最后一个版本是 typora 0.11.18 版本，还好能下到：\n下载地址：https://download.typora.io/linux/typora_0.11.18_amd64.deb\n回退方法：\n1sudo apt autoremove typora 2sudo dpkg -i typora_0.11.18_amd64.deb 3 4sudo add-apt-repository --remove \u0026#39;deb https://typora.io/linux ./\u0026#39; ","date":"2021-10-15","img":"","permalink":"https://bajie.dev/posts/20211015-hugo_blog/","series":null,"tags":null,"title":"本站博客的由来以及搭建使用教程"},{"categories":null,"content":"上一篇文章，我们实施了Ubuntu下wifi热点的搭建，那么，其实我是想抓我iphone手机的https明文流量包来着。\n怎么抓取呢？\n方法也很简单\n一、安装 mitmproxy 有wifi热点那台机器的wlan网卡地址是192.168.222.1，就在那台上安装，便于抓取\n1pip install mitmproxy 二、运行mitmproxy 1mitmproxy -p 8080 三、配置iphone安装mitm证书 打开手机，获取wifi热点，设置这个wifi使用手机代理，192.168.222.1:8080\n然后用Safari浏览器打开网址：http://mitm.it\n找到IOS那行，仔细看一看说明\n点击 Get mitmproxy-ca-cert.pem 安装描述文件\n安装完以后，在设置 \u0026ndash;\u0026gt; 已下载描述文件，安装描述文件\n安装好以后会显示绿色的已验证\n然后在手机上用 safari 访问网址：https://ip138.com\n回到 Ubuntu 的命令行窗口，上下选中抓到的包，然后按回车查看，左右光标键移动，可以看到response是明文的，q键是返回上一级\n四、包的拦截修改 上面演示的是常规的查看操作，下面介绍一下 mitmproxy 的另一强大功能，拦截修改 request 和 response。\n输入 i，然后输入 ~s 再按回车键，这时候就进入了 response 拦截模式。如果输入 ~q 则进入 request 的拦截模式，更多的命令可以输入 ？ 查看。拦截模式下的页面显示如下图所示：\n其中红色的表示请求正被拦截，这时 Enter 进入后 再按 e 就可以修改 request 或者 response。修改时是用 vim 进行编辑的，修改完成后按 a 将请求放行，如果要放行所有请求输入 A 即可。\n","date":"2021-10-14","img":"","permalink":"https://bajie.dev/posts/20211014-iphone_hijack/","series":null,"tags":null,"title":"Iphone手机的https抓包"},{"categories":null,"content":"公司的wifi信号很弱，也不保险。省事起见，还是自己建立一个好\nusb无线网卡设备必须是一个 nl80211 兼容的无线设备，所以驱动就是这个：nl80211\n我的操作系统是Ubuntu，如果是CentOS命令基本一样\n插上wifi usb卡后 ip a 看一下网卡的名称，我这里是：wlx00a1b0817651，够长\n一、安装hostapd软件 1sudo apt install -y hostapd 二、建立hostapd.conf文件 1vi /etc/hostapd/hostapd.conf 2driver=nl80211 3ssid=Fast_8188 4channel=10 5interface=wlx00a1b0817651 6wpa=2 7wpa_passphrase=GreatWall2021! 8wpa_key_mgmt=WPA-PSK 9wpa_pairwise=TKIP 三、建立启动脚本/usr/local/bin/initAP.sh 1cat /usr/local/bin/initAP.sh 2 3#!/bin/bash 4 5start() { 6rfkill unblock all 7ifconfig wlx00a1b0817651 up 192.168.222.1 netmask 255.255.255.0 8sleep 2 9 10dnsmasq -i wlx00a1b0817651 --dhcp-range=192.168.222.10,192.168.222.20,2h 11 12#Enable NAT 13sysctl -w net.ipv4.ip_forward=1 14iptables -F 15iptables -X 16iptables -t nat -A POSTROUTING -s 192.168.222.0/24 -j SNAT --to 192.168.41.15 17 18hostapd -B /etc/hostapd/hostapd.conf 19} 20 21stop() { 22iptables -P INPUT ACCEPT 23iptables -P FORWARD ACCEPT 24iptables -P OUTPUT ACCEPT 25iptables -F 26iptables -X 27iptables -t nat -F 28iptables -t nat -X 29iptables -t mangle -F 30iptables -t mangle -X 31systemctl stop dnsmasq 32pkill hostapd 33/sbin/ip link set down dev wlx00a1b0817651 34} 35 36case $1 in 37 start) 38 start 39 ;; 40 stop) 41 stop 42 ;; 43 *) 44 echo \u0026#34;Usage: $0 {start|stop}\u0026#34; 45 exit 2 46esac 四、用root身份执行即可 1sudo chmod 755 /usr/local/bin/initAP.sh 2sudo /usr/local/bin/initAP.sh start 这样就可以用自己的手机连上这个wifi热点，尽情冲浪啦。\n","date":"2021-10-14","img":"","permalink":"https://bajie.dev/posts/20211014-linux_wifi/","series":null,"tags":null,"title":"Ubuntu下自建一个wifi热点供手机使用"},{"categories":null,"content":"某些场合，很有可能需要启动ISO或者USB盘，自带Linux系统，然后拯救当前损坏的系统\n或者直接启动一个LIVE CentOS系统，去做某些事，比如用MegaRaid划分Raid、测试系统等等\n这时候就需要制作出来一个LIVE CD的系统了\n制作步骤如下：\n一、安装live-tools 1yum -y install livecd-tools 二、准备Kickstart文件centos7-live-docker.ks 下载地址：centos7-live-docker.ks 1lang en_GB.UTF-8 2keyboard us 3timezone Asia/Shanghai --isUtc 4 5#selinux --enforcing 6selinux --disabled 7 8#firewall --enabled --service=cockpit 9firewall --disabled 10 11#xconfig --startxonboot 12part / --size 8192 --fstype ext4 13services --enabled=NetworkManager,sshd --disabled=network 14 15 16# Root password 17auth --useshadow --enablemd5 18rootpw --plaintext Kalaisadog2021 19 20repo --name=base --baseurl=http://mirror.centos.org/centos/7/os/x86_64/ 21repo --name=updates --baseurl=http://mirror.centos.org/centos/7/updates/x86_64/ 22repo --name=extras --baseurl=http://mirror.centos.org/centos/7/extras/x86_64/ 23repo --name=epel --baseurl=http://dl.fedoraproject.org/pub/epel/7/x86_64/ 24 25%packages 26@core 27kernel 28dracut 29bash 30firewalld 31NetworkManager 32e2fsprogs 33rootfiles 34docker 35openssh-server 36 37#By zhang ranrui 38unzip 39net-tools 40binutils 41wget 42bash-completion 43bc 44dmidecode 45dmraid 46dmraid-events 47lvm2 48lvm2-libs 49kpartx 50mdadm 51parted 52xfsdump 53xfsprogs 54gdisk 55bzip2 56extundelete 57libHX 58libHX-devel 59autoconf 60gcc 61gcc-c++ 62make 63screen 64telnet 65 66%end 67 68%post 69 70systemctl enable docker 71 72# By Zhang Ranrui, Add your custom script 73#wget http://www.rendoumi.com/soft/other/xfs_irecover -O /usr/local/bin/xfs_irecover 74#chmod 755 /usr/local/bin/xfs_irecover 75 76echo \u0026#34;Banner /etc/issue\u0026#34; \u0026gt;\u0026gt; /etc/ssh/sshd_config 77 78sed -i \u0026#34;s/After=network\\.target/After=network-online\\.target\\nWants=network-online\\.target/g\u0026#34; /usr/lib/systemd/system/rc-local.service 79 80chmod 755 /etc/systemd/system/rc.local.service.d 81chmod 644 /etc/systemd/system/rc.local.service.d/local.conf 82 83chmod 755 /etc/rc.d/rc.local 84systemctl enable rc-local 85systemctl start rc-local 86 87# FIXME: it\u0026#39;d be better to get this installed from a package 88cat \u0026gt; /etc/rc.d/init.d/livesys \u0026lt;\u0026lt; EOF 89#!/bin/bash 90# 91# live: Init script for live image 92# 93# chkconfig: 345 00 99 94# description: Init script for live image. 95### BEGIN INIT INFO 96# X-Start-Before: display-manager 97### END INIT INFO 98 99. /etc/init.d/functions 100 101if ! strstr \u0026#34;\\`cat /proc/cmdline\\`\u0026#34; rd.live.image || [ \u0026#34;\\$1\u0026#34; != \u0026#34;start\u0026#34; ]; then 102 exit 0 103fi 104 105if [ -e /.liveimg-configured ] ; then 106 configdone=1 107fi 108 109exists() { 110 which \\$1 \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 || return 111 \\$* 112} 113 114# Make sure we don\u0026#39;t mangle the hardware clock on shutdown 115ln -sf /dev/null /etc/systemd/system/hwclock-save.service 116 117livedir=\u0026#34;LiveOS\u0026#34; 118for arg in \\`cat /proc/cmdline\\` ; do 119 if [ \u0026#34;\\${arg##rd.live.dir=}\u0026#34; != \u0026#34;\\${arg}\u0026#34; ]; then 120 livedir=\\${arg##rd.live.dir=} 121 return 122 fi 123 if [ \u0026#34;\\${arg##live_dir=}\u0026#34; != \u0026#34;\\${arg}\u0026#34; ]; then 124 livedir=\\${arg##live_dir=} 125 return 126 fi 127done 128 129# enable swaps unless requested otherwise 130swaps=\\`blkid -t TYPE=swap -o device\\` 131if ! strstr \u0026#34;\\`cat /proc/cmdline\\`\u0026#34; noswap \u0026amp;\u0026amp; [ -n \u0026#34;\\$swaps\u0026#34; ] ; then 132 for s in \\$swaps ; do 133 action \u0026#34;Enabling swap partition \\$s\u0026#34; swapon \\$s 134 done 135fi 136if ! strstr \u0026#34;\\`cat /proc/cmdline\\`\u0026#34; noswap \u0026amp;\u0026amp; [ -f /run/initramfs/live/\\${livedir}/swap.img ] ; then 137 action \u0026#34;Enabling swap file\u0026#34; swapon /run/initramfs/live/\\${livedir}/swap.img 138fi 139 140mountDockerDisk() { 141 # support label/uuid 142 if [ \u0026#34;\\${dockerdev##LABEL=}\u0026#34; != \u0026#34;\\${dockerdev}\u0026#34; -o \u0026#34;\\${dockerdev##UUID=}\u0026#34; != \u0026#34;\\${dockerdev}\u0026#34; ]; then 143 dockerdev=\\`/sbin/blkid -o device -t \u0026#34;\\$dockerdev\u0026#34;\\` 144 fi 145 146 # if we\u0026#39;re given a file rather than a blockdev, loopback it 147 if [ \u0026#34;\\${dockerdev##mtd}\u0026#34; != \u0026#34;\\${dockerdev}\u0026#34; ]; then 148 # mtd devs don\u0026#39;t have a block device but get magic-mounted with -t jffs2 149 mountopts=\u0026#34;-t jffs2\u0026#34; 150 elif [ ! -b \u0026#34;\\$dockerdev\u0026#34; ]; then 151 loopdev=\\`losetup -f\\` 152 if [ \u0026#34;\\${dockerdev##/run/initramfs/live}\u0026#34; != \u0026#34;\\${dockerdev}\u0026#34; ]; then 153 action \u0026#34;Remounting live store r/w\u0026#34; mount -o remount,rw /run/initramfs/live 154 fi 155 losetup \\$loopdev \\$dockerdev 156 dockerdev=\\$loopdev 157 fi 158 159 # if it\u0026#39;s encrypted, we need to unlock it 160 if [ \u0026#34;\\$(/sbin/blkid -s TYPE -o value \\$dockerdev 2\u0026gt;/dev/null)\u0026#34; = \u0026#34;crypto_LUKS\u0026#34; ]; then 161 echo 162 echo \u0026#34;Setting up encrypted Docker device\u0026#34; 163 plymouth ask-for-password --command=\u0026#34;cryptsetup luksOpen \\$dockerdev EncDocker\u0026#34; 164 dockerdev=/dev/mapper/EncDocker 165 fi 166 167 # and finally do the mount 168 mount \\$mountopts \\$dockerdev /var/lib/docker 169 # if we have /home under what\u0026#39;s passed for persistent home, then 170 # we should make that the real /home. useful for mtd device on olpc 171 if [ -d /var/lib/docker/docker ]; then mount --bind /var/lib/docker/docker /var/lib/docker ; fi 172 [ -x /sbin/restorecon ] \u0026amp;\u0026amp; /sbin/restorecon /var/lib/docker 173} 174 175findDockerDisk() { 176 for arg in \\`cat /proc/cmdline\\` ; do 177 if [ \u0026#34;\\${arg##dockerdisk=}\u0026#34; != \u0026#34;\\${arg}\u0026#34; ]; then 178 dockerdev=\\${arg##dockerdisk=} 179 return 180 fi 181 done 182} 183 184if strstr \u0026#34;\\`cat /proc/cmdline\\`\u0026#34; dockerdisk= ; then 185 findDockerDisk 186elif [ -e /run/initramfs/live/\\${livedir}/docker.img ]; then 187 dockerdev=/run/initramfs/live/\\${livedir}/docker.img 188fi 189 190# if we have a persistent /home, then we want to go ahead and mount it 191if ! strstr \u0026#34;\\`cat /proc/cmdline\\`\u0026#34; nodockerdisk \u0026amp;\u0026amp; [ -n \u0026#34;\\$dockerdev\u0026#34; ] ; then 192 action \u0026#34;Mounting persistent /var/lib/docker\u0026#34; mountDockerDisk 193fi 194 195# make it so that we don\u0026#39;t do writing to the overlay for things which 196# are just tmpdirs/caches 197mount -t tmpfs -o mode=0755 varcacheyum /var/cache/yum 198mount -t tmpfs vartmp /var/tmp 199[ -x /sbin/restorecon ] \u0026amp;\u0026amp; /sbin/restorecon /var/cache/yum /var/tmp \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 200 201if [ -n \u0026#34;\\$configdone\u0026#34; ]; then 202 exit 0 203fi 204 205# add fedora user with no passwd 206action \u0026#34;Adding live user\u0026#34; useradd \\$USERADDARGS -c \u0026#34;Live System User\u0026#34; liveuser 207passwd -d liveuser \u0026gt; /dev/null 208usermod -aG wheel,docker liveuser \u0026gt; /dev/null 209 210# Remove root password lock 211passwd -d root \u0026gt; /dev/null 212(echo Kalaisadog2021; echo Kalaisadog2021)|passwd root --stdin 213 214# turn off firstboot for livecd boots 215systemctl --no-reload disable firstboot-text.service 2\u0026gt; /dev/null || : 216systemctl --no-reload disable firstboot-graphical.service 2\u0026gt; /dev/null || : 217systemctl stop firstboot-text.service 2\u0026gt; /dev/null || : 218systemctl stop firstboot-graphical.service 2\u0026gt; /dev/null || : 219 220# don\u0026#39;t use prelink on a running live image 221sed -i \u0026#39;s/PRELINKING=yes/PRELINKING=no/\u0026#39; /etc/sysconfig/prelink \u0026amp;\u0026gt;/dev/null || : 222 223# turn off mdmonitor by default 224systemctl --no-reload disable mdmonitor.service 2\u0026gt; /dev/null || : 225systemctl --no-reload disable mdmonitor-takeover.service 2\u0026gt; /dev/null || : 226systemctl stop mdmonitor.service 2\u0026gt; /dev/null || : 227systemctl stop mdmonitor-takeover.service 2\u0026gt; /dev/null || : 228 229# don\u0026#39;t enable the gnome-settings-daemon packagekit plugin 230gsettings set org.gnome.settings-daemon.plugins.updates active \u0026#39;false\u0026#39; || : 231 232# don\u0026#39;t start cron/at as they tend to spawn things which are 233# disk intensive that are painful on a live image 234systemctl --no-reload disable crond.service 2\u0026gt; /dev/null || : 235systemctl --no-reload disable atd.service 2\u0026gt; /dev/null || : 236systemctl stop crond.service 2\u0026gt; /dev/null || : 237systemctl stop atd.service 2\u0026gt; /dev/null || : 238 239# Mark things as configured 240touch /.liveimg-configured 241 242# add static hostname to work around xauth bug 243# https://bugzilla.redhat.com/show_bug.cgi?id=679486 244echo \u0026#34;localhost\u0026#34; \u0026gt; /etc/hostname 245 246# Fixing the lang install issue when other lang than English is selected . See http://bugs.centos.org/view.php?id=7217 247/usr/bin/cp /usr/lib/python2.7/site-packages/blivet/size.py /usr/lib/python2.7/site-packages/blivet/size.py.orig 248/usr/bin/sed -i \u0026#34;s#return self.humanReadable()#return self.humanReadable().encode(\u0026#39;utf-8\u0026#39;)#g\u0026#34; /usr/lib/python2.7/site-packages/blivet/size.py 249 250EOF 251 252# bah, hal starts way too late 253cat \u0026gt; /etc/rc.d/init.d/livesys-late \u0026lt;\u0026lt; EOF 254#!/bin/bash 255# 256# live: Late init script for live image 257# 258# chkconfig: 345 99 01 259# description: Late init script for live image. 260 261. /etc/init.d/functions 262 263if ! strstr \u0026#34;\\`cat /proc/cmdline\\`\u0026#34; rd.live.image || [ \u0026#34;\\$1\u0026#34; != \u0026#34;start\u0026#34; ] || [ -e /.liveimg-late-configured ] ; then 264 exit 0 265fi 266 267exists() { 268 which \\$1 \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 || return 269 \\$* 270} 271 272touch /.liveimg-late-configured 273 274# read some variables out of /proc/cmdline 275for o in \\`cat /proc/cmdline\\` ; do 276 case \\$o in 277 ks=*) 278 ks=\u0026#34;--kickstart=\\${o#ks=}\u0026#34; 279 ;; 280 xdriver=*) 281 xdriver=\u0026#34;\\${o#xdriver=}\u0026#34; 282 ;; 283 esac 284done 285 286# if liveinst or textinst is given, start anaconda 287if strstr \u0026#34;\\`cat /proc/cmdline\\`\u0026#34; liveinst ; then 288 plymouth --quit 289 /usr/sbin/liveinst \\$ks 290fi 291if strstr \u0026#34;\\`cat /proc/cmdline\\`\u0026#34; textinst ; then 292 plymouth --quit 293 /usr/sbin/liveinst --text \\$ks 294fi 295 296# configure X, allowing user to override xdriver 297if [ -n \u0026#34;\\$xdriver\u0026#34; ]; then 298 cat \u0026gt; /etc/X11/xorg.conf.d/00-xdriver.conf \u0026lt;\u0026lt;FOE 299Section \u0026#34;Device\u0026#34; 300 Identifier \u0026#34;Videocard0\u0026#34; 301 Driver \u0026#34;\\$xdriver\u0026#34; 302EndSection 303FOE 304fi 305 306EOF 307 308chmod 755 /etc/rc.d/init.d/livesys 309/sbin/restorecon /etc/rc.d/init.d/livesys 310/sbin/chkconfig --add livesys 311 312chmod 755 /etc/rc.d/init.d/livesys-late 313/sbin/restorecon /etc/rc.d/init.d/livesys-late 314/sbin/chkconfig --add livesys-late 315 316# enable tmpfs for /tmp 317systemctl enable tmp.mount 318 319 320# enable docker 321systemctl enable docker.service 322 323# work around for poor key import UI in PackageKit 324rm -f /var/lib/rpm/__db* 325releasever=$(rpm -q --qf \u0026#39;%{version}\\n\u0026#39; --whatprovides system-release) 326basearch=$(uname -i) 327rpm --import /etc/pki/rpm-gpg/RPM-GPG-KEY-fedora-$releasever-$basearch 328echo \u0026#34;Packages within this LiveCD\u0026#34; 329rpm -qa 330# Note that running rpm recreates the rpm db files which aren\u0026#39;t needed or wanted 331rm -f /var/lib/rpm/__db* 332 333# go ahead and pre-make the man -k cache (#455968) 334/usr/bin/mandb 335 336# save a little bit of space at least... 337rm -f /boot/initramfs* 338# make sure there aren\u0026#39;t core files lying around 339rm -f /core* 340 341# convince readahead not to collect 342# FIXME: for systemd 343 344cat \u0026gt;\u0026gt; /etc/rc.d/init.d/livesys \u0026lt;\u0026lt; EOF 345 346 347# disable updates plugin 348cat \u0026gt;\u0026gt; /usr/share/glib-2.0/schemas/org.gnome.settings-daemon.plugins.updates.gschema.override \u0026lt;\u0026lt; FOE 349[org.gnome.settings-daemon.plugins.updates] 350active=false 351FOE 352 353# Show the system-config-keyboard tool on the desktop 354mkdir /home/liveuser/Desktop -p \u0026gt;/dev/null 355cat /usr/share/applications/system-config-keyboard.desktop | sed \u0026#39;/NotShowIn/d\u0026#39; |sed \u0026#39;s/Terminal=false/Terminal=true/\u0026#39; \u0026gt; /home/liveuser/Desktop/system-config-keyboard.desktop 356cat /usr/share/applications/liveinst.desktop | sed \u0026#39;/NoDisplay/d\u0026#39; \u0026gt; /home/liveuser/Desktop/liveinst.desktop 357chmod +x /home/liveuser/Desktop/*.desktop 358chown -R liveuser:liveuser /home/liveuser 359 360# Liveuser face 361if [ -e /usr/share/icons/hicolor/96x96/apps/fedora-logo-icon.png ] ; then 362 cp /usr/share/icons/hicolor/96x96/apps/fedora-logo-icon.png /home/liveuser/.face 363 chown liveuser:liveuser /home/liveuser/.face 364fi 365 366# make the installer show up 367if [ -f /usr/share/applications/liveinst.desktop ]; then 368 # Show harddisk install in shell dash 369 sed -i -e \u0026#39;s/NoDisplay=true/NoDisplay=false/\u0026#39; /usr/share/applications/liveinst.desktop 370 # need to move it to anaconda.desktop to make shell happy 371 #cp /usr/share/applications/liveinst.desktop /usr/share/applications/anaconda.desktop 372fi 373 cat \u0026gt;\u0026gt; /usr/share/glib-2.0/schemas/org.gnome.shell.gschema.override \u0026lt;\u0026lt; FOE 374[org.gnome.shell] 375favorite-apps=[\u0026#39;liveinst.desktop\u0026#39;,\u0026#39;firefox.desktop\u0026#39;, \u0026#39;evolution.desktop\u0026#39;, \u0026#39;empathy.desktop\u0026#39;, \u0026#39;rhythmbox.desktop\u0026#39;, \u0026#39;shotwell.desktop\u0026#39;, \u0026#39;libreoffice-writer.desktop\u0026#39;, \u0026#39;nautilus.desktop\u0026#39;, \u0026#39;gnome-documents.desktop\u0026#39;, \u0026#39;anaconda.desktop\u0026#39;] 376FOE 377 378 379# set up auto-login 380cat \u0026gt; /etc/gdm/custom.conf \u0026lt;\u0026lt; FOE 381[daemon] 382AutomaticLoginEnable=True 383AutomaticLogin=liveuser 384FOE 385 386# Turn off PackageKit-command-not-found while uninstalled 387if [ -f /etc/PackageKit/CommandNotFound.conf ]; then 388 sed -i -e \u0026#39;s/^SoftwareSourceSearch=true/SoftwareSourceSearch=false/\u0026#39; /etc/PackageKit/CommandNotFound.conf 389fi 390 391# make sure to set the right permissions and selinux contexts 392chown -R liveuser:liveuser /home/liveuser/ 393restorecon -R /home/liveuser/ 394 395# Fixing default locale to us 396localectl set-keymap us 397localectl set-x11-keymap us 398EOF 399 400 401# rebuild schema cache with any overrides we installed 402glib-compile-schemas /usr/share/glib-2.0/schemas 403 404 405%end 注意，上面注释了两个地方，都可以添加软件或者运行脚本\n三、build出iso文件 1livecd-creator --verbose -c centos7-live-docker.ks --cache=cache -f centos7-live-docker 然后就会得到centos7-live-docker.iso的文件，注意在build过程中的报错信息，多数是无法下载包导致的。\n直接加载ISO文件启动或者刻录到USB上启动，就可以进入这个自制的Live系统了\n千万注意，启动一定要选Bios legacy，不要用Uefi。\n相关一些有用的链接：\nhttps://github.com/minishift/minishift-centos-iso https://github.com/livecd-tools/livecd-tools https://blog.csdn.net/sharpbladepan/article/details/107423468 ","date":"2021-10-10","img":"","permalink":"https://bajie.dev/posts/20211010-live_cd/","series":null,"tags":null,"title":"CentOS 7 Live-CD 的制作"},{"categories":[],"content":"This article offers a sample of basic Markdown syntax that can be used in Hugo content files, also it shows whether basic HTML elements are decorated with CSS in a Hugo theme.\nHeadings The following HTML \u0026lt;h1\u0026gt;—\u0026lt;h6\u0026gt; elements represent six levels of section headings. \u0026lt;h1\u0026gt; is the highest section level while \u0026lt;h6\u0026gt; is the lowest.\nH1 H2 H3 H4 H5 H6 Paragraph Xerum, quo qui aut unt expliquam qui dolut labo. Aque venitatiusda cum, voluptionse latur sitiae dolessi aut parist aut dollo enim qui voluptate ma dolestendit peritin re plis aut quas inctum laceat est volestemque commosa as cus endigna tectur, offic to cor sequas etum rerum idem sintibus eiur? Quianimin porecus evelectur, cum que nis nust voloribus ratem aut omnimi, sitatur? Quiatem. Nam, omnis sum am facea corem alique molestrunt et eos evelece arcillit ut aut eos eos nus, sin conecerem erum fuga. Ri oditatquam, ad quibus unda veliamenimin cusam et facea ipsamus es exerum sitate dolores editium rerore eost, temped molorro ratiae volorro te reribus dolorer sperchicium faceata tiustia prat.\nItatur? Quiatae cullecum rem ent aut odis in re eossequodi nonsequ idebis ne sapicia is sinveli squiatum, core et que aut hariosam ex eat.\nBlockquotes The blockquote element represents content that is quoted from another source, optionally with a citation which must be within a footer or cite element, and optionally with in-line changes such as annotations and abbreviations.\nBlockquote without attribution Tiam, ad mint andaepu dandae nostion secatur sequo quae. Note that you can use Markdown syntax within a blockquote.\nBlockquote with attribution Don\u0026rsquo;t communicate by sharing memory, share memory by communicating. — Rob Pike1\nTables Tables aren\u0026rsquo;t part of the core Markdown spec, but Hugo supports supports them out-of-the-box.\nName Age Bob 27 Alice 23 Inline Markdown within tables Italics Bold Code italics bold code Code Blocks Code block with backticks 1\u0026lt;!doctype html\u0026gt; 2\u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; 3\u0026lt;head\u0026gt; 4 \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; 5 \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; 6\u0026lt;/head\u0026gt; 7\u0026lt;body\u0026gt; 8 \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; 9\u0026lt;/body\u0026gt; 10\u0026lt;/html\u0026gt; Code block with Hugo\u0026rsquo;s internal highlight shortcode 1\u0026lt;!doctype html\u0026gt; 2\u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; 3\u0026lt;head\u0026gt; 4 \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; 5 \u0026lt;title\u0026gt;Another Example HTML5 Document\u0026lt;/title\u0026gt; 6\u0026lt;/head\u0026gt; 7\u0026lt;body\u0026gt; 8 \u0026lt;p\u0026gt;A looooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooong text\u0026lt;/p\u0026gt; 9\u0026lt;/body\u0026gt; 10\u0026lt;/html\u0026gt; List Types Ordered List First item Second item Third item Unordered List List item Another item And another item Nested List Fruit Apple Orange Banana Dairy Milk Cheese TODO List Done WIP Other Elements — abbr, sub, sup, kbd, mark GIF is a bitmap image format.\nH2O\nXn + Yn = Zn\nPress CTRL+ALT+Delete to end the session.\nMost salamanders are nocturnal, and hunt for insects, worms, and other small creatures.\nThe above quote is excerpted from Rob Pike\u0026rsquo;s talk during Gopherfest, November 18, 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2020-11-09","img":"https://bajie.dev/images/markdown.png","permalink":"https://bajie.dev/posts/markdown-syntax/","series":["Manual"],"tags":["Markdown","CSS","HTML"],"title":"Markdown Syntax Guide"},{"categories":null,"content":"kvm生产的虚机内仍然可以再产虚拟机，这就是嵌套虚拟化\n我们的目的是在kvm虚机中再装一套proxmox\n在实体机上检查当前的Linux是否支持嵌套\nIntel的CPU\rcat /sys/module/kvm_intel/parameters/nested\rAMD的CPU\rcat /sys/module/kvm_amd/parameters/nested\r​\t修改支持，以intel为例：\nvi /etc/modprobe.d/kvm.conf\roptions kvm_intel nested=1\rreboot就好\n不重启的话\nmodprobe -r kvm_intel\rmodprobe kvm_intel\r然后看看\ncat /sys/module/kvm_intel/parameters/nested\r安装的时候，指定cpu=host：\n1#!/bin/sh 2qemu-img create -f qcow2 /export/kvm/proxmox-168-86-103/proxmox-168-86-103.qcow2 200G 3virt-install \\ 4 --name=proxmox-168-86-103 \\ 5 --cpu=host \\ 6 --ram=8192 \\ 7 --disk path=/export/kvm/proxmox-168-86-103/proxmox-168-86-103.qcow2,format=qcow2,size=200 \\ 8 --cdrom=/export/kvm/iso/proxmox-ve_7.0-2.iso \\ 9 --os-type=Linux \\ 10 --network bridge=br0 \\ 11 --vnc --vnclisten=0.0.0.0 --vncport=5903 ","date":"0001-01-01","img":"","permalink":"https://bajie.dev/posts/untitled/","series":null,"tags":null,"title":""}]