<!doctype html><html lang=zh dir=auto data-theme=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-3988508441804851" crossorigin=anonymous></script><title>GlusterFS的实际应用 | 八戒的技术博客</title>
<meta name=keywords content><meta name=description content="特别注意
两台GFS主机172.18.30.18和172.18.30.36上面务必配置/etc/hosts，否则peer的时候会有问题
172.18.30.18 renhe-18-30-18
172.18.30.18 renhe-18-30-36

客户端安装
yum -y install epel-release

然后 yum  install glusterfs-fuse 就可以了
挂载：

mount.glusterfs 172.18.30.18:/borui-vol /data/br/nfs
fstab 自动挂载

172.18.30.18:/borui-vol /data/br/nfs glusterfs defaults,_netdev,backupvolfile-server=172.18.30.36 0 0
在172.18.30.18上建立新卷，因为只有2个节点，就必须force了
gluster volume create test-zhichi-vol replica 2 transport tcp 172.18.30.18:/glusterfs/test-zhichi-vol 172.18.30.36:/glusterfs/test-zhichi-vol force
启动

gluster volume start test-zhichi-vol
查看一下

gluster volume info test-zhichi-vol
查看卷信息(禁止查看inode，太多了)
gluster volume status test-zhichi-vol detail
gluster volume status test-zhichi-vol clients
gluster volume status test-zhichi-vol mem
gluster volume status test-zhichi-vol fd
gluster volume status test-zhichi-vol inode
开启限额
gluster volume quota test-zhichi-vol enable
gluster volume quota test-zhichi-vol limit-usage / 50GB
gluster volume quota test-zhichi-vol list
限制IP访问
例如允许172.18.31.*网段的主机访问rep-volume："><meta name=author content><link rel=canonical href=https://rendoumi.com/posts/20230330-glusterfs/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.181dcf08b5d0ad7a24019e99c09910b83c66dc2a9b7be7a1215b3bb7af4cc829.css integrity="sha256-GB3PCLXQrXokAZ6ZwJkQuDxm3Cqbe+ehIVs7t69MyCk=" rel="preload stylesheet" as=style><link rel=icon href=https://rendoumi.com/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://rendoumi.com/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://rendoumi.com/favicon-32x32.png><link rel=apple-touch-icon href=https://rendoumi.com/apple-touch-icon.png><link rel=mask-icon href=https://rendoumi.com/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=zh href=https://rendoumi.com/posts/20230330-glusterfs/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><meta property="og:url" content="https://rendoumi.com/posts/20230330-glusterfs/"><meta property="og:site_name" content="八戒的技术博客"><meta property="og:title" content="GlusterFS的实际应用"><meta property="og:description" content="特别注意 两台GFS主机172.18.30.18和172.18.30.36上面务必配置/etc/hosts，否则peer的时候会有问题
172.18.30.18 renhe-18-30-18172.18.30.18 renhe-18-30-36客户端安装 yum -y install epel-release 然后 yum install glusterfs-fuse 就可以了 挂载： mount.glusterfs 172.18.30.18:/borui-vol /data/br/nfs
fstab 自动挂载 172.18.30.18:/borui-vol /data/br/nfs glusterfs defaults,_netdev,backupvolfile-server=172.18.30.36 0 0
在172.18.30.18上建立新卷，因为只有2个节点，就必须force了 gluster volume create test-zhichi-vol replica 2 transport tcp 172.18.30.18:/glusterfs/test-zhichi-vol 172.18.30.36:/glusterfs/test-zhichi-vol force 启动 gluster volume start test-zhichi-vol
查看一下 gluster volume info test-zhichi-vol
查看卷信息(禁止查看inode，太多了) gluster volume status test-zhichi-vol detail gluster volume status test-zhichi-vol clients gluster volume status test-zhichi-vol mem gluster volume status test-zhichi-vol fd gluster volume status test-zhichi-vol inode 开启限额 gluster volume quota test-zhichi-vol enable gluster volume quota test-zhichi-vol limit-usage / 50GB gluster volume quota test-zhichi-vol list 限制IP访问 例如允许172.18.31.*网段的主机访问rep-volume："><meta property="og:locale" content="zh"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-03-30T09:05:11+08:00"><meta property="article:modified_time" content="2023-03-30T09:05:11+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="GlusterFS的实际应用"><meta name=twitter:description content="特别注意
两台GFS主机172.18.30.18和172.18.30.36上面务必配置/etc/hosts，否则peer的时候会有问题
172.18.30.18 renhe-18-30-18
172.18.30.18 renhe-18-30-36

客户端安装
yum -y install epel-release

然后 yum  install glusterfs-fuse 就可以了
挂载：

mount.glusterfs 172.18.30.18:/borui-vol /data/br/nfs
fstab 自动挂载

172.18.30.18:/borui-vol /data/br/nfs glusterfs defaults,_netdev,backupvolfile-server=172.18.30.36 0 0
在172.18.30.18上建立新卷，因为只有2个节点，就必须force了
gluster volume create test-zhichi-vol replica 2 transport tcp 172.18.30.18:/glusterfs/test-zhichi-vol 172.18.30.36:/glusterfs/test-zhichi-vol force
启动

gluster volume start test-zhichi-vol
查看一下

gluster volume info test-zhichi-vol
查看卷信息(禁止查看inode，太多了)
gluster volume status test-zhichi-vol detail
gluster volume status test-zhichi-vol clients
gluster volume status test-zhichi-vol mem
gluster volume status test-zhichi-vol fd
gluster volume status test-zhichi-vol inode
开启限额
gluster volume quota test-zhichi-vol enable
gluster volume quota test-zhichi-vol limit-usage / 50GB
gluster volume quota test-zhichi-vol list
限制IP访问
例如允许172.18.31.*网段的主机访问rep-volume："><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"所有文章","item":"https://rendoumi.com/posts/"},{"@type":"ListItem","position":2,"name":"GlusterFS的实际应用","item":"https://rendoumi.com/posts/20230330-glusterfs/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"GlusterFS的实际应用","name":"GlusterFS的实际应用","description":"特别注意 两台GFS主机172.18.30.18和172.18.30.36上面务必配置/etc/hosts，否则peer的时候会有问题\n172.18.30.18 renhe-18-30-18\r172.18.30.18 renhe-18-30-36\r客户端安装 yum -y install epel-release 然后 yum install glusterfs-fuse 就可以了 挂载： mount.glusterfs 172.18.30.18:/borui-vol /data/br/nfs\nfstab 自动挂载 172.18.30.18:/borui-vol /data/br/nfs glusterfs defaults,_netdev,backupvolfile-server=172.18.30.36 0 0\n在172.18.30.18上建立新卷，因为只有2个节点，就必须force了 gluster volume create test-zhichi-vol replica 2 transport tcp 172.18.30.18:/glusterfs/test-zhichi-vol 172.18.30.36:/glusterfs/test-zhichi-vol force 启动 gluster volume start test-zhichi-vol\n查看一下 gluster volume info test-zhichi-vol\n查看卷信息(禁止查看inode，太多了) gluster volume status test-zhichi-vol detail gluster volume status test-zhichi-vol clients gluster volume status test-zhichi-vol mem gluster volume status test-zhichi-vol fd gluster volume status test-zhichi-vol inode 开启限额 gluster volume quota test-zhichi-vol enable gluster volume quota test-zhichi-vol limit-usage / 50GB gluster volume quota test-zhichi-vol list 限制IP访问 例如允许172.18.31.*网段的主机访问rep-volume：\n","keywords":[],"articleBody":"特别注意 两台GFS主机172.18.30.18和172.18.30.36上面务必配置/etc/hosts，否则peer的时候会有问题\n172.18.30.18 renhe-18-30-18\r172.18.30.18 renhe-18-30-36\r客户端安装 yum -y install epel-release 然后 yum install glusterfs-fuse 就可以了 挂载： mount.glusterfs 172.18.30.18:/borui-vol /data/br/nfs\nfstab 自动挂载 172.18.30.18:/borui-vol /data/br/nfs glusterfs defaults,_netdev,backupvolfile-server=172.18.30.36 0 0\n在172.18.30.18上建立新卷，因为只有2个节点，就必须force了 gluster volume create test-zhichi-vol replica 2 transport tcp 172.18.30.18:/glusterfs/test-zhichi-vol 172.18.30.36:/glusterfs/test-zhichi-vol force 启动 gluster volume start test-zhichi-vol\n查看一下 gluster volume info test-zhichi-vol\n查看卷信息(禁止查看inode，太多了) gluster volume status test-zhichi-vol detail gluster volume status test-zhichi-vol clients gluster volume status test-zhichi-vol mem gluster volume status test-zhichi-vol fd gluster volume status test-zhichi-vol inode 开启限额 gluster volume quota test-zhichi-vol enable gluster volume quota test-zhichi-vol limit-usage / 50GB gluster volume quota test-zhichi-vol list 限制IP访问 例如允许172.18.31.*网段的主机访问rep-volume：\ngluster volume set test-zhichi-vol auth.allow 172.18.31.*`\n如果客户端想用NFS挂载 gluster volume set test-zhichi-vol nfs.disable off\n开启性能分析（慎用） gluster volume profile test-zhichi-vol start\n开启后性能分析后，显示I/O信息: gluster volume profile gv0 info\nGlusterd 启动失败，原因未知处理办法（慎用，拷出数据后再用）： rm -rf /var/lib/glusterd/* systemctl start glusterd 脑裂解决方法： 脑裂： 使用replica模式的时候，如果发生网络故障（比如交换机坏了、网线被碰掉了），而两台机器都还活着的时候，它们各自的数据读写还会继续。\n当网络恢复时，它们都会认为自己的数据才是正确的，对方的是错误的，这就是俗称的脑裂。\n双方谁都不肯妥协，结果就是文件数据读取错误，系统无法正常运行。 在正常的机器上执行以下操作：\ngluster volume status nfsp （看看这个节点有没有在线） gluster volume heal nfsp full （启动完全修复） gluster volume heal nfsp info （查看需要修复的文件） gluster volume heal nfsp info healed （查看修复成功的文件） gluster volume heal nfsp info heal-failed （查看修复失败的文件） gluster volume heal nfsp info split-brain （查看脑裂的文件） Gathering Heal info on volume nfsp has been successful Brick gls03.mycloud.com.cn:/glusterfs/nfsp Number of entries: 24 at path on brick ----------------------------------- 2014-05-30 10:22:20 /36c741b8-2de2-46e9-9e3c-8c7475e4dd10 。。。。。。。。。。。。。。。。。。。。。。。。。。。。。 。。。。。。。。。。。。。。。。。。。。。。。。。。。。。 。。。。。。。。。。。。。。。。。。。。。。。。。。。。。 在有病的那台机器上，删除脑裂的文件： （注意！要删除的文件是在gluster volume info nfsp看到的目录中，不要去删除挂载的目录中的文件，不然就等着哭吧） 把硬链接文件找出来，也要删除：\nfind /glusterfs/nfsp/ -samefile /glusterfs/nfsp/36c741b8-2de2-46e9-9e3c-8c7475e4dd10 -print /glusterfs/nfsp/36c741b8-2de2-46e9-9e3c-8c7475e4dd10 /glusterfs/nfsp/.glusterfs/65/26/6526e766-cb26-434c-9605-eacb21316447 （这里看得出硬链接文件的目录名和日志中的gfid的对应关系）\n删除掉：\nrm /glusterfs/nfsp/36c741b8-2de2-46e9-9e3c-8c7475e4dd10 rm /glusterfs/nfsp/.glusterfs/65/26/6526e766-cb26-434c-9605-eacb21316447 在正常的机器上执行\ntail -1 /nfsprimary/36c741b8-2de2-46e9-9e3c-8c7475e4dd10 （读一下这个文件，触发修复） ls -l /glusterfs/nfsp/36c741b8-2de2-46e9-9e3c-8c7475e4dd10 人工查看一下两台机器的数据是否一致\n其它的脑裂文件也是一样处理。 没问题的话，重新挂载目录:\numount /nfsprimary /bin/mount -t glusterfs 10.10.10.21:/nfsp /nfsprimary/ ","wordCount":"236","inLanguage":"zh","datePublished":"2023-03-30T09:05:11+08:00","dateModified":"2023-03-30T09:05:11+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://rendoumi.com/posts/20230330-glusterfs/"},"publisher":{"@type":"Organization","name":"八戒的技术博客","logo":{"@type":"ImageObject","url":"https://rendoumi.com/favicon.ico"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://rendoumi.com/ accesskey=h title="八戒的技术博客 (Alt + H)">八戒的技术博客</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://rendoumi.com/ title=首页><span>首页</span></a></li><li><a href=https://rendoumi.com/posts/ title=文章><span>文章</span></a></li><li><a href=https://rendoumi.com/archives/ title=归档><span>归档</span></a></li><li><a href=https://blog.rendoumi.com title=生活><span>生活</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li><li><a href=https://rendoumi.com/search/ title=搜索><span>搜索</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">GlusterFS的实际应用
<span class=entry-hint title=Draft><svg height="35" viewBox="0 -960 960 960" fill="currentcolor"><path d="M160-410v-60h3e2v60H160zm0-165v-60h470v60H160zm0-165v-60h470v60H160zm360 580v-123l221-220q9-9 20-13t22-4q12 0 23 4.5t20 13.5l37 37q9 9 13 20t4 22-4.5 22.5T862.09-380L643-160H520zm3e2-263-37-37 37 37zM580-220h38l121-122-18-19-19-18-122 121v38zm141-141-19-18 37 37-18-19z"/></svg></span></h1><div class=post-meta><span title='2023-03-30 09:05:11 +0800 CST'>2023年3月30日</span></div></header><div class=post-content><h2 id=特别注意>特别注意<a hidden class=anchor aria-hidden=true href=#特别注意>#</a></h2><p>两台GFS主机172.18.30.18和172.18.30.36上面务必配置/etc/hosts，否则peer的时候会有问题</p><pre><code>172.18.30.18 renhe-18-30-18
172.18.30.18 renhe-18-30-36
</code></pre><h4 id=客户端安装>客户端安装<a hidden class=anchor aria-hidden=true href=#客户端安装>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>yum -y install epel-release
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>然后 yum  install glusterfs-fuse 就可以了
</span></span></code></pre></div><h4 id=挂载>挂载：<a hidden class=anchor aria-hidden=true href=#挂载>#</a></h4><blockquote><p>mount.glusterfs 172.18.30.18:/borui-vol /data/br/nfs</p></blockquote><h4 id=fstab-自动挂载>fstab 自动挂载<a hidden class=anchor aria-hidden=true href=#fstab-自动挂载>#</a></h4><blockquote><p>172.18.30.18:/borui-vol /data/br/nfs glusterfs defaults,_netdev,backupvolfile-server=172.18.30.36 0 0</p></blockquote><h4 id=在172183018上建立新卷因为只有2个节点就必须force了>在172.18.30.18上建立新卷，因为只有2个节点，就必须force了<a hidden class=anchor aria-hidden=true href=#在172183018上建立新卷因为只有2个节点就必须force了>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>gluster volume create test-zhichi-vol replica <span class=m>2</span> transport tcp 172.18.30.18:/glusterfs/test-zhichi-vol 172.18.30.36:/glusterfs/test-zhichi-vol force
</span></span></code></pre></div><h4 id=启动>启动<a hidden class=anchor aria-hidden=true href=#启动>#</a></h4><blockquote><p>gluster volume start test-zhichi-vol</p></blockquote><h4 id=查看一下>查看一下<a hidden class=anchor aria-hidden=true href=#查看一下>#</a></h4><blockquote><p>gluster volume info test-zhichi-vol</p></blockquote><h4 id=查看卷信息禁止查看inode太多了>查看卷信息(禁止查看inode，太多了)<a hidden class=anchor aria-hidden=true href=#查看卷信息禁止查看inode太多了>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>gluster volume status test-zhichi-vol detail
</span></span><span class=line><span class=cl>gluster volume status test-zhichi-vol clients
</span></span><span class=line><span class=cl>gluster volume status test-zhichi-vol mem
</span></span><span class=line><span class=cl>gluster volume status test-zhichi-vol fd
</span></span><span class=line><span class=cl>gluster volume status test-zhichi-vol inode
</span></span></code></pre></div><h4 id=开启限额>开启限额<a hidden class=anchor aria-hidden=true href=#开启限额>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>gluster volume quota test-zhichi-vol <span class=nb>enable</span>
</span></span><span class=line><span class=cl>gluster volume quota test-zhichi-vol limit-usage / 50GB
</span></span><span class=line><span class=cl>gluster volume quota test-zhichi-vol list
</span></span></code></pre></div><h4 id=限制ip访问>限制IP访问<a hidden class=anchor aria-hidden=true href=#限制ip访问>#</a></h4><p>例如允许172.18.31.*网段的主机访问rep-volume：</p><blockquote><p>gluster volume set test-zhichi-vol auth.allow 172.18.31.*`</p></blockquote><h4 id=如果客户端想用nfs挂载>如果客户端想用NFS挂载<a hidden class=anchor aria-hidden=true href=#如果客户端想用nfs挂载>#</a></h4><blockquote><p>gluster volume set test-zhichi-vol nfs.disable off</p></blockquote><h4 id=开启性能分析慎用>开启性能分析（慎用）<a hidden class=anchor aria-hidden=true href=#开启性能分析慎用>#</a></h4><blockquote><p>gluster volume profile test-zhichi-vol start</p></blockquote><h4 id=开启后性能分析后显示io信息>开启后性能分析后，显示I/O信息:<a hidden class=anchor aria-hidden=true href=#开启后性能分析后显示io信息>#</a></h4><blockquote><p>gluster volume profile gv0 info</p></blockquote><h4 id=glusterd-启动失败原因未知处理办法慎用拷出数据后再用>Glusterd 启动失败，原因未知处理办法（慎用，拷出数据后再用）：<a hidden class=anchor aria-hidden=true href=#glusterd-启动失败原因未知处理办法慎用拷出数据后再用>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>rm -rf /var/lib/glusterd/*
</span></span><span class=line><span class=cl>systemctl start glusterd
</span></span></code></pre></div><h1 id=脑裂解决方法>脑裂解决方法：<a hidden class=anchor aria-hidden=true href=#脑裂解决方法>#</a></h1><p>脑裂：
使用replica模式的时候，如果发生网络故障（比如交换机坏了、网线被碰掉了），而两台机器都还活着的时候，它们各自的数据读写还会继续。</p><p>当网络恢复时，它们都会认为自己的数据才是正确的，对方的是错误的，这就是俗称的脑裂。</p><p>双方谁都不肯妥协，结果就是文件数据读取错误，系统无法正常运行。
在正常的机器上执行以下操作：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>gluster volume status nfsp （看看这个节点有没有在线）
</span></span><span class=line><span class=cl>gluster volume heal nfsp full （启动完全修复）
</span></span><span class=line><span class=cl>gluster volume heal nfsp info  （查看需要修复的文件）
</span></span><span class=line><span class=cl>gluster volume heal nfsp info healed  （查看修复成功的文件）
</span></span><span class=line><span class=cl>gluster volume heal nfsp info heal-failed  （查看修复失败的文件）
</span></span><span class=line><span class=cl>gluster volume heal nfsp info split-brain  （查看脑裂的文件）
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>Gathering Heal info on volume nfsp has been successful
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Brick gls03.mycloud.com.cn:/glusterfs/nfsp
</span></span><span class=line><span class=cl>Number of entries: <span class=m>24</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>at                    path on brick
</span></span><span class=line><span class=cl>-----------------------------------
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>2014-05-30 10:22:20 /36c741b8-2de2-46e9-9e3c-8c7475e4dd10
</span></span><span class=line><span class=cl>。。。。。。。。。。。。。。。。。。。。。。。。。。。。。
</span></span><span class=line><span class=cl>。。。。。。。。。。。。。。。。。。。。。。。。。。。。。
</span></span><span class=line><span class=cl>。。。。。。。。。。。。。。。。。。。。。。。。。。。。。
</span></span></code></pre></div><p>在有病的那台机器上，删除脑裂的文件：
（注意！要删除的文件是在gluster volume info nfsp看到的目录中，不要去删除挂载的目录中的文件，不然就等着哭吧）
把硬链接文件找出来，也要删除：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>find /glusterfs/nfsp/ -samefile /glusterfs/nfsp/36c741b8-2de2-46e9-9e3c-8c7475e4dd10 -print
</span></span></code></pre></div><p>/glusterfs/nfsp/36c741b8-2de2-46e9-9e3c-8c7475e4dd10
/glusterfs/nfsp/.glusterfs/65/26/6526e766-cb26-434c-9605-eacb21316447 （这里看得出硬链接文件的目录名和日志中的gfid的对应关系）</p><p>删除掉：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>rm /glusterfs/nfsp/36c741b8-2de2-46e9-9e3c-8c7475e4dd10
</span></span><span class=line><span class=cl>rm /glusterfs/nfsp/.glusterfs/65/26/6526e766-cb26-434c-9605-eacb21316447
</span></span></code></pre></div><p>在正常的机器上执行</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>tail -1 /nfsprimary/36c741b8-2de2-46e9-9e3c-8c7475e4dd10   （读一下这个文件，触发修复）
</span></span><span class=line><span class=cl>ls -l /glusterfs/nfsp/36c741b8-2de2-46e9-9e3c-8c7475e4dd10 
</span></span></code></pre></div><p>人工查看一下两台机器的数据是否一致</p><p>其它的脑裂文件也是一样处理。
没问题的话，重新挂载目录:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>umount /nfsprimary
</span></span><span class=line><span class=cl>/bin/mount -t glusterfs 10.10.10.21:/nfsp /nfsprimary/
</span></span></code></pre></div></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>Copyright © 2020-2025 Zhang Ranrui. All Rights Reserved.</span><br>·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script></body></html>